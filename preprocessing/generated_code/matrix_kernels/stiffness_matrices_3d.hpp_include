// @file
// This file is part of SeisSol.
// 
// @author Alexander Breuer (breuera AT in.tum.de, http://www5.in.tum.de/wiki/index.php/Dipl.-Math._Alexander_Breuer)
// @author Alexander Heinecke (heinecke AT in.tum.de, http://www5.in.tum.de/wiki/index.php/Alexander_Heinecke,_M.Sc.,_M.Sc._with_honors)
// 
// @date 2014-03-13 22:33:41.495626
// 
// @section LICENSE
// This software was developed at Technische Universitaet Muenchen, who is the owner of the software.
// 
// According to good scientific practice, publications on results achieved in whole or in part due to this software should cite at least one paper or referring to an URL presenting this software.
// 
// The owner wishes to make the software available to all users to use, reproduce, modify, distribute and redistribute also for commercial purposes under the following conditions of the original BSD license. Linking this software module statically or dynamically with other modules is making a combined work based on this software. Thus, the terms and conditions of this license cover the whole combination. As a special exception, the copyright holders of this software give you permission to link it with independent modules or to instantiate templates and macros from this software's source files to produce an executable, regardless of the license terms of these independent modules, and to copy and distribute the resulting executable under terms of your choice, provided that you also meet, for each linked independent module, the terms and conditions of this license of that module.
// 
// Copyright (c) 2012, 2013
// Technische Universitaet Muenchen
// Department of Informatics
// Chair of Scientific Computing
// http://www5.in.tum.de/
// 
// All rights reserved.
// 
// Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
// 
// Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
// Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
// All advertising materials mentioning features or use of this software must display the following acknowledgement: This product includes software developed by the Technische Universitaet Muenchen (TUM), Germany, and its contributors.
// Neither the name of the Technische Universitaet Muenchen, Munich, Germany nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
// 
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
// 
// @section DESCRIPTION
// Remark: This file was generated.
#ifndef STIFFNESSMATRICESDHPPINCLUDE
#define STIFFNESSMATRICESDHPPINCLUDE

#if defined( __SSE3__) || defined(__MIC__)
#include <immintrin.h>
#endif

inline void generatedMatrixMultiplication_kXiDivM_9_4(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 4; m++) {
    C[(i*4)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*4)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*4)+0]);
#endif
__m128d c0_0 = _mm_load_sd(&C[(i*4)+1]);
__m128d a0_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, b0));
#endif
_mm_store_sd(&C[(i*4)+1], c0_0);
#else
C[(i*4)+1] += values[0] * B[(i*4)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 18;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivM_9_4(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 4; m++) {
    C[(i*4)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*4)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*4)+0]);
#endif
__m128d c0_0 = _mm_loadu_pd(&C[(i*4)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
#endif
_mm_storeu_pd(&C[(i*4)+1], c0_0);
#else
C[(i*4)+1] += values[0] * B[(i*4)+0];
C[(i*4)+2] += values[1] * B[(i*4)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 36;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivM_9_4(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 4; m++) {
    C[(i*4)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*4)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*4)+0]);
#endif
__m128d c0_0 = _mm_loadu_pd(&C[(i*4)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
#endif
_mm_storeu_pd(&C[(i*4)+1], c0_0);
__m128d c0_2 = _mm_load_sd(&C[(i*4)+3]);
__m128d a0_2 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, b0));
#endif
_mm_store_sd(&C[(i*4)+3], c0_2);
#else
C[(i*4)+1] += values[0] * B[(i*4)+0];
C[(i*4)+2] += values[1] * B[(i*4)+0];
C[(i*4)+3] += values[2] * B[(i*4)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 54;
#endif

}

inline void generatedMatrixMultiplication_kXiDivMT_9_4(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*4)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*4)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*4)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*4)+0], c1_0);
#else
C[(i*4)+0] += values[0] * B[(i*4)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivMT_9_4(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*4)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*4)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*4)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*4)+0], c1_0);
#else
C[(i*4)+0] += values[0] * B[(i*4)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*4)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*4)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*4)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*4)+0], c2_0);
#else
C[(i*4)+0] += values[1] * B[(i*4)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivMT_9_4(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*4)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*4)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*4)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*4)+0], c1_0);
#else
C[(i*4)+0] += values[0] * B[(i*4)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*4)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*4)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*4)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*4)+0], c2_0);
#else
C[(i*4)+0] += values[1] * B[(i*4)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*4)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*4)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*4)+0]);
__m128d a3_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*4)+0], c3_0);
#else
C[(i*4)+0] += values[2] * B[(i*4)+3];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kXiDivM_9_10(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 10; m++) {
    C[(i*10)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*10)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*10)+0]);
#endif
__m128d c0_0 = _mm_load_sd(&C[(i*10)+1]);
__m128d a0_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, b0));
#endif
_mm_store_sd(&C[(i*10)+1], c0_0);
__m128d c0_1 = _mm_load_sd(&C[(i*10)+5]);
__m128d a0_1 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, b0));
#endif
_mm_store_sd(&C[(i*10)+5], c0_1);
__m128d c0_2 = _mm_load_sd(&C[(i*10)+7]);
__m128d a0_2 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, b0));
#endif
_mm_store_sd(&C[(i*10)+7], c0_2);
#else
C[(i*10)+1] += values[0] * B[(i*10)+0];
C[(i*10)+5] += values[1] * B[(i*10)+0];
C[(i*10)+7] += values[2] * B[(i*10)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*10)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*10)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*10)+4]);
__m128d a1_0 = _mm_load_sd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*10)+4], c1_0);
#else
C[(i*10)+4] += values[3] * B[(i*10)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*10)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*10)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*10)+5]);
__m128d a2_0 = _mm_load_sd(&values[4]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*10)+5], c2_0);
#else
C[(i*10)+5] += values[4] * B[(i*10)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*10)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*10)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*10)+5]);
__m128d a3_0 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*10)+5], c3_0);
__m128d c3_1 = _mm_load_sd(&C[(i*10)+7]);
__m128d a3_1 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, b3));
#endif
_mm_store_sd(&C[(i*10)+7], c3_1);
#else
C[(i*10)+5] += values[5] * B[(i*10)+3];
C[(i*10)+7] += values[6] * B[(i*10)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 126;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivM_9_10(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 10; m++) {
    C[(i*10)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*10)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*10)+0]);
#endif
__m128d c0_0 = _mm_loadu_pd(&C[(i*10)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
#endif
_mm_storeu_pd(&C[(i*10)+1], c0_0);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_2 = _mm256_loadu_pd(&C[(i*10)+4]);
__m256d a0_2 = _mm256_loadu_pd(&values[2]);
c0_2 = _mm256_add_pd(c0_2, _mm256_mul_pd(a0_2, b0));
_mm256_storeu_pd(&C[(i*10)+4], c0_2);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_2 = _mm_loadu_pd(&C[(i*10)+4]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*10)+4], c0_2);
__m128d c0_4 = _mm_loadu_pd(&C[(i*10)+6]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*10)+6], c0_4);
#endif
__m128d c0_6 = _mm_load_sd(&C[(i*10)+8]);
__m128d a0_6 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, b0));
#endif
_mm_store_sd(&C[(i*10)+8], c0_6);
#else
C[(i*10)+1] += values[0] * B[(i*10)+0];
C[(i*10)+2] += values[1] * B[(i*10)+0];
C[(i*10)+4] += values[2] * B[(i*10)+0];
C[(i*10)+5] += values[3] * B[(i*10)+0];
C[(i*10)+6] += values[4] * B[(i*10)+0];
C[(i*10)+7] += values[5] * B[(i*10)+0];
C[(i*10)+8] += values[6] * B[(i*10)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*10)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*10)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*10)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[7]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*10)+4], c1_0);
#else
C[(i*10)+4] += values[7] * B[(i*10)+1];
C[(i*10)+5] += values[8] * B[(i*10)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*10)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*10)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*10)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[9]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*10)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*10)+6]);
__m128d a2_2 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*10)+6], c2_2);
#else
C[(i*10)+4] += values[9] * B[(i*10)+2];
C[(i*10)+5] += values[10] * B[(i*10)+2];
C[(i*10)+6] += values[11] * B[(i*10)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*10)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*10)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*10)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[12]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*10)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*10)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[12]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*10)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*10)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[14]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*10)+6], c3_2);
#endif
__m128d c3_4 = _mm_load_sd(&C[(i*10)+8]);
__m128d a3_4 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, b3));
#endif
_mm_store_sd(&C[(i*10)+8], c3_4);
#else
C[(i*10)+4] += values[12] * B[(i*10)+3];
C[(i*10)+5] += values[13] * B[(i*10)+3];
C[(i*10)+6] += values[14] * B[(i*10)+3];
C[(i*10)+7] += values[15] * B[(i*10)+3];
C[(i*10)+8] += values[16] * B[(i*10)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 306;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivM_9_10(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 10; m++) {
    C[(i*10)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*10)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*10)+0]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_0 = _mm256_loadu_pd(&C[(i*10)+1]);
__m256d a0_0 = _mm256_loadu_pd(&values[0]);
c0_0 = _mm256_add_pd(c0_0, _mm256_mul_pd(a0_0, b0));
_mm256_storeu_pd(&C[(i*10)+1], c0_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_0 = _mm_loadu_pd(&C[(i*10)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
_mm_storeu_pd(&C[(i*10)+1], c0_0);
__m128d c0_2 = _mm_loadu_pd(&C[(i*10)+3]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*10)+3], c0_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_4 = _mm256_loadu_pd(&C[(i*10)+5]);
__m256d a0_4 = _mm256_loadu_pd(&values[4]);
c0_4 = _mm256_add_pd(c0_4, _mm256_mul_pd(a0_4, b0));
_mm256_storeu_pd(&C[(i*10)+5], c0_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_4 = _mm_loadu_pd(&C[(i*10)+5]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*10)+5], c0_4);
__m128d c0_6 = _mm_loadu_pd(&C[(i*10)+7]);
__m128d a0_6 = _mm_loadu_pd(&values[6]);
c0_6 = _mm_add_pd(c0_6, _mm_mul_pd(a0_6, b0));
_mm_storeu_pd(&C[(i*10)+7], c0_6);
#endif
__m128d c0_8 = _mm_load_sd(&C[(i*10)+9]);
__m128d a0_8 = _mm_load_sd(&values[8]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_8 = _mm_add_sd(c0_8, _mm_mul_sd(a0_8, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_8 = _mm_add_sd(c0_8, _mm_mul_sd(a0_8, b0));
#endif
_mm_store_sd(&C[(i*10)+9], c0_8);
#else
C[(i*10)+1] += values[0] * B[(i*10)+0];
C[(i*10)+2] += values[1] * B[(i*10)+0];
C[(i*10)+3] += values[2] * B[(i*10)+0];
C[(i*10)+4] += values[3] * B[(i*10)+0];
C[(i*10)+5] += values[4] * B[(i*10)+0];
C[(i*10)+6] += values[5] * B[(i*10)+0];
C[(i*10)+7] += values[6] * B[(i*10)+0];
C[(i*10)+8] += values[7] * B[(i*10)+0];
C[(i*10)+9] += values[8] * B[(i*10)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*10)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*10)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*10)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[9]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*10)+4], c1_0);
__m128d c1_2 = _mm_load_sd(&C[(i*10)+7]);
__m128d a1_2 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, b1));
#endif
_mm_store_sd(&C[(i*10)+7], c1_2);
#else
C[(i*10)+4] += values[9] * B[(i*10)+1];
C[(i*10)+5] += values[10] * B[(i*10)+1];
C[(i*10)+7] += values[11] * B[(i*10)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*10)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*10)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*10)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*10)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*10)+6]);
__m128d a2_2 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*10)+6], c2_2);
__m128d c2_3 = _mm_load_sd(&C[(i*10)+8]);
__m128d a2_3 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, b2));
#endif
_mm_store_sd(&C[(i*10)+8], c2_3);
#else
C[(i*10)+4] += values[12] * B[(i*10)+2];
C[(i*10)+5] += values[13] * B[(i*10)+2];
C[(i*10)+6] += values[14] * B[(i*10)+2];
C[(i*10)+8] += values[15] * B[(i*10)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*10)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*10)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*10)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[16]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*10)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*10)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[16]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*10)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*10)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[18]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*10)+6], c3_2);
#endif
__m128d c3_4 = _mm_loadu_pd(&C[(i*10)+8]);
__m128d a3_4 = _mm_loadu_pd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_pd(c3_4, _mm_mul_pd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_pd(c3_4, _mm_mul_pd(a3_4, b3));
#endif
_mm_storeu_pd(&C[(i*10)+8], c3_4);
#else
C[(i*10)+4] += values[16] * B[(i*10)+3];
C[(i*10)+5] += values[17] * B[(i*10)+3];
C[(i*10)+6] += values[18] * B[(i*10)+3];
C[(i*10)+7] += values[19] * B[(i*10)+3];
C[(i*10)+8] += values[20] * B[(i*10)+3];
C[(i*10)+9] += values[21] * B[(i*10)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 396;
#endif

}

inline void generatedMatrixMultiplication_kXiDivMT_9_10(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*10)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*10)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*10)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*10)+0], c1_0);
#else
C[(i*10)+0] += values[0] * B[(i*10)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*10)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*10)+4]);
#endif
__m128d c4_0 = _mm_load_sd(&C[(i*10)+1]);
__m128d a4_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, b4));
#endif
_mm_store_sd(&C[(i*10)+1], c4_0);
#else
C[(i*10)+1] += values[1] * B[(i*10)+4];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*10)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*10)+5]);
#endif
__m128d c5_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a5_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, b5));
#endif
_mm_store_sd(&C[(i*10)+0], c5_0);
__m128d c5_1 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a5_1 = _mm_loadu_pd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, b5));
#endif
_mm_storeu_pd(&C[(i*10)+2], c5_1);
#else
C[(i*10)+0] += values[2] * B[(i*10)+5];
C[(i*10)+2] += values[3] * B[(i*10)+5];
C[(i*10)+3] += values[4] * B[(i*10)+5];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*10)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*10)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a7_0 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*10)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*10)+3]);
__m128d a7_1 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*10)+3], c7_1);
#else
C[(i*10)+0] += values[5] * B[(i*10)+7];
C[(i*10)+3] += values[6] * B[(i*10)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivMT_9_10(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*10)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*10)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*10)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*10)+0], c1_0);
#else
C[(i*10)+0] += values[0] * B[(i*10)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*10)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*10)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*10)+0], c2_0);
#else
C[(i*10)+0] += values[1] * B[(i*10)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*10)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*10)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*10)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[2]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*10)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*10)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[2]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*10)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[4]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*10)+2], c4_2);
#endif
#else
C[(i*10)+0] += values[2] * B[(i*10)+4];
C[(i*10)+1] += values[3] * B[(i*10)+4];
C[(i*10)+2] += values[4] * B[(i*10)+4];
C[(i*10)+3] += values[5] * B[(i*10)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*10)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*10)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*10)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[6]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*10)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*10)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[6]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*10)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[8]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*10)+2], c5_2);
#endif
#else
C[(i*10)+0] += values[6] * B[(i*10)+5];
C[(i*10)+1] += values[7] * B[(i*10)+5];
C[(i*10)+2] += values[8] * B[(i*10)+5];
C[(i*10)+3] += values[9] * B[(i*10)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*10)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*10)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a6_0 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*10)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*10)+2], c6_1);
#else
C[(i*10)+0] += values[10] * B[(i*10)+6];
C[(i*10)+2] += values[11] * B[(i*10)+6];
C[(i*10)+3] += values[12] * B[(i*10)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*10)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*10)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a7_0 = _mm_load_sd(&values[13]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*10)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*10)+3]);
__m128d a7_1 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*10)+3], c7_1);
#else
C[(i*10)+0] += values[13] * B[(i*10)+7];
C[(i*10)+3] += values[14] * B[(i*10)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*10)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*10)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a8_0 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*10)+0], c8_0);
__m128d c8_1 = _mm_load_sd(&C[(i*10)+3]);
__m128d a8_1 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, b8));
#endif
_mm_store_sd(&C[(i*10)+3], c8_1);
#else
C[(i*10)+0] += values[15] * B[(i*10)+8];
C[(i*10)+3] += values[16] * B[(i*10)+8];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivMT_9_10(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*10)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*10)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*10)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*10)+0], c1_0);
#else
C[(i*10)+0] += values[0] * B[(i*10)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*10)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*10)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*10)+0], c2_0);
#else
C[(i*10)+0] += values[1] * B[(i*10)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*10)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*10)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a3_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*10)+0], c3_0);
#else
C[(i*10)+0] += values[2] * B[(i*10)+3];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*10)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*10)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*10)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[3]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*10)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*10)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[3]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*10)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[5]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*10)+2], c4_2);
#endif
#else
C[(i*10)+0] += values[3] * B[(i*10)+4];
C[(i*10)+1] += values[4] * B[(i*10)+4];
C[(i*10)+2] += values[5] * B[(i*10)+4];
C[(i*10)+3] += values[6] * B[(i*10)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*10)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*10)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*10)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[7]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*10)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*10)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[7]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*10)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[9]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*10)+2], c5_2);
#endif
#else
C[(i*10)+0] += values[7] * B[(i*10)+5];
C[(i*10)+1] += values[8] * B[(i*10)+5];
C[(i*10)+2] += values[9] * B[(i*10)+5];
C[(i*10)+3] += values[10] * B[(i*10)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*10)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*10)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a6_0 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*10)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*10)+2], c6_1);
#else
C[(i*10)+0] += values[11] * B[(i*10)+6];
C[(i*10)+2] += values[12] * B[(i*10)+6];
C[(i*10)+3] += values[13] * B[(i*10)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*10)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*10)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*10)+0]);
__m128d a7_0 = _mm_loadu_pd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*10)+0], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*10)+3]);
__m128d a7_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*10)+3], c7_2);
#else
C[(i*10)+0] += values[14] * B[(i*10)+7];
C[(i*10)+1] += values[15] * B[(i*10)+7];
C[(i*10)+3] += values[16] * B[(i*10)+7];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*10)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*10)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a8_0 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*10)+0], c8_0);
__m128d c8_1 = _mm_loadu_pd(&C[(i*10)+2]);
__m128d a8_1 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, b8));
#endif
_mm_storeu_pd(&C[(i*10)+2], c8_1);
#else
C[(i*10)+0] += values[17] * B[(i*10)+8];
C[(i*10)+2] += values[18] * B[(i*10)+8];
C[(i*10)+3] += values[19] * B[(i*10)+8];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*10)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*10)+9]);
#endif
__m128d c9_0 = _mm_load_sd(&C[(i*10)+0]);
__m128d a9_0 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, b9));
#endif
_mm_store_sd(&C[(i*10)+0], c9_0);
__m128d c9_1 = _mm_load_sd(&C[(i*10)+3]);
__m128d a9_1 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, b9));
#endif
_mm_store_sd(&C[(i*10)+3], c9_1);
#else
C[(i*10)+0] += values[20] * B[(i*10)+9];
C[(i*10)+3] += values[21] * B[(i*10)+9];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kXiDivM_9_20(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 20; m++) {
    C[(i*20)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*20)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*20)+0]);
#endif
__m128d c0_0 = _mm_load_sd(&C[(i*20)+1]);
__m128d a0_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, b0));
#endif
_mm_store_sd(&C[(i*20)+1], c0_0);
__m128d c0_1 = _mm_load_sd(&C[(i*20)+5]);
__m128d a0_1 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, b0));
#endif
_mm_store_sd(&C[(i*20)+5], c0_1);
__m128d c0_2 = _mm_load_sd(&C[(i*20)+7]);
__m128d a0_2 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, b0));
#endif
_mm_store_sd(&C[(i*20)+7], c0_2);
__m128d c0_3 = _mm_load_sd(&C[(i*20)+10]);
__m128d a0_3 = _mm_load_sd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_3 = _mm_add_sd(c0_3, _mm_mul_sd(a0_3, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_3 = _mm_add_sd(c0_3, _mm_mul_sd(a0_3, b0));
#endif
_mm_store_sd(&C[(i*20)+10], c0_3);
__m128d c0_4 = _mm_load_sd(&C[(i*20)+12]);
__m128d a0_4 = _mm_load_sd(&values[4]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_4 = _mm_add_sd(c0_4, _mm_mul_sd(a0_4, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_4 = _mm_add_sd(c0_4, _mm_mul_sd(a0_4, b0));
#endif
_mm_store_sd(&C[(i*20)+12], c0_4);
__m128d c0_5 = _mm_load_sd(&C[(i*20)+15]);
__m128d a0_5 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_5 = _mm_add_sd(c0_5, _mm_mul_sd(a0_5, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_5 = _mm_add_sd(c0_5, _mm_mul_sd(a0_5, b0));
#endif
_mm_store_sd(&C[(i*20)+15], c0_5);
__m128d c0_6 = _mm_load_sd(&C[(i*20)+17]);
__m128d a0_6 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, b0));
#endif
_mm_store_sd(&C[(i*20)+17], c0_6);
#else
C[(i*20)+1] += values[0] * B[(i*20)+0];
C[(i*20)+5] += values[1] * B[(i*20)+0];
C[(i*20)+7] += values[2] * B[(i*20)+0];
C[(i*20)+10] += values[3] * B[(i*20)+0];
C[(i*20)+12] += values[4] * B[(i*20)+0];
C[(i*20)+15] += values[5] * B[(i*20)+0];
C[(i*20)+17] += values[6] * B[(i*20)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*20)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*20)+4]);
__m128d a1_0 = _mm_load_sd(&values[7]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*20)+4], c1_0);
__m128d c1_1 = _mm_load_sd(&C[(i*20)+11]);
__m128d a1_1 = _mm_load_sd(&values[8]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_1 = _mm_add_sd(c1_1, _mm_mul_sd(a1_1, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_1 = _mm_add_sd(c1_1, _mm_mul_sd(a1_1, b1));
#endif
_mm_store_sd(&C[(i*20)+11], c1_1);
__m128d c1_2 = _mm_load_sd(&C[(i*20)+14]);
__m128d a1_2 = _mm_load_sd(&values[9]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, b1));
#endif
_mm_store_sd(&C[(i*20)+14], c1_2);
#else
C[(i*20)+4] += values[7] * B[(i*20)+1];
C[(i*20)+11] += values[8] * B[(i*20)+1];
C[(i*20)+14] += values[9] * B[(i*20)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*20)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*20)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*20)+5]);
__m128d a2_0 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*20)+5], c2_0);
__m128d c2_1 = _mm_load_sd(&C[(i*20)+10]);
__m128d a2_1 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_1 = _mm_add_sd(c2_1, _mm_mul_sd(a2_1, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_1 = _mm_add_sd(c2_1, _mm_mul_sd(a2_1, b2));
#endif
_mm_store_sd(&C[(i*20)+10], c2_1);
__m128d c2_2 = _mm_load_sd(&C[(i*20)+12]);
__m128d a2_2 = _mm_load_sd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*20)+12], c2_2);
__m128d c2_3 = _mm_load_sd(&C[(i*20)+15]);
__m128d a2_3 = _mm_load_sd(&values[13]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, b2));
#endif
_mm_store_sd(&C[(i*20)+15], c2_3);
#else
C[(i*20)+5] += values[10] * B[(i*20)+2];
C[(i*20)+10] += values[11] * B[(i*20)+2];
C[(i*20)+12] += values[12] * B[(i*20)+2];
C[(i*20)+15] += values[13] * B[(i*20)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*20)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*20)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*20)+5]);
__m128d a3_0 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*20)+5], c3_0);
__m128d c3_1 = _mm_load_sd(&C[(i*20)+7]);
__m128d a3_1 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, b3));
#endif
_mm_store_sd(&C[(i*20)+7], c3_1);
__m128d c3_2 = _mm_load_sd(&C[(i*20)+10]);
__m128d a3_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_2 = _mm_add_sd(c3_2, _mm_mul_sd(a3_2, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_2 = _mm_add_sd(c3_2, _mm_mul_sd(a3_2, b3));
#endif
_mm_store_sd(&C[(i*20)+10], c3_2);
__m128d c3_3 = _mm_load_sd(&C[(i*20)+12]);
__m128d a3_3 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_3 = _mm_add_sd(c3_3, _mm_mul_sd(a3_3, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_3 = _mm_add_sd(c3_3, _mm_mul_sd(a3_3, b3));
#endif
_mm_store_sd(&C[(i*20)+12], c3_3);
__m128d c3_4 = _mm_load_sd(&C[(i*20)+15]);
__m128d a3_4 = _mm_load_sd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, b3));
#endif
_mm_store_sd(&C[(i*20)+15], c3_4);
__m128d c3_5 = _mm_load_sd(&C[(i*20)+17]);
__m128d a3_5 = _mm_load_sd(&values[19]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_5 = _mm_add_sd(c3_5, _mm_mul_sd(a3_5, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_5 = _mm_add_sd(c3_5, _mm_mul_sd(a3_5, b3));
#endif
_mm_store_sd(&C[(i*20)+17], c3_5);
#else
C[(i*20)+5] += values[14] * B[(i*20)+3];
C[(i*20)+7] += values[15] * B[(i*20)+3];
C[(i*20)+10] += values[16] * B[(i*20)+3];
C[(i*20)+12] += values[17] * B[(i*20)+3];
C[(i*20)+15] += values[18] * B[(i*20)+3];
C[(i*20)+17] += values[19] * B[(i*20)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*20)+4]);
#endif
__m128d c4_0 = _mm_load_sd(&C[(i*20)+10]);
__m128d a4_0 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, b4));
#endif
_mm_store_sd(&C[(i*20)+10], c4_0);
#else
C[(i*20)+10] += values[20] * B[(i*20)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*20)+5]);
#endif
__m128d c5_0 = _mm_load_sd(&C[(i*20)+11]);
__m128d a5_0 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, b5));
#endif
_mm_store_sd(&C[(i*20)+11], c5_0);
#else
C[(i*20)+11] += values[21] * B[(i*20)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*20)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*20)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*20)+10]);
__m128d a6_0 = _mm_load_sd(&values[22]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*20)+10], c6_0);
__m128d c6_1 = _mm_load_sd(&C[(i*20)+12]);
__m128d a6_1 = _mm_load_sd(&values[23]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_sd(c6_1, _mm_mul_sd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_sd(c6_1, _mm_mul_sd(a6_1, b6));
#endif
_mm_store_sd(&C[(i*20)+12], c6_1);
#else
C[(i*20)+10] += values[22] * B[(i*20)+6];
C[(i*20)+12] += values[23] * B[(i*20)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*20)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*20)+11]);
__m128d a7_0 = _mm_load_sd(&values[24]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*20)+11], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*20)+14]);
__m128d a7_1 = _mm_load_sd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*20)+14], c7_1);
#else
C[(i*20)+11] += values[24] * B[(i*20)+7];
C[(i*20)+14] += values[25] * B[(i*20)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*20)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*20)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*20)+10]);
__m128d a8_0 = _mm_load_sd(&values[26]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*20)+10], c8_0);
__m128d c8_1 = _mm_load_sd(&C[(i*20)+12]);
__m128d a8_1 = _mm_load_sd(&values[27]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, b8));
#endif
_mm_store_sd(&C[(i*20)+12], c8_1);
__m128d c8_2 = _mm_load_sd(&C[(i*20)+15]);
__m128d a8_2 = _mm_load_sd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_2 = _mm_add_sd(c8_2, _mm_mul_sd(a8_2, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_2 = _mm_add_sd(c8_2, _mm_mul_sd(a8_2, b8));
#endif
_mm_store_sd(&C[(i*20)+15], c8_2);
#else
C[(i*20)+10] += values[26] * B[(i*20)+8];
C[(i*20)+12] += values[27] * B[(i*20)+8];
C[(i*20)+15] += values[28] * B[(i*20)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*20)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*20)+9]);
#endif
__m128d c9_0 = _mm_load_sd(&C[(i*20)+10]);
__m128d a9_0 = _mm_load_sd(&values[29]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, b9));
#endif
_mm_store_sd(&C[(i*20)+10], c9_0);
__m128d c9_1 = _mm_load_sd(&C[(i*20)+12]);
__m128d a9_1 = _mm_load_sd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, b9));
#endif
_mm_store_sd(&C[(i*20)+12], c9_1);
__m128d c9_2 = _mm_load_sd(&C[(i*20)+15]);
__m128d a9_2 = _mm_load_sd(&values[31]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_2 = _mm_add_sd(c9_2, _mm_mul_sd(a9_2, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_2 = _mm_add_sd(c9_2, _mm_mul_sd(a9_2, b9));
#endif
_mm_store_sd(&C[(i*20)+15], c9_2);
__m128d c9_3 = _mm_load_sd(&C[(i*20)+17]);
__m128d a9_3 = _mm_load_sd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_3 = _mm_add_sd(c9_3, _mm_mul_sd(a9_3, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_3 = _mm_add_sd(c9_3, _mm_mul_sd(a9_3, b9));
#endif
_mm_store_sd(&C[(i*20)+17], c9_3);
#else
C[(i*20)+10] += values[29] * B[(i*20)+9];
C[(i*20)+12] += values[30] * B[(i*20)+9];
C[(i*20)+15] += values[31] * B[(i*20)+9];
C[(i*20)+17] += values[32] * B[(i*20)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 594;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivM_9_20(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 20; m++) {
    C[(i*20)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*20)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*20)+0]);
#endif
__m128d c0_0 = _mm_loadu_pd(&C[(i*20)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
#endif
_mm_storeu_pd(&C[(i*20)+1], c0_0);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_2 = _mm256_loadu_pd(&C[(i*20)+4]);
__m256d a0_2 = _mm256_loadu_pd(&values[2]);
c0_2 = _mm256_add_pd(c0_2, _mm256_mul_pd(a0_2, b0));
_mm256_storeu_pd(&C[(i*20)+4], c0_2);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_2 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*20)+4], c0_2);
__m128d c0_4 = _mm_loadu_pd(&C[(i*20)+6]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*20)+6], c0_4);
#endif
__m128d c0_6 = _mm_load_sd(&C[(i*20)+8]);
__m128d a0_6 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, b0));
#endif
_mm_store_sd(&C[(i*20)+8], c0_6);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_7 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a0_7 = _mm256_loadu_pd(&values[7]);
c0_7 = _mm256_add_pd(c0_7, _mm256_mul_pd(a0_7, b0));
_mm256_storeu_pd(&C[(i*20)+10], c0_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_7 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a0_7 = _mm_loadu_pd(&values[7]);
c0_7 = _mm_add_pd(c0_7, _mm_mul_pd(a0_7, b0));
_mm_storeu_pd(&C[(i*20)+10], c0_7);
__m128d c0_9 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a0_9 = _mm_loadu_pd(&values[9]);
c0_9 = _mm_add_pd(c0_9, _mm_mul_pd(a0_9, b0));
_mm_storeu_pd(&C[(i*20)+12], c0_9);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_11 = _mm256_loadu_pd(&C[(i*20)+14]);
__m256d a0_11 = _mm256_loadu_pd(&values[11]);
c0_11 = _mm256_add_pd(c0_11, _mm256_mul_pd(a0_11, b0));
_mm256_storeu_pd(&C[(i*20)+14], c0_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_11 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a0_11 = _mm_loadu_pd(&values[11]);
c0_11 = _mm_add_pd(c0_11, _mm_mul_pd(a0_11, b0));
_mm_storeu_pd(&C[(i*20)+14], c0_11);
__m128d c0_13 = _mm_loadu_pd(&C[(i*20)+16]);
__m128d a0_13 = _mm_loadu_pd(&values[13]);
c0_13 = _mm_add_pd(c0_13, _mm_mul_pd(a0_13, b0));
_mm_storeu_pd(&C[(i*20)+16], c0_13);
#endif
__m128d c0_15 = _mm_load_sd(&C[(i*20)+18]);
__m128d a0_15 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, b0));
#endif
_mm_store_sd(&C[(i*20)+18], c0_15);
#else
C[(i*20)+1] += values[0] * B[(i*20)+0];
C[(i*20)+2] += values[1] * B[(i*20)+0];
C[(i*20)+4] += values[2] * B[(i*20)+0];
C[(i*20)+5] += values[3] * B[(i*20)+0];
C[(i*20)+6] += values[4] * B[(i*20)+0];
C[(i*20)+7] += values[5] * B[(i*20)+0];
C[(i*20)+8] += values[6] * B[(i*20)+0];
C[(i*20)+10] += values[7] * B[(i*20)+0];
C[(i*20)+11] += values[8] * B[(i*20)+0];
C[(i*20)+12] += values[9] * B[(i*20)+0];
C[(i*20)+13] += values[10] * B[(i*20)+0];
C[(i*20)+14] += values[11] * B[(i*20)+0];
C[(i*20)+15] += values[12] * B[(i*20)+0];
C[(i*20)+16] += values[13] * B[(i*20)+0];
C[(i*20)+17] += values[14] * B[(i*20)+0];
C[(i*20)+18] += values[15] * B[(i*20)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*20)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*20)+4], c1_0);
__m128d c1_2 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a1_2 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_pd(c1_2, _mm_mul_pd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_pd(c1_2, _mm_mul_pd(a1_2, b1));
#endif
_mm_storeu_pd(&C[(i*20)+10], c1_2);
__m128d c1_4 = _mm_load_sd(&C[(i*20)+12]);
__m128d a1_4 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, b1));
#endif
_mm_store_sd(&C[(i*20)+12], c1_4);
__m128d c1_5 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a1_5 = _mm_loadu_pd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_pd(c1_5, _mm_mul_pd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_pd(c1_5, _mm_mul_pd(a1_5, b1));
#endif
_mm_storeu_pd(&C[(i*20)+14], c1_5);
#else
C[(i*20)+4] += values[16] * B[(i*20)+1];
C[(i*20)+5] += values[17] * B[(i*20)+1];
C[(i*20)+10] += values[18] * B[(i*20)+1];
C[(i*20)+11] += values[19] * B[(i*20)+1];
C[(i*20)+12] += values[20] * B[(i*20)+1];
C[(i*20)+14] += values[21] * B[(i*20)+1];
C[(i*20)+15] += values[22] * B[(i*20)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*20)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*20)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[23]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*20)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*20)+6]);
__m128d a2_2 = _mm_load_sd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*20)+6], c2_2);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_3 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a2_3 = _mm256_loadu_pd(&values[26]);
c2_3 = _mm256_add_pd(c2_3, _mm256_mul_pd(a2_3, b2));
_mm256_storeu_pd(&C[(i*20)+10], c2_3);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_3 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a2_3 = _mm_loadu_pd(&values[26]);
c2_3 = _mm_add_pd(c2_3, _mm_mul_pd(a2_3, b2));
_mm_storeu_pd(&C[(i*20)+10], c2_3);
__m128d c2_5 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a2_5 = _mm_loadu_pd(&values[28]);
c2_5 = _mm_add_pd(c2_5, _mm_mul_pd(a2_5, b2));
_mm_storeu_pd(&C[(i*20)+12], c2_5);
#endif
__m128d c2_7 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a2_7 = _mm_loadu_pd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_7 = _mm_add_pd(c2_7, _mm_mul_pd(a2_7, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_7 = _mm_add_pd(c2_7, _mm_mul_pd(a2_7, b2));
#endif
_mm_storeu_pd(&C[(i*20)+14], c2_7);
__m128d c2_9 = _mm_load_sd(&C[(i*20)+16]);
__m128d a2_9 = _mm_load_sd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, b2));
#endif
_mm_store_sd(&C[(i*20)+16], c2_9);
#else
C[(i*20)+4] += values[23] * B[(i*20)+2];
C[(i*20)+5] += values[24] * B[(i*20)+2];
C[(i*20)+6] += values[25] * B[(i*20)+2];
C[(i*20)+10] += values[26] * B[(i*20)+2];
C[(i*20)+11] += values[27] * B[(i*20)+2];
C[(i*20)+12] += values[28] * B[(i*20)+2];
C[(i*20)+13] += values[29] * B[(i*20)+2];
C[(i*20)+14] += values[30] * B[(i*20)+2];
C[(i*20)+15] += values[31] * B[(i*20)+2];
C[(i*20)+16] += values[32] * B[(i*20)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*20)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*20)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*20)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[33]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*20)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[33]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*20)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*20)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[35]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*20)+6], c3_2);
#endif
__m128d c3_4 = _mm_load_sd(&C[(i*20)+8]);
__m128d a3_4 = _mm_load_sd(&values[37]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, b3));
#endif
_mm_store_sd(&C[(i*20)+8], c3_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_5 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a3_5 = _mm256_loadu_pd(&values[38]);
c3_5 = _mm256_add_pd(c3_5, _mm256_mul_pd(a3_5, b3));
_mm256_storeu_pd(&C[(i*20)+10], c3_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_5 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a3_5 = _mm_loadu_pd(&values[38]);
c3_5 = _mm_add_pd(c3_5, _mm_mul_pd(a3_5, b3));
_mm_storeu_pd(&C[(i*20)+10], c3_5);
__m128d c3_7 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a3_7 = _mm_loadu_pd(&values[40]);
c3_7 = _mm_add_pd(c3_7, _mm_mul_pd(a3_7, b3));
_mm_storeu_pd(&C[(i*20)+12], c3_7);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_9 = _mm256_loadu_pd(&C[(i*20)+14]);
__m256d a3_9 = _mm256_loadu_pd(&values[42]);
c3_9 = _mm256_add_pd(c3_9, _mm256_mul_pd(a3_9, b3));
_mm256_storeu_pd(&C[(i*20)+14], c3_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_9 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a3_9 = _mm_loadu_pd(&values[42]);
c3_9 = _mm_add_pd(c3_9, _mm_mul_pd(a3_9, b3));
_mm_storeu_pd(&C[(i*20)+14], c3_9);
__m128d c3_11 = _mm_loadu_pd(&C[(i*20)+16]);
__m128d a3_11 = _mm_loadu_pd(&values[44]);
c3_11 = _mm_add_pd(c3_11, _mm_mul_pd(a3_11, b3));
_mm_storeu_pd(&C[(i*20)+16], c3_11);
#endif
__m128d c3_13 = _mm_load_sd(&C[(i*20)+18]);
__m128d a3_13 = _mm_load_sd(&values[46]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, b3));
#endif
_mm_store_sd(&C[(i*20)+18], c3_13);
#else
C[(i*20)+4] += values[33] * B[(i*20)+3];
C[(i*20)+5] += values[34] * B[(i*20)+3];
C[(i*20)+6] += values[35] * B[(i*20)+3];
C[(i*20)+7] += values[36] * B[(i*20)+3];
C[(i*20)+8] += values[37] * B[(i*20)+3];
C[(i*20)+10] += values[38] * B[(i*20)+3];
C[(i*20)+11] += values[39] * B[(i*20)+3];
C[(i*20)+12] += values[40] * B[(i*20)+3];
C[(i*20)+13] += values[41] * B[(i*20)+3];
C[(i*20)+14] += values[42] * B[(i*20)+3];
C[(i*20)+15] += values[43] * B[(i*20)+3];
C[(i*20)+16] += values[44] * B[(i*20)+3];
C[(i*20)+17] += values[45] * B[(i*20)+3];
C[(i*20)+18] += values[46] * B[(i*20)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*20)+4]);
#endif
__m128d c4_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a4_0 = _mm_loadu_pd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
#endif
_mm_storeu_pd(&C[(i*20)+10], c4_0);
#else
C[(i*20)+10] += values[47] * B[(i*20)+4];
C[(i*20)+11] += values[48] * B[(i*20)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*20)+5]);
#endif
__m128d c5_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a5_0 = _mm_loadu_pd(&values[49]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
#endif
_mm_storeu_pd(&C[(i*20)+10], c5_0);
__m128d c5_2 = _mm_load_sd(&C[(i*20)+12]);
__m128d a5_2 = _mm_load_sd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*20)+12], c5_2);
#else
C[(i*20)+10] += values[49] * B[(i*20)+5];
C[(i*20)+11] += values[50] * B[(i*20)+5];
C[(i*20)+12] += values[51] * B[(i*20)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*20)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*20)+6]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_0 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a6_0 = _mm256_loadu_pd(&values[52]);
c6_0 = _mm256_add_pd(c6_0, _mm256_mul_pd(a6_0, b6));
_mm256_storeu_pd(&C[(i*20)+10], c6_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a6_0 = _mm_loadu_pd(&values[52]);
c6_0 = _mm_add_pd(c6_0, _mm_mul_pd(a6_0, b6));
_mm_storeu_pd(&C[(i*20)+10], c6_0);
__m128d c6_2 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a6_2 = _mm_loadu_pd(&values[54]);
c6_2 = _mm_add_pd(c6_2, _mm_mul_pd(a6_2, b6));
_mm_storeu_pd(&C[(i*20)+12], c6_2);
#endif
#else
C[(i*20)+10] += values[52] * B[(i*20)+6];
C[(i*20)+11] += values[53] * B[(i*20)+6];
C[(i*20)+12] += values[54] * B[(i*20)+6];
C[(i*20)+13] += values[55] * B[(i*20)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*20)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a7_0 = _mm_loadu_pd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*20)+10], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*20)+12]);
__m128d a7_2 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*20)+12], c7_2);
__m128d c7_3 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a7_3 = _mm_loadu_pd(&values[59]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, b7));
#endif
_mm_storeu_pd(&C[(i*20)+14], c7_3);
#else
C[(i*20)+10] += values[56] * B[(i*20)+7];
C[(i*20)+11] += values[57] * B[(i*20)+7];
C[(i*20)+12] += values[58] * B[(i*20)+7];
C[(i*20)+14] += values[59] * B[(i*20)+7];
C[(i*20)+15] += values[60] * B[(i*20)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*20)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*20)+8]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_0 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a8_0 = _mm256_loadu_pd(&values[61]);
c8_0 = _mm256_add_pd(c8_0, _mm256_mul_pd(a8_0, b8));
_mm256_storeu_pd(&C[(i*20)+10], c8_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a8_0 = _mm_loadu_pd(&values[61]);
c8_0 = _mm_add_pd(c8_0, _mm_mul_pd(a8_0, b8));
_mm_storeu_pd(&C[(i*20)+10], c8_0);
__m128d c8_2 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a8_2 = _mm_loadu_pd(&values[63]);
c8_2 = _mm_add_pd(c8_2, _mm_mul_pd(a8_2, b8));
_mm_storeu_pd(&C[(i*20)+12], c8_2);
#endif
__m128d c8_4 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a8_4 = _mm_loadu_pd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, b8));
#endif
_mm_storeu_pd(&C[(i*20)+14], c8_4);
__m128d c8_6 = _mm_load_sd(&C[(i*20)+16]);
__m128d a8_6 = _mm_load_sd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*20)+16], c8_6);
#else
C[(i*20)+10] += values[61] * B[(i*20)+8];
C[(i*20)+11] += values[62] * B[(i*20)+8];
C[(i*20)+12] += values[63] * B[(i*20)+8];
C[(i*20)+13] += values[64] * B[(i*20)+8];
C[(i*20)+14] += values[65] * B[(i*20)+8];
C[(i*20)+15] += values[66] * B[(i*20)+8];
C[(i*20)+16] += values[67] * B[(i*20)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*20)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*20)+9]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_0 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a9_0 = _mm256_loadu_pd(&values[68]);
c9_0 = _mm256_add_pd(c9_0, _mm256_mul_pd(a9_0, b9));
_mm256_storeu_pd(&C[(i*20)+10], c9_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a9_0 = _mm_loadu_pd(&values[68]);
c9_0 = _mm_add_pd(c9_0, _mm_mul_pd(a9_0, b9));
_mm_storeu_pd(&C[(i*20)+10], c9_0);
__m128d c9_2 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a9_2 = _mm_loadu_pd(&values[70]);
c9_2 = _mm_add_pd(c9_2, _mm_mul_pd(a9_2, b9));
_mm_storeu_pd(&C[(i*20)+12], c9_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_4 = _mm256_loadu_pd(&C[(i*20)+14]);
__m256d a9_4 = _mm256_loadu_pd(&values[72]);
c9_4 = _mm256_add_pd(c9_4, _mm256_mul_pd(a9_4, b9));
_mm256_storeu_pd(&C[(i*20)+14], c9_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_4 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a9_4 = _mm_loadu_pd(&values[72]);
c9_4 = _mm_add_pd(c9_4, _mm_mul_pd(a9_4, b9));
_mm_storeu_pd(&C[(i*20)+14], c9_4);
__m128d c9_6 = _mm_loadu_pd(&C[(i*20)+16]);
__m128d a9_6 = _mm_loadu_pd(&values[74]);
c9_6 = _mm_add_pd(c9_6, _mm_mul_pd(a9_6, b9));
_mm_storeu_pd(&C[(i*20)+16], c9_6);
#endif
__m128d c9_8 = _mm_load_sd(&C[(i*20)+18]);
__m128d a9_8 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, b9));
#endif
_mm_store_sd(&C[(i*20)+18], c9_8);
#else
C[(i*20)+10] += values[68] * B[(i*20)+9];
C[(i*20)+11] += values[69] * B[(i*20)+9];
C[(i*20)+12] += values[70] * B[(i*20)+9];
C[(i*20)+13] += values[71] * B[(i*20)+9];
C[(i*20)+14] += values[72] * B[(i*20)+9];
C[(i*20)+15] += values[73] * B[(i*20)+9];
C[(i*20)+16] += values[74] * B[(i*20)+9];
C[(i*20)+17] += values[75] * B[(i*20)+9];
C[(i*20)+18] += values[76] * B[(i*20)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 1386;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivM_9_20(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 20; m++) {
    C[(i*20)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*20)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*20)+0]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_0 = _mm256_loadu_pd(&C[(i*20)+1]);
__m256d a0_0 = _mm256_loadu_pd(&values[0]);
c0_0 = _mm256_add_pd(c0_0, _mm256_mul_pd(a0_0, b0));
_mm256_storeu_pd(&C[(i*20)+1], c0_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_0 = _mm_loadu_pd(&C[(i*20)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
_mm_storeu_pd(&C[(i*20)+1], c0_0);
__m128d c0_2 = _mm_loadu_pd(&C[(i*20)+3]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*20)+3], c0_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_4 = _mm256_loadu_pd(&C[(i*20)+5]);
__m256d a0_4 = _mm256_loadu_pd(&values[4]);
c0_4 = _mm256_add_pd(c0_4, _mm256_mul_pd(a0_4, b0));
_mm256_storeu_pd(&C[(i*20)+5], c0_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_4 = _mm_loadu_pd(&C[(i*20)+5]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*20)+5], c0_4);
__m128d c0_6 = _mm_loadu_pd(&C[(i*20)+7]);
__m128d a0_6 = _mm_loadu_pd(&values[6]);
c0_6 = _mm_add_pd(c0_6, _mm_mul_pd(a0_6, b0));
_mm_storeu_pd(&C[(i*20)+7], c0_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_8 = _mm256_loadu_pd(&C[(i*20)+9]);
__m256d a0_8 = _mm256_loadu_pd(&values[8]);
c0_8 = _mm256_add_pd(c0_8, _mm256_mul_pd(a0_8, b0));
_mm256_storeu_pd(&C[(i*20)+9], c0_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_8 = _mm_loadu_pd(&C[(i*20)+9]);
__m128d a0_8 = _mm_loadu_pd(&values[8]);
c0_8 = _mm_add_pd(c0_8, _mm_mul_pd(a0_8, b0));
_mm_storeu_pd(&C[(i*20)+9], c0_8);
__m128d c0_10 = _mm_loadu_pd(&C[(i*20)+11]);
__m128d a0_10 = _mm_loadu_pd(&values[10]);
c0_10 = _mm_add_pd(c0_10, _mm_mul_pd(a0_10, b0));
_mm_storeu_pd(&C[(i*20)+11], c0_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_12 = _mm256_loadu_pd(&C[(i*20)+13]);
__m256d a0_12 = _mm256_loadu_pd(&values[12]);
c0_12 = _mm256_add_pd(c0_12, _mm256_mul_pd(a0_12, b0));
_mm256_storeu_pd(&C[(i*20)+13], c0_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_12 = _mm_loadu_pd(&C[(i*20)+13]);
__m128d a0_12 = _mm_loadu_pd(&values[12]);
c0_12 = _mm_add_pd(c0_12, _mm_mul_pd(a0_12, b0));
_mm_storeu_pd(&C[(i*20)+13], c0_12);
__m128d c0_14 = _mm_loadu_pd(&C[(i*20)+15]);
__m128d a0_14 = _mm_loadu_pd(&values[14]);
c0_14 = _mm_add_pd(c0_14, _mm_mul_pd(a0_14, b0));
_mm_storeu_pd(&C[(i*20)+15], c0_14);
#endif
__m128d c0_16 = _mm_loadu_pd(&C[(i*20)+17]);
__m128d a0_16 = _mm_loadu_pd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_16 = _mm_add_pd(c0_16, _mm_mul_pd(a0_16, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_16 = _mm_add_pd(c0_16, _mm_mul_pd(a0_16, b0));
#endif
_mm_storeu_pd(&C[(i*20)+17], c0_16);
__m128d c0_18 = _mm_load_sd(&C[(i*20)+19]);
__m128d a0_18 = _mm_load_sd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_18 = _mm_add_sd(c0_18, _mm_mul_sd(a0_18, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_18 = _mm_add_sd(c0_18, _mm_mul_sd(a0_18, b0));
#endif
_mm_store_sd(&C[(i*20)+19], c0_18);
#else
C[(i*20)+1] += values[0] * B[(i*20)+0];
C[(i*20)+2] += values[1] * B[(i*20)+0];
C[(i*20)+3] += values[2] * B[(i*20)+0];
C[(i*20)+4] += values[3] * B[(i*20)+0];
C[(i*20)+5] += values[4] * B[(i*20)+0];
C[(i*20)+6] += values[5] * B[(i*20)+0];
C[(i*20)+7] += values[6] * B[(i*20)+0];
C[(i*20)+8] += values[7] * B[(i*20)+0];
C[(i*20)+9] += values[8] * B[(i*20)+0];
C[(i*20)+10] += values[9] * B[(i*20)+0];
C[(i*20)+11] += values[10] * B[(i*20)+0];
C[(i*20)+12] += values[11] * B[(i*20)+0];
C[(i*20)+13] += values[12] * B[(i*20)+0];
C[(i*20)+14] += values[13] * B[(i*20)+0];
C[(i*20)+15] += values[14] * B[(i*20)+0];
C[(i*20)+16] += values[15] * B[(i*20)+0];
C[(i*20)+17] += values[16] * B[(i*20)+0];
C[(i*20)+18] += values[17] * B[(i*20)+0];
C[(i*20)+19] += values[18] * B[(i*20)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*20)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[19]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*20)+4], c1_0);
__m128d c1_2 = _mm_load_sd(&C[(i*20)+7]);
__m128d a1_2 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, b1));
#endif
_mm_store_sd(&C[(i*20)+7], c1_2);
__m128d c1_3 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a1_3 = _mm_loadu_pd(&values[22]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_3 = _mm_add_pd(c1_3, _mm_mul_pd(a1_3, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_3 = _mm_add_pd(c1_3, _mm_mul_pd(a1_3, b1));
#endif
_mm_storeu_pd(&C[(i*20)+10], c1_3);
__m128d c1_5 = _mm_load_sd(&C[(i*20)+12]);
__m128d a1_5 = _mm_load_sd(&values[24]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, b1));
#endif
_mm_store_sd(&C[(i*20)+12], c1_5);
__m128d c1_6 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a1_6 = _mm_loadu_pd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_6 = _mm_add_pd(c1_6, _mm_mul_pd(a1_6, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_6 = _mm_add_pd(c1_6, _mm_mul_pd(a1_6, b1));
#endif
_mm_storeu_pd(&C[(i*20)+14], c1_6);
__m128d c1_8 = _mm_load_sd(&C[(i*20)+17]);
__m128d a1_8 = _mm_load_sd(&values[27]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, b1));
#endif
_mm_store_sd(&C[(i*20)+17], c1_8);
#else
C[(i*20)+4] += values[19] * B[(i*20)+1];
C[(i*20)+5] += values[20] * B[(i*20)+1];
C[(i*20)+7] += values[21] * B[(i*20)+1];
C[(i*20)+10] += values[22] * B[(i*20)+1];
C[(i*20)+11] += values[23] * B[(i*20)+1];
C[(i*20)+12] += values[24] * B[(i*20)+1];
C[(i*20)+14] += values[25] * B[(i*20)+1];
C[(i*20)+15] += values[26] * B[(i*20)+1];
C[(i*20)+17] += values[27] * B[(i*20)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*20)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*20)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*20)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*20)+6]);
__m128d a2_2 = _mm_load_sd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*20)+6], c2_2);
__m128d c2_3 = _mm_load_sd(&C[(i*20)+8]);
__m128d a2_3 = _mm_load_sd(&values[31]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, b2));
#endif
_mm_store_sd(&C[(i*20)+8], c2_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_4 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a2_4 = _mm256_loadu_pd(&values[32]);
c2_4 = _mm256_add_pd(c2_4, _mm256_mul_pd(a2_4, b2));
_mm256_storeu_pd(&C[(i*20)+10], c2_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_4 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a2_4 = _mm_loadu_pd(&values[32]);
c2_4 = _mm_add_pd(c2_4, _mm_mul_pd(a2_4, b2));
_mm_storeu_pd(&C[(i*20)+10], c2_4);
__m128d c2_6 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a2_6 = _mm_loadu_pd(&values[34]);
c2_6 = _mm_add_pd(c2_6, _mm_mul_pd(a2_6, b2));
_mm_storeu_pd(&C[(i*20)+12], c2_6);
#endif
__m128d c2_8 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a2_8 = _mm_loadu_pd(&values[36]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_8 = _mm_add_pd(c2_8, _mm_mul_pd(a2_8, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_8 = _mm_add_pd(c2_8, _mm_mul_pd(a2_8, b2));
#endif
_mm_storeu_pd(&C[(i*20)+14], c2_8);
__m128d c2_10 = _mm_load_sd(&C[(i*20)+16]);
__m128d a2_10 = _mm_load_sd(&values[38]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, b2));
#endif
_mm_store_sd(&C[(i*20)+16], c2_10);
__m128d c2_11 = _mm_load_sd(&C[(i*20)+18]);
__m128d a2_11 = _mm_load_sd(&values[39]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, b2));
#endif
_mm_store_sd(&C[(i*20)+18], c2_11);
#else
C[(i*20)+4] += values[28] * B[(i*20)+2];
C[(i*20)+5] += values[29] * B[(i*20)+2];
C[(i*20)+6] += values[30] * B[(i*20)+2];
C[(i*20)+8] += values[31] * B[(i*20)+2];
C[(i*20)+10] += values[32] * B[(i*20)+2];
C[(i*20)+11] += values[33] * B[(i*20)+2];
C[(i*20)+12] += values[34] * B[(i*20)+2];
C[(i*20)+13] += values[35] * B[(i*20)+2];
C[(i*20)+14] += values[36] * B[(i*20)+2];
C[(i*20)+15] += values[37] * B[(i*20)+2];
C[(i*20)+16] += values[38] * B[(i*20)+2];
C[(i*20)+18] += values[39] * B[(i*20)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*20)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*20)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*20)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[40]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*20)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[40]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*20)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*20)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[42]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*20)+6], c3_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_4 = _mm256_loadu_pd(&C[(i*20)+8]);
__m256d a3_4 = _mm256_loadu_pd(&values[44]);
c3_4 = _mm256_add_pd(c3_4, _mm256_mul_pd(a3_4, b3));
_mm256_storeu_pd(&C[(i*20)+8], c3_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_4 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a3_4 = _mm_loadu_pd(&values[44]);
c3_4 = _mm_add_pd(c3_4, _mm_mul_pd(a3_4, b3));
_mm_storeu_pd(&C[(i*20)+8], c3_4);
__m128d c3_6 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a3_6 = _mm_loadu_pd(&values[46]);
c3_6 = _mm_add_pd(c3_6, _mm_mul_pd(a3_6, b3));
_mm_storeu_pd(&C[(i*20)+10], c3_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_8 = _mm256_loadu_pd(&C[(i*20)+12]);
__m256d a3_8 = _mm256_loadu_pd(&values[48]);
c3_8 = _mm256_add_pd(c3_8, _mm256_mul_pd(a3_8, b3));
_mm256_storeu_pd(&C[(i*20)+12], c3_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_8 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a3_8 = _mm_loadu_pd(&values[48]);
c3_8 = _mm_add_pd(c3_8, _mm_mul_pd(a3_8, b3));
_mm_storeu_pd(&C[(i*20)+12], c3_8);
__m128d c3_10 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a3_10 = _mm_loadu_pd(&values[50]);
c3_10 = _mm_add_pd(c3_10, _mm_mul_pd(a3_10, b3));
_mm_storeu_pd(&C[(i*20)+14], c3_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_12 = _mm256_loadu_pd(&C[(i*20)+16]);
__m256d a3_12 = _mm256_loadu_pd(&values[52]);
c3_12 = _mm256_add_pd(c3_12, _mm256_mul_pd(a3_12, b3));
_mm256_storeu_pd(&C[(i*20)+16], c3_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_12 = _mm_loadu_pd(&C[(i*20)+16]);
__m128d a3_12 = _mm_loadu_pd(&values[52]);
c3_12 = _mm_add_pd(c3_12, _mm_mul_pd(a3_12, b3));
_mm_storeu_pd(&C[(i*20)+16], c3_12);
__m128d c3_14 = _mm_loadu_pd(&C[(i*20)+18]);
__m128d a3_14 = _mm_loadu_pd(&values[54]);
c3_14 = _mm_add_pd(c3_14, _mm_mul_pd(a3_14, b3));
_mm_storeu_pd(&C[(i*20)+18], c3_14);
#endif
#else
C[(i*20)+4] += values[40] * B[(i*20)+3];
C[(i*20)+5] += values[41] * B[(i*20)+3];
C[(i*20)+6] += values[42] * B[(i*20)+3];
C[(i*20)+7] += values[43] * B[(i*20)+3];
C[(i*20)+8] += values[44] * B[(i*20)+3];
C[(i*20)+9] += values[45] * B[(i*20)+3];
C[(i*20)+10] += values[46] * B[(i*20)+3];
C[(i*20)+11] += values[47] * B[(i*20)+3];
C[(i*20)+12] += values[48] * B[(i*20)+3];
C[(i*20)+13] += values[49] * B[(i*20)+3];
C[(i*20)+14] += values[50] * B[(i*20)+3];
C[(i*20)+15] += values[51] * B[(i*20)+3];
C[(i*20)+16] += values[52] * B[(i*20)+3];
C[(i*20)+17] += values[53] * B[(i*20)+3];
C[(i*20)+18] += values[54] * B[(i*20)+3];
C[(i*20)+19] += values[55] * B[(i*20)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*20)+4]);
#endif
__m128d c4_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a4_0 = _mm_loadu_pd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
#endif
_mm_storeu_pd(&C[(i*20)+10], c4_0);
__m128d c4_2 = _mm_load_sd(&C[(i*20)+14]);
__m128d a4_2 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, b4));
#endif
_mm_store_sd(&C[(i*20)+14], c4_2);
#else
C[(i*20)+10] += values[56] * B[(i*20)+4];
C[(i*20)+11] += values[57] * B[(i*20)+4];
C[(i*20)+14] += values[58] * B[(i*20)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*20)+5]);
#endif
__m128d c5_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a5_0 = _mm_loadu_pd(&values[59]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
#endif
_mm_storeu_pd(&C[(i*20)+10], c5_0);
__m128d c5_2 = _mm_load_sd(&C[(i*20)+12]);
__m128d a5_2 = _mm_load_sd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*20)+12], c5_2);
__m128d c5_3 = _mm_load_sd(&C[(i*20)+15]);
__m128d a5_3 = _mm_load_sd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, b5));
#endif
_mm_store_sd(&C[(i*20)+15], c5_3);
#else
C[(i*20)+10] += values[59] * B[(i*20)+5];
C[(i*20)+11] += values[60] * B[(i*20)+5];
C[(i*20)+12] += values[61] * B[(i*20)+5];
C[(i*20)+15] += values[62] * B[(i*20)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*20)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*20)+6]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_0 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a6_0 = _mm256_loadu_pd(&values[63]);
c6_0 = _mm256_add_pd(c6_0, _mm256_mul_pd(a6_0, b6));
_mm256_storeu_pd(&C[(i*20)+10], c6_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a6_0 = _mm_loadu_pd(&values[63]);
c6_0 = _mm_add_pd(c6_0, _mm_mul_pd(a6_0, b6));
_mm_storeu_pd(&C[(i*20)+10], c6_0);
__m128d c6_2 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a6_2 = _mm_loadu_pd(&values[65]);
c6_2 = _mm_add_pd(c6_2, _mm_mul_pd(a6_2, b6));
_mm_storeu_pd(&C[(i*20)+12], c6_2);
#endif
__m128d c6_4 = _mm_load_sd(&C[(i*20)+16]);
__m128d a6_4 = _mm_load_sd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, b6));
#endif
_mm_store_sd(&C[(i*20)+16], c6_4);
#else
C[(i*20)+10] += values[63] * B[(i*20)+6];
C[(i*20)+11] += values[64] * B[(i*20)+6];
C[(i*20)+12] += values[65] * B[(i*20)+6];
C[(i*20)+13] += values[66] * B[(i*20)+6];
C[(i*20)+16] += values[67] * B[(i*20)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*20)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a7_0 = _mm_loadu_pd(&values[68]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*20)+10], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*20)+12]);
__m128d a7_2 = _mm_load_sd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*20)+12], c7_2);
__m128d c7_3 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a7_3 = _mm_loadu_pd(&values[71]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, b7));
#endif
_mm_storeu_pd(&C[(i*20)+14], c7_3);
__m128d c7_5 = _mm_load_sd(&C[(i*20)+17]);
__m128d a7_5 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, b7));
#endif
_mm_store_sd(&C[(i*20)+17], c7_5);
#else
C[(i*20)+10] += values[68] * B[(i*20)+7];
C[(i*20)+11] += values[69] * B[(i*20)+7];
C[(i*20)+12] += values[70] * B[(i*20)+7];
C[(i*20)+14] += values[71] * B[(i*20)+7];
C[(i*20)+15] += values[72] * B[(i*20)+7];
C[(i*20)+17] += values[73] * B[(i*20)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*20)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*20)+8]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_0 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a8_0 = _mm256_loadu_pd(&values[74]);
c8_0 = _mm256_add_pd(c8_0, _mm256_mul_pd(a8_0, b8));
_mm256_storeu_pd(&C[(i*20)+10], c8_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a8_0 = _mm_loadu_pd(&values[74]);
c8_0 = _mm_add_pd(c8_0, _mm_mul_pd(a8_0, b8));
_mm_storeu_pd(&C[(i*20)+10], c8_0);
__m128d c8_2 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a8_2 = _mm_loadu_pd(&values[76]);
c8_2 = _mm_add_pd(c8_2, _mm_mul_pd(a8_2, b8));
_mm_storeu_pd(&C[(i*20)+12], c8_2);
#endif
__m128d c8_4 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a8_4 = _mm_loadu_pd(&values[78]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, b8));
#endif
_mm_storeu_pd(&C[(i*20)+14], c8_4);
__m128d c8_6 = _mm_load_sd(&C[(i*20)+16]);
__m128d a8_6 = _mm_load_sd(&values[80]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*20)+16], c8_6);
__m128d c8_7 = _mm_load_sd(&C[(i*20)+18]);
__m128d a8_7 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, b8));
#endif
_mm_store_sd(&C[(i*20)+18], c8_7);
#else
C[(i*20)+10] += values[74] * B[(i*20)+8];
C[(i*20)+11] += values[75] * B[(i*20)+8];
C[(i*20)+12] += values[76] * B[(i*20)+8];
C[(i*20)+13] += values[77] * B[(i*20)+8];
C[(i*20)+14] += values[78] * B[(i*20)+8];
C[(i*20)+15] += values[79] * B[(i*20)+8];
C[(i*20)+16] += values[80] * B[(i*20)+8];
C[(i*20)+18] += values[81] * B[(i*20)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*20)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*20)+9]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_0 = _mm256_loadu_pd(&C[(i*20)+10]);
__m256d a9_0 = _mm256_loadu_pd(&values[82]);
c9_0 = _mm256_add_pd(c9_0, _mm256_mul_pd(a9_0, b9));
_mm256_storeu_pd(&C[(i*20)+10], c9_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_0 = _mm_loadu_pd(&C[(i*20)+10]);
__m128d a9_0 = _mm_loadu_pd(&values[82]);
c9_0 = _mm_add_pd(c9_0, _mm_mul_pd(a9_0, b9));
_mm_storeu_pd(&C[(i*20)+10], c9_0);
__m128d c9_2 = _mm_loadu_pd(&C[(i*20)+12]);
__m128d a9_2 = _mm_loadu_pd(&values[84]);
c9_2 = _mm_add_pd(c9_2, _mm_mul_pd(a9_2, b9));
_mm_storeu_pd(&C[(i*20)+12], c9_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_4 = _mm256_loadu_pd(&C[(i*20)+14]);
__m256d a9_4 = _mm256_loadu_pd(&values[86]);
c9_4 = _mm256_add_pd(c9_4, _mm256_mul_pd(a9_4, b9));
_mm256_storeu_pd(&C[(i*20)+14], c9_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_4 = _mm_loadu_pd(&C[(i*20)+14]);
__m128d a9_4 = _mm_loadu_pd(&values[86]);
c9_4 = _mm_add_pd(c9_4, _mm_mul_pd(a9_4, b9));
_mm_storeu_pd(&C[(i*20)+14], c9_4);
__m128d c9_6 = _mm_loadu_pd(&C[(i*20)+16]);
__m128d a9_6 = _mm_loadu_pd(&values[88]);
c9_6 = _mm_add_pd(c9_6, _mm_mul_pd(a9_6, b9));
_mm_storeu_pd(&C[(i*20)+16], c9_6);
#endif
__m128d c9_8 = _mm_loadu_pd(&C[(i*20)+18]);
__m128d a9_8 = _mm_loadu_pd(&values[90]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_8 = _mm_add_pd(c9_8, _mm_mul_pd(a9_8, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_8 = _mm_add_pd(c9_8, _mm_mul_pd(a9_8, b9));
#endif
_mm_storeu_pd(&C[(i*20)+18], c9_8);
#else
C[(i*20)+10] += values[82] * B[(i*20)+9];
C[(i*20)+11] += values[83] * B[(i*20)+9];
C[(i*20)+12] += values[84] * B[(i*20)+9];
C[(i*20)+13] += values[85] * B[(i*20)+9];
C[(i*20)+14] += values[86] * B[(i*20)+9];
C[(i*20)+15] += values[87] * B[(i*20)+9];
C[(i*20)+16] += values[88] * B[(i*20)+9];
C[(i*20)+17] += values[89] * B[(i*20)+9];
C[(i*20)+18] += values[90] * B[(i*20)+9];
C[(i*20)+19] += values[91] * B[(i*20)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 1656;
#endif

}

inline void generatedMatrixMultiplication_kXiDivMT_9_20(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*20)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*20)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*20)+0], c1_0);
#else
C[(i*20)+0] += values[0] * B[(i*20)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*20)+4]);
#endif
__m128d c4_0 = _mm_load_sd(&C[(i*20)+1]);
__m128d a4_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, b4));
#endif
_mm_store_sd(&C[(i*20)+1], c4_0);
#else
C[(i*20)+1] += values[1] * B[(i*20)+4];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*20)+5]);
#endif
__m128d c5_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a5_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, b5));
#endif
_mm_store_sd(&C[(i*20)+0], c5_0);
__m128d c5_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a5_1 = _mm_loadu_pd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, b5));
#endif
_mm_storeu_pd(&C[(i*20)+2], c5_1);
#else
C[(i*20)+0] += values[2] * B[(i*20)+5];
C[(i*20)+2] += values[3] * B[(i*20)+5];
C[(i*20)+3] += values[4] * B[(i*20)+5];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*20)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a7_0 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*20)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a7_1 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*20)+3], c7_1);
#else
C[(i*20)+0] += values[5] * B[(i*20)+7];
C[(i*20)+3] += values[6] * B[(i*20)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*20)+10]);
#endif
__m128d c10_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a10_0 = _mm_load_sd(&values[7]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, b10));
#endif
_mm_store_sd(&C[(i*20)+0], c10_0);
__m128d c10_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a10_1 = _mm_loadu_pd(&values[8]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_1 = _mm_add_pd(c10_1, _mm_mul_pd(a10_1, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_1 = _mm_add_pd(c10_1, _mm_mul_pd(a10_1, b10));
#endif
_mm_storeu_pd(&C[(i*20)+2], c10_1);
__m128d c10_3 = _mm_load_sd(&C[(i*20)+4]);
__m128d a10_3 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_3 = _mm_add_sd(c10_3, _mm_mul_sd(a10_3, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_3 = _mm_add_sd(c10_3, _mm_mul_sd(a10_3, b10));
#endif
_mm_store_sd(&C[(i*20)+4], c10_3);
__m128d c10_4 = _mm_load_sd(&C[(i*20)+6]);
__m128d a10_4 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, b10));
#endif
_mm_store_sd(&C[(i*20)+6], c10_4);
__m128d c10_5 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a10_5 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, b10));
#endif
_mm_storeu_pd(&C[(i*20)+8], c10_5);
#else
C[(i*20)+0] += values[7] * B[(i*20)+10];
C[(i*20)+2] += values[8] * B[(i*20)+10];
C[(i*20)+3] += values[9] * B[(i*20)+10];
C[(i*20)+4] += values[10] * B[(i*20)+10];
C[(i*20)+6] += values[11] * B[(i*20)+10];
C[(i*20)+8] += values[12] * B[(i*20)+10];
C[(i*20)+9] += values[13] * B[(i*20)+10];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*20)+11]);
#endif
__m128d c11_0 = _mm_load_sd(&C[(i*20)+1]);
__m128d a11_0 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, b11));
#endif
_mm_store_sd(&C[(i*20)+1], c11_0);
__m128d c11_1 = _mm_load_sd(&C[(i*20)+5]);
__m128d a11_1 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, b11));
#endif
_mm_store_sd(&C[(i*20)+5], c11_1);
__m128d c11_2 = _mm_load_sd(&C[(i*20)+7]);
__m128d a11_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*20)+7], c11_2);
#else
C[(i*20)+1] += values[14] * B[(i*20)+11];
C[(i*20)+5] += values[15] * B[(i*20)+11];
C[(i*20)+7] += values[16] * B[(i*20)+11];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*20)+12]);
#endif
__m128d c12_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a12_0 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, b12));
#endif
_mm_store_sd(&C[(i*20)+0], c12_0);
__m128d c12_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a12_1 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_1 = _mm_add_pd(c12_1, _mm_mul_pd(a12_1, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_1 = _mm_add_pd(c12_1, _mm_mul_pd(a12_1, b12));
#endif
_mm_storeu_pd(&C[(i*20)+2], c12_1);
__m128d c12_3 = _mm_load_sd(&C[(i*20)+6]);
__m128d a12_3 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, b12));
#endif
_mm_store_sd(&C[(i*20)+6], c12_3);
__m128d c12_4 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a12_4 = _mm_loadu_pd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
#endif
_mm_storeu_pd(&C[(i*20)+8], c12_4);
#else
C[(i*20)+0] += values[17] * B[(i*20)+12];
C[(i*20)+2] += values[18] * B[(i*20)+12];
C[(i*20)+3] += values[19] * B[(i*20)+12];
C[(i*20)+6] += values[20] * B[(i*20)+12];
C[(i*20)+8] += values[21] * B[(i*20)+12];
C[(i*20)+9] += values[22] * B[(i*20)+12];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*20)+14]);
#endif
__m128d c14_0 = _mm_load_sd(&C[(i*20)+1]);
__m128d a14_0 = _mm_load_sd(&values[23]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, b14));
#endif
_mm_store_sd(&C[(i*20)+1], c14_0);
__m128d c14_1 = _mm_load_sd(&C[(i*20)+7]);
__m128d a14_1 = _mm_load_sd(&values[24]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, b14));
#endif
_mm_store_sd(&C[(i*20)+7], c14_1);
#else
C[(i*20)+1] += values[23] * B[(i*20)+14];
C[(i*20)+7] += values[24] * B[(i*20)+14];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*20)+15]);
#endif
__m128d c15_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a15_0 = _mm_load_sd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, b15));
#endif
_mm_store_sd(&C[(i*20)+0], c15_0);
__m128d c15_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a15_1 = _mm_loadu_pd(&values[26]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_1 = _mm_add_pd(c15_1, _mm_mul_pd(a15_1, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_1 = _mm_add_pd(c15_1, _mm_mul_pd(a15_1, b15));
#endif
_mm_storeu_pd(&C[(i*20)+2], c15_1);
__m128d c15_3 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a15_3 = _mm_loadu_pd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_3 = _mm_add_pd(c15_3, _mm_mul_pd(a15_3, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_3 = _mm_add_pd(c15_3, _mm_mul_pd(a15_3, b15));
#endif
_mm_storeu_pd(&C[(i*20)+8], c15_3);
#else
C[(i*20)+0] += values[25] * B[(i*20)+15];
C[(i*20)+2] += values[26] * B[(i*20)+15];
C[(i*20)+3] += values[27] * B[(i*20)+15];
C[(i*20)+8] += values[28] * B[(i*20)+15];
C[(i*20)+9] += values[29] * B[(i*20)+15];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*20)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a17_0 = _mm_load_sd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*20)+0], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a17_1 = _mm_load_sd(&values[31]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*20)+3], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*20)+9]);
__m128d a17_2 = _mm_load_sd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*20)+9], c17_2);
#else
C[(i*20)+0] += values[30] * B[(i*20)+17];
C[(i*20)+3] += values[31] * B[(i*20)+17];
C[(i*20)+9] += values[32] * B[(i*20)+17];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivMT_9_20(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*20)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*20)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*20)+0], c1_0);
#else
C[(i*20)+0] += values[0] * B[(i*20)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*20)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*20)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*20)+0], c2_0);
#else
C[(i*20)+0] += values[1] * B[(i*20)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[2]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*20)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[2]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*20)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[4]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*20)+2], c4_2);
#endif
#else
C[(i*20)+0] += values[2] * B[(i*20)+4];
C[(i*20)+1] += values[3] * B[(i*20)+4];
C[(i*20)+2] += values[4] * B[(i*20)+4];
C[(i*20)+3] += values[5] * B[(i*20)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[6]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*20)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[6]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*20)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[8]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*20)+2], c5_2);
#endif
#else
C[(i*20)+0] += values[6] * B[(i*20)+5];
C[(i*20)+1] += values[7] * B[(i*20)+5];
C[(i*20)+2] += values[8] * B[(i*20)+5];
C[(i*20)+3] += values[9] * B[(i*20)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*20)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*20)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a6_0 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*20)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*20)+2], c6_1);
#else
C[(i*20)+0] += values[10] * B[(i*20)+6];
C[(i*20)+2] += values[11] * B[(i*20)+6];
C[(i*20)+3] += values[12] * B[(i*20)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*20)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a7_0 = _mm_load_sd(&values[13]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*20)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a7_1 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*20)+3], c7_1);
#else
C[(i*20)+0] += values[13] * B[(i*20)+7];
C[(i*20)+3] += values[14] * B[(i*20)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*20)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*20)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a8_0 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*20)+0], c8_0);
__m128d c8_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a8_1 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, b8));
#endif
_mm_store_sd(&C[(i*20)+3], c8_1);
#else
C[(i*20)+0] += values[15] * B[(i*20)+8];
C[(i*20)+3] += values[16] * B[(i*20)+8];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*20)+10]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a10_0 = _mm256_loadu_pd(&values[17]);
c10_0 = _mm256_add_pd(c10_0, _mm256_mul_pd(a10_0, b10));
_mm256_storeu_pd(&C[(i*20)+0], c10_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a10_0 = _mm_loadu_pd(&values[17]);
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
_mm_storeu_pd(&C[(i*20)+0], c10_0);
__m128d c10_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a10_2 = _mm_loadu_pd(&values[19]);
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, b10));
_mm_storeu_pd(&C[(i*20)+2], c10_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_4 = _mm256_loadu_pd(&C[(i*20)+4]);
__m256d a10_4 = _mm256_loadu_pd(&values[21]);
c10_4 = _mm256_add_pd(c10_4, _mm256_mul_pd(a10_4, b10));
_mm256_storeu_pd(&C[(i*20)+4], c10_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_4 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a10_4 = _mm_loadu_pd(&values[21]);
c10_4 = _mm_add_pd(c10_4, _mm_mul_pd(a10_4, b10));
_mm_storeu_pd(&C[(i*20)+4], c10_4);
__m128d c10_6 = _mm_loadu_pd(&C[(i*20)+6]);
__m128d a10_6 = _mm_loadu_pd(&values[23]);
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, b10));
_mm_storeu_pd(&C[(i*20)+6], c10_6);
#endif
__m128d c10_8 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a10_8 = _mm_loadu_pd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, b10));
#endif
_mm_storeu_pd(&C[(i*20)+8], c10_8);
#else
C[(i*20)+0] += values[17] * B[(i*20)+10];
C[(i*20)+1] += values[18] * B[(i*20)+10];
C[(i*20)+2] += values[19] * B[(i*20)+10];
C[(i*20)+3] += values[20] * B[(i*20)+10];
C[(i*20)+4] += values[21] * B[(i*20)+10];
C[(i*20)+5] += values[22] * B[(i*20)+10];
C[(i*20)+6] += values[23] * B[(i*20)+10];
C[(i*20)+7] += values[24] * B[(i*20)+10];
C[(i*20)+8] += values[25] * B[(i*20)+10];
C[(i*20)+9] += values[26] * B[(i*20)+10];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*20)+11]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a11_0 = _mm256_loadu_pd(&values[27]);
c11_0 = _mm256_add_pd(c11_0, _mm256_mul_pd(a11_0, b11));
_mm256_storeu_pd(&C[(i*20)+0], c11_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a11_0 = _mm_loadu_pd(&values[27]);
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
_mm_storeu_pd(&C[(i*20)+0], c11_0);
__m128d c11_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a11_2 = _mm_loadu_pd(&values[29]);
c11_2 = _mm_add_pd(c11_2, _mm_mul_pd(a11_2, b11));
_mm_storeu_pd(&C[(i*20)+2], c11_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_4 = _mm256_loadu_pd(&C[(i*20)+4]);
__m256d a11_4 = _mm256_loadu_pd(&values[31]);
c11_4 = _mm256_add_pd(c11_4, _mm256_mul_pd(a11_4, b11));
_mm256_storeu_pd(&C[(i*20)+4], c11_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_4 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a11_4 = _mm_loadu_pd(&values[31]);
c11_4 = _mm_add_pd(c11_4, _mm_mul_pd(a11_4, b11));
_mm_storeu_pd(&C[(i*20)+4], c11_4);
__m128d c11_6 = _mm_loadu_pd(&C[(i*20)+6]);
__m128d a11_6 = _mm_loadu_pd(&values[33]);
c11_6 = _mm_add_pd(c11_6, _mm_mul_pd(a11_6, b11));
_mm_storeu_pd(&C[(i*20)+6], c11_6);
#endif
__m128d c11_8 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a11_8 = _mm_loadu_pd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, b11));
#endif
_mm_storeu_pd(&C[(i*20)+8], c11_8);
#else
C[(i*20)+0] += values[27] * B[(i*20)+11];
C[(i*20)+1] += values[28] * B[(i*20)+11];
C[(i*20)+2] += values[29] * B[(i*20)+11];
C[(i*20)+3] += values[30] * B[(i*20)+11];
C[(i*20)+4] += values[31] * B[(i*20)+11];
C[(i*20)+5] += values[32] * B[(i*20)+11];
C[(i*20)+6] += values[33] * B[(i*20)+11];
C[(i*20)+7] += values[34] * B[(i*20)+11];
C[(i*20)+8] += values[35] * B[(i*20)+11];
C[(i*20)+9] += values[36] * B[(i*20)+11];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*20)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a12_0 = _mm256_loadu_pd(&values[37]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*20)+0], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a12_0 = _mm_loadu_pd(&values[37]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*20)+0], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a12_2 = _mm_loadu_pd(&values[39]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*20)+2], c12_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_4 = _mm256_loadu_pd(&C[(i*20)+5]);
__m256d a12_4 = _mm256_loadu_pd(&values[41]);
c12_4 = _mm256_add_pd(c12_4, _mm256_mul_pd(a12_4, b12));
_mm256_storeu_pd(&C[(i*20)+5], c12_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_4 = _mm_loadu_pd(&C[(i*20)+5]);
__m128d a12_4 = _mm_loadu_pd(&values[41]);
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
_mm_storeu_pd(&C[(i*20)+5], c12_4);
__m128d c12_6 = _mm_loadu_pd(&C[(i*20)+7]);
__m128d a12_6 = _mm_loadu_pd(&values[43]);
c12_6 = _mm_add_pd(c12_6, _mm_mul_pd(a12_6, b12));
_mm_storeu_pd(&C[(i*20)+7], c12_6);
#endif
__m128d c12_8 = _mm_load_sd(&C[(i*20)+9]);
__m128d a12_8 = _mm_load_sd(&values[45]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, b12));
#endif
_mm_store_sd(&C[(i*20)+9], c12_8);
#else
C[(i*20)+0] += values[37] * B[(i*20)+12];
C[(i*20)+1] += values[38] * B[(i*20)+12];
C[(i*20)+2] += values[39] * B[(i*20)+12];
C[(i*20)+3] += values[40] * B[(i*20)+12];
C[(i*20)+5] += values[41] * B[(i*20)+12];
C[(i*20)+6] += values[42] * B[(i*20)+12];
C[(i*20)+7] += values[43] * B[(i*20)+12];
C[(i*20)+8] += values[44] * B[(i*20)+12];
C[(i*20)+9] += values[45] * B[(i*20)+12];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*20)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*20)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a13_0 = _mm_load_sd(&values[46]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*20)+0], c13_0);
__m128d c13_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a13_1 = _mm_loadu_pd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, b13));
#endif
_mm_storeu_pd(&C[(i*20)+2], c13_1);
__m128d c13_3 = _mm_load_sd(&C[(i*20)+6]);
__m128d a13_3 = _mm_load_sd(&values[49]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, b13));
#endif
_mm_store_sd(&C[(i*20)+6], c13_3);
__m128d c13_4 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a13_4 = _mm_loadu_pd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, b13));
#endif
_mm_storeu_pd(&C[(i*20)+8], c13_4);
#else
C[(i*20)+0] += values[46] * B[(i*20)+13];
C[(i*20)+2] += values[47] * B[(i*20)+13];
C[(i*20)+3] += values[48] * B[(i*20)+13];
C[(i*20)+6] += values[49] * B[(i*20)+13];
C[(i*20)+8] += values[50] * B[(i*20)+13];
C[(i*20)+9] += values[51] * B[(i*20)+13];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*20)+14]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a14_0 = _mm256_loadu_pd(&values[52]);
c14_0 = _mm256_add_pd(c14_0, _mm256_mul_pd(a14_0, b14));
_mm256_storeu_pd(&C[(i*20)+0], c14_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a14_0 = _mm_loadu_pd(&values[52]);
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
_mm_storeu_pd(&C[(i*20)+0], c14_0);
__m128d c14_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a14_2 = _mm_loadu_pd(&values[54]);
c14_2 = _mm_add_pd(c14_2, _mm_mul_pd(a14_2, b14));
_mm_storeu_pd(&C[(i*20)+2], c14_2);
#endif
__m128d c14_4 = _mm_loadu_pd(&C[(i*20)+7]);
__m128d a14_4 = _mm_loadu_pd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_4 = _mm_add_pd(c14_4, _mm_mul_pd(a14_4, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_4 = _mm_add_pd(c14_4, _mm_mul_pd(a14_4, b14));
#endif
_mm_storeu_pd(&C[(i*20)+7], c14_4);
__m128d c14_6 = _mm_load_sd(&C[(i*20)+9]);
__m128d a14_6 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_6 = _mm_add_sd(c14_6, _mm_mul_sd(a14_6, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_6 = _mm_add_sd(c14_6, _mm_mul_sd(a14_6, b14));
#endif
_mm_store_sd(&C[(i*20)+9], c14_6);
#else
C[(i*20)+0] += values[52] * B[(i*20)+14];
C[(i*20)+1] += values[53] * B[(i*20)+14];
C[(i*20)+2] += values[54] * B[(i*20)+14];
C[(i*20)+3] += values[55] * B[(i*20)+14];
C[(i*20)+7] += values[56] * B[(i*20)+14];
C[(i*20)+8] += values[57] * B[(i*20)+14];
C[(i*20)+9] += values[58] * B[(i*20)+14];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*20)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a15_0 = _mm256_loadu_pd(&values[59]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*20)+0], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a15_0 = _mm_loadu_pd(&values[59]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*20)+0], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a15_2 = _mm_loadu_pd(&values[61]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*20)+2], c15_2);
#endif
__m128d c15_4 = _mm_loadu_pd(&C[(i*20)+7]);
__m128d a15_4 = _mm_loadu_pd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, b15));
#endif
_mm_storeu_pd(&C[(i*20)+7], c15_4);
__m128d c15_6 = _mm_load_sd(&C[(i*20)+9]);
__m128d a15_6 = _mm_load_sd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*20)+9], c15_6);
#else
C[(i*20)+0] += values[59] * B[(i*20)+15];
C[(i*20)+1] += values[60] * B[(i*20)+15];
C[(i*20)+2] += values[61] * B[(i*20)+15];
C[(i*20)+3] += values[62] * B[(i*20)+15];
C[(i*20)+7] += values[63] * B[(i*20)+15];
C[(i*20)+8] += values[64] * B[(i*20)+15];
C[(i*20)+9] += values[65] * B[(i*20)+15];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*20)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*20)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a16_0 = _mm_load_sd(&values[66]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*20)+0], c16_0);
__m128d c16_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a16_1 = _mm_loadu_pd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, b16));
#endif
_mm_storeu_pd(&C[(i*20)+2], c16_1);
__m128d c16_3 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a16_3 = _mm_loadu_pd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_pd(c16_3, _mm_mul_pd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_pd(c16_3, _mm_mul_pd(a16_3, b16));
#endif
_mm_storeu_pd(&C[(i*20)+8], c16_3);
#else
C[(i*20)+0] += values[66] * B[(i*20)+16];
C[(i*20)+2] += values[67] * B[(i*20)+16];
C[(i*20)+3] += values[68] * B[(i*20)+16];
C[(i*20)+8] += values[69] * B[(i*20)+16];
C[(i*20)+9] += values[70] * B[(i*20)+16];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*20)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a17_0 = _mm_load_sd(&values[71]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*20)+0], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a17_1 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*20)+3], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*20)+9]);
__m128d a17_2 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*20)+9], c17_2);
#else
C[(i*20)+0] += values[71] * B[(i*20)+17];
C[(i*20)+3] += values[72] * B[(i*20)+17];
C[(i*20)+9] += values[73] * B[(i*20)+17];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*20)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*20)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a18_0 = _mm_load_sd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*20)+0], c18_0);
__m128d c18_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a18_1 = _mm_load_sd(&values[75]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, b18));
#endif
_mm_store_sd(&C[(i*20)+3], c18_1);
__m128d c18_2 = _mm_load_sd(&C[(i*20)+9]);
__m128d a18_2 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, b18));
#endif
_mm_store_sd(&C[(i*20)+9], c18_2);
#else
C[(i*20)+0] += values[74] * B[(i*20)+18];
C[(i*20)+3] += values[75] * B[(i*20)+18];
C[(i*20)+9] += values[76] * B[(i*20)+18];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivMT_9_20(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*20)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*20)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*20)+0], c1_0);
#else
C[(i*20)+0] += values[0] * B[(i*20)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*20)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*20)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*20)+0], c2_0);
#else
C[(i*20)+0] += values[1] * B[(i*20)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*20)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*20)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a3_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*20)+0], c3_0);
#else
C[(i*20)+0] += values[2] * B[(i*20)+3];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*20)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[3]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*20)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[3]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*20)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[5]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*20)+2], c4_2);
#endif
#else
C[(i*20)+0] += values[3] * B[(i*20)+4];
C[(i*20)+1] += values[4] * B[(i*20)+4];
C[(i*20)+2] += values[5] * B[(i*20)+4];
C[(i*20)+3] += values[6] * B[(i*20)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*20)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[7]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*20)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[7]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*20)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[9]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*20)+2], c5_2);
#endif
#else
C[(i*20)+0] += values[7] * B[(i*20)+5];
C[(i*20)+1] += values[8] * B[(i*20)+5];
C[(i*20)+2] += values[9] * B[(i*20)+5];
C[(i*20)+3] += values[10] * B[(i*20)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*20)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*20)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a6_0 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*20)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*20)+2], c6_1);
#else
C[(i*20)+0] += values[11] * B[(i*20)+6];
C[(i*20)+2] += values[12] * B[(i*20)+6];
C[(i*20)+3] += values[13] * B[(i*20)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*20)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a7_0 = _mm_loadu_pd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*20)+0], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*20)+3]);
__m128d a7_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*20)+3], c7_2);
#else
C[(i*20)+0] += values[14] * B[(i*20)+7];
C[(i*20)+1] += values[15] * B[(i*20)+7];
C[(i*20)+3] += values[16] * B[(i*20)+7];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*20)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*20)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a8_0 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*20)+0], c8_0);
__m128d c8_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a8_1 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, b8));
#endif
_mm_storeu_pd(&C[(i*20)+2], c8_1);
#else
C[(i*20)+0] += values[17] * B[(i*20)+8];
C[(i*20)+2] += values[18] * B[(i*20)+8];
C[(i*20)+3] += values[19] * B[(i*20)+8];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*20)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*20)+9]);
#endif
__m128d c9_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a9_0 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, b9));
#endif
_mm_store_sd(&C[(i*20)+0], c9_0);
__m128d c9_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a9_1 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, b9));
#endif
_mm_store_sd(&C[(i*20)+3], c9_1);
#else
C[(i*20)+0] += values[20] * B[(i*20)+9];
C[(i*20)+3] += values[21] * B[(i*20)+9];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*20)+10]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a10_0 = _mm256_loadu_pd(&values[22]);
c10_0 = _mm256_add_pd(c10_0, _mm256_mul_pd(a10_0, b10));
_mm256_storeu_pd(&C[(i*20)+0], c10_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a10_0 = _mm_loadu_pd(&values[22]);
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
_mm_storeu_pd(&C[(i*20)+0], c10_0);
__m128d c10_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a10_2 = _mm_loadu_pd(&values[24]);
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, b10));
_mm_storeu_pd(&C[(i*20)+2], c10_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_4 = _mm256_loadu_pd(&C[(i*20)+4]);
__m256d a10_4 = _mm256_loadu_pd(&values[26]);
c10_4 = _mm256_add_pd(c10_4, _mm256_mul_pd(a10_4, b10));
_mm256_storeu_pd(&C[(i*20)+4], c10_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_4 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a10_4 = _mm_loadu_pd(&values[26]);
c10_4 = _mm_add_pd(c10_4, _mm_mul_pd(a10_4, b10));
_mm_storeu_pd(&C[(i*20)+4], c10_4);
__m128d c10_6 = _mm_loadu_pd(&C[(i*20)+6]);
__m128d a10_6 = _mm_loadu_pd(&values[28]);
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, b10));
_mm_storeu_pd(&C[(i*20)+6], c10_6);
#endif
__m128d c10_8 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a10_8 = _mm_loadu_pd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, b10));
#endif
_mm_storeu_pd(&C[(i*20)+8], c10_8);
#else
C[(i*20)+0] += values[22] * B[(i*20)+10];
C[(i*20)+1] += values[23] * B[(i*20)+10];
C[(i*20)+2] += values[24] * B[(i*20)+10];
C[(i*20)+3] += values[25] * B[(i*20)+10];
C[(i*20)+4] += values[26] * B[(i*20)+10];
C[(i*20)+5] += values[27] * B[(i*20)+10];
C[(i*20)+6] += values[28] * B[(i*20)+10];
C[(i*20)+7] += values[29] * B[(i*20)+10];
C[(i*20)+8] += values[30] * B[(i*20)+10];
C[(i*20)+9] += values[31] * B[(i*20)+10];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*20)+11]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a11_0 = _mm256_loadu_pd(&values[32]);
c11_0 = _mm256_add_pd(c11_0, _mm256_mul_pd(a11_0, b11));
_mm256_storeu_pd(&C[(i*20)+0], c11_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a11_0 = _mm_loadu_pd(&values[32]);
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
_mm_storeu_pd(&C[(i*20)+0], c11_0);
__m128d c11_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a11_2 = _mm_loadu_pd(&values[34]);
c11_2 = _mm_add_pd(c11_2, _mm_mul_pd(a11_2, b11));
_mm_storeu_pd(&C[(i*20)+2], c11_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_4 = _mm256_loadu_pd(&C[(i*20)+4]);
__m256d a11_4 = _mm256_loadu_pd(&values[36]);
c11_4 = _mm256_add_pd(c11_4, _mm256_mul_pd(a11_4, b11));
_mm256_storeu_pd(&C[(i*20)+4], c11_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_4 = _mm_loadu_pd(&C[(i*20)+4]);
__m128d a11_4 = _mm_loadu_pd(&values[36]);
c11_4 = _mm_add_pd(c11_4, _mm_mul_pd(a11_4, b11));
_mm_storeu_pd(&C[(i*20)+4], c11_4);
__m128d c11_6 = _mm_loadu_pd(&C[(i*20)+6]);
__m128d a11_6 = _mm_loadu_pd(&values[38]);
c11_6 = _mm_add_pd(c11_6, _mm_mul_pd(a11_6, b11));
_mm_storeu_pd(&C[(i*20)+6], c11_6);
#endif
__m128d c11_8 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a11_8 = _mm_loadu_pd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, b11));
#endif
_mm_storeu_pd(&C[(i*20)+8], c11_8);
#else
C[(i*20)+0] += values[32] * B[(i*20)+11];
C[(i*20)+1] += values[33] * B[(i*20)+11];
C[(i*20)+2] += values[34] * B[(i*20)+11];
C[(i*20)+3] += values[35] * B[(i*20)+11];
C[(i*20)+4] += values[36] * B[(i*20)+11];
C[(i*20)+5] += values[37] * B[(i*20)+11];
C[(i*20)+6] += values[38] * B[(i*20)+11];
C[(i*20)+7] += values[39] * B[(i*20)+11];
C[(i*20)+8] += values[40] * B[(i*20)+11];
C[(i*20)+9] += values[41] * B[(i*20)+11];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*20)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a12_0 = _mm256_loadu_pd(&values[42]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*20)+0], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a12_0 = _mm_loadu_pd(&values[42]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*20)+0], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a12_2 = _mm_loadu_pd(&values[44]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*20)+2], c12_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_4 = _mm256_loadu_pd(&C[(i*20)+5]);
__m256d a12_4 = _mm256_loadu_pd(&values[46]);
c12_4 = _mm256_add_pd(c12_4, _mm256_mul_pd(a12_4, b12));
_mm256_storeu_pd(&C[(i*20)+5], c12_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_4 = _mm_loadu_pd(&C[(i*20)+5]);
__m128d a12_4 = _mm_loadu_pd(&values[46]);
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
_mm_storeu_pd(&C[(i*20)+5], c12_4);
__m128d c12_6 = _mm_loadu_pd(&C[(i*20)+7]);
__m128d a12_6 = _mm_loadu_pd(&values[48]);
c12_6 = _mm_add_pd(c12_6, _mm_mul_pd(a12_6, b12));
_mm_storeu_pd(&C[(i*20)+7], c12_6);
#endif
__m128d c12_8 = _mm_load_sd(&C[(i*20)+9]);
__m128d a12_8 = _mm_load_sd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, b12));
#endif
_mm_store_sd(&C[(i*20)+9], c12_8);
#else
C[(i*20)+0] += values[42] * B[(i*20)+12];
C[(i*20)+1] += values[43] * B[(i*20)+12];
C[(i*20)+2] += values[44] * B[(i*20)+12];
C[(i*20)+3] += values[45] * B[(i*20)+12];
C[(i*20)+5] += values[46] * B[(i*20)+12];
C[(i*20)+6] += values[47] * B[(i*20)+12];
C[(i*20)+7] += values[48] * B[(i*20)+12];
C[(i*20)+8] += values[49] * B[(i*20)+12];
C[(i*20)+9] += values[50] * B[(i*20)+12];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*20)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*20)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a13_0 = _mm_load_sd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*20)+0], c13_0);
__m128d c13_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a13_1 = _mm_loadu_pd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, b13));
#endif
_mm_storeu_pd(&C[(i*20)+2], c13_1);
__m128d c13_3 = _mm_load_sd(&C[(i*20)+6]);
__m128d a13_3 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, b13));
#endif
_mm_store_sd(&C[(i*20)+6], c13_3);
__m128d c13_4 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a13_4 = _mm_loadu_pd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, b13));
#endif
_mm_storeu_pd(&C[(i*20)+8], c13_4);
#else
C[(i*20)+0] += values[51] * B[(i*20)+13];
C[(i*20)+2] += values[52] * B[(i*20)+13];
C[(i*20)+3] += values[53] * B[(i*20)+13];
C[(i*20)+6] += values[54] * B[(i*20)+13];
C[(i*20)+8] += values[55] * B[(i*20)+13];
C[(i*20)+9] += values[56] * B[(i*20)+13];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*20)+14]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a14_0 = _mm256_loadu_pd(&values[57]);
c14_0 = _mm256_add_pd(c14_0, _mm256_mul_pd(a14_0, b14));
_mm256_storeu_pd(&C[(i*20)+0], c14_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a14_0 = _mm_loadu_pd(&values[57]);
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
_mm_storeu_pd(&C[(i*20)+0], c14_0);
__m128d c14_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a14_2 = _mm_loadu_pd(&values[59]);
c14_2 = _mm_add_pd(c14_2, _mm_mul_pd(a14_2, b14));
_mm_storeu_pd(&C[(i*20)+2], c14_2);
#endif
__m128d c14_4 = _mm_load_sd(&C[(i*20)+4]);
__m128d a14_4 = _mm_load_sd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, b14));
#endif
_mm_store_sd(&C[(i*20)+4], c14_4);
__m128d c14_5 = _mm_loadu_pd(&C[(i*20)+7]);
__m128d a14_5 = _mm_loadu_pd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_5 = _mm_add_pd(c14_5, _mm_mul_pd(a14_5, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_5 = _mm_add_pd(c14_5, _mm_mul_pd(a14_5, b14));
#endif
_mm_storeu_pd(&C[(i*20)+7], c14_5);
__m128d c14_7 = _mm_load_sd(&C[(i*20)+9]);
__m128d a14_7 = _mm_load_sd(&values[64]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_7 = _mm_add_sd(c14_7, _mm_mul_sd(a14_7, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_7 = _mm_add_sd(c14_7, _mm_mul_sd(a14_7, b14));
#endif
_mm_store_sd(&C[(i*20)+9], c14_7);
#else
C[(i*20)+0] += values[57] * B[(i*20)+14];
C[(i*20)+1] += values[58] * B[(i*20)+14];
C[(i*20)+2] += values[59] * B[(i*20)+14];
C[(i*20)+3] += values[60] * B[(i*20)+14];
C[(i*20)+4] += values[61] * B[(i*20)+14];
C[(i*20)+7] += values[62] * B[(i*20)+14];
C[(i*20)+8] += values[63] * B[(i*20)+14];
C[(i*20)+9] += values[64] * B[(i*20)+14];
#endif
#ifndef NDEBUG
num_flops += 16;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*20)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*20)+0]);
__m256d a15_0 = _mm256_loadu_pd(&values[65]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*20)+0], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a15_0 = _mm_loadu_pd(&values[65]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*20)+0], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a15_2 = _mm_loadu_pd(&values[67]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*20)+2], c15_2);
#endif
__m128d c15_4 = _mm_load_sd(&C[(i*20)+5]);
__m128d a15_4 = _mm_load_sd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, b15));
#endif
_mm_store_sd(&C[(i*20)+5], c15_4);
__m128d c15_5 = _mm_loadu_pd(&C[(i*20)+7]);
__m128d a15_5 = _mm_loadu_pd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_5 = _mm_add_pd(c15_5, _mm_mul_pd(a15_5, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_5 = _mm_add_pd(c15_5, _mm_mul_pd(a15_5, b15));
#endif
_mm_storeu_pd(&C[(i*20)+7], c15_5);
__m128d c15_7 = _mm_load_sd(&C[(i*20)+9]);
__m128d a15_7 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, b15));
#endif
_mm_store_sd(&C[(i*20)+9], c15_7);
#else
C[(i*20)+0] += values[65] * B[(i*20)+15];
C[(i*20)+1] += values[66] * B[(i*20)+15];
C[(i*20)+2] += values[67] * B[(i*20)+15];
C[(i*20)+3] += values[68] * B[(i*20)+15];
C[(i*20)+5] += values[69] * B[(i*20)+15];
C[(i*20)+7] += values[70] * B[(i*20)+15];
C[(i*20)+8] += values[71] * B[(i*20)+15];
C[(i*20)+9] += values[72] * B[(i*20)+15];
#endif
#ifndef NDEBUG
num_flops += 16;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*20)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*20)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a16_0 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*20)+0], c16_0);
__m128d c16_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a16_1 = _mm_loadu_pd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, b16));
#endif
_mm_storeu_pd(&C[(i*20)+2], c16_1);
__m128d c16_3 = _mm_load_sd(&C[(i*20)+6]);
__m128d a16_3 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, b16));
#endif
_mm_store_sd(&C[(i*20)+6], c16_3);
__m128d c16_4 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a16_4 = _mm_loadu_pd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, b16));
#endif
_mm_storeu_pd(&C[(i*20)+8], c16_4);
#else
C[(i*20)+0] += values[73] * B[(i*20)+16];
C[(i*20)+2] += values[74] * B[(i*20)+16];
C[(i*20)+3] += values[75] * B[(i*20)+16];
C[(i*20)+6] += values[76] * B[(i*20)+16];
C[(i*20)+8] += values[77] * B[(i*20)+16];
C[(i*20)+9] += values[78] * B[(i*20)+16];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*20)+17]);
#endif
__m128d c17_0 = _mm_loadu_pd(&C[(i*20)+0]);
__m128d a17_0 = _mm_loadu_pd(&values[79]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, b17));
#endif
_mm_storeu_pd(&C[(i*20)+0], c17_0);
__m128d c17_2 = _mm_load_sd(&C[(i*20)+3]);
__m128d a17_2 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*20)+3], c17_2);
__m128d c17_3 = _mm_load_sd(&C[(i*20)+7]);
__m128d a17_3 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, b17));
#endif
_mm_store_sd(&C[(i*20)+7], c17_3);
__m128d c17_4 = _mm_load_sd(&C[(i*20)+9]);
__m128d a17_4 = _mm_load_sd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, b17));
#endif
_mm_store_sd(&C[(i*20)+9], c17_4);
#else
C[(i*20)+0] += values[79] * B[(i*20)+17];
C[(i*20)+1] += values[80] * B[(i*20)+17];
C[(i*20)+3] += values[81] * B[(i*20)+17];
C[(i*20)+7] += values[82] * B[(i*20)+17];
C[(i*20)+9] += values[83] * B[(i*20)+17];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*20)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*20)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a18_0 = _mm_load_sd(&values[84]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*20)+0], c18_0);
__m128d c18_1 = _mm_loadu_pd(&C[(i*20)+2]);
__m128d a18_1 = _mm_loadu_pd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_pd(c18_1, _mm_mul_pd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_pd(c18_1, _mm_mul_pd(a18_1, b18));
#endif
_mm_storeu_pd(&C[(i*20)+2], c18_1);
__m128d c18_3 = _mm_loadu_pd(&C[(i*20)+8]);
__m128d a18_3 = _mm_loadu_pd(&values[87]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_3 = _mm_add_pd(c18_3, _mm_mul_pd(a18_3, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_3 = _mm_add_pd(c18_3, _mm_mul_pd(a18_3, b18));
#endif
_mm_storeu_pd(&C[(i*20)+8], c18_3);
#else
C[(i*20)+0] += values[84] * B[(i*20)+18];
C[(i*20)+2] += values[85] * B[(i*20)+18];
C[(i*20)+3] += values[86] * B[(i*20)+18];
C[(i*20)+8] += values[87] * B[(i*20)+18];
C[(i*20)+9] += values[88] * B[(i*20)+18];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*20)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*20)+19]);
#endif
__m128d c19_0 = _mm_load_sd(&C[(i*20)+0]);
__m128d a19_0 = _mm_load_sd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, b19));
#endif
_mm_store_sd(&C[(i*20)+0], c19_0);
__m128d c19_1 = _mm_load_sd(&C[(i*20)+3]);
__m128d a19_1 = _mm_load_sd(&values[90]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, b19));
#endif
_mm_store_sd(&C[(i*20)+3], c19_1);
__m128d c19_2 = _mm_load_sd(&C[(i*20)+9]);
__m128d a19_2 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, b19));
#endif
_mm_store_sd(&C[(i*20)+9], c19_2);
#else
C[(i*20)+0] += values[89] * B[(i*20)+19];
C[(i*20)+3] += values[90] * B[(i*20)+19];
C[(i*20)+9] += values[91] * B[(i*20)+19];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kXiDivM_9_35(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 35; m++) {
    C[(i*35)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*35)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*35)+0]);
#endif
__m128d c0_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a0_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, b0));
#endif
_mm_store_sd(&C[(i*35)+1], c0_0);
__m128d c0_1 = _mm_load_sd(&C[(i*35)+5]);
__m128d a0_1 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, b0));
#endif
_mm_store_sd(&C[(i*35)+5], c0_1);
__m128d c0_2 = _mm_load_sd(&C[(i*35)+7]);
__m128d a0_2 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, b0));
#endif
_mm_store_sd(&C[(i*35)+7], c0_2);
__m128d c0_3 = _mm_load_sd(&C[(i*35)+10]);
__m128d a0_3 = _mm_load_sd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_3 = _mm_add_sd(c0_3, _mm_mul_sd(a0_3, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_3 = _mm_add_sd(c0_3, _mm_mul_sd(a0_3, b0));
#endif
_mm_store_sd(&C[(i*35)+10], c0_3);
__m128d c0_4 = _mm_load_sd(&C[(i*35)+12]);
__m128d a0_4 = _mm_load_sd(&values[4]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_4 = _mm_add_sd(c0_4, _mm_mul_sd(a0_4, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_4 = _mm_add_sd(c0_4, _mm_mul_sd(a0_4, b0));
#endif
_mm_store_sd(&C[(i*35)+12], c0_4);
__m128d c0_5 = _mm_load_sd(&C[(i*35)+15]);
__m128d a0_5 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_5 = _mm_add_sd(c0_5, _mm_mul_sd(a0_5, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_5 = _mm_add_sd(c0_5, _mm_mul_sd(a0_5, b0));
#endif
_mm_store_sd(&C[(i*35)+15], c0_5);
__m128d c0_6 = _mm_load_sd(&C[(i*35)+17]);
__m128d a0_6 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, b0));
#endif
_mm_store_sd(&C[(i*35)+17], c0_6);
__m128d c0_7 = _mm_load_sd(&C[(i*35)+21]);
__m128d a0_7 = _mm_load_sd(&values[7]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_7 = _mm_add_sd(c0_7, _mm_mul_sd(a0_7, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_7 = _mm_add_sd(c0_7, _mm_mul_sd(a0_7, b0));
#endif
_mm_store_sd(&C[(i*35)+21], c0_7);
__m128d c0_8 = _mm_load_sd(&C[(i*35)+23]);
__m128d a0_8 = _mm_load_sd(&values[8]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_8 = _mm_add_sd(c0_8, _mm_mul_sd(a0_8, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_8 = _mm_add_sd(c0_8, _mm_mul_sd(a0_8, b0));
#endif
_mm_store_sd(&C[(i*35)+23], c0_8);
__m128d c0_9 = _mm_load_sd(&C[(i*35)+25]);
__m128d a0_9 = _mm_load_sd(&values[9]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_9 = _mm_add_sd(c0_9, _mm_mul_sd(a0_9, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_9 = _mm_add_sd(c0_9, _mm_mul_sd(a0_9, b0));
#endif
_mm_store_sd(&C[(i*35)+25], c0_9);
__m128d c0_10 = _mm_load_sd(&C[(i*35)+27]);
__m128d a0_10 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_10 = _mm_add_sd(c0_10, _mm_mul_sd(a0_10, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_10 = _mm_add_sd(c0_10, _mm_mul_sd(a0_10, b0));
#endif
_mm_store_sd(&C[(i*35)+27], c0_10);
__m128d c0_11 = _mm_load_sd(&C[(i*35)+30]);
__m128d a0_11 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_11 = _mm_add_sd(c0_11, _mm_mul_sd(a0_11, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_11 = _mm_add_sd(c0_11, _mm_mul_sd(a0_11, b0));
#endif
_mm_store_sd(&C[(i*35)+30], c0_11);
__m128d c0_12 = _mm_load_sd(&C[(i*35)+32]);
__m128d a0_12 = _mm_load_sd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_12 = _mm_add_sd(c0_12, _mm_mul_sd(a0_12, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_12 = _mm_add_sd(c0_12, _mm_mul_sd(a0_12, b0));
#endif
_mm_store_sd(&C[(i*35)+32], c0_12);
#else
C[(i*35)+1] += values[0] * B[(i*35)+0];
C[(i*35)+5] += values[1] * B[(i*35)+0];
C[(i*35)+7] += values[2] * B[(i*35)+0];
C[(i*35)+10] += values[3] * B[(i*35)+0];
C[(i*35)+12] += values[4] * B[(i*35)+0];
C[(i*35)+15] += values[5] * B[(i*35)+0];
C[(i*35)+17] += values[6] * B[(i*35)+0];
C[(i*35)+21] += values[7] * B[(i*35)+0];
C[(i*35)+23] += values[8] * B[(i*35)+0];
C[(i*35)+25] += values[9] * B[(i*35)+0];
C[(i*35)+27] += values[10] * B[(i*35)+0];
C[(i*35)+30] += values[11] * B[(i*35)+0];
C[(i*35)+32] += values[12] * B[(i*35)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*35)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*35)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*35)+4]);
__m128d a1_0 = _mm_load_sd(&values[13]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*35)+4], c1_0);
__m128d c1_1 = _mm_load_sd(&C[(i*35)+11]);
__m128d a1_1 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_1 = _mm_add_sd(c1_1, _mm_mul_sd(a1_1, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_1 = _mm_add_sd(c1_1, _mm_mul_sd(a1_1, b1));
#endif
_mm_store_sd(&C[(i*35)+11], c1_1);
__m128d c1_2 = _mm_load_sd(&C[(i*35)+14]);
__m128d a1_2 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, b1));
#endif
_mm_store_sd(&C[(i*35)+14], c1_2);
__m128d c1_3 = _mm_load_sd(&C[(i*35)+20]);
__m128d a1_3 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_3 = _mm_add_sd(c1_3, _mm_mul_sd(a1_3, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_3 = _mm_add_sd(c1_3, _mm_mul_sd(a1_3, b1));
#endif
_mm_store_sd(&C[(i*35)+20], c1_3);
__m128d c1_4 = _mm_load_sd(&C[(i*35)+22]);
__m128d a1_4 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, b1));
#endif
_mm_store_sd(&C[(i*35)+22], c1_4);
__m128d c1_5 = _mm_load_sd(&C[(i*35)+26]);
__m128d a1_5 = _mm_load_sd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, b1));
#endif
_mm_store_sd(&C[(i*35)+26], c1_5);
__m128d c1_6 = _mm_load_sd(&C[(i*35)+29]);
__m128d a1_6 = _mm_load_sd(&values[19]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_6 = _mm_add_sd(c1_6, _mm_mul_sd(a1_6, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_6 = _mm_add_sd(c1_6, _mm_mul_sd(a1_6, b1));
#endif
_mm_store_sd(&C[(i*35)+29], c1_6);
#else
C[(i*35)+4] += values[13] * B[(i*35)+1];
C[(i*35)+11] += values[14] * B[(i*35)+1];
C[(i*35)+14] += values[15] * B[(i*35)+1];
C[(i*35)+20] += values[16] * B[(i*35)+1];
C[(i*35)+22] += values[17] * B[(i*35)+1];
C[(i*35)+26] += values[18] * B[(i*35)+1];
C[(i*35)+29] += values[19] * B[(i*35)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*35)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*35)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*35)+5]);
__m128d a2_0 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*35)+5], c2_0);
__m128d c2_1 = _mm_load_sd(&C[(i*35)+10]);
__m128d a2_1 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_1 = _mm_add_sd(c2_1, _mm_mul_sd(a2_1, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_1 = _mm_add_sd(c2_1, _mm_mul_sd(a2_1, b2));
#endif
_mm_store_sd(&C[(i*35)+10], c2_1);
__m128d c2_2 = _mm_load_sd(&C[(i*35)+12]);
__m128d a2_2 = _mm_load_sd(&values[22]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*35)+12], c2_2);
__m128d c2_3 = _mm_load_sd(&C[(i*35)+15]);
__m128d a2_3 = _mm_load_sd(&values[23]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, b2));
#endif
_mm_store_sd(&C[(i*35)+15], c2_3);
__m128d c2_4 = _mm_load_sd(&C[(i*35)+21]);
__m128d a2_4 = _mm_load_sd(&values[24]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_4 = _mm_add_sd(c2_4, _mm_mul_sd(a2_4, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_4 = _mm_add_sd(c2_4, _mm_mul_sd(a2_4, b2));
#endif
_mm_store_sd(&C[(i*35)+21], c2_4);
__m128d c2_5 = _mm_load_sd(&C[(i*35)+23]);
__m128d a2_5 = _mm_load_sd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_5 = _mm_add_sd(c2_5, _mm_mul_sd(a2_5, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_5 = _mm_add_sd(c2_5, _mm_mul_sd(a2_5, b2));
#endif
_mm_store_sd(&C[(i*35)+23], c2_5);
__m128d c2_6 = _mm_load_sd(&C[(i*35)+25]);
__m128d a2_6 = _mm_load_sd(&values[26]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_6 = _mm_add_sd(c2_6, _mm_mul_sd(a2_6, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_6 = _mm_add_sd(c2_6, _mm_mul_sd(a2_6, b2));
#endif
_mm_store_sd(&C[(i*35)+25], c2_6);
__m128d c2_7 = _mm_load_sd(&C[(i*35)+27]);
__m128d a2_7 = _mm_load_sd(&values[27]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_7 = _mm_add_sd(c2_7, _mm_mul_sd(a2_7, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_7 = _mm_add_sd(c2_7, _mm_mul_sd(a2_7, b2));
#endif
_mm_store_sd(&C[(i*35)+27], c2_7);
__m128d c2_8 = _mm_load_sd(&C[(i*35)+30]);
__m128d a2_8 = _mm_load_sd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_8 = _mm_add_sd(c2_8, _mm_mul_sd(a2_8, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_8 = _mm_add_sd(c2_8, _mm_mul_sd(a2_8, b2));
#endif
_mm_store_sd(&C[(i*35)+30], c2_8);
#else
C[(i*35)+5] += values[20] * B[(i*35)+2];
C[(i*35)+10] += values[21] * B[(i*35)+2];
C[(i*35)+12] += values[22] * B[(i*35)+2];
C[(i*35)+15] += values[23] * B[(i*35)+2];
C[(i*35)+21] += values[24] * B[(i*35)+2];
C[(i*35)+23] += values[25] * B[(i*35)+2];
C[(i*35)+25] += values[26] * B[(i*35)+2];
C[(i*35)+27] += values[27] * B[(i*35)+2];
C[(i*35)+30] += values[28] * B[(i*35)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*35)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*35)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*35)+5]);
__m128d a3_0 = _mm_load_sd(&values[29]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*35)+5], c3_0);
__m128d c3_1 = _mm_load_sd(&C[(i*35)+7]);
__m128d a3_1 = _mm_load_sd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, b3));
#endif
_mm_store_sd(&C[(i*35)+7], c3_1);
__m128d c3_2 = _mm_load_sd(&C[(i*35)+10]);
__m128d a3_2 = _mm_load_sd(&values[31]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_2 = _mm_add_sd(c3_2, _mm_mul_sd(a3_2, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_2 = _mm_add_sd(c3_2, _mm_mul_sd(a3_2, b3));
#endif
_mm_store_sd(&C[(i*35)+10], c3_2);
__m128d c3_3 = _mm_load_sd(&C[(i*35)+12]);
__m128d a3_3 = _mm_load_sd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_3 = _mm_add_sd(c3_3, _mm_mul_sd(a3_3, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_3 = _mm_add_sd(c3_3, _mm_mul_sd(a3_3, b3));
#endif
_mm_store_sd(&C[(i*35)+12], c3_3);
__m128d c3_4 = _mm_load_sd(&C[(i*35)+15]);
__m128d a3_4 = _mm_load_sd(&values[33]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, b3));
#endif
_mm_store_sd(&C[(i*35)+15], c3_4);
__m128d c3_5 = _mm_load_sd(&C[(i*35)+17]);
__m128d a3_5 = _mm_load_sd(&values[34]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_5 = _mm_add_sd(c3_5, _mm_mul_sd(a3_5, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_5 = _mm_add_sd(c3_5, _mm_mul_sd(a3_5, b3));
#endif
_mm_store_sd(&C[(i*35)+17], c3_5);
__m128d c3_6 = _mm_load_sd(&C[(i*35)+21]);
__m128d a3_6 = _mm_load_sd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_6 = _mm_add_sd(c3_6, _mm_mul_sd(a3_6, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_6 = _mm_add_sd(c3_6, _mm_mul_sd(a3_6, b3));
#endif
_mm_store_sd(&C[(i*35)+21], c3_6);
__m128d c3_7 = _mm_load_sd(&C[(i*35)+23]);
__m128d a3_7 = _mm_load_sd(&values[36]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_7 = _mm_add_sd(c3_7, _mm_mul_sd(a3_7, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_7 = _mm_add_sd(c3_7, _mm_mul_sd(a3_7, b3));
#endif
_mm_store_sd(&C[(i*35)+23], c3_7);
__m128d c3_8 = _mm_load_sd(&C[(i*35)+25]);
__m128d a3_8 = _mm_load_sd(&values[37]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_8 = _mm_add_sd(c3_8, _mm_mul_sd(a3_8, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_8 = _mm_add_sd(c3_8, _mm_mul_sd(a3_8, b3));
#endif
_mm_store_sd(&C[(i*35)+25], c3_8);
__m128d c3_9 = _mm_load_sd(&C[(i*35)+27]);
__m128d a3_9 = _mm_load_sd(&values[38]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_9 = _mm_add_sd(c3_9, _mm_mul_sd(a3_9, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_9 = _mm_add_sd(c3_9, _mm_mul_sd(a3_9, b3));
#endif
_mm_store_sd(&C[(i*35)+27], c3_9);
__m128d c3_10 = _mm_load_sd(&C[(i*35)+30]);
__m128d a3_10 = _mm_load_sd(&values[39]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_10 = _mm_add_sd(c3_10, _mm_mul_sd(a3_10, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_10 = _mm_add_sd(c3_10, _mm_mul_sd(a3_10, b3));
#endif
_mm_store_sd(&C[(i*35)+30], c3_10);
__m128d c3_11 = _mm_load_sd(&C[(i*35)+32]);
__m128d a3_11 = _mm_load_sd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_11 = _mm_add_sd(c3_11, _mm_mul_sd(a3_11, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_11 = _mm_add_sd(c3_11, _mm_mul_sd(a3_11, b3));
#endif
_mm_store_sd(&C[(i*35)+32], c3_11);
#else
C[(i*35)+5] += values[29] * B[(i*35)+3];
C[(i*35)+7] += values[30] * B[(i*35)+3];
C[(i*35)+10] += values[31] * B[(i*35)+3];
C[(i*35)+12] += values[32] * B[(i*35)+3];
C[(i*35)+15] += values[33] * B[(i*35)+3];
C[(i*35)+17] += values[34] * B[(i*35)+3];
C[(i*35)+21] += values[35] * B[(i*35)+3];
C[(i*35)+23] += values[36] * B[(i*35)+3];
C[(i*35)+25] += values[37] * B[(i*35)+3];
C[(i*35)+27] += values[38] * B[(i*35)+3];
C[(i*35)+30] += values[39] * B[(i*35)+3];
C[(i*35)+32] += values[40] * B[(i*35)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*35)+4]);
#endif
__m128d c4_0 = _mm_load_sd(&C[(i*35)+10]);
__m128d a4_0 = _mm_load_sd(&values[41]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, b4));
#endif
_mm_store_sd(&C[(i*35)+10], c4_0);
__m128d c4_1 = _mm_load_sd(&C[(i*35)+21]);
__m128d a4_1 = _mm_load_sd(&values[42]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_1 = _mm_add_sd(c4_1, _mm_mul_sd(a4_1, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_1 = _mm_add_sd(c4_1, _mm_mul_sd(a4_1, b4));
#endif
_mm_store_sd(&C[(i*35)+21], c4_1);
__m128d c4_2 = _mm_load_sd(&C[(i*35)+25]);
__m128d a4_2 = _mm_load_sd(&values[43]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, b4));
#endif
_mm_store_sd(&C[(i*35)+25], c4_2);
#else
C[(i*35)+10] += values[41] * B[(i*35)+4];
C[(i*35)+21] += values[42] * B[(i*35)+4];
C[(i*35)+25] += values[43] * B[(i*35)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*35)+5]);
#endif
__m128d c5_0 = _mm_load_sd(&C[(i*35)+11]);
__m128d a5_0 = _mm_load_sd(&values[44]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, b5));
#endif
_mm_store_sd(&C[(i*35)+11], c5_0);
__m128d c5_1 = _mm_load_sd(&C[(i*35)+20]);
__m128d a5_1 = _mm_load_sd(&values[45]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_1 = _mm_add_sd(c5_1, _mm_mul_sd(a5_1, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_1 = _mm_add_sd(c5_1, _mm_mul_sd(a5_1, b5));
#endif
_mm_store_sd(&C[(i*35)+20], c5_1);
__m128d c5_2 = _mm_load_sd(&C[(i*35)+22]);
__m128d a5_2 = _mm_load_sd(&values[46]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*35)+22], c5_2);
__m128d c5_3 = _mm_load_sd(&C[(i*35)+26]);
__m128d a5_3 = _mm_load_sd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, b5));
#endif
_mm_store_sd(&C[(i*35)+26], c5_3);
#else
C[(i*35)+11] += values[44] * B[(i*35)+5];
C[(i*35)+20] += values[45] * B[(i*35)+5];
C[(i*35)+22] += values[46] * B[(i*35)+5];
C[(i*35)+26] += values[47] * B[(i*35)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*35)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*35)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*35)+10]);
__m128d a6_0 = _mm_load_sd(&values[48]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*35)+10], c6_0);
__m128d c6_1 = _mm_load_sd(&C[(i*35)+12]);
__m128d a6_1 = _mm_load_sd(&values[49]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_sd(c6_1, _mm_mul_sd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_sd(c6_1, _mm_mul_sd(a6_1, b6));
#endif
_mm_store_sd(&C[(i*35)+12], c6_1);
__m128d c6_2 = _mm_load_sd(&C[(i*35)+21]);
__m128d a6_2 = _mm_load_sd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_2 = _mm_add_sd(c6_2, _mm_mul_sd(a6_2, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_2 = _mm_add_sd(c6_2, _mm_mul_sd(a6_2, b6));
#endif
_mm_store_sd(&C[(i*35)+21], c6_2);
__m128d c6_3 = _mm_load_sd(&C[(i*35)+23]);
__m128d a6_3 = _mm_load_sd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_3 = _mm_add_sd(c6_3, _mm_mul_sd(a6_3, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_3 = _mm_add_sd(c6_3, _mm_mul_sd(a6_3, b6));
#endif
_mm_store_sd(&C[(i*35)+23], c6_3);
__m128d c6_4 = _mm_load_sd(&C[(i*35)+25]);
__m128d a6_4 = _mm_load_sd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, b6));
#endif
_mm_store_sd(&C[(i*35)+25], c6_4);
__m128d c6_5 = _mm_load_sd(&C[(i*35)+27]);
__m128d a6_5 = _mm_load_sd(&values[53]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_5 = _mm_add_sd(c6_5, _mm_mul_sd(a6_5, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_5 = _mm_add_sd(c6_5, _mm_mul_sd(a6_5, b6));
#endif
_mm_store_sd(&C[(i*35)+27], c6_5);
#else
C[(i*35)+10] += values[48] * B[(i*35)+6];
C[(i*35)+12] += values[49] * B[(i*35)+6];
C[(i*35)+21] += values[50] * B[(i*35)+6];
C[(i*35)+23] += values[51] * B[(i*35)+6];
C[(i*35)+25] += values[52] * B[(i*35)+6];
C[(i*35)+27] += values[53] * B[(i*35)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*35)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*35)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*35)+11]);
__m128d a7_0 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*35)+11], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*35)+14]);
__m128d a7_1 = _mm_load_sd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*35)+14], c7_1);
__m128d c7_2 = _mm_load_sd(&C[(i*35)+20]);
__m128d a7_2 = _mm_load_sd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*35)+20], c7_2);
__m128d c7_3 = _mm_load_sd(&C[(i*35)+22]);
__m128d a7_3 = _mm_load_sd(&values[57]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_sd(c7_3, _mm_mul_sd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_sd(c7_3, _mm_mul_sd(a7_3, b7));
#endif
_mm_store_sd(&C[(i*35)+22], c7_3);
__m128d c7_4 = _mm_load_sd(&C[(i*35)+26]);
__m128d a7_4 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_4 = _mm_add_sd(c7_4, _mm_mul_sd(a7_4, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_4 = _mm_add_sd(c7_4, _mm_mul_sd(a7_4, b7));
#endif
_mm_store_sd(&C[(i*35)+26], c7_4);
__m128d c7_5 = _mm_load_sd(&C[(i*35)+29]);
__m128d a7_5 = _mm_load_sd(&values[59]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, b7));
#endif
_mm_store_sd(&C[(i*35)+29], c7_5);
#else
C[(i*35)+11] += values[54] * B[(i*35)+7];
C[(i*35)+14] += values[55] * B[(i*35)+7];
C[(i*35)+20] += values[56] * B[(i*35)+7];
C[(i*35)+22] += values[57] * B[(i*35)+7];
C[(i*35)+26] += values[58] * B[(i*35)+7];
C[(i*35)+29] += values[59] * B[(i*35)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*35)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*35)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*35)+10]);
__m128d a8_0 = _mm_load_sd(&values[60]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*35)+10], c8_0);
__m128d c8_1 = _mm_load_sd(&C[(i*35)+12]);
__m128d a8_1 = _mm_load_sd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, b8));
#endif
_mm_store_sd(&C[(i*35)+12], c8_1);
__m128d c8_2 = _mm_load_sd(&C[(i*35)+15]);
__m128d a8_2 = _mm_load_sd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_2 = _mm_add_sd(c8_2, _mm_mul_sd(a8_2, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_2 = _mm_add_sd(c8_2, _mm_mul_sd(a8_2, b8));
#endif
_mm_store_sd(&C[(i*35)+15], c8_2);
__m128d c8_3 = _mm_load_sd(&C[(i*35)+21]);
__m128d a8_3 = _mm_load_sd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_3 = _mm_add_sd(c8_3, _mm_mul_sd(a8_3, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_3 = _mm_add_sd(c8_3, _mm_mul_sd(a8_3, b8));
#endif
_mm_store_sd(&C[(i*35)+21], c8_3);
__m128d c8_4 = _mm_load_sd(&C[(i*35)+23]);
__m128d a8_4 = _mm_load_sd(&values[64]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_sd(c8_4, _mm_mul_sd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_sd(c8_4, _mm_mul_sd(a8_4, b8));
#endif
_mm_store_sd(&C[(i*35)+23], c8_4);
__m128d c8_5 = _mm_load_sd(&C[(i*35)+25]);
__m128d a8_5 = _mm_load_sd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_5 = _mm_add_sd(c8_5, _mm_mul_sd(a8_5, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_5 = _mm_add_sd(c8_5, _mm_mul_sd(a8_5, b8));
#endif
_mm_store_sd(&C[(i*35)+25], c8_5);
__m128d c8_6 = _mm_load_sd(&C[(i*35)+27]);
__m128d a8_6 = _mm_load_sd(&values[66]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*35)+27], c8_6);
__m128d c8_7 = _mm_load_sd(&C[(i*35)+30]);
__m128d a8_7 = _mm_load_sd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, b8));
#endif
_mm_store_sd(&C[(i*35)+30], c8_7);
#else
C[(i*35)+10] += values[60] * B[(i*35)+8];
C[(i*35)+12] += values[61] * B[(i*35)+8];
C[(i*35)+15] += values[62] * B[(i*35)+8];
C[(i*35)+21] += values[63] * B[(i*35)+8];
C[(i*35)+23] += values[64] * B[(i*35)+8];
C[(i*35)+25] += values[65] * B[(i*35)+8];
C[(i*35)+27] += values[66] * B[(i*35)+8];
C[(i*35)+30] += values[67] * B[(i*35)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*35)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*35)+9]);
#endif
__m128d c9_0 = _mm_load_sd(&C[(i*35)+10]);
__m128d a9_0 = _mm_load_sd(&values[68]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, b9));
#endif
_mm_store_sd(&C[(i*35)+10], c9_0);
__m128d c9_1 = _mm_load_sd(&C[(i*35)+12]);
__m128d a9_1 = _mm_load_sd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, b9));
#endif
_mm_store_sd(&C[(i*35)+12], c9_1);
__m128d c9_2 = _mm_load_sd(&C[(i*35)+15]);
__m128d a9_2 = _mm_load_sd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_2 = _mm_add_sd(c9_2, _mm_mul_sd(a9_2, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_2 = _mm_add_sd(c9_2, _mm_mul_sd(a9_2, b9));
#endif
_mm_store_sd(&C[(i*35)+15], c9_2);
__m128d c9_3 = _mm_load_sd(&C[(i*35)+17]);
__m128d a9_3 = _mm_load_sd(&values[71]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_3 = _mm_add_sd(c9_3, _mm_mul_sd(a9_3, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_3 = _mm_add_sd(c9_3, _mm_mul_sd(a9_3, b9));
#endif
_mm_store_sd(&C[(i*35)+17], c9_3);
__m128d c9_4 = _mm_load_sd(&C[(i*35)+21]);
__m128d a9_4 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_4 = _mm_add_sd(c9_4, _mm_mul_sd(a9_4, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_4 = _mm_add_sd(c9_4, _mm_mul_sd(a9_4, b9));
#endif
_mm_store_sd(&C[(i*35)+21], c9_4);
__m128d c9_5 = _mm_load_sd(&C[(i*35)+23]);
__m128d a9_5 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_5 = _mm_add_sd(c9_5, _mm_mul_sd(a9_5, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_5 = _mm_add_sd(c9_5, _mm_mul_sd(a9_5, b9));
#endif
_mm_store_sd(&C[(i*35)+23], c9_5);
__m128d c9_6 = _mm_load_sd(&C[(i*35)+25]);
__m128d a9_6 = _mm_load_sd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_6 = _mm_add_sd(c9_6, _mm_mul_sd(a9_6, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_6 = _mm_add_sd(c9_6, _mm_mul_sd(a9_6, b9));
#endif
_mm_store_sd(&C[(i*35)+25], c9_6);
__m128d c9_7 = _mm_load_sd(&C[(i*35)+27]);
__m128d a9_7 = _mm_load_sd(&values[75]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_7 = _mm_add_sd(c9_7, _mm_mul_sd(a9_7, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_7 = _mm_add_sd(c9_7, _mm_mul_sd(a9_7, b9));
#endif
_mm_store_sd(&C[(i*35)+27], c9_7);
__m128d c9_8 = _mm_load_sd(&C[(i*35)+30]);
__m128d a9_8 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, b9));
#endif
_mm_store_sd(&C[(i*35)+30], c9_8);
__m128d c9_9 = _mm_load_sd(&C[(i*35)+32]);
__m128d a9_9 = _mm_load_sd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_9 = _mm_add_sd(c9_9, _mm_mul_sd(a9_9, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_9 = _mm_add_sd(c9_9, _mm_mul_sd(a9_9, b9));
#endif
_mm_store_sd(&C[(i*35)+32], c9_9);
#else
C[(i*35)+10] += values[68] * B[(i*35)+9];
C[(i*35)+12] += values[69] * B[(i*35)+9];
C[(i*35)+15] += values[70] * B[(i*35)+9];
C[(i*35)+17] += values[71] * B[(i*35)+9];
C[(i*35)+21] += values[72] * B[(i*35)+9];
C[(i*35)+23] += values[73] * B[(i*35)+9];
C[(i*35)+25] += values[74] * B[(i*35)+9];
C[(i*35)+27] += values[75] * B[(i*35)+9];
C[(i*35)+30] += values[76] * B[(i*35)+9];
C[(i*35)+32] += values[77] * B[(i*35)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*35)+10]);
#endif
__m128d c10_0 = _mm_load_sd(&C[(i*35)+20]);
__m128d a10_0 = _mm_load_sd(&values[78]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, b10));
#endif
_mm_store_sd(&C[(i*35)+20], c10_0);
#else
C[(i*35)+20] += values[78] * B[(i*35)+10];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*35)+11]);
#endif
__m128d c11_0 = _mm_load_sd(&C[(i*35)+21]);
__m128d a11_0 = _mm_load_sd(&values[79]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, b11));
#endif
_mm_store_sd(&C[(i*35)+21], c11_0);
#else
C[(i*35)+21] += values[79] * B[(i*35)+11];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*35)+12]);
#endif
__m128d c12_0 = _mm_load_sd(&C[(i*35)+20]);
__m128d a12_0 = _mm_load_sd(&values[80]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, b12));
#endif
_mm_store_sd(&C[(i*35)+20], c12_0);
__m128d c12_1 = _mm_load_sd(&C[(i*35)+22]);
__m128d a12_1 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_1 = _mm_add_sd(c12_1, _mm_mul_sd(a12_1, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_1 = _mm_add_sd(c12_1, _mm_mul_sd(a12_1, b12));
#endif
_mm_store_sd(&C[(i*35)+22], c12_1);
#else
C[(i*35)+20] += values[80] * B[(i*35)+12];
C[(i*35)+22] += values[81] * B[(i*35)+12];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*35)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*35)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*35)+21]);
__m128d a13_0 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*35)+21], c13_0);
__m128d c13_1 = _mm_load_sd(&C[(i*35)+23]);
__m128d a13_1 = _mm_load_sd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_sd(c13_1, _mm_mul_sd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_sd(c13_1, _mm_mul_sd(a13_1, b13));
#endif
_mm_store_sd(&C[(i*35)+23], c13_1);
#else
C[(i*35)+21] += values[82] * B[(i*35)+13];
C[(i*35)+23] += values[83] * B[(i*35)+13];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*35)+14]);
#endif
__m128d c14_0 = _mm_load_sd(&C[(i*35)+21]);
__m128d a14_0 = _mm_load_sd(&values[84]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, b14));
#endif
_mm_store_sd(&C[(i*35)+21], c14_0);
__m128d c14_1 = _mm_load_sd(&C[(i*35)+25]);
__m128d a14_1 = _mm_load_sd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, b14));
#endif
_mm_store_sd(&C[(i*35)+25], c14_1);
#else
C[(i*35)+21] += values[84] * B[(i*35)+14];
C[(i*35)+25] += values[85] * B[(i*35)+14];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*35)+15]);
#endif
__m128d c15_0 = _mm_load_sd(&C[(i*35)+20]);
__m128d a15_0 = _mm_load_sd(&values[86]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, b15));
#endif
_mm_store_sd(&C[(i*35)+20], c15_0);
__m128d c15_1 = _mm_load_sd(&C[(i*35)+22]);
__m128d a15_1 = _mm_load_sd(&values[87]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_1 = _mm_add_sd(c15_1, _mm_mul_sd(a15_1, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_1 = _mm_add_sd(c15_1, _mm_mul_sd(a15_1, b15));
#endif
_mm_store_sd(&C[(i*35)+22], c15_1);
__m128d c15_2 = _mm_load_sd(&C[(i*35)+26]);
__m128d a15_2 = _mm_load_sd(&values[88]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_2 = _mm_add_sd(c15_2, _mm_mul_sd(a15_2, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_2 = _mm_add_sd(c15_2, _mm_mul_sd(a15_2, b15));
#endif
_mm_store_sd(&C[(i*35)+26], c15_2);
#else
C[(i*35)+20] += values[86] * B[(i*35)+15];
C[(i*35)+22] += values[87] * B[(i*35)+15];
C[(i*35)+26] += values[88] * B[(i*35)+15];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*35)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*35)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*35)+21]);
__m128d a16_0 = _mm_load_sd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*35)+21], c16_0);
__m128d c16_1 = _mm_load_sd(&C[(i*35)+23]);
__m128d a16_1 = _mm_load_sd(&values[90]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_sd(c16_1, _mm_mul_sd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_sd(c16_1, _mm_mul_sd(a16_1, b16));
#endif
_mm_store_sd(&C[(i*35)+23], c16_1);
__m128d c16_2 = _mm_load_sd(&C[(i*35)+25]);
__m128d a16_2 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_2 = _mm_add_sd(c16_2, _mm_mul_sd(a16_2, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_2 = _mm_add_sd(c16_2, _mm_mul_sd(a16_2, b16));
#endif
_mm_store_sd(&C[(i*35)+25], c16_2);
__m128d c16_3 = _mm_load_sd(&C[(i*35)+27]);
__m128d a16_3 = _mm_load_sd(&values[92]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, b16));
#endif
_mm_store_sd(&C[(i*35)+27], c16_3);
#else
C[(i*35)+21] += values[89] * B[(i*35)+16];
C[(i*35)+23] += values[90] * B[(i*35)+16];
C[(i*35)+25] += values[91] * B[(i*35)+16];
C[(i*35)+27] += values[92] * B[(i*35)+16];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*35)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*35)+20]);
__m128d a17_0 = _mm_load_sd(&values[93]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*35)+20], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*35)+22]);
__m128d a17_1 = _mm_load_sd(&values[94]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*35)+22], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*35)+26]);
__m128d a17_2 = _mm_load_sd(&values[95]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*35)+26], c17_2);
__m128d c17_3 = _mm_load_sd(&C[(i*35)+29]);
__m128d a17_3 = _mm_load_sd(&values[96]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, b17));
#endif
_mm_store_sd(&C[(i*35)+29], c17_3);
#else
C[(i*35)+20] += values[93] * B[(i*35)+17];
C[(i*35)+22] += values[94] * B[(i*35)+17];
C[(i*35)+26] += values[95] * B[(i*35)+17];
C[(i*35)+29] += values[96] * B[(i*35)+17];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*35)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*35)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*35)+21]);
__m128d a18_0 = _mm_load_sd(&values[97]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*35)+21], c18_0);
__m128d c18_1 = _mm_load_sd(&C[(i*35)+23]);
__m128d a18_1 = _mm_load_sd(&values[98]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, b18));
#endif
_mm_store_sd(&C[(i*35)+23], c18_1);
__m128d c18_2 = _mm_load_sd(&C[(i*35)+25]);
__m128d a18_2 = _mm_load_sd(&values[99]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, b18));
#endif
_mm_store_sd(&C[(i*35)+25], c18_2);
__m128d c18_3 = _mm_load_sd(&C[(i*35)+27]);
__m128d a18_3 = _mm_load_sd(&values[100]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_3 = _mm_add_sd(c18_3, _mm_mul_sd(a18_3, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_3 = _mm_add_sd(c18_3, _mm_mul_sd(a18_3, b18));
#endif
_mm_store_sd(&C[(i*35)+27], c18_3);
__m128d c18_4 = _mm_load_sd(&C[(i*35)+30]);
__m128d a18_4 = _mm_load_sd(&values[101]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_4 = _mm_add_sd(c18_4, _mm_mul_sd(a18_4, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_4 = _mm_add_sd(c18_4, _mm_mul_sd(a18_4, b18));
#endif
_mm_store_sd(&C[(i*35)+30], c18_4);
#else
C[(i*35)+21] += values[97] * B[(i*35)+18];
C[(i*35)+23] += values[98] * B[(i*35)+18];
C[(i*35)+25] += values[99] * B[(i*35)+18];
C[(i*35)+27] += values[100] * B[(i*35)+18];
C[(i*35)+30] += values[101] * B[(i*35)+18];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*35)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*35)+19]);
#endif
__m128d c19_0 = _mm_load_sd(&C[(i*35)+21]);
__m128d a19_0 = _mm_load_sd(&values[102]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, b19));
#endif
_mm_store_sd(&C[(i*35)+21], c19_0);
__m128d c19_1 = _mm_load_sd(&C[(i*35)+23]);
__m128d a19_1 = _mm_load_sd(&values[103]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, b19));
#endif
_mm_store_sd(&C[(i*35)+23], c19_1);
__m128d c19_2 = _mm_load_sd(&C[(i*35)+25]);
__m128d a19_2 = _mm_load_sd(&values[104]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, b19));
#endif
_mm_store_sd(&C[(i*35)+25], c19_2);
__m128d c19_3 = _mm_load_sd(&C[(i*35)+27]);
__m128d a19_3 = _mm_load_sd(&values[105]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_3 = _mm_add_sd(c19_3, _mm_mul_sd(a19_3, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_3 = _mm_add_sd(c19_3, _mm_mul_sd(a19_3, b19));
#endif
_mm_store_sd(&C[(i*35)+27], c19_3);
__m128d c19_4 = _mm_load_sd(&C[(i*35)+30]);
__m128d a19_4 = _mm_load_sd(&values[106]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_4 = _mm_add_sd(c19_4, _mm_mul_sd(a19_4, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_4 = _mm_add_sd(c19_4, _mm_mul_sd(a19_4, b19));
#endif
_mm_store_sd(&C[(i*35)+30], c19_4);
__m128d c19_5 = _mm_load_sd(&C[(i*35)+32]);
__m128d a19_5 = _mm_load_sd(&values[107]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_5 = _mm_add_sd(c19_5, _mm_mul_sd(a19_5, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_5 = _mm_add_sd(c19_5, _mm_mul_sd(a19_5, b19));
#endif
_mm_store_sd(&C[(i*35)+32], c19_5);
#else
C[(i*35)+21] += values[102] * B[(i*35)+19];
C[(i*35)+23] += values[103] * B[(i*35)+19];
C[(i*35)+25] += values[104] * B[(i*35)+19];
C[(i*35)+27] += values[105] * B[(i*35)+19];
C[(i*35)+30] += values[106] * B[(i*35)+19];
C[(i*35)+32] += values[107] * B[(i*35)+19];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 1944;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivM_9_35(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 35; m++) {
    C[(i*35)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*35)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*35)+0]);
#endif
__m128d c0_0 = _mm_loadu_pd(&C[(i*35)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
#endif
_mm_storeu_pd(&C[(i*35)+1], c0_0);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_2 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a0_2 = _mm256_loadu_pd(&values[2]);
c0_2 = _mm256_add_pd(c0_2, _mm256_mul_pd(a0_2, b0));
_mm256_storeu_pd(&C[(i*35)+4], c0_2);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_2 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*35)+4], c0_2);
__m128d c0_4 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*35)+6], c0_4);
#endif
__m128d c0_6 = _mm_load_sd(&C[(i*35)+8]);
__m128d a0_6 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, b0));
#endif
_mm_store_sd(&C[(i*35)+8], c0_6);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_7 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a0_7 = _mm256_loadu_pd(&values[7]);
c0_7 = _mm256_add_pd(c0_7, _mm256_mul_pd(a0_7, b0));
_mm256_storeu_pd(&C[(i*35)+10], c0_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_7 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a0_7 = _mm_loadu_pd(&values[7]);
c0_7 = _mm_add_pd(c0_7, _mm_mul_pd(a0_7, b0));
_mm_storeu_pd(&C[(i*35)+10], c0_7);
__m128d c0_9 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a0_9 = _mm_loadu_pd(&values[9]);
c0_9 = _mm_add_pd(c0_9, _mm_mul_pd(a0_9, b0));
_mm_storeu_pd(&C[(i*35)+12], c0_9);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_11 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a0_11 = _mm256_loadu_pd(&values[11]);
c0_11 = _mm256_add_pd(c0_11, _mm256_mul_pd(a0_11, b0));
_mm256_storeu_pd(&C[(i*35)+14], c0_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_11 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a0_11 = _mm_loadu_pd(&values[11]);
c0_11 = _mm_add_pd(c0_11, _mm_mul_pd(a0_11, b0));
_mm_storeu_pd(&C[(i*35)+14], c0_11);
__m128d c0_13 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a0_13 = _mm_loadu_pd(&values[13]);
c0_13 = _mm_add_pd(c0_13, _mm_mul_pd(a0_13, b0));
_mm_storeu_pd(&C[(i*35)+16], c0_13);
#endif
__m128d c0_15 = _mm_load_sd(&C[(i*35)+18]);
__m128d a0_15 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, b0));
#endif
_mm_store_sd(&C[(i*35)+18], c0_15);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_16 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a0_16 = _mm256_loadu_pd(&values[16]);
c0_16 = _mm256_add_pd(c0_16, _mm256_mul_pd(a0_16, b0));
_mm256_storeu_pd(&C[(i*35)+20], c0_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_16 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a0_16 = _mm_loadu_pd(&values[16]);
c0_16 = _mm_add_pd(c0_16, _mm_mul_pd(a0_16, b0));
_mm_storeu_pd(&C[(i*35)+20], c0_16);
__m128d c0_18 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a0_18 = _mm_loadu_pd(&values[18]);
c0_18 = _mm_add_pd(c0_18, _mm_mul_pd(a0_18, b0));
_mm_storeu_pd(&C[(i*35)+22], c0_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_20 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a0_20 = _mm256_loadu_pd(&values[20]);
c0_20 = _mm256_add_pd(c0_20, _mm256_mul_pd(a0_20, b0));
_mm256_storeu_pd(&C[(i*35)+24], c0_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_20 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a0_20 = _mm_loadu_pd(&values[20]);
c0_20 = _mm_add_pd(c0_20, _mm_mul_pd(a0_20, b0));
_mm_storeu_pd(&C[(i*35)+24], c0_20);
__m128d c0_22 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a0_22 = _mm_loadu_pd(&values[22]);
c0_22 = _mm_add_pd(c0_22, _mm_mul_pd(a0_22, b0));
_mm_storeu_pd(&C[(i*35)+26], c0_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_24 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a0_24 = _mm256_loadu_pd(&values[24]);
c0_24 = _mm256_add_pd(c0_24, _mm256_mul_pd(a0_24, b0));
_mm256_storeu_pd(&C[(i*35)+28], c0_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_24 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a0_24 = _mm_loadu_pd(&values[24]);
c0_24 = _mm_add_pd(c0_24, _mm_mul_pd(a0_24, b0));
_mm_storeu_pd(&C[(i*35)+28], c0_24);
__m128d c0_26 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a0_26 = _mm_loadu_pd(&values[26]);
c0_26 = _mm_add_pd(c0_26, _mm_mul_pd(a0_26, b0));
_mm_storeu_pd(&C[(i*35)+30], c0_26);
#endif
__m128d c0_28 = _mm_loadu_pd(&C[(i*35)+32]);
__m128d a0_28 = _mm_loadu_pd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_28 = _mm_add_pd(c0_28, _mm_mul_pd(a0_28, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_28 = _mm_add_pd(c0_28, _mm_mul_pd(a0_28, b0));
#endif
_mm_storeu_pd(&C[(i*35)+32], c0_28);
#else
C[(i*35)+1] += values[0] * B[(i*35)+0];
C[(i*35)+2] += values[1] * B[(i*35)+0];
C[(i*35)+4] += values[2] * B[(i*35)+0];
C[(i*35)+5] += values[3] * B[(i*35)+0];
C[(i*35)+6] += values[4] * B[(i*35)+0];
C[(i*35)+7] += values[5] * B[(i*35)+0];
C[(i*35)+8] += values[6] * B[(i*35)+0];
C[(i*35)+10] += values[7] * B[(i*35)+0];
C[(i*35)+11] += values[8] * B[(i*35)+0];
C[(i*35)+12] += values[9] * B[(i*35)+0];
C[(i*35)+13] += values[10] * B[(i*35)+0];
C[(i*35)+14] += values[11] * B[(i*35)+0];
C[(i*35)+15] += values[12] * B[(i*35)+0];
C[(i*35)+16] += values[13] * B[(i*35)+0];
C[(i*35)+17] += values[14] * B[(i*35)+0];
C[(i*35)+18] += values[15] * B[(i*35)+0];
C[(i*35)+20] += values[16] * B[(i*35)+0];
C[(i*35)+21] += values[17] * B[(i*35)+0];
C[(i*35)+22] += values[18] * B[(i*35)+0];
C[(i*35)+23] += values[19] * B[(i*35)+0];
C[(i*35)+24] += values[20] * B[(i*35)+0];
C[(i*35)+25] += values[21] * B[(i*35)+0];
C[(i*35)+26] += values[22] * B[(i*35)+0];
C[(i*35)+27] += values[23] * B[(i*35)+0];
C[(i*35)+28] += values[24] * B[(i*35)+0];
C[(i*35)+29] += values[25] * B[(i*35)+0];
C[(i*35)+30] += values[26] * B[(i*35)+0];
C[(i*35)+31] += values[27] * B[(i*35)+0];
C[(i*35)+32] += values[28] * B[(i*35)+0];
C[(i*35)+33] += values[29] * B[(i*35)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*35)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*35)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*35)+4], c1_0);
__m128d c1_2 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a1_2 = _mm_loadu_pd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_pd(c1_2, _mm_mul_pd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_pd(c1_2, _mm_mul_pd(a1_2, b1));
#endif
_mm_storeu_pd(&C[(i*35)+10], c1_2);
__m128d c1_4 = _mm_load_sd(&C[(i*35)+12]);
__m128d a1_4 = _mm_load_sd(&values[34]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, b1));
#endif
_mm_store_sd(&C[(i*35)+12], c1_4);
__m128d c1_5 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a1_5 = _mm_loadu_pd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_pd(c1_5, _mm_mul_pd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_pd(c1_5, _mm_mul_pd(a1_5, b1));
#endif
_mm_storeu_pd(&C[(i*35)+14], c1_5);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_7 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a1_7 = _mm256_loadu_pd(&values[37]);
c1_7 = _mm256_add_pd(c1_7, _mm256_mul_pd(a1_7, b1));
_mm256_storeu_pd(&C[(i*35)+20], c1_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_7 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a1_7 = _mm_loadu_pd(&values[37]);
c1_7 = _mm_add_pd(c1_7, _mm_mul_pd(a1_7, b1));
_mm_storeu_pd(&C[(i*35)+20], c1_7);
__m128d c1_9 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a1_9 = _mm_loadu_pd(&values[39]);
c1_9 = _mm_add_pd(c1_9, _mm_mul_pd(a1_9, b1));
_mm_storeu_pd(&C[(i*35)+22], c1_9);
#endif
__m128d c1_11 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a1_11 = _mm_loadu_pd(&values[41]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_11 = _mm_add_pd(c1_11, _mm_mul_pd(a1_11, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_11 = _mm_add_pd(c1_11, _mm_mul_pd(a1_11, b1));
#endif
_mm_storeu_pd(&C[(i*35)+25], c1_11);
__m128d c1_13 = _mm_load_sd(&C[(i*35)+27]);
__m128d a1_13 = _mm_load_sd(&values[43]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_13 = _mm_add_sd(c1_13, _mm_mul_sd(a1_13, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_13 = _mm_add_sd(c1_13, _mm_mul_sd(a1_13, b1));
#endif
_mm_store_sd(&C[(i*35)+27], c1_13);
__m128d c1_14 = _mm_loadu_pd(&C[(i*35)+29]);
__m128d a1_14 = _mm_loadu_pd(&values[44]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_14 = _mm_add_pd(c1_14, _mm_mul_pd(a1_14, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_14 = _mm_add_pd(c1_14, _mm_mul_pd(a1_14, b1));
#endif
_mm_storeu_pd(&C[(i*35)+29], c1_14);
#else
C[(i*35)+4] += values[30] * B[(i*35)+1];
C[(i*35)+5] += values[31] * B[(i*35)+1];
C[(i*35)+10] += values[32] * B[(i*35)+1];
C[(i*35)+11] += values[33] * B[(i*35)+1];
C[(i*35)+12] += values[34] * B[(i*35)+1];
C[(i*35)+14] += values[35] * B[(i*35)+1];
C[(i*35)+15] += values[36] * B[(i*35)+1];
C[(i*35)+20] += values[37] * B[(i*35)+1];
C[(i*35)+21] += values[38] * B[(i*35)+1];
C[(i*35)+22] += values[39] * B[(i*35)+1];
C[(i*35)+23] += values[40] * B[(i*35)+1];
C[(i*35)+25] += values[41] * B[(i*35)+1];
C[(i*35)+26] += values[42] * B[(i*35)+1];
C[(i*35)+27] += values[43] * B[(i*35)+1];
C[(i*35)+29] += values[44] * B[(i*35)+1];
C[(i*35)+30] += values[45] * B[(i*35)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*35)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*35)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[46]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*35)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*35)+6]);
__m128d a2_2 = _mm_load_sd(&values[48]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*35)+6], c2_2);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_3 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a2_3 = _mm256_loadu_pd(&values[49]);
c2_3 = _mm256_add_pd(c2_3, _mm256_mul_pd(a2_3, b2));
_mm256_storeu_pd(&C[(i*35)+10], c2_3);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_3 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a2_3 = _mm_loadu_pd(&values[49]);
c2_3 = _mm_add_pd(c2_3, _mm_mul_pd(a2_3, b2));
_mm_storeu_pd(&C[(i*35)+10], c2_3);
__m128d c2_5 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a2_5 = _mm_loadu_pd(&values[51]);
c2_5 = _mm_add_pd(c2_5, _mm_mul_pd(a2_5, b2));
_mm_storeu_pd(&C[(i*35)+12], c2_5);
#endif
__m128d c2_7 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a2_7 = _mm_loadu_pd(&values[53]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_7 = _mm_add_pd(c2_7, _mm_mul_pd(a2_7, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_7 = _mm_add_pd(c2_7, _mm_mul_pd(a2_7, b2));
#endif
_mm_storeu_pd(&C[(i*35)+14], c2_7);
__m128d c2_9 = _mm_load_sd(&C[(i*35)+16]);
__m128d a2_9 = _mm_load_sd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, b2));
#endif
_mm_store_sd(&C[(i*35)+16], c2_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_10 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a2_10 = _mm256_loadu_pd(&values[56]);
c2_10 = _mm256_add_pd(c2_10, _mm256_mul_pd(a2_10, b2));
_mm256_storeu_pd(&C[(i*35)+20], c2_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_10 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a2_10 = _mm_loadu_pd(&values[56]);
c2_10 = _mm_add_pd(c2_10, _mm_mul_pd(a2_10, b2));
_mm_storeu_pd(&C[(i*35)+20], c2_10);
__m128d c2_12 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a2_12 = _mm_loadu_pd(&values[58]);
c2_12 = _mm_add_pd(c2_12, _mm_mul_pd(a2_12, b2));
_mm_storeu_pd(&C[(i*35)+22], c2_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_14 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a2_14 = _mm256_loadu_pd(&values[60]);
c2_14 = _mm256_add_pd(c2_14, _mm256_mul_pd(a2_14, b2));
_mm256_storeu_pd(&C[(i*35)+24], c2_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_14 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a2_14 = _mm_loadu_pd(&values[60]);
c2_14 = _mm_add_pd(c2_14, _mm_mul_pd(a2_14, b2));
_mm_storeu_pd(&C[(i*35)+24], c2_14);
__m128d c2_16 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a2_16 = _mm_loadu_pd(&values[62]);
c2_16 = _mm_add_pd(c2_16, _mm_mul_pd(a2_16, b2));
_mm_storeu_pd(&C[(i*35)+26], c2_16);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_18 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a2_18 = _mm256_loadu_pd(&values[64]);
c2_18 = _mm256_add_pd(c2_18, _mm256_mul_pd(a2_18, b2));
_mm256_storeu_pd(&C[(i*35)+28], c2_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_18 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a2_18 = _mm_loadu_pd(&values[64]);
c2_18 = _mm_add_pd(c2_18, _mm_mul_pd(a2_18, b2));
_mm_storeu_pd(&C[(i*35)+28], c2_18);
__m128d c2_20 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a2_20 = _mm_loadu_pd(&values[66]);
c2_20 = _mm_add_pd(c2_20, _mm_mul_pd(a2_20, b2));
_mm_storeu_pd(&C[(i*35)+30], c2_20);
#endif
#else
C[(i*35)+4] += values[46] * B[(i*35)+2];
C[(i*35)+5] += values[47] * B[(i*35)+2];
C[(i*35)+6] += values[48] * B[(i*35)+2];
C[(i*35)+10] += values[49] * B[(i*35)+2];
C[(i*35)+11] += values[50] * B[(i*35)+2];
C[(i*35)+12] += values[51] * B[(i*35)+2];
C[(i*35)+13] += values[52] * B[(i*35)+2];
C[(i*35)+14] += values[53] * B[(i*35)+2];
C[(i*35)+15] += values[54] * B[(i*35)+2];
C[(i*35)+16] += values[55] * B[(i*35)+2];
C[(i*35)+20] += values[56] * B[(i*35)+2];
C[(i*35)+21] += values[57] * B[(i*35)+2];
C[(i*35)+22] += values[58] * B[(i*35)+2];
C[(i*35)+23] += values[59] * B[(i*35)+2];
C[(i*35)+24] += values[60] * B[(i*35)+2];
C[(i*35)+25] += values[61] * B[(i*35)+2];
C[(i*35)+26] += values[62] * B[(i*35)+2];
C[(i*35)+27] += values[63] * B[(i*35)+2];
C[(i*35)+28] += values[64] * B[(i*35)+2];
C[(i*35)+29] += values[65] * B[(i*35)+2];
C[(i*35)+30] += values[66] * B[(i*35)+2];
C[(i*35)+31] += values[67] * B[(i*35)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*35)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*35)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[68]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*35)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[68]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*35)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[70]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*35)+6], c3_2);
#endif
__m128d c3_4 = _mm_load_sd(&C[(i*35)+8]);
__m128d a3_4 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, b3));
#endif
_mm_store_sd(&C[(i*35)+8], c3_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_5 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a3_5 = _mm256_loadu_pd(&values[73]);
c3_5 = _mm256_add_pd(c3_5, _mm256_mul_pd(a3_5, b3));
_mm256_storeu_pd(&C[(i*35)+10], c3_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_5 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a3_5 = _mm_loadu_pd(&values[73]);
c3_5 = _mm_add_pd(c3_5, _mm_mul_pd(a3_5, b3));
_mm_storeu_pd(&C[(i*35)+10], c3_5);
__m128d c3_7 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a3_7 = _mm_loadu_pd(&values[75]);
c3_7 = _mm_add_pd(c3_7, _mm_mul_pd(a3_7, b3));
_mm_storeu_pd(&C[(i*35)+12], c3_7);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_9 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a3_9 = _mm256_loadu_pd(&values[77]);
c3_9 = _mm256_add_pd(c3_9, _mm256_mul_pd(a3_9, b3));
_mm256_storeu_pd(&C[(i*35)+14], c3_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_9 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a3_9 = _mm_loadu_pd(&values[77]);
c3_9 = _mm_add_pd(c3_9, _mm_mul_pd(a3_9, b3));
_mm_storeu_pd(&C[(i*35)+14], c3_9);
__m128d c3_11 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a3_11 = _mm_loadu_pd(&values[79]);
c3_11 = _mm_add_pd(c3_11, _mm_mul_pd(a3_11, b3));
_mm_storeu_pd(&C[(i*35)+16], c3_11);
#endif
__m128d c3_13 = _mm_load_sd(&C[(i*35)+18]);
__m128d a3_13 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, b3));
#endif
_mm_store_sd(&C[(i*35)+18], c3_13);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_14 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a3_14 = _mm256_loadu_pd(&values[82]);
c3_14 = _mm256_add_pd(c3_14, _mm256_mul_pd(a3_14, b3));
_mm256_storeu_pd(&C[(i*35)+20], c3_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_14 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a3_14 = _mm_loadu_pd(&values[82]);
c3_14 = _mm_add_pd(c3_14, _mm_mul_pd(a3_14, b3));
_mm_storeu_pd(&C[(i*35)+20], c3_14);
__m128d c3_16 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a3_16 = _mm_loadu_pd(&values[84]);
c3_16 = _mm_add_pd(c3_16, _mm_mul_pd(a3_16, b3));
_mm_storeu_pd(&C[(i*35)+22], c3_16);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_18 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a3_18 = _mm256_loadu_pd(&values[86]);
c3_18 = _mm256_add_pd(c3_18, _mm256_mul_pd(a3_18, b3));
_mm256_storeu_pd(&C[(i*35)+24], c3_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_18 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a3_18 = _mm_loadu_pd(&values[86]);
c3_18 = _mm_add_pd(c3_18, _mm_mul_pd(a3_18, b3));
_mm_storeu_pd(&C[(i*35)+24], c3_18);
__m128d c3_20 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a3_20 = _mm_loadu_pd(&values[88]);
c3_20 = _mm_add_pd(c3_20, _mm_mul_pd(a3_20, b3));
_mm_storeu_pd(&C[(i*35)+26], c3_20);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_22 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a3_22 = _mm256_loadu_pd(&values[90]);
c3_22 = _mm256_add_pd(c3_22, _mm256_mul_pd(a3_22, b3));
_mm256_storeu_pd(&C[(i*35)+28], c3_22);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_22 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a3_22 = _mm_loadu_pd(&values[90]);
c3_22 = _mm_add_pd(c3_22, _mm_mul_pd(a3_22, b3));
_mm_storeu_pd(&C[(i*35)+28], c3_22);
__m128d c3_24 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a3_24 = _mm_loadu_pd(&values[92]);
c3_24 = _mm_add_pd(c3_24, _mm_mul_pd(a3_24, b3));
_mm_storeu_pd(&C[(i*35)+30], c3_24);
#endif
__m128d c3_26 = _mm_loadu_pd(&C[(i*35)+32]);
__m128d a3_26 = _mm_loadu_pd(&values[94]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_26 = _mm_add_pd(c3_26, _mm_mul_pd(a3_26, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_26 = _mm_add_pd(c3_26, _mm_mul_pd(a3_26, b3));
#endif
_mm_storeu_pd(&C[(i*35)+32], c3_26);
#else
C[(i*35)+4] += values[68] * B[(i*35)+3];
C[(i*35)+5] += values[69] * B[(i*35)+3];
C[(i*35)+6] += values[70] * B[(i*35)+3];
C[(i*35)+7] += values[71] * B[(i*35)+3];
C[(i*35)+8] += values[72] * B[(i*35)+3];
C[(i*35)+10] += values[73] * B[(i*35)+3];
C[(i*35)+11] += values[74] * B[(i*35)+3];
C[(i*35)+12] += values[75] * B[(i*35)+3];
C[(i*35)+13] += values[76] * B[(i*35)+3];
C[(i*35)+14] += values[77] * B[(i*35)+3];
C[(i*35)+15] += values[78] * B[(i*35)+3];
C[(i*35)+16] += values[79] * B[(i*35)+3];
C[(i*35)+17] += values[80] * B[(i*35)+3];
C[(i*35)+18] += values[81] * B[(i*35)+3];
C[(i*35)+20] += values[82] * B[(i*35)+3];
C[(i*35)+21] += values[83] * B[(i*35)+3];
C[(i*35)+22] += values[84] * B[(i*35)+3];
C[(i*35)+23] += values[85] * B[(i*35)+3];
C[(i*35)+24] += values[86] * B[(i*35)+3];
C[(i*35)+25] += values[87] * B[(i*35)+3];
C[(i*35)+26] += values[88] * B[(i*35)+3];
C[(i*35)+27] += values[89] * B[(i*35)+3];
C[(i*35)+28] += values[90] * B[(i*35)+3];
C[(i*35)+29] += values[91] * B[(i*35)+3];
C[(i*35)+30] += values[92] * B[(i*35)+3];
C[(i*35)+31] += values[93] * B[(i*35)+3];
C[(i*35)+32] += values[94] * B[(i*35)+3];
C[(i*35)+33] += values[95] * B[(i*35)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*35)+4]);
#endif
__m128d c4_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a4_0 = _mm_loadu_pd(&values[96]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
#endif
_mm_storeu_pd(&C[(i*35)+10], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a4_2 = _mm_loadu_pd(&values[98]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
#endif
_mm_storeu_pd(&C[(i*35)+20], c4_2);
__m128d c4_4 = _mm_load_sd(&C[(i*35)+22]);
__m128d a4_4 = _mm_load_sd(&values[100]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_4 = _mm_add_sd(c4_4, _mm_mul_sd(a4_4, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_4 = _mm_add_sd(c4_4, _mm_mul_sd(a4_4, b4));
#endif
_mm_store_sd(&C[(i*35)+22], c4_4);
__m128d c4_5 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a4_5 = _mm_loadu_pd(&values[101]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_5 = _mm_add_pd(c4_5, _mm_mul_pd(a4_5, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_5 = _mm_add_pd(c4_5, _mm_mul_pd(a4_5, b4));
#endif
_mm_storeu_pd(&C[(i*35)+25], c4_5);
#else
C[(i*35)+10] += values[96] * B[(i*35)+4];
C[(i*35)+11] += values[97] * B[(i*35)+4];
C[(i*35)+20] += values[98] * B[(i*35)+4];
C[(i*35)+21] += values[99] * B[(i*35)+4];
C[(i*35)+22] += values[100] * B[(i*35)+4];
C[(i*35)+25] += values[101] * B[(i*35)+4];
C[(i*35)+26] += values[102] * B[(i*35)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*35)+5]);
#endif
__m128d c5_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a5_0 = _mm_loadu_pd(&values[103]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
#endif
_mm_storeu_pd(&C[(i*35)+10], c5_0);
__m128d c5_2 = _mm_load_sd(&C[(i*35)+12]);
__m128d a5_2 = _mm_load_sd(&values[105]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*35)+12], c5_2);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_3 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a5_3 = _mm256_loadu_pd(&values[106]);
c5_3 = _mm256_add_pd(c5_3, _mm256_mul_pd(a5_3, b5));
_mm256_storeu_pd(&C[(i*35)+20], c5_3);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_3 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a5_3 = _mm_loadu_pd(&values[106]);
c5_3 = _mm_add_pd(c5_3, _mm_mul_pd(a5_3, b5));
_mm_storeu_pd(&C[(i*35)+20], c5_3);
__m128d c5_5 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a5_5 = _mm_loadu_pd(&values[108]);
c5_5 = _mm_add_pd(c5_5, _mm_mul_pd(a5_5, b5));
_mm_storeu_pd(&C[(i*35)+22], c5_5);
#endif
__m128d c5_7 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a5_7 = _mm_loadu_pd(&values[110]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_7 = _mm_add_pd(c5_7, _mm_mul_pd(a5_7, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_7 = _mm_add_pd(c5_7, _mm_mul_pd(a5_7, b5));
#endif
_mm_storeu_pd(&C[(i*35)+25], c5_7);
__m128d c5_9 = _mm_load_sd(&C[(i*35)+27]);
__m128d a5_9 = _mm_load_sd(&values[112]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_9 = _mm_add_sd(c5_9, _mm_mul_sd(a5_9, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_9 = _mm_add_sd(c5_9, _mm_mul_sd(a5_9, b5));
#endif
_mm_store_sd(&C[(i*35)+27], c5_9);
#else
C[(i*35)+10] += values[103] * B[(i*35)+5];
C[(i*35)+11] += values[104] * B[(i*35)+5];
C[(i*35)+12] += values[105] * B[(i*35)+5];
C[(i*35)+20] += values[106] * B[(i*35)+5];
C[(i*35)+21] += values[107] * B[(i*35)+5];
C[(i*35)+22] += values[108] * B[(i*35)+5];
C[(i*35)+23] += values[109] * B[(i*35)+5];
C[(i*35)+25] += values[110] * B[(i*35)+5];
C[(i*35)+26] += values[111] * B[(i*35)+5];
C[(i*35)+27] += values[112] * B[(i*35)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*35)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*35)+6]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_0 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a6_0 = _mm256_loadu_pd(&values[113]);
c6_0 = _mm256_add_pd(c6_0, _mm256_mul_pd(a6_0, b6));
_mm256_storeu_pd(&C[(i*35)+10], c6_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a6_0 = _mm_loadu_pd(&values[113]);
c6_0 = _mm_add_pd(c6_0, _mm_mul_pd(a6_0, b6));
_mm_storeu_pd(&C[(i*35)+10], c6_0);
__m128d c6_2 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a6_2 = _mm_loadu_pd(&values[115]);
c6_2 = _mm_add_pd(c6_2, _mm_mul_pd(a6_2, b6));
_mm_storeu_pd(&C[(i*35)+12], c6_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_4 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a6_4 = _mm256_loadu_pd(&values[117]);
c6_4 = _mm256_add_pd(c6_4, _mm256_mul_pd(a6_4, b6));
_mm256_storeu_pd(&C[(i*35)+20], c6_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_4 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a6_4 = _mm_loadu_pd(&values[117]);
c6_4 = _mm_add_pd(c6_4, _mm_mul_pd(a6_4, b6));
_mm_storeu_pd(&C[(i*35)+20], c6_4);
__m128d c6_6 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a6_6 = _mm_loadu_pd(&values[119]);
c6_6 = _mm_add_pd(c6_6, _mm_mul_pd(a6_6, b6));
_mm_storeu_pd(&C[(i*35)+22], c6_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_8 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a6_8 = _mm256_loadu_pd(&values[121]);
c6_8 = _mm256_add_pd(c6_8, _mm256_mul_pd(a6_8, b6));
_mm256_storeu_pd(&C[(i*35)+24], c6_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_8 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a6_8 = _mm_loadu_pd(&values[121]);
c6_8 = _mm_add_pd(c6_8, _mm_mul_pd(a6_8, b6));
_mm_storeu_pd(&C[(i*35)+24], c6_8);
__m128d c6_10 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a6_10 = _mm_loadu_pd(&values[123]);
c6_10 = _mm_add_pd(c6_10, _mm_mul_pd(a6_10, b6));
_mm_storeu_pd(&C[(i*35)+26], c6_10);
#endif
__m128d c6_12 = _mm_load_sd(&C[(i*35)+28]);
__m128d a6_12 = _mm_load_sd(&values[125]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_12 = _mm_add_sd(c6_12, _mm_mul_sd(a6_12, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_12 = _mm_add_sd(c6_12, _mm_mul_sd(a6_12, b6));
#endif
_mm_store_sd(&C[(i*35)+28], c6_12);
#else
C[(i*35)+10] += values[113] * B[(i*35)+6];
C[(i*35)+11] += values[114] * B[(i*35)+6];
C[(i*35)+12] += values[115] * B[(i*35)+6];
C[(i*35)+13] += values[116] * B[(i*35)+6];
C[(i*35)+20] += values[117] * B[(i*35)+6];
C[(i*35)+21] += values[118] * B[(i*35)+6];
C[(i*35)+22] += values[119] * B[(i*35)+6];
C[(i*35)+23] += values[120] * B[(i*35)+6];
C[(i*35)+24] += values[121] * B[(i*35)+6];
C[(i*35)+25] += values[122] * B[(i*35)+6];
C[(i*35)+26] += values[123] * B[(i*35)+6];
C[(i*35)+27] += values[124] * B[(i*35)+6];
C[(i*35)+28] += values[125] * B[(i*35)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*35)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*35)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a7_0 = _mm_loadu_pd(&values[126]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*35)+10], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*35)+12]);
__m128d a7_2 = _mm_load_sd(&values[128]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*35)+12], c7_2);
__m128d c7_3 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a7_3 = _mm_loadu_pd(&values[129]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, b7));
#endif
_mm_storeu_pd(&C[(i*35)+14], c7_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_5 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a7_5 = _mm256_loadu_pd(&values[131]);
c7_5 = _mm256_add_pd(c7_5, _mm256_mul_pd(a7_5, b7));
_mm256_storeu_pd(&C[(i*35)+20], c7_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_5 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a7_5 = _mm_loadu_pd(&values[131]);
c7_5 = _mm_add_pd(c7_5, _mm_mul_pd(a7_5, b7));
_mm_storeu_pd(&C[(i*35)+20], c7_5);
__m128d c7_7 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a7_7 = _mm_loadu_pd(&values[133]);
c7_7 = _mm_add_pd(c7_7, _mm_mul_pd(a7_7, b7));
_mm_storeu_pd(&C[(i*35)+22], c7_7);
#endif
__m128d c7_9 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a7_9 = _mm_loadu_pd(&values[135]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_9 = _mm_add_pd(c7_9, _mm_mul_pd(a7_9, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_9 = _mm_add_pd(c7_9, _mm_mul_pd(a7_9, b7));
#endif
_mm_storeu_pd(&C[(i*35)+25], c7_9);
__m128d c7_11 = _mm_load_sd(&C[(i*35)+27]);
__m128d a7_11 = _mm_load_sd(&values[137]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_11 = _mm_add_sd(c7_11, _mm_mul_sd(a7_11, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_11 = _mm_add_sd(c7_11, _mm_mul_sd(a7_11, b7));
#endif
_mm_store_sd(&C[(i*35)+27], c7_11);
__m128d c7_12 = _mm_loadu_pd(&C[(i*35)+29]);
__m128d a7_12 = _mm_loadu_pd(&values[138]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_12 = _mm_add_pd(c7_12, _mm_mul_pd(a7_12, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_12 = _mm_add_pd(c7_12, _mm_mul_pd(a7_12, b7));
#endif
_mm_storeu_pd(&C[(i*35)+29], c7_12);
#else
C[(i*35)+10] += values[126] * B[(i*35)+7];
C[(i*35)+11] += values[127] * B[(i*35)+7];
C[(i*35)+12] += values[128] * B[(i*35)+7];
C[(i*35)+14] += values[129] * B[(i*35)+7];
C[(i*35)+15] += values[130] * B[(i*35)+7];
C[(i*35)+20] += values[131] * B[(i*35)+7];
C[(i*35)+21] += values[132] * B[(i*35)+7];
C[(i*35)+22] += values[133] * B[(i*35)+7];
C[(i*35)+23] += values[134] * B[(i*35)+7];
C[(i*35)+25] += values[135] * B[(i*35)+7];
C[(i*35)+26] += values[136] * B[(i*35)+7];
C[(i*35)+27] += values[137] * B[(i*35)+7];
C[(i*35)+29] += values[138] * B[(i*35)+7];
C[(i*35)+30] += values[139] * B[(i*35)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*35)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*35)+8]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_0 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a8_0 = _mm256_loadu_pd(&values[140]);
c8_0 = _mm256_add_pd(c8_0, _mm256_mul_pd(a8_0, b8));
_mm256_storeu_pd(&C[(i*35)+10], c8_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a8_0 = _mm_loadu_pd(&values[140]);
c8_0 = _mm_add_pd(c8_0, _mm_mul_pd(a8_0, b8));
_mm_storeu_pd(&C[(i*35)+10], c8_0);
__m128d c8_2 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a8_2 = _mm_loadu_pd(&values[142]);
c8_2 = _mm_add_pd(c8_2, _mm_mul_pd(a8_2, b8));
_mm_storeu_pd(&C[(i*35)+12], c8_2);
#endif
__m128d c8_4 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a8_4 = _mm_loadu_pd(&values[144]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, b8));
#endif
_mm_storeu_pd(&C[(i*35)+14], c8_4);
__m128d c8_6 = _mm_load_sd(&C[(i*35)+16]);
__m128d a8_6 = _mm_load_sd(&values[146]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*35)+16], c8_6);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_7 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a8_7 = _mm256_loadu_pd(&values[147]);
c8_7 = _mm256_add_pd(c8_7, _mm256_mul_pd(a8_7, b8));
_mm256_storeu_pd(&C[(i*35)+20], c8_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_7 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a8_7 = _mm_loadu_pd(&values[147]);
c8_7 = _mm_add_pd(c8_7, _mm_mul_pd(a8_7, b8));
_mm_storeu_pd(&C[(i*35)+20], c8_7);
__m128d c8_9 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a8_9 = _mm_loadu_pd(&values[149]);
c8_9 = _mm_add_pd(c8_9, _mm_mul_pd(a8_9, b8));
_mm_storeu_pd(&C[(i*35)+22], c8_9);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_11 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a8_11 = _mm256_loadu_pd(&values[151]);
c8_11 = _mm256_add_pd(c8_11, _mm256_mul_pd(a8_11, b8));
_mm256_storeu_pd(&C[(i*35)+24], c8_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_11 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a8_11 = _mm_loadu_pd(&values[151]);
c8_11 = _mm_add_pd(c8_11, _mm_mul_pd(a8_11, b8));
_mm_storeu_pd(&C[(i*35)+24], c8_11);
__m128d c8_13 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a8_13 = _mm_loadu_pd(&values[153]);
c8_13 = _mm_add_pd(c8_13, _mm_mul_pd(a8_13, b8));
_mm_storeu_pd(&C[(i*35)+26], c8_13);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_15 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a8_15 = _mm256_loadu_pd(&values[155]);
c8_15 = _mm256_add_pd(c8_15, _mm256_mul_pd(a8_15, b8));
_mm256_storeu_pd(&C[(i*35)+28], c8_15);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_15 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a8_15 = _mm_loadu_pd(&values[155]);
c8_15 = _mm_add_pd(c8_15, _mm_mul_pd(a8_15, b8));
_mm_storeu_pd(&C[(i*35)+28], c8_15);
__m128d c8_17 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a8_17 = _mm_loadu_pd(&values[157]);
c8_17 = _mm_add_pd(c8_17, _mm_mul_pd(a8_17, b8));
_mm_storeu_pd(&C[(i*35)+30], c8_17);
#endif
#else
C[(i*35)+10] += values[140] * B[(i*35)+8];
C[(i*35)+11] += values[141] * B[(i*35)+8];
C[(i*35)+12] += values[142] * B[(i*35)+8];
C[(i*35)+13] += values[143] * B[(i*35)+8];
C[(i*35)+14] += values[144] * B[(i*35)+8];
C[(i*35)+15] += values[145] * B[(i*35)+8];
C[(i*35)+16] += values[146] * B[(i*35)+8];
C[(i*35)+20] += values[147] * B[(i*35)+8];
C[(i*35)+21] += values[148] * B[(i*35)+8];
C[(i*35)+22] += values[149] * B[(i*35)+8];
C[(i*35)+23] += values[150] * B[(i*35)+8];
C[(i*35)+24] += values[151] * B[(i*35)+8];
C[(i*35)+25] += values[152] * B[(i*35)+8];
C[(i*35)+26] += values[153] * B[(i*35)+8];
C[(i*35)+27] += values[154] * B[(i*35)+8];
C[(i*35)+28] += values[155] * B[(i*35)+8];
C[(i*35)+29] += values[156] * B[(i*35)+8];
C[(i*35)+30] += values[157] * B[(i*35)+8];
C[(i*35)+31] += values[158] * B[(i*35)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*35)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*35)+9]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_0 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a9_0 = _mm256_loadu_pd(&values[159]);
c9_0 = _mm256_add_pd(c9_0, _mm256_mul_pd(a9_0, b9));
_mm256_storeu_pd(&C[(i*35)+10], c9_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a9_0 = _mm_loadu_pd(&values[159]);
c9_0 = _mm_add_pd(c9_0, _mm_mul_pd(a9_0, b9));
_mm_storeu_pd(&C[(i*35)+10], c9_0);
__m128d c9_2 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a9_2 = _mm_loadu_pd(&values[161]);
c9_2 = _mm_add_pd(c9_2, _mm_mul_pd(a9_2, b9));
_mm_storeu_pd(&C[(i*35)+12], c9_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_4 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a9_4 = _mm256_loadu_pd(&values[163]);
c9_4 = _mm256_add_pd(c9_4, _mm256_mul_pd(a9_4, b9));
_mm256_storeu_pd(&C[(i*35)+14], c9_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_4 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a9_4 = _mm_loadu_pd(&values[163]);
c9_4 = _mm_add_pd(c9_4, _mm_mul_pd(a9_4, b9));
_mm_storeu_pd(&C[(i*35)+14], c9_4);
__m128d c9_6 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a9_6 = _mm_loadu_pd(&values[165]);
c9_6 = _mm_add_pd(c9_6, _mm_mul_pd(a9_6, b9));
_mm_storeu_pd(&C[(i*35)+16], c9_6);
#endif
__m128d c9_8 = _mm_load_sd(&C[(i*35)+18]);
__m128d a9_8 = _mm_load_sd(&values[167]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, b9));
#endif
_mm_store_sd(&C[(i*35)+18], c9_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_9 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a9_9 = _mm256_loadu_pd(&values[168]);
c9_9 = _mm256_add_pd(c9_9, _mm256_mul_pd(a9_9, b9));
_mm256_storeu_pd(&C[(i*35)+20], c9_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_9 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a9_9 = _mm_loadu_pd(&values[168]);
c9_9 = _mm_add_pd(c9_9, _mm_mul_pd(a9_9, b9));
_mm_storeu_pd(&C[(i*35)+20], c9_9);
__m128d c9_11 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a9_11 = _mm_loadu_pd(&values[170]);
c9_11 = _mm_add_pd(c9_11, _mm_mul_pd(a9_11, b9));
_mm_storeu_pd(&C[(i*35)+22], c9_11);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_13 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a9_13 = _mm256_loadu_pd(&values[172]);
c9_13 = _mm256_add_pd(c9_13, _mm256_mul_pd(a9_13, b9));
_mm256_storeu_pd(&C[(i*35)+24], c9_13);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_13 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a9_13 = _mm_loadu_pd(&values[172]);
c9_13 = _mm_add_pd(c9_13, _mm_mul_pd(a9_13, b9));
_mm_storeu_pd(&C[(i*35)+24], c9_13);
__m128d c9_15 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a9_15 = _mm_loadu_pd(&values[174]);
c9_15 = _mm_add_pd(c9_15, _mm_mul_pd(a9_15, b9));
_mm_storeu_pd(&C[(i*35)+26], c9_15);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_17 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a9_17 = _mm256_loadu_pd(&values[176]);
c9_17 = _mm256_add_pd(c9_17, _mm256_mul_pd(a9_17, b9));
_mm256_storeu_pd(&C[(i*35)+28], c9_17);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_17 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a9_17 = _mm_loadu_pd(&values[176]);
c9_17 = _mm_add_pd(c9_17, _mm_mul_pd(a9_17, b9));
_mm_storeu_pd(&C[(i*35)+28], c9_17);
__m128d c9_19 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a9_19 = _mm_loadu_pd(&values[178]);
c9_19 = _mm_add_pd(c9_19, _mm_mul_pd(a9_19, b9));
_mm_storeu_pd(&C[(i*35)+30], c9_19);
#endif
__m128d c9_21 = _mm_loadu_pd(&C[(i*35)+32]);
__m128d a9_21 = _mm_loadu_pd(&values[180]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_21 = _mm_add_pd(c9_21, _mm_mul_pd(a9_21, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_21 = _mm_add_pd(c9_21, _mm_mul_pd(a9_21, b9));
#endif
_mm_storeu_pd(&C[(i*35)+32], c9_21);
#else
C[(i*35)+10] += values[159] * B[(i*35)+9];
C[(i*35)+11] += values[160] * B[(i*35)+9];
C[(i*35)+12] += values[161] * B[(i*35)+9];
C[(i*35)+13] += values[162] * B[(i*35)+9];
C[(i*35)+14] += values[163] * B[(i*35)+9];
C[(i*35)+15] += values[164] * B[(i*35)+9];
C[(i*35)+16] += values[165] * B[(i*35)+9];
C[(i*35)+17] += values[166] * B[(i*35)+9];
C[(i*35)+18] += values[167] * B[(i*35)+9];
C[(i*35)+20] += values[168] * B[(i*35)+9];
C[(i*35)+21] += values[169] * B[(i*35)+9];
C[(i*35)+22] += values[170] * B[(i*35)+9];
C[(i*35)+23] += values[171] * B[(i*35)+9];
C[(i*35)+24] += values[172] * B[(i*35)+9];
C[(i*35)+25] += values[173] * B[(i*35)+9];
C[(i*35)+26] += values[174] * B[(i*35)+9];
C[(i*35)+27] += values[175] * B[(i*35)+9];
C[(i*35)+28] += values[176] * B[(i*35)+9];
C[(i*35)+29] += values[177] * B[(i*35)+9];
C[(i*35)+30] += values[178] * B[(i*35)+9];
C[(i*35)+31] += values[179] * B[(i*35)+9];
C[(i*35)+32] += values[180] * B[(i*35)+9];
C[(i*35)+33] += values[181] * B[(i*35)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*35)+10]);
#endif
__m128d c10_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a10_0 = _mm_loadu_pd(&values[182]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
#endif
_mm_storeu_pd(&C[(i*35)+20], c10_0);
#else
C[(i*35)+20] += values[182] * B[(i*35)+10];
C[(i*35)+21] += values[183] * B[(i*35)+10];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*35)+11]);
#endif
__m128d c11_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a11_0 = _mm_loadu_pd(&values[184]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
#endif
_mm_storeu_pd(&C[(i*35)+20], c11_0);
__m128d c11_2 = _mm_load_sd(&C[(i*35)+22]);
__m128d a11_2 = _mm_load_sd(&values[186]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*35)+22], c11_2);
#else
C[(i*35)+20] += values[184] * B[(i*35)+11];
C[(i*35)+21] += values[185] * B[(i*35)+11];
C[(i*35)+22] += values[186] * B[(i*35)+11];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a12_0 = _mm256_loadu_pd(&values[187]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*35)+20], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a12_0 = _mm_loadu_pd(&values[187]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*35)+20], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a12_2 = _mm_loadu_pd(&values[189]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*35)+22], c12_2);
#endif
#else
C[(i*35)+20] += values[187] * B[(i*35)+12];
C[(i*35)+21] += values[188] * B[(i*35)+12];
C[(i*35)+22] += values[189] * B[(i*35)+12];
C[(i*35)+23] += values[190] * B[(i*35)+12];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*35)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*35)+13]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a13_0 = _mm256_loadu_pd(&values[191]);
c13_0 = _mm256_add_pd(c13_0, _mm256_mul_pd(a13_0, b13));
_mm256_storeu_pd(&C[(i*35)+20], c13_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a13_0 = _mm_loadu_pd(&values[191]);
c13_0 = _mm_add_pd(c13_0, _mm_mul_pd(a13_0, b13));
_mm_storeu_pd(&C[(i*35)+20], c13_0);
__m128d c13_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a13_2 = _mm_loadu_pd(&values[193]);
c13_2 = _mm_add_pd(c13_2, _mm_mul_pd(a13_2, b13));
_mm_storeu_pd(&C[(i*35)+22], c13_2);
#endif
__m128d c13_4 = _mm_load_sd(&C[(i*35)+24]);
__m128d a13_4 = _mm_load_sd(&values[195]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, b13));
#endif
_mm_store_sd(&C[(i*35)+24], c13_4);
#else
C[(i*35)+20] += values[191] * B[(i*35)+13];
C[(i*35)+21] += values[192] * B[(i*35)+13];
C[(i*35)+22] += values[193] * B[(i*35)+13];
C[(i*35)+23] += values[194] * B[(i*35)+13];
C[(i*35)+24] += values[195] * B[(i*35)+13];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*35)+14]);
#endif
__m128d c14_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a14_0 = _mm_loadu_pd(&values[196]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
#endif
_mm_storeu_pd(&C[(i*35)+20], c14_0);
__m128d c14_2 = _mm_load_sd(&C[(i*35)+22]);
__m128d a14_2 = _mm_load_sd(&values[198]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, b14));
#endif
_mm_store_sd(&C[(i*35)+22], c14_2);
__m128d c14_3 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a14_3 = _mm_loadu_pd(&values[199]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, b14));
#endif
_mm_storeu_pd(&C[(i*35)+25], c14_3);
#else
C[(i*35)+20] += values[196] * B[(i*35)+14];
C[(i*35)+21] += values[197] * B[(i*35)+14];
C[(i*35)+22] += values[198] * B[(i*35)+14];
C[(i*35)+25] += values[199] * B[(i*35)+14];
C[(i*35)+26] += values[200] * B[(i*35)+14];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a15_0 = _mm256_loadu_pd(&values[201]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*35)+20], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a15_0 = _mm_loadu_pd(&values[201]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*35)+20], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a15_2 = _mm_loadu_pd(&values[203]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*35)+22], c15_2);
#endif
__m128d c15_4 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a15_4 = _mm_loadu_pd(&values[205]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, b15));
#endif
_mm_storeu_pd(&C[(i*35)+25], c15_4);
__m128d c15_6 = _mm_load_sd(&C[(i*35)+27]);
__m128d a15_6 = _mm_load_sd(&values[207]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*35)+27], c15_6);
#else
C[(i*35)+20] += values[201] * B[(i*35)+15];
C[(i*35)+21] += values[202] * B[(i*35)+15];
C[(i*35)+22] += values[203] * B[(i*35)+15];
C[(i*35)+23] += values[204] * B[(i*35)+15];
C[(i*35)+25] += values[205] * B[(i*35)+15];
C[(i*35)+26] += values[206] * B[(i*35)+15];
C[(i*35)+27] += values[207] * B[(i*35)+15];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*35)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*35)+16]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a16_0 = _mm256_loadu_pd(&values[208]);
c16_0 = _mm256_add_pd(c16_0, _mm256_mul_pd(a16_0, b16));
_mm256_storeu_pd(&C[(i*35)+20], c16_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a16_0 = _mm_loadu_pd(&values[208]);
c16_0 = _mm_add_pd(c16_0, _mm_mul_pd(a16_0, b16));
_mm_storeu_pd(&C[(i*35)+20], c16_0);
__m128d c16_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a16_2 = _mm_loadu_pd(&values[210]);
c16_2 = _mm_add_pd(c16_2, _mm_mul_pd(a16_2, b16));
_mm_storeu_pd(&C[(i*35)+22], c16_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_4 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a16_4 = _mm256_loadu_pd(&values[212]);
c16_4 = _mm256_add_pd(c16_4, _mm256_mul_pd(a16_4, b16));
_mm256_storeu_pd(&C[(i*35)+24], c16_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_4 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a16_4 = _mm_loadu_pd(&values[212]);
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, b16));
_mm_storeu_pd(&C[(i*35)+24], c16_4);
__m128d c16_6 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a16_6 = _mm_loadu_pd(&values[214]);
c16_6 = _mm_add_pd(c16_6, _mm_mul_pd(a16_6, b16));
_mm_storeu_pd(&C[(i*35)+26], c16_6);
#endif
__m128d c16_8 = _mm_load_sd(&C[(i*35)+28]);
__m128d a16_8 = _mm_load_sd(&values[216]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, b16));
#endif
_mm_store_sd(&C[(i*35)+28], c16_8);
#else
C[(i*35)+20] += values[208] * B[(i*35)+16];
C[(i*35)+21] += values[209] * B[(i*35)+16];
C[(i*35)+22] += values[210] * B[(i*35)+16];
C[(i*35)+23] += values[211] * B[(i*35)+16];
C[(i*35)+24] += values[212] * B[(i*35)+16];
C[(i*35)+25] += values[213] * B[(i*35)+16];
C[(i*35)+26] += values[214] * B[(i*35)+16];
C[(i*35)+27] += values[215] * B[(i*35)+16];
C[(i*35)+28] += values[216] * B[(i*35)+16];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a17_0 = _mm256_loadu_pd(&values[217]);
c17_0 = _mm256_add_pd(c17_0, _mm256_mul_pd(a17_0, b17));
_mm256_storeu_pd(&C[(i*35)+20], c17_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a17_0 = _mm_loadu_pd(&values[217]);
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, b17));
_mm_storeu_pd(&C[(i*35)+20], c17_0);
__m128d c17_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a17_2 = _mm_loadu_pd(&values[219]);
c17_2 = _mm_add_pd(c17_2, _mm_mul_pd(a17_2, b17));
_mm_storeu_pd(&C[(i*35)+22], c17_2);
#endif
__m128d c17_4 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a17_4 = _mm_loadu_pd(&values[221]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, b17));
#endif
_mm_storeu_pd(&C[(i*35)+25], c17_4);
__m128d c17_6 = _mm_load_sd(&C[(i*35)+27]);
__m128d a17_6 = _mm_load_sd(&values[223]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, b17));
#endif
_mm_store_sd(&C[(i*35)+27], c17_6);
__m128d c17_7 = _mm_loadu_pd(&C[(i*35)+29]);
__m128d a17_7 = _mm_loadu_pd(&values[224]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, b17));
#endif
_mm_storeu_pd(&C[(i*35)+29], c17_7);
#else
C[(i*35)+20] += values[217] * B[(i*35)+17];
C[(i*35)+21] += values[218] * B[(i*35)+17];
C[(i*35)+22] += values[219] * B[(i*35)+17];
C[(i*35)+23] += values[220] * B[(i*35)+17];
C[(i*35)+25] += values[221] * B[(i*35)+17];
C[(i*35)+26] += values[222] * B[(i*35)+17];
C[(i*35)+27] += values[223] * B[(i*35)+17];
C[(i*35)+29] += values[224] * B[(i*35)+17];
C[(i*35)+30] += values[225] * B[(i*35)+17];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*35)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*35)+18]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a18_0 = _mm256_loadu_pd(&values[226]);
c18_0 = _mm256_add_pd(c18_0, _mm256_mul_pd(a18_0, b18));
_mm256_storeu_pd(&C[(i*35)+20], c18_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a18_0 = _mm_loadu_pd(&values[226]);
c18_0 = _mm_add_pd(c18_0, _mm_mul_pd(a18_0, b18));
_mm_storeu_pd(&C[(i*35)+20], c18_0);
__m128d c18_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a18_2 = _mm_loadu_pd(&values[228]);
c18_2 = _mm_add_pd(c18_2, _mm_mul_pd(a18_2, b18));
_mm_storeu_pd(&C[(i*35)+22], c18_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_4 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a18_4 = _mm256_loadu_pd(&values[230]);
c18_4 = _mm256_add_pd(c18_4, _mm256_mul_pd(a18_4, b18));
_mm256_storeu_pd(&C[(i*35)+24], c18_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_4 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a18_4 = _mm_loadu_pd(&values[230]);
c18_4 = _mm_add_pd(c18_4, _mm_mul_pd(a18_4, b18));
_mm_storeu_pd(&C[(i*35)+24], c18_4);
__m128d c18_6 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a18_6 = _mm_loadu_pd(&values[232]);
c18_6 = _mm_add_pd(c18_6, _mm_mul_pd(a18_6, b18));
_mm_storeu_pd(&C[(i*35)+26], c18_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_8 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a18_8 = _mm256_loadu_pd(&values[234]);
c18_8 = _mm256_add_pd(c18_8, _mm256_mul_pd(a18_8, b18));
_mm256_storeu_pd(&C[(i*35)+28], c18_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_8 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a18_8 = _mm_loadu_pd(&values[234]);
c18_8 = _mm_add_pd(c18_8, _mm_mul_pd(a18_8, b18));
_mm_storeu_pd(&C[(i*35)+28], c18_8);
__m128d c18_10 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a18_10 = _mm_loadu_pd(&values[236]);
c18_10 = _mm_add_pd(c18_10, _mm_mul_pd(a18_10, b18));
_mm_storeu_pd(&C[(i*35)+30], c18_10);
#endif
#else
C[(i*35)+20] += values[226] * B[(i*35)+18];
C[(i*35)+21] += values[227] * B[(i*35)+18];
C[(i*35)+22] += values[228] * B[(i*35)+18];
C[(i*35)+23] += values[229] * B[(i*35)+18];
C[(i*35)+24] += values[230] * B[(i*35)+18];
C[(i*35)+25] += values[231] * B[(i*35)+18];
C[(i*35)+26] += values[232] * B[(i*35)+18];
C[(i*35)+27] += values[233] * B[(i*35)+18];
C[(i*35)+28] += values[234] * B[(i*35)+18];
C[(i*35)+29] += values[235] * B[(i*35)+18];
C[(i*35)+30] += values[236] * B[(i*35)+18];
C[(i*35)+31] += values[237] * B[(i*35)+18];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*35)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*35)+19]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a19_0 = _mm256_loadu_pd(&values[238]);
c19_0 = _mm256_add_pd(c19_0, _mm256_mul_pd(a19_0, b19));
_mm256_storeu_pd(&C[(i*35)+20], c19_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a19_0 = _mm_loadu_pd(&values[238]);
c19_0 = _mm_add_pd(c19_0, _mm_mul_pd(a19_0, b19));
_mm_storeu_pd(&C[(i*35)+20], c19_0);
__m128d c19_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a19_2 = _mm_loadu_pd(&values[240]);
c19_2 = _mm_add_pd(c19_2, _mm_mul_pd(a19_2, b19));
_mm_storeu_pd(&C[(i*35)+22], c19_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_4 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a19_4 = _mm256_loadu_pd(&values[242]);
c19_4 = _mm256_add_pd(c19_4, _mm256_mul_pd(a19_4, b19));
_mm256_storeu_pd(&C[(i*35)+24], c19_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_4 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a19_4 = _mm_loadu_pd(&values[242]);
c19_4 = _mm_add_pd(c19_4, _mm_mul_pd(a19_4, b19));
_mm_storeu_pd(&C[(i*35)+24], c19_4);
__m128d c19_6 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a19_6 = _mm_loadu_pd(&values[244]);
c19_6 = _mm_add_pd(c19_6, _mm_mul_pd(a19_6, b19));
_mm_storeu_pd(&C[(i*35)+26], c19_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_8 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a19_8 = _mm256_loadu_pd(&values[246]);
c19_8 = _mm256_add_pd(c19_8, _mm256_mul_pd(a19_8, b19));
_mm256_storeu_pd(&C[(i*35)+28], c19_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_8 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a19_8 = _mm_loadu_pd(&values[246]);
c19_8 = _mm_add_pd(c19_8, _mm_mul_pd(a19_8, b19));
_mm_storeu_pd(&C[(i*35)+28], c19_8);
__m128d c19_10 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a19_10 = _mm_loadu_pd(&values[248]);
c19_10 = _mm_add_pd(c19_10, _mm_mul_pd(a19_10, b19));
_mm_storeu_pd(&C[(i*35)+30], c19_10);
#endif
__m128d c19_12 = _mm_loadu_pd(&C[(i*35)+32]);
__m128d a19_12 = _mm_loadu_pd(&values[250]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_12 = _mm_add_pd(c19_12, _mm_mul_pd(a19_12, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_12 = _mm_add_pd(c19_12, _mm_mul_pd(a19_12, b19));
#endif
_mm_storeu_pd(&C[(i*35)+32], c19_12);
#else
C[(i*35)+20] += values[238] * B[(i*35)+19];
C[(i*35)+21] += values[239] * B[(i*35)+19];
C[(i*35)+22] += values[240] * B[(i*35)+19];
C[(i*35)+23] += values[241] * B[(i*35)+19];
C[(i*35)+24] += values[242] * B[(i*35)+19];
C[(i*35)+25] += values[243] * B[(i*35)+19];
C[(i*35)+26] += values[244] * B[(i*35)+19];
C[(i*35)+27] += values[245] * B[(i*35)+19];
C[(i*35)+28] += values[246] * B[(i*35)+19];
C[(i*35)+29] += values[247] * B[(i*35)+19];
C[(i*35)+30] += values[248] * B[(i*35)+19];
C[(i*35)+31] += values[249] * B[(i*35)+19];
C[(i*35)+32] += values[250] * B[(i*35)+19];
C[(i*35)+33] += values[251] * B[(i*35)+19];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 4536;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivM_9_35(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 35; m++) {
    C[(i*35)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*35)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*35)+0]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_0 = _mm256_loadu_pd(&C[(i*35)+1]);
__m256d a0_0 = _mm256_loadu_pd(&values[0]);
c0_0 = _mm256_add_pd(c0_0, _mm256_mul_pd(a0_0, b0));
_mm256_storeu_pd(&C[(i*35)+1], c0_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_0 = _mm_loadu_pd(&C[(i*35)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
_mm_storeu_pd(&C[(i*35)+1], c0_0);
__m128d c0_2 = _mm_loadu_pd(&C[(i*35)+3]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*35)+3], c0_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_4 = _mm256_loadu_pd(&C[(i*35)+5]);
__m256d a0_4 = _mm256_loadu_pd(&values[4]);
c0_4 = _mm256_add_pd(c0_4, _mm256_mul_pd(a0_4, b0));
_mm256_storeu_pd(&C[(i*35)+5], c0_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_4 = _mm_loadu_pd(&C[(i*35)+5]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*35)+5], c0_4);
__m128d c0_6 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a0_6 = _mm_loadu_pd(&values[6]);
c0_6 = _mm_add_pd(c0_6, _mm_mul_pd(a0_6, b0));
_mm_storeu_pd(&C[(i*35)+7], c0_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_8 = _mm256_loadu_pd(&C[(i*35)+9]);
__m256d a0_8 = _mm256_loadu_pd(&values[8]);
c0_8 = _mm256_add_pd(c0_8, _mm256_mul_pd(a0_8, b0));
_mm256_storeu_pd(&C[(i*35)+9], c0_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_8 = _mm_loadu_pd(&C[(i*35)+9]);
__m128d a0_8 = _mm_loadu_pd(&values[8]);
c0_8 = _mm_add_pd(c0_8, _mm_mul_pd(a0_8, b0));
_mm_storeu_pd(&C[(i*35)+9], c0_8);
__m128d c0_10 = _mm_loadu_pd(&C[(i*35)+11]);
__m128d a0_10 = _mm_loadu_pd(&values[10]);
c0_10 = _mm_add_pd(c0_10, _mm_mul_pd(a0_10, b0));
_mm_storeu_pd(&C[(i*35)+11], c0_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_12 = _mm256_loadu_pd(&C[(i*35)+13]);
__m256d a0_12 = _mm256_loadu_pd(&values[12]);
c0_12 = _mm256_add_pd(c0_12, _mm256_mul_pd(a0_12, b0));
_mm256_storeu_pd(&C[(i*35)+13], c0_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_12 = _mm_loadu_pd(&C[(i*35)+13]);
__m128d a0_12 = _mm_loadu_pd(&values[12]);
c0_12 = _mm_add_pd(c0_12, _mm_mul_pd(a0_12, b0));
_mm_storeu_pd(&C[(i*35)+13], c0_12);
__m128d c0_14 = _mm_loadu_pd(&C[(i*35)+15]);
__m128d a0_14 = _mm_loadu_pd(&values[14]);
c0_14 = _mm_add_pd(c0_14, _mm_mul_pd(a0_14, b0));
_mm_storeu_pd(&C[(i*35)+15], c0_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_16 = _mm256_loadu_pd(&C[(i*35)+17]);
__m256d a0_16 = _mm256_loadu_pd(&values[16]);
c0_16 = _mm256_add_pd(c0_16, _mm256_mul_pd(a0_16, b0));
_mm256_storeu_pd(&C[(i*35)+17], c0_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_16 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a0_16 = _mm_loadu_pd(&values[16]);
c0_16 = _mm_add_pd(c0_16, _mm_mul_pd(a0_16, b0));
_mm_storeu_pd(&C[(i*35)+17], c0_16);
__m128d c0_18 = _mm_loadu_pd(&C[(i*35)+19]);
__m128d a0_18 = _mm_loadu_pd(&values[18]);
c0_18 = _mm_add_pd(c0_18, _mm_mul_pd(a0_18, b0));
_mm_storeu_pd(&C[(i*35)+19], c0_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_20 = _mm256_loadu_pd(&C[(i*35)+21]);
__m256d a0_20 = _mm256_loadu_pd(&values[20]);
c0_20 = _mm256_add_pd(c0_20, _mm256_mul_pd(a0_20, b0));
_mm256_storeu_pd(&C[(i*35)+21], c0_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_20 = _mm_loadu_pd(&C[(i*35)+21]);
__m128d a0_20 = _mm_loadu_pd(&values[20]);
c0_20 = _mm_add_pd(c0_20, _mm_mul_pd(a0_20, b0));
_mm_storeu_pd(&C[(i*35)+21], c0_20);
__m128d c0_22 = _mm_loadu_pd(&C[(i*35)+23]);
__m128d a0_22 = _mm_loadu_pd(&values[22]);
c0_22 = _mm_add_pd(c0_22, _mm_mul_pd(a0_22, b0));
_mm_storeu_pd(&C[(i*35)+23], c0_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_24 = _mm256_loadu_pd(&C[(i*35)+25]);
__m256d a0_24 = _mm256_loadu_pd(&values[24]);
c0_24 = _mm256_add_pd(c0_24, _mm256_mul_pd(a0_24, b0));
_mm256_storeu_pd(&C[(i*35)+25], c0_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_24 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a0_24 = _mm_loadu_pd(&values[24]);
c0_24 = _mm_add_pd(c0_24, _mm_mul_pd(a0_24, b0));
_mm_storeu_pd(&C[(i*35)+25], c0_24);
__m128d c0_26 = _mm_loadu_pd(&C[(i*35)+27]);
__m128d a0_26 = _mm_loadu_pd(&values[26]);
c0_26 = _mm_add_pd(c0_26, _mm_mul_pd(a0_26, b0));
_mm_storeu_pd(&C[(i*35)+27], c0_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_28 = _mm256_loadu_pd(&C[(i*35)+29]);
__m256d a0_28 = _mm256_loadu_pd(&values[28]);
c0_28 = _mm256_add_pd(c0_28, _mm256_mul_pd(a0_28, b0));
_mm256_storeu_pd(&C[(i*35)+29], c0_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_28 = _mm_loadu_pd(&C[(i*35)+29]);
__m128d a0_28 = _mm_loadu_pd(&values[28]);
c0_28 = _mm_add_pd(c0_28, _mm_mul_pd(a0_28, b0));
_mm_storeu_pd(&C[(i*35)+29], c0_28);
__m128d c0_30 = _mm_loadu_pd(&C[(i*35)+31]);
__m128d a0_30 = _mm_loadu_pd(&values[30]);
c0_30 = _mm_add_pd(c0_30, _mm_mul_pd(a0_30, b0));
_mm_storeu_pd(&C[(i*35)+31], c0_30);
#endif
__m128d c0_32 = _mm_loadu_pd(&C[(i*35)+33]);
__m128d a0_32 = _mm_loadu_pd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_32 = _mm_add_pd(c0_32, _mm_mul_pd(a0_32, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_32 = _mm_add_pd(c0_32, _mm_mul_pd(a0_32, b0));
#endif
_mm_storeu_pd(&C[(i*35)+33], c0_32);
#else
C[(i*35)+1] += values[0] * B[(i*35)+0];
C[(i*35)+2] += values[1] * B[(i*35)+0];
C[(i*35)+3] += values[2] * B[(i*35)+0];
C[(i*35)+4] += values[3] * B[(i*35)+0];
C[(i*35)+5] += values[4] * B[(i*35)+0];
C[(i*35)+6] += values[5] * B[(i*35)+0];
C[(i*35)+7] += values[6] * B[(i*35)+0];
C[(i*35)+8] += values[7] * B[(i*35)+0];
C[(i*35)+9] += values[8] * B[(i*35)+0];
C[(i*35)+10] += values[9] * B[(i*35)+0];
C[(i*35)+11] += values[10] * B[(i*35)+0];
C[(i*35)+12] += values[11] * B[(i*35)+0];
C[(i*35)+13] += values[12] * B[(i*35)+0];
C[(i*35)+14] += values[13] * B[(i*35)+0];
C[(i*35)+15] += values[14] * B[(i*35)+0];
C[(i*35)+16] += values[15] * B[(i*35)+0];
C[(i*35)+17] += values[16] * B[(i*35)+0];
C[(i*35)+18] += values[17] * B[(i*35)+0];
C[(i*35)+19] += values[18] * B[(i*35)+0];
C[(i*35)+20] += values[19] * B[(i*35)+0];
C[(i*35)+21] += values[20] * B[(i*35)+0];
C[(i*35)+22] += values[21] * B[(i*35)+0];
C[(i*35)+23] += values[22] * B[(i*35)+0];
C[(i*35)+24] += values[23] * B[(i*35)+0];
C[(i*35)+25] += values[24] * B[(i*35)+0];
C[(i*35)+26] += values[25] * B[(i*35)+0];
C[(i*35)+27] += values[26] * B[(i*35)+0];
C[(i*35)+28] += values[27] * B[(i*35)+0];
C[(i*35)+29] += values[28] * B[(i*35)+0];
C[(i*35)+30] += values[29] * B[(i*35)+0];
C[(i*35)+31] += values[30] * B[(i*35)+0];
C[(i*35)+32] += values[31] * B[(i*35)+0];
C[(i*35)+33] += values[32] * B[(i*35)+0];
C[(i*35)+34] += values[33] * B[(i*35)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*35)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*35)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[34]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*35)+4], c1_0);
__m128d c1_2 = _mm_load_sd(&C[(i*35)+7]);
__m128d a1_2 = _mm_load_sd(&values[36]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, b1));
#endif
_mm_store_sd(&C[(i*35)+7], c1_2);
__m128d c1_3 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a1_3 = _mm_loadu_pd(&values[37]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_3 = _mm_add_pd(c1_3, _mm_mul_pd(a1_3, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_3 = _mm_add_pd(c1_3, _mm_mul_pd(a1_3, b1));
#endif
_mm_storeu_pd(&C[(i*35)+10], c1_3);
__m128d c1_5 = _mm_load_sd(&C[(i*35)+12]);
__m128d a1_5 = _mm_load_sd(&values[39]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, b1));
#endif
_mm_store_sd(&C[(i*35)+12], c1_5);
__m128d c1_6 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a1_6 = _mm_loadu_pd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_6 = _mm_add_pd(c1_6, _mm_mul_pd(a1_6, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_6 = _mm_add_pd(c1_6, _mm_mul_pd(a1_6, b1));
#endif
_mm_storeu_pd(&C[(i*35)+14], c1_6);
__m128d c1_8 = _mm_load_sd(&C[(i*35)+17]);
__m128d a1_8 = _mm_load_sd(&values[42]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, b1));
#endif
_mm_store_sd(&C[(i*35)+17], c1_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_9 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a1_9 = _mm256_loadu_pd(&values[43]);
c1_9 = _mm256_add_pd(c1_9, _mm256_mul_pd(a1_9, b1));
_mm256_storeu_pd(&C[(i*35)+20], c1_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_9 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a1_9 = _mm_loadu_pd(&values[43]);
c1_9 = _mm_add_pd(c1_9, _mm_mul_pd(a1_9, b1));
_mm_storeu_pd(&C[(i*35)+20], c1_9);
__m128d c1_11 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a1_11 = _mm_loadu_pd(&values[45]);
c1_11 = _mm_add_pd(c1_11, _mm_mul_pd(a1_11, b1));
_mm_storeu_pd(&C[(i*35)+22], c1_11);
#endif
__m128d c1_13 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a1_13 = _mm_loadu_pd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_13 = _mm_add_pd(c1_13, _mm_mul_pd(a1_13, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_13 = _mm_add_pd(c1_13, _mm_mul_pd(a1_13, b1));
#endif
_mm_storeu_pd(&C[(i*35)+25], c1_13);
__m128d c1_15 = _mm_load_sd(&C[(i*35)+27]);
__m128d a1_15 = _mm_load_sd(&values[49]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_15 = _mm_add_sd(c1_15, _mm_mul_sd(a1_15, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_15 = _mm_add_sd(c1_15, _mm_mul_sd(a1_15, b1));
#endif
_mm_store_sd(&C[(i*35)+27], c1_15);
__m128d c1_16 = _mm_loadu_pd(&C[(i*35)+29]);
__m128d a1_16 = _mm_loadu_pd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_16 = _mm_add_pd(c1_16, _mm_mul_pd(a1_16, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_16 = _mm_add_pd(c1_16, _mm_mul_pd(a1_16, b1));
#endif
_mm_storeu_pd(&C[(i*35)+29], c1_16);
__m128d c1_18 = _mm_load_sd(&C[(i*35)+32]);
__m128d a1_18 = _mm_load_sd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_18 = _mm_add_sd(c1_18, _mm_mul_sd(a1_18, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_18 = _mm_add_sd(c1_18, _mm_mul_sd(a1_18, b1));
#endif
_mm_store_sd(&C[(i*35)+32], c1_18);
#else
C[(i*35)+4] += values[34] * B[(i*35)+1];
C[(i*35)+5] += values[35] * B[(i*35)+1];
C[(i*35)+7] += values[36] * B[(i*35)+1];
C[(i*35)+10] += values[37] * B[(i*35)+1];
C[(i*35)+11] += values[38] * B[(i*35)+1];
C[(i*35)+12] += values[39] * B[(i*35)+1];
C[(i*35)+14] += values[40] * B[(i*35)+1];
C[(i*35)+15] += values[41] * B[(i*35)+1];
C[(i*35)+17] += values[42] * B[(i*35)+1];
C[(i*35)+20] += values[43] * B[(i*35)+1];
C[(i*35)+21] += values[44] * B[(i*35)+1];
C[(i*35)+22] += values[45] * B[(i*35)+1];
C[(i*35)+23] += values[46] * B[(i*35)+1];
C[(i*35)+25] += values[47] * B[(i*35)+1];
C[(i*35)+26] += values[48] * B[(i*35)+1];
C[(i*35)+27] += values[49] * B[(i*35)+1];
C[(i*35)+29] += values[50] * B[(i*35)+1];
C[(i*35)+30] += values[51] * B[(i*35)+1];
C[(i*35)+32] += values[52] * B[(i*35)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*35)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*35)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[53]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*35)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*35)+6]);
__m128d a2_2 = _mm_load_sd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*35)+6], c2_2);
__m128d c2_3 = _mm_load_sd(&C[(i*35)+8]);
__m128d a2_3 = _mm_load_sd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, b2));
#endif
_mm_store_sd(&C[(i*35)+8], c2_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_4 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a2_4 = _mm256_loadu_pd(&values[57]);
c2_4 = _mm256_add_pd(c2_4, _mm256_mul_pd(a2_4, b2));
_mm256_storeu_pd(&C[(i*35)+10], c2_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_4 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a2_4 = _mm_loadu_pd(&values[57]);
c2_4 = _mm_add_pd(c2_4, _mm_mul_pd(a2_4, b2));
_mm_storeu_pd(&C[(i*35)+10], c2_4);
__m128d c2_6 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a2_6 = _mm_loadu_pd(&values[59]);
c2_6 = _mm_add_pd(c2_6, _mm_mul_pd(a2_6, b2));
_mm_storeu_pd(&C[(i*35)+12], c2_6);
#endif
__m128d c2_8 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a2_8 = _mm_loadu_pd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_8 = _mm_add_pd(c2_8, _mm_mul_pd(a2_8, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_8 = _mm_add_pd(c2_8, _mm_mul_pd(a2_8, b2));
#endif
_mm_storeu_pd(&C[(i*35)+14], c2_8);
__m128d c2_10 = _mm_load_sd(&C[(i*35)+16]);
__m128d a2_10 = _mm_load_sd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, b2));
#endif
_mm_store_sd(&C[(i*35)+16], c2_10);
__m128d c2_11 = _mm_load_sd(&C[(i*35)+18]);
__m128d a2_11 = _mm_load_sd(&values[64]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, b2));
#endif
_mm_store_sd(&C[(i*35)+18], c2_11);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_12 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a2_12 = _mm256_loadu_pd(&values[65]);
c2_12 = _mm256_add_pd(c2_12, _mm256_mul_pd(a2_12, b2));
_mm256_storeu_pd(&C[(i*35)+20], c2_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_12 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a2_12 = _mm_loadu_pd(&values[65]);
c2_12 = _mm_add_pd(c2_12, _mm_mul_pd(a2_12, b2));
_mm_storeu_pd(&C[(i*35)+20], c2_12);
__m128d c2_14 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a2_14 = _mm_loadu_pd(&values[67]);
c2_14 = _mm_add_pd(c2_14, _mm_mul_pd(a2_14, b2));
_mm_storeu_pd(&C[(i*35)+22], c2_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_16 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a2_16 = _mm256_loadu_pd(&values[69]);
c2_16 = _mm256_add_pd(c2_16, _mm256_mul_pd(a2_16, b2));
_mm256_storeu_pd(&C[(i*35)+24], c2_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_16 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a2_16 = _mm_loadu_pd(&values[69]);
c2_16 = _mm_add_pd(c2_16, _mm_mul_pd(a2_16, b2));
_mm_storeu_pd(&C[(i*35)+24], c2_16);
__m128d c2_18 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a2_18 = _mm_loadu_pd(&values[71]);
c2_18 = _mm_add_pd(c2_18, _mm_mul_pd(a2_18, b2));
_mm_storeu_pd(&C[(i*35)+26], c2_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_20 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a2_20 = _mm256_loadu_pd(&values[73]);
c2_20 = _mm256_add_pd(c2_20, _mm256_mul_pd(a2_20, b2));
_mm256_storeu_pd(&C[(i*35)+28], c2_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_20 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a2_20 = _mm_loadu_pd(&values[73]);
c2_20 = _mm_add_pd(c2_20, _mm_mul_pd(a2_20, b2));
_mm_storeu_pd(&C[(i*35)+28], c2_20);
__m128d c2_22 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a2_22 = _mm_loadu_pd(&values[75]);
c2_22 = _mm_add_pd(c2_22, _mm_mul_pd(a2_22, b2));
_mm_storeu_pd(&C[(i*35)+30], c2_22);
#endif
__m128d c2_24 = _mm_load_sd(&C[(i*35)+33]);
__m128d a2_24 = _mm_load_sd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_24 = _mm_add_sd(c2_24, _mm_mul_sd(a2_24, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_24 = _mm_add_sd(c2_24, _mm_mul_sd(a2_24, b2));
#endif
_mm_store_sd(&C[(i*35)+33], c2_24);
#else
C[(i*35)+4] += values[53] * B[(i*35)+2];
C[(i*35)+5] += values[54] * B[(i*35)+2];
C[(i*35)+6] += values[55] * B[(i*35)+2];
C[(i*35)+8] += values[56] * B[(i*35)+2];
C[(i*35)+10] += values[57] * B[(i*35)+2];
C[(i*35)+11] += values[58] * B[(i*35)+2];
C[(i*35)+12] += values[59] * B[(i*35)+2];
C[(i*35)+13] += values[60] * B[(i*35)+2];
C[(i*35)+14] += values[61] * B[(i*35)+2];
C[(i*35)+15] += values[62] * B[(i*35)+2];
C[(i*35)+16] += values[63] * B[(i*35)+2];
C[(i*35)+18] += values[64] * B[(i*35)+2];
C[(i*35)+20] += values[65] * B[(i*35)+2];
C[(i*35)+21] += values[66] * B[(i*35)+2];
C[(i*35)+22] += values[67] * B[(i*35)+2];
C[(i*35)+23] += values[68] * B[(i*35)+2];
C[(i*35)+24] += values[69] * B[(i*35)+2];
C[(i*35)+25] += values[70] * B[(i*35)+2];
C[(i*35)+26] += values[71] * B[(i*35)+2];
C[(i*35)+27] += values[72] * B[(i*35)+2];
C[(i*35)+28] += values[73] * B[(i*35)+2];
C[(i*35)+29] += values[74] * B[(i*35)+2];
C[(i*35)+30] += values[75] * B[(i*35)+2];
C[(i*35)+31] += values[76] * B[(i*35)+2];
C[(i*35)+33] += values[77] * B[(i*35)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*35)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*35)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[78]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*35)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[78]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*35)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[80]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*35)+6], c3_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_4 = _mm256_loadu_pd(&C[(i*35)+8]);
__m256d a3_4 = _mm256_loadu_pd(&values[82]);
c3_4 = _mm256_add_pd(c3_4, _mm256_mul_pd(a3_4, b3));
_mm256_storeu_pd(&C[(i*35)+8], c3_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a3_4 = _mm_loadu_pd(&values[82]);
c3_4 = _mm_add_pd(c3_4, _mm_mul_pd(a3_4, b3));
_mm_storeu_pd(&C[(i*35)+8], c3_4);
__m128d c3_6 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a3_6 = _mm_loadu_pd(&values[84]);
c3_6 = _mm_add_pd(c3_6, _mm_mul_pd(a3_6, b3));
_mm_storeu_pd(&C[(i*35)+10], c3_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_8 = _mm256_loadu_pd(&C[(i*35)+12]);
__m256d a3_8 = _mm256_loadu_pd(&values[86]);
c3_8 = _mm256_add_pd(c3_8, _mm256_mul_pd(a3_8, b3));
_mm256_storeu_pd(&C[(i*35)+12], c3_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_8 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a3_8 = _mm_loadu_pd(&values[86]);
c3_8 = _mm_add_pd(c3_8, _mm_mul_pd(a3_8, b3));
_mm_storeu_pd(&C[(i*35)+12], c3_8);
__m128d c3_10 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a3_10 = _mm_loadu_pd(&values[88]);
c3_10 = _mm_add_pd(c3_10, _mm_mul_pd(a3_10, b3));
_mm_storeu_pd(&C[(i*35)+14], c3_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_12 = _mm256_loadu_pd(&C[(i*35)+16]);
__m256d a3_12 = _mm256_loadu_pd(&values[90]);
c3_12 = _mm256_add_pd(c3_12, _mm256_mul_pd(a3_12, b3));
_mm256_storeu_pd(&C[(i*35)+16], c3_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_12 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a3_12 = _mm_loadu_pd(&values[90]);
c3_12 = _mm_add_pd(c3_12, _mm_mul_pd(a3_12, b3));
_mm_storeu_pd(&C[(i*35)+16], c3_12);
__m128d c3_14 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a3_14 = _mm_loadu_pd(&values[92]);
c3_14 = _mm_add_pd(c3_14, _mm_mul_pd(a3_14, b3));
_mm_storeu_pd(&C[(i*35)+18], c3_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_16 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a3_16 = _mm256_loadu_pd(&values[94]);
c3_16 = _mm256_add_pd(c3_16, _mm256_mul_pd(a3_16, b3));
_mm256_storeu_pd(&C[(i*35)+20], c3_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_16 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a3_16 = _mm_loadu_pd(&values[94]);
c3_16 = _mm_add_pd(c3_16, _mm_mul_pd(a3_16, b3));
_mm_storeu_pd(&C[(i*35)+20], c3_16);
__m128d c3_18 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a3_18 = _mm_loadu_pd(&values[96]);
c3_18 = _mm_add_pd(c3_18, _mm_mul_pd(a3_18, b3));
_mm_storeu_pd(&C[(i*35)+22], c3_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_20 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a3_20 = _mm256_loadu_pd(&values[98]);
c3_20 = _mm256_add_pd(c3_20, _mm256_mul_pd(a3_20, b3));
_mm256_storeu_pd(&C[(i*35)+24], c3_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_20 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a3_20 = _mm_loadu_pd(&values[98]);
c3_20 = _mm_add_pd(c3_20, _mm_mul_pd(a3_20, b3));
_mm_storeu_pd(&C[(i*35)+24], c3_20);
__m128d c3_22 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a3_22 = _mm_loadu_pd(&values[100]);
c3_22 = _mm_add_pd(c3_22, _mm_mul_pd(a3_22, b3));
_mm_storeu_pd(&C[(i*35)+26], c3_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_24 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a3_24 = _mm256_loadu_pd(&values[102]);
c3_24 = _mm256_add_pd(c3_24, _mm256_mul_pd(a3_24, b3));
_mm256_storeu_pd(&C[(i*35)+28], c3_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_24 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a3_24 = _mm_loadu_pd(&values[102]);
c3_24 = _mm_add_pd(c3_24, _mm_mul_pd(a3_24, b3));
_mm_storeu_pd(&C[(i*35)+28], c3_24);
__m128d c3_26 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a3_26 = _mm_loadu_pd(&values[104]);
c3_26 = _mm_add_pd(c3_26, _mm_mul_pd(a3_26, b3));
_mm_storeu_pd(&C[(i*35)+30], c3_26);
#endif
__m128d c3_28 = _mm_loadu_pd(&C[(i*35)+32]);
__m128d a3_28 = _mm_loadu_pd(&values[106]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_28 = _mm_add_pd(c3_28, _mm_mul_pd(a3_28, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_28 = _mm_add_pd(c3_28, _mm_mul_pd(a3_28, b3));
#endif
_mm_storeu_pd(&C[(i*35)+32], c3_28);
__m128d c3_30 = _mm_load_sd(&C[(i*35)+34]);
__m128d a3_30 = _mm_load_sd(&values[108]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_30 = _mm_add_sd(c3_30, _mm_mul_sd(a3_30, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_30 = _mm_add_sd(c3_30, _mm_mul_sd(a3_30, b3));
#endif
_mm_store_sd(&C[(i*35)+34], c3_30);
#else
C[(i*35)+4] += values[78] * B[(i*35)+3];
C[(i*35)+5] += values[79] * B[(i*35)+3];
C[(i*35)+6] += values[80] * B[(i*35)+3];
C[(i*35)+7] += values[81] * B[(i*35)+3];
C[(i*35)+8] += values[82] * B[(i*35)+3];
C[(i*35)+9] += values[83] * B[(i*35)+3];
C[(i*35)+10] += values[84] * B[(i*35)+3];
C[(i*35)+11] += values[85] * B[(i*35)+3];
C[(i*35)+12] += values[86] * B[(i*35)+3];
C[(i*35)+13] += values[87] * B[(i*35)+3];
C[(i*35)+14] += values[88] * B[(i*35)+3];
C[(i*35)+15] += values[89] * B[(i*35)+3];
C[(i*35)+16] += values[90] * B[(i*35)+3];
C[(i*35)+17] += values[91] * B[(i*35)+3];
C[(i*35)+18] += values[92] * B[(i*35)+3];
C[(i*35)+19] += values[93] * B[(i*35)+3];
C[(i*35)+20] += values[94] * B[(i*35)+3];
C[(i*35)+21] += values[95] * B[(i*35)+3];
C[(i*35)+22] += values[96] * B[(i*35)+3];
C[(i*35)+23] += values[97] * B[(i*35)+3];
C[(i*35)+24] += values[98] * B[(i*35)+3];
C[(i*35)+25] += values[99] * B[(i*35)+3];
C[(i*35)+26] += values[100] * B[(i*35)+3];
C[(i*35)+27] += values[101] * B[(i*35)+3];
C[(i*35)+28] += values[102] * B[(i*35)+3];
C[(i*35)+29] += values[103] * B[(i*35)+3];
C[(i*35)+30] += values[104] * B[(i*35)+3];
C[(i*35)+31] += values[105] * B[(i*35)+3];
C[(i*35)+32] += values[106] * B[(i*35)+3];
C[(i*35)+33] += values[107] * B[(i*35)+3];
C[(i*35)+34] += values[108] * B[(i*35)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*35)+4]);
#endif
__m128d c4_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a4_0 = _mm_loadu_pd(&values[109]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
#endif
_mm_storeu_pd(&C[(i*35)+10], c4_0);
__m128d c4_2 = _mm_load_sd(&C[(i*35)+14]);
__m128d a4_2 = _mm_load_sd(&values[111]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, b4));
#endif
_mm_store_sd(&C[(i*35)+14], c4_2);
__m128d c4_3 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a4_3 = _mm_loadu_pd(&values[112]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_3 = _mm_add_pd(c4_3, _mm_mul_pd(a4_3, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_3 = _mm_add_pd(c4_3, _mm_mul_pd(a4_3, b4));
#endif
_mm_storeu_pd(&C[(i*35)+20], c4_3);
__m128d c4_5 = _mm_load_sd(&C[(i*35)+22]);
__m128d a4_5 = _mm_load_sd(&values[114]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_5 = _mm_add_sd(c4_5, _mm_mul_sd(a4_5, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_5 = _mm_add_sd(c4_5, _mm_mul_sd(a4_5, b4));
#endif
_mm_store_sd(&C[(i*35)+22], c4_5);
__m128d c4_6 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a4_6 = _mm_loadu_pd(&values[115]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_6 = _mm_add_pd(c4_6, _mm_mul_pd(a4_6, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_6 = _mm_add_pd(c4_6, _mm_mul_pd(a4_6, b4));
#endif
_mm_storeu_pd(&C[(i*35)+25], c4_6);
__m128d c4_8 = _mm_load_sd(&C[(i*35)+29]);
__m128d a4_8 = _mm_load_sd(&values[117]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_8 = _mm_add_sd(c4_8, _mm_mul_sd(a4_8, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_8 = _mm_add_sd(c4_8, _mm_mul_sd(a4_8, b4));
#endif
_mm_store_sd(&C[(i*35)+29], c4_8);
#else
C[(i*35)+10] += values[109] * B[(i*35)+4];
C[(i*35)+11] += values[110] * B[(i*35)+4];
C[(i*35)+14] += values[111] * B[(i*35)+4];
C[(i*35)+20] += values[112] * B[(i*35)+4];
C[(i*35)+21] += values[113] * B[(i*35)+4];
C[(i*35)+22] += values[114] * B[(i*35)+4];
C[(i*35)+25] += values[115] * B[(i*35)+4];
C[(i*35)+26] += values[116] * B[(i*35)+4];
C[(i*35)+29] += values[117] * B[(i*35)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*35)+5]);
#endif
__m128d c5_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a5_0 = _mm_loadu_pd(&values[118]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
#endif
_mm_storeu_pd(&C[(i*35)+10], c5_0);
__m128d c5_2 = _mm_load_sd(&C[(i*35)+12]);
__m128d a5_2 = _mm_load_sd(&values[120]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*35)+12], c5_2);
__m128d c5_3 = _mm_load_sd(&C[(i*35)+15]);
__m128d a5_3 = _mm_load_sd(&values[121]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, b5));
#endif
_mm_store_sd(&C[(i*35)+15], c5_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_4 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a5_4 = _mm256_loadu_pd(&values[122]);
c5_4 = _mm256_add_pd(c5_4, _mm256_mul_pd(a5_4, b5));
_mm256_storeu_pd(&C[(i*35)+20], c5_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_4 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a5_4 = _mm_loadu_pd(&values[122]);
c5_4 = _mm_add_pd(c5_4, _mm_mul_pd(a5_4, b5));
_mm_storeu_pd(&C[(i*35)+20], c5_4);
__m128d c5_6 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a5_6 = _mm_loadu_pd(&values[124]);
c5_6 = _mm_add_pd(c5_6, _mm_mul_pd(a5_6, b5));
_mm_storeu_pd(&C[(i*35)+22], c5_6);
#endif
__m128d c5_8 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a5_8 = _mm_loadu_pd(&values[126]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_8 = _mm_add_pd(c5_8, _mm_mul_pd(a5_8, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_8 = _mm_add_pd(c5_8, _mm_mul_pd(a5_8, b5));
#endif
_mm_storeu_pd(&C[(i*35)+25], c5_8);
__m128d c5_10 = _mm_load_sd(&C[(i*35)+27]);
__m128d a5_10 = _mm_load_sd(&values[128]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_10 = _mm_add_sd(c5_10, _mm_mul_sd(a5_10, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_10 = _mm_add_sd(c5_10, _mm_mul_sd(a5_10, b5));
#endif
_mm_store_sd(&C[(i*35)+27], c5_10);
__m128d c5_11 = _mm_load_sd(&C[(i*35)+30]);
__m128d a5_11 = _mm_load_sd(&values[129]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_11 = _mm_add_sd(c5_11, _mm_mul_sd(a5_11, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_11 = _mm_add_sd(c5_11, _mm_mul_sd(a5_11, b5));
#endif
_mm_store_sd(&C[(i*35)+30], c5_11);
#else
C[(i*35)+10] += values[118] * B[(i*35)+5];
C[(i*35)+11] += values[119] * B[(i*35)+5];
C[(i*35)+12] += values[120] * B[(i*35)+5];
C[(i*35)+15] += values[121] * B[(i*35)+5];
C[(i*35)+20] += values[122] * B[(i*35)+5];
C[(i*35)+21] += values[123] * B[(i*35)+5];
C[(i*35)+22] += values[124] * B[(i*35)+5];
C[(i*35)+23] += values[125] * B[(i*35)+5];
C[(i*35)+25] += values[126] * B[(i*35)+5];
C[(i*35)+26] += values[127] * B[(i*35)+5];
C[(i*35)+27] += values[128] * B[(i*35)+5];
C[(i*35)+30] += values[129] * B[(i*35)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*35)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*35)+6]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_0 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a6_0 = _mm256_loadu_pd(&values[130]);
c6_0 = _mm256_add_pd(c6_0, _mm256_mul_pd(a6_0, b6));
_mm256_storeu_pd(&C[(i*35)+10], c6_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a6_0 = _mm_loadu_pd(&values[130]);
c6_0 = _mm_add_pd(c6_0, _mm_mul_pd(a6_0, b6));
_mm_storeu_pd(&C[(i*35)+10], c6_0);
__m128d c6_2 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a6_2 = _mm_loadu_pd(&values[132]);
c6_2 = _mm_add_pd(c6_2, _mm_mul_pd(a6_2, b6));
_mm_storeu_pd(&C[(i*35)+12], c6_2);
#endif
__m128d c6_4 = _mm_load_sd(&C[(i*35)+16]);
__m128d a6_4 = _mm_load_sd(&values[134]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, b6));
#endif
_mm_store_sd(&C[(i*35)+16], c6_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_5 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a6_5 = _mm256_loadu_pd(&values[135]);
c6_5 = _mm256_add_pd(c6_5, _mm256_mul_pd(a6_5, b6));
_mm256_storeu_pd(&C[(i*35)+20], c6_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_5 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a6_5 = _mm_loadu_pd(&values[135]);
c6_5 = _mm_add_pd(c6_5, _mm_mul_pd(a6_5, b6));
_mm_storeu_pd(&C[(i*35)+20], c6_5);
__m128d c6_7 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a6_7 = _mm_loadu_pd(&values[137]);
c6_7 = _mm_add_pd(c6_7, _mm_mul_pd(a6_7, b6));
_mm_storeu_pd(&C[(i*35)+22], c6_7);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_9 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a6_9 = _mm256_loadu_pd(&values[139]);
c6_9 = _mm256_add_pd(c6_9, _mm256_mul_pd(a6_9, b6));
_mm256_storeu_pd(&C[(i*35)+24], c6_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_9 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a6_9 = _mm_loadu_pd(&values[139]);
c6_9 = _mm_add_pd(c6_9, _mm_mul_pd(a6_9, b6));
_mm_storeu_pd(&C[(i*35)+24], c6_9);
__m128d c6_11 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a6_11 = _mm_loadu_pd(&values[141]);
c6_11 = _mm_add_pd(c6_11, _mm_mul_pd(a6_11, b6));
_mm_storeu_pd(&C[(i*35)+26], c6_11);
#endif
__m128d c6_13 = _mm_load_sd(&C[(i*35)+28]);
__m128d a6_13 = _mm_load_sd(&values[143]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_13 = _mm_add_sd(c6_13, _mm_mul_sd(a6_13, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_13 = _mm_add_sd(c6_13, _mm_mul_sd(a6_13, b6));
#endif
_mm_store_sd(&C[(i*35)+28], c6_13);
__m128d c6_14 = _mm_load_sd(&C[(i*35)+31]);
__m128d a6_14 = _mm_load_sd(&values[144]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_14 = _mm_add_sd(c6_14, _mm_mul_sd(a6_14, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_14 = _mm_add_sd(c6_14, _mm_mul_sd(a6_14, b6));
#endif
_mm_store_sd(&C[(i*35)+31], c6_14);
#else
C[(i*35)+10] += values[130] * B[(i*35)+6];
C[(i*35)+11] += values[131] * B[(i*35)+6];
C[(i*35)+12] += values[132] * B[(i*35)+6];
C[(i*35)+13] += values[133] * B[(i*35)+6];
C[(i*35)+16] += values[134] * B[(i*35)+6];
C[(i*35)+20] += values[135] * B[(i*35)+6];
C[(i*35)+21] += values[136] * B[(i*35)+6];
C[(i*35)+22] += values[137] * B[(i*35)+6];
C[(i*35)+23] += values[138] * B[(i*35)+6];
C[(i*35)+24] += values[139] * B[(i*35)+6];
C[(i*35)+25] += values[140] * B[(i*35)+6];
C[(i*35)+26] += values[141] * B[(i*35)+6];
C[(i*35)+27] += values[142] * B[(i*35)+6];
C[(i*35)+28] += values[143] * B[(i*35)+6];
C[(i*35)+31] += values[144] * B[(i*35)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*35)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*35)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a7_0 = _mm_loadu_pd(&values[145]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*35)+10], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*35)+12]);
__m128d a7_2 = _mm_load_sd(&values[147]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*35)+12], c7_2);
__m128d c7_3 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a7_3 = _mm_loadu_pd(&values[148]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, b7));
#endif
_mm_storeu_pd(&C[(i*35)+14], c7_3);
__m128d c7_5 = _mm_load_sd(&C[(i*35)+17]);
__m128d a7_5 = _mm_load_sd(&values[150]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, b7));
#endif
_mm_store_sd(&C[(i*35)+17], c7_5);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_6 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a7_6 = _mm256_loadu_pd(&values[151]);
c7_6 = _mm256_add_pd(c7_6, _mm256_mul_pd(a7_6, b7));
_mm256_storeu_pd(&C[(i*35)+20], c7_6);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_6 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a7_6 = _mm_loadu_pd(&values[151]);
c7_6 = _mm_add_pd(c7_6, _mm_mul_pd(a7_6, b7));
_mm_storeu_pd(&C[(i*35)+20], c7_6);
__m128d c7_8 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a7_8 = _mm_loadu_pd(&values[153]);
c7_8 = _mm_add_pd(c7_8, _mm_mul_pd(a7_8, b7));
_mm_storeu_pd(&C[(i*35)+22], c7_8);
#endif
__m128d c7_10 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a7_10 = _mm_loadu_pd(&values[155]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_10 = _mm_add_pd(c7_10, _mm_mul_pd(a7_10, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_10 = _mm_add_pd(c7_10, _mm_mul_pd(a7_10, b7));
#endif
_mm_storeu_pd(&C[(i*35)+25], c7_10);
__m128d c7_12 = _mm_load_sd(&C[(i*35)+27]);
__m128d a7_12 = _mm_load_sd(&values[157]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_12 = _mm_add_sd(c7_12, _mm_mul_sd(a7_12, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_12 = _mm_add_sd(c7_12, _mm_mul_sd(a7_12, b7));
#endif
_mm_store_sd(&C[(i*35)+27], c7_12);
__m128d c7_13 = _mm_loadu_pd(&C[(i*35)+29]);
__m128d a7_13 = _mm_loadu_pd(&values[158]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_13 = _mm_add_pd(c7_13, _mm_mul_pd(a7_13, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_13 = _mm_add_pd(c7_13, _mm_mul_pd(a7_13, b7));
#endif
_mm_storeu_pd(&C[(i*35)+29], c7_13);
__m128d c7_15 = _mm_load_sd(&C[(i*35)+32]);
__m128d a7_15 = _mm_load_sd(&values[160]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_15 = _mm_add_sd(c7_15, _mm_mul_sd(a7_15, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_15 = _mm_add_sd(c7_15, _mm_mul_sd(a7_15, b7));
#endif
_mm_store_sd(&C[(i*35)+32], c7_15);
#else
C[(i*35)+10] += values[145] * B[(i*35)+7];
C[(i*35)+11] += values[146] * B[(i*35)+7];
C[(i*35)+12] += values[147] * B[(i*35)+7];
C[(i*35)+14] += values[148] * B[(i*35)+7];
C[(i*35)+15] += values[149] * B[(i*35)+7];
C[(i*35)+17] += values[150] * B[(i*35)+7];
C[(i*35)+20] += values[151] * B[(i*35)+7];
C[(i*35)+21] += values[152] * B[(i*35)+7];
C[(i*35)+22] += values[153] * B[(i*35)+7];
C[(i*35)+23] += values[154] * B[(i*35)+7];
C[(i*35)+25] += values[155] * B[(i*35)+7];
C[(i*35)+26] += values[156] * B[(i*35)+7];
C[(i*35)+27] += values[157] * B[(i*35)+7];
C[(i*35)+29] += values[158] * B[(i*35)+7];
C[(i*35)+30] += values[159] * B[(i*35)+7];
C[(i*35)+32] += values[160] * B[(i*35)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*35)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*35)+8]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_0 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a8_0 = _mm256_loadu_pd(&values[161]);
c8_0 = _mm256_add_pd(c8_0, _mm256_mul_pd(a8_0, b8));
_mm256_storeu_pd(&C[(i*35)+10], c8_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a8_0 = _mm_loadu_pd(&values[161]);
c8_0 = _mm_add_pd(c8_0, _mm_mul_pd(a8_0, b8));
_mm_storeu_pd(&C[(i*35)+10], c8_0);
__m128d c8_2 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a8_2 = _mm_loadu_pd(&values[163]);
c8_2 = _mm_add_pd(c8_2, _mm_mul_pd(a8_2, b8));
_mm_storeu_pd(&C[(i*35)+12], c8_2);
#endif
__m128d c8_4 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a8_4 = _mm_loadu_pd(&values[165]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, b8));
#endif
_mm_storeu_pd(&C[(i*35)+14], c8_4);
__m128d c8_6 = _mm_load_sd(&C[(i*35)+16]);
__m128d a8_6 = _mm_load_sd(&values[167]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*35)+16], c8_6);
__m128d c8_7 = _mm_load_sd(&C[(i*35)+18]);
__m128d a8_7 = _mm_load_sd(&values[168]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, b8));
#endif
_mm_store_sd(&C[(i*35)+18], c8_7);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_8 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a8_8 = _mm256_loadu_pd(&values[169]);
c8_8 = _mm256_add_pd(c8_8, _mm256_mul_pd(a8_8, b8));
_mm256_storeu_pd(&C[(i*35)+20], c8_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_8 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a8_8 = _mm_loadu_pd(&values[169]);
c8_8 = _mm_add_pd(c8_8, _mm_mul_pd(a8_8, b8));
_mm_storeu_pd(&C[(i*35)+20], c8_8);
__m128d c8_10 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a8_10 = _mm_loadu_pd(&values[171]);
c8_10 = _mm_add_pd(c8_10, _mm_mul_pd(a8_10, b8));
_mm_storeu_pd(&C[(i*35)+22], c8_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_12 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a8_12 = _mm256_loadu_pd(&values[173]);
c8_12 = _mm256_add_pd(c8_12, _mm256_mul_pd(a8_12, b8));
_mm256_storeu_pd(&C[(i*35)+24], c8_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_12 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a8_12 = _mm_loadu_pd(&values[173]);
c8_12 = _mm_add_pd(c8_12, _mm_mul_pd(a8_12, b8));
_mm_storeu_pd(&C[(i*35)+24], c8_12);
__m128d c8_14 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a8_14 = _mm_loadu_pd(&values[175]);
c8_14 = _mm_add_pd(c8_14, _mm_mul_pd(a8_14, b8));
_mm_storeu_pd(&C[(i*35)+26], c8_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_16 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a8_16 = _mm256_loadu_pd(&values[177]);
c8_16 = _mm256_add_pd(c8_16, _mm256_mul_pd(a8_16, b8));
_mm256_storeu_pd(&C[(i*35)+28], c8_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_16 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a8_16 = _mm_loadu_pd(&values[177]);
c8_16 = _mm_add_pd(c8_16, _mm_mul_pd(a8_16, b8));
_mm_storeu_pd(&C[(i*35)+28], c8_16);
__m128d c8_18 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a8_18 = _mm_loadu_pd(&values[179]);
c8_18 = _mm_add_pd(c8_18, _mm_mul_pd(a8_18, b8));
_mm_storeu_pd(&C[(i*35)+30], c8_18);
#endif
__m128d c8_20 = _mm_load_sd(&C[(i*35)+33]);
__m128d a8_20 = _mm_load_sd(&values[181]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_20 = _mm_add_sd(c8_20, _mm_mul_sd(a8_20, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_20 = _mm_add_sd(c8_20, _mm_mul_sd(a8_20, b8));
#endif
_mm_store_sd(&C[(i*35)+33], c8_20);
#else
C[(i*35)+10] += values[161] * B[(i*35)+8];
C[(i*35)+11] += values[162] * B[(i*35)+8];
C[(i*35)+12] += values[163] * B[(i*35)+8];
C[(i*35)+13] += values[164] * B[(i*35)+8];
C[(i*35)+14] += values[165] * B[(i*35)+8];
C[(i*35)+15] += values[166] * B[(i*35)+8];
C[(i*35)+16] += values[167] * B[(i*35)+8];
C[(i*35)+18] += values[168] * B[(i*35)+8];
C[(i*35)+20] += values[169] * B[(i*35)+8];
C[(i*35)+21] += values[170] * B[(i*35)+8];
C[(i*35)+22] += values[171] * B[(i*35)+8];
C[(i*35)+23] += values[172] * B[(i*35)+8];
C[(i*35)+24] += values[173] * B[(i*35)+8];
C[(i*35)+25] += values[174] * B[(i*35)+8];
C[(i*35)+26] += values[175] * B[(i*35)+8];
C[(i*35)+27] += values[176] * B[(i*35)+8];
C[(i*35)+28] += values[177] * B[(i*35)+8];
C[(i*35)+29] += values[178] * B[(i*35)+8];
C[(i*35)+30] += values[179] * B[(i*35)+8];
C[(i*35)+31] += values[180] * B[(i*35)+8];
C[(i*35)+33] += values[181] * B[(i*35)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*35)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*35)+9]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_0 = _mm256_loadu_pd(&C[(i*35)+10]);
__m256d a9_0 = _mm256_loadu_pd(&values[182]);
c9_0 = _mm256_add_pd(c9_0, _mm256_mul_pd(a9_0, b9));
_mm256_storeu_pd(&C[(i*35)+10], c9_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_0 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a9_0 = _mm_loadu_pd(&values[182]);
c9_0 = _mm_add_pd(c9_0, _mm_mul_pd(a9_0, b9));
_mm_storeu_pd(&C[(i*35)+10], c9_0);
__m128d c9_2 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a9_2 = _mm_loadu_pd(&values[184]);
c9_2 = _mm_add_pd(c9_2, _mm_mul_pd(a9_2, b9));
_mm_storeu_pd(&C[(i*35)+12], c9_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_4 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a9_4 = _mm256_loadu_pd(&values[186]);
c9_4 = _mm256_add_pd(c9_4, _mm256_mul_pd(a9_4, b9));
_mm256_storeu_pd(&C[(i*35)+14], c9_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_4 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a9_4 = _mm_loadu_pd(&values[186]);
c9_4 = _mm_add_pd(c9_4, _mm_mul_pd(a9_4, b9));
_mm_storeu_pd(&C[(i*35)+14], c9_4);
__m128d c9_6 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a9_6 = _mm_loadu_pd(&values[188]);
c9_6 = _mm_add_pd(c9_6, _mm_mul_pd(a9_6, b9));
_mm_storeu_pd(&C[(i*35)+16], c9_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_8 = _mm256_loadu_pd(&C[(i*35)+18]);
__m256d a9_8 = _mm256_loadu_pd(&values[190]);
c9_8 = _mm256_add_pd(c9_8, _mm256_mul_pd(a9_8, b9));
_mm256_storeu_pd(&C[(i*35)+18], c9_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_8 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a9_8 = _mm_loadu_pd(&values[190]);
c9_8 = _mm_add_pd(c9_8, _mm_mul_pd(a9_8, b9));
_mm_storeu_pd(&C[(i*35)+18], c9_8);
__m128d c9_10 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a9_10 = _mm_loadu_pd(&values[192]);
c9_10 = _mm_add_pd(c9_10, _mm_mul_pd(a9_10, b9));
_mm_storeu_pd(&C[(i*35)+20], c9_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_12 = _mm256_loadu_pd(&C[(i*35)+22]);
__m256d a9_12 = _mm256_loadu_pd(&values[194]);
c9_12 = _mm256_add_pd(c9_12, _mm256_mul_pd(a9_12, b9));
_mm256_storeu_pd(&C[(i*35)+22], c9_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_12 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a9_12 = _mm_loadu_pd(&values[194]);
c9_12 = _mm_add_pd(c9_12, _mm_mul_pd(a9_12, b9));
_mm_storeu_pd(&C[(i*35)+22], c9_12);
__m128d c9_14 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a9_14 = _mm_loadu_pd(&values[196]);
c9_14 = _mm_add_pd(c9_14, _mm_mul_pd(a9_14, b9));
_mm_storeu_pd(&C[(i*35)+24], c9_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_16 = _mm256_loadu_pd(&C[(i*35)+26]);
__m256d a9_16 = _mm256_loadu_pd(&values[198]);
c9_16 = _mm256_add_pd(c9_16, _mm256_mul_pd(a9_16, b9));
_mm256_storeu_pd(&C[(i*35)+26], c9_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_16 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a9_16 = _mm_loadu_pd(&values[198]);
c9_16 = _mm_add_pd(c9_16, _mm_mul_pd(a9_16, b9));
_mm_storeu_pd(&C[(i*35)+26], c9_16);
__m128d c9_18 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a9_18 = _mm_loadu_pd(&values[200]);
c9_18 = _mm_add_pd(c9_18, _mm_mul_pd(a9_18, b9));
_mm_storeu_pd(&C[(i*35)+28], c9_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_20 = _mm256_loadu_pd(&C[(i*35)+30]);
__m256d a9_20 = _mm256_loadu_pd(&values[202]);
c9_20 = _mm256_add_pd(c9_20, _mm256_mul_pd(a9_20, b9));
_mm256_storeu_pd(&C[(i*35)+30], c9_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_20 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a9_20 = _mm_loadu_pd(&values[202]);
c9_20 = _mm_add_pd(c9_20, _mm_mul_pd(a9_20, b9));
_mm_storeu_pd(&C[(i*35)+30], c9_20);
__m128d c9_22 = _mm_loadu_pd(&C[(i*35)+32]);
__m128d a9_22 = _mm_loadu_pd(&values[204]);
c9_22 = _mm_add_pd(c9_22, _mm_mul_pd(a9_22, b9));
_mm_storeu_pd(&C[(i*35)+32], c9_22);
#endif
__m128d c9_24 = _mm_load_sd(&C[(i*35)+34]);
__m128d a9_24 = _mm_load_sd(&values[206]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_24 = _mm_add_sd(c9_24, _mm_mul_sd(a9_24, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_24 = _mm_add_sd(c9_24, _mm_mul_sd(a9_24, b9));
#endif
_mm_store_sd(&C[(i*35)+34], c9_24);
#else
C[(i*35)+10] += values[182] * B[(i*35)+9];
C[(i*35)+11] += values[183] * B[(i*35)+9];
C[(i*35)+12] += values[184] * B[(i*35)+9];
C[(i*35)+13] += values[185] * B[(i*35)+9];
C[(i*35)+14] += values[186] * B[(i*35)+9];
C[(i*35)+15] += values[187] * B[(i*35)+9];
C[(i*35)+16] += values[188] * B[(i*35)+9];
C[(i*35)+17] += values[189] * B[(i*35)+9];
C[(i*35)+18] += values[190] * B[(i*35)+9];
C[(i*35)+19] += values[191] * B[(i*35)+9];
C[(i*35)+20] += values[192] * B[(i*35)+9];
C[(i*35)+21] += values[193] * B[(i*35)+9];
C[(i*35)+22] += values[194] * B[(i*35)+9];
C[(i*35)+23] += values[195] * B[(i*35)+9];
C[(i*35)+24] += values[196] * B[(i*35)+9];
C[(i*35)+25] += values[197] * B[(i*35)+9];
C[(i*35)+26] += values[198] * B[(i*35)+9];
C[(i*35)+27] += values[199] * B[(i*35)+9];
C[(i*35)+28] += values[200] * B[(i*35)+9];
C[(i*35)+29] += values[201] * B[(i*35)+9];
C[(i*35)+30] += values[202] * B[(i*35)+9];
C[(i*35)+31] += values[203] * B[(i*35)+9];
C[(i*35)+32] += values[204] * B[(i*35)+9];
C[(i*35)+33] += values[205] * B[(i*35)+9];
C[(i*35)+34] += values[206] * B[(i*35)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*35)+10]);
#endif
__m128d c10_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a10_0 = _mm_loadu_pd(&values[207]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
#endif
_mm_storeu_pd(&C[(i*35)+20], c10_0);
__m128d c10_2 = _mm_load_sd(&C[(i*35)+25]);
__m128d a10_2 = _mm_load_sd(&values[209]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_2 = _mm_add_sd(c10_2, _mm_mul_sd(a10_2, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_2 = _mm_add_sd(c10_2, _mm_mul_sd(a10_2, b10));
#endif
_mm_store_sd(&C[(i*35)+25], c10_2);
#else
C[(i*35)+20] += values[207] * B[(i*35)+10];
C[(i*35)+21] += values[208] * B[(i*35)+10];
C[(i*35)+25] += values[209] * B[(i*35)+10];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*35)+11]);
#endif
__m128d c11_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a11_0 = _mm_loadu_pd(&values[210]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
#endif
_mm_storeu_pd(&C[(i*35)+20], c11_0);
__m128d c11_2 = _mm_load_sd(&C[(i*35)+22]);
__m128d a11_2 = _mm_load_sd(&values[212]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*35)+22], c11_2);
__m128d c11_3 = _mm_load_sd(&C[(i*35)+26]);
__m128d a11_3 = _mm_load_sd(&values[213]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_3 = _mm_add_sd(c11_3, _mm_mul_sd(a11_3, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_3 = _mm_add_sd(c11_3, _mm_mul_sd(a11_3, b11));
#endif
_mm_store_sd(&C[(i*35)+26], c11_3);
#else
C[(i*35)+20] += values[210] * B[(i*35)+11];
C[(i*35)+21] += values[211] * B[(i*35)+11];
C[(i*35)+22] += values[212] * B[(i*35)+11];
C[(i*35)+26] += values[213] * B[(i*35)+11];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a12_0 = _mm256_loadu_pd(&values[214]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*35)+20], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a12_0 = _mm_loadu_pd(&values[214]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*35)+20], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a12_2 = _mm_loadu_pd(&values[216]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*35)+22], c12_2);
#endif
__m128d c12_4 = _mm_load_sd(&C[(i*35)+27]);
__m128d a12_4 = _mm_load_sd(&values[218]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_4 = _mm_add_sd(c12_4, _mm_mul_sd(a12_4, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_4 = _mm_add_sd(c12_4, _mm_mul_sd(a12_4, b12));
#endif
_mm_store_sd(&C[(i*35)+27], c12_4);
#else
C[(i*35)+20] += values[214] * B[(i*35)+12];
C[(i*35)+21] += values[215] * B[(i*35)+12];
C[(i*35)+22] += values[216] * B[(i*35)+12];
C[(i*35)+23] += values[217] * B[(i*35)+12];
C[(i*35)+27] += values[218] * B[(i*35)+12];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*35)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*35)+13]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a13_0 = _mm256_loadu_pd(&values[219]);
c13_0 = _mm256_add_pd(c13_0, _mm256_mul_pd(a13_0, b13));
_mm256_storeu_pd(&C[(i*35)+20], c13_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a13_0 = _mm_loadu_pd(&values[219]);
c13_0 = _mm_add_pd(c13_0, _mm_mul_pd(a13_0, b13));
_mm_storeu_pd(&C[(i*35)+20], c13_0);
__m128d c13_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a13_2 = _mm_loadu_pd(&values[221]);
c13_2 = _mm_add_pd(c13_2, _mm_mul_pd(a13_2, b13));
_mm_storeu_pd(&C[(i*35)+22], c13_2);
#endif
__m128d c13_4 = _mm_load_sd(&C[(i*35)+24]);
__m128d a13_4 = _mm_load_sd(&values[223]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, b13));
#endif
_mm_store_sd(&C[(i*35)+24], c13_4);
__m128d c13_5 = _mm_load_sd(&C[(i*35)+28]);
__m128d a13_5 = _mm_load_sd(&values[224]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_5 = _mm_add_sd(c13_5, _mm_mul_sd(a13_5, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_5 = _mm_add_sd(c13_5, _mm_mul_sd(a13_5, b13));
#endif
_mm_store_sd(&C[(i*35)+28], c13_5);
#else
C[(i*35)+20] += values[219] * B[(i*35)+13];
C[(i*35)+21] += values[220] * B[(i*35)+13];
C[(i*35)+22] += values[221] * B[(i*35)+13];
C[(i*35)+23] += values[222] * B[(i*35)+13];
C[(i*35)+24] += values[223] * B[(i*35)+13];
C[(i*35)+28] += values[224] * B[(i*35)+13];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*35)+14]);
#endif
__m128d c14_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a14_0 = _mm_loadu_pd(&values[225]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
#endif
_mm_storeu_pd(&C[(i*35)+20], c14_0);
__m128d c14_2 = _mm_load_sd(&C[(i*35)+22]);
__m128d a14_2 = _mm_load_sd(&values[227]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, b14));
#endif
_mm_store_sd(&C[(i*35)+22], c14_2);
__m128d c14_3 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a14_3 = _mm_loadu_pd(&values[228]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, b14));
#endif
_mm_storeu_pd(&C[(i*35)+25], c14_3);
__m128d c14_5 = _mm_load_sd(&C[(i*35)+29]);
__m128d a14_5 = _mm_load_sd(&values[230]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_5 = _mm_add_sd(c14_5, _mm_mul_sd(a14_5, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_5 = _mm_add_sd(c14_5, _mm_mul_sd(a14_5, b14));
#endif
_mm_store_sd(&C[(i*35)+29], c14_5);
#else
C[(i*35)+20] += values[225] * B[(i*35)+14];
C[(i*35)+21] += values[226] * B[(i*35)+14];
C[(i*35)+22] += values[227] * B[(i*35)+14];
C[(i*35)+25] += values[228] * B[(i*35)+14];
C[(i*35)+26] += values[229] * B[(i*35)+14];
C[(i*35)+29] += values[230] * B[(i*35)+14];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a15_0 = _mm256_loadu_pd(&values[231]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*35)+20], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a15_0 = _mm_loadu_pd(&values[231]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*35)+20], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a15_2 = _mm_loadu_pd(&values[233]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*35)+22], c15_2);
#endif
__m128d c15_4 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a15_4 = _mm_loadu_pd(&values[235]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, b15));
#endif
_mm_storeu_pd(&C[(i*35)+25], c15_4);
__m128d c15_6 = _mm_load_sd(&C[(i*35)+27]);
__m128d a15_6 = _mm_load_sd(&values[237]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*35)+27], c15_6);
__m128d c15_7 = _mm_load_sd(&C[(i*35)+30]);
__m128d a15_7 = _mm_load_sd(&values[238]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, b15));
#endif
_mm_store_sd(&C[(i*35)+30], c15_7);
#else
C[(i*35)+20] += values[231] * B[(i*35)+15];
C[(i*35)+21] += values[232] * B[(i*35)+15];
C[(i*35)+22] += values[233] * B[(i*35)+15];
C[(i*35)+23] += values[234] * B[(i*35)+15];
C[(i*35)+25] += values[235] * B[(i*35)+15];
C[(i*35)+26] += values[236] * B[(i*35)+15];
C[(i*35)+27] += values[237] * B[(i*35)+15];
C[(i*35)+30] += values[238] * B[(i*35)+15];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*35)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*35)+16]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a16_0 = _mm256_loadu_pd(&values[239]);
c16_0 = _mm256_add_pd(c16_0, _mm256_mul_pd(a16_0, b16));
_mm256_storeu_pd(&C[(i*35)+20], c16_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a16_0 = _mm_loadu_pd(&values[239]);
c16_0 = _mm_add_pd(c16_0, _mm_mul_pd(a16_0, b16));
_mm_storeu_pd(&C[(i*35)+20], c16_0);
__m128d c16_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a16_2 = _mm_loadu_pd(&values[241]);
c16_2 = _mm_add_pd(c16_2, _mm_mul_pd(a16_2, b16));
_mm_storeu_pd(&C[(i*35)+22], c16_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_4 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a16_4 = _mm256_loadu_pd(&values[243]);
c16_4 = _mm256_add_pd(c16_4, _mm256_mul_pd(a16_4, b16));
_mm256_storeu_pd(&C[(i*35)+24], c16_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_4 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a16_4 = _mm_loadu_pd(&values[243]);
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, b16));
_mm_storeu_pd(&C[(i*35)+24], c16_4);
__m128d c16_6 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a16_6 = _mm_loadu_pd(&values[245]);
c16_6 = _mm_add_pd(c16_6, _mm_mul_pd(a16_6, b16));
_mm_storeu_pd(&C[(i*35)+26], c16_6);
#endif
__m128d c16_8 = _mm_load_sd(&C[(i*35)+28]);
__m128d a16_8 = _mm_load_sd(&values[247]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, b16));
#endif
_mm_store_sd(&C[(i*35)+28], c16_8);
__m128d c16_9 = _mm_load_sd(&C[(i*35)+31]);
__m128d a16_9 = _mm_load_sd(&values[248]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_9 = _mm_add_sd(c16_9, _mm_mul_sd(a16_9, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_9 = _mm_add_sd(c16_9, _mm_mul_sd(a16_9, b16));
#endif
_mm_store_sd(&C[(i*35)+31], c16_9);
#else
C[(i*35)+20] += values[239] * B[(i*35)+16];
C[(i*35)+21] += values[240] * B[(i*35)+16];
C[(i*35)+22] += values[241] * B[(i*35)+16];
C[(i*35)+23] += values[242] * B[(i*35)+16];
C[(i*35)+24] += values[243] * B[(i*35)+16];
C[(i*35)+25] += values[244] * B[(i*35)+16];
C[(i*35)+26] += values[245] * B[(i*35)+16];
C[(i*35)+27] += values[246] * B[(i*35)+16];
C[(i*35)+28] += values[247] * B[(i*35)+16];
C[(i*35)+31] += values[248] * B[(i*35)+16];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a17_0 = _mm256_loadu_pd(&values[249]);
c17_0 = _mm256_add_pd(c17_0, _mm256_mul_pd(a17_0, b17));
_mm256_storeu_pd(&C[(i*35)+20], c17_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a17_0 = _mm_loadu_pd(&values[249]);
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, b17));
_mm_storeu_pd(&C[(i*35)+20], c17_0);
__m128d c17_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a17_2 = _mm_loadu_pd(&values[251]);
c17_2 = _mm_add_pd(c17_2, _mm_mul_pd(a17_2, b17));
_mm_storeu_pd(&C[(i*35)+22], c17_2);
#endif
__m128d c17_4 = _mm_loadu_pd(&C[(i*35)+25]);
__m128d a17_4 = _mm_loadu_pd(&values[253]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, b17));
#endif
_mm_storeu_pd(&C[(i*35)+25], c17_4);
__m128d c17_6 = _mm_load_sd(&C[(i*35)+27]);
__m128d a17_6 = _mm_load_sd(&values[255]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, b17));
#endif
_mm_store_sd(&C[(i*35)+27], c17_6);
__m128d c17_7 = _mm_loadu_pd(&C[(i*35)+29]);
__m128d a17_7 = _mm_loadu_pd(&values[256]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, b17));
#endif
_mm_storeu_pd(&C[(i*35)+29], c17_7);
__m128d c17_9 = _mm_load_sd(&C[(i*35)+32]);
__m128d a17_9 = _mm_load_sd(&values[258]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_9 = _mm_add_sd(c17_9, _mm_mul_sd(a17_9, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_9 = _mm_add_sd(c17_9, _mm_mul_sd(a17_9, b17));
#endif
_mm_store_sd(&C[(i*35)+32], c17_9);
#else
C[(i*35)+20] += values[249] * B[(i*35)+17];
C[(i*35)+21] += values[250] * B[(i*35)+17];
C[(i*35)+22] += values[251] * B[(i*35)+17];
C[(i*35)+23] += values[252] * B[(i*35)+17];
C[(i*35)+25] += values[253] * B[(i*35)+17];
C[(i*35)+26] += values[254] * B[(i*35)+17];
C[(i*35)+27] += values[255] * B[(i*35)+17];
C[(i*35)+29] += values[256] * B[(i*35)+17];
C[(i*35)+30] += values[257] * B[(i*35)+17];
C[(i*35)+32] += values[258] * B[(i*35)+17];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*35)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*35)+18]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a18_0 = _mm256_loadu_pd(&values[259]);
c18_0 = _mm256_add_pd(c18_0, _mm256_mul_pd(a18_0, b18));
_mm256_storeu_pd(&C[(i*35)+20], c18_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a18_0 = _mm_loadu_pd(&values[259]);
c18_0 = _mm_add_pd(c18_0, _mm_mul_pd(a18_0, b18));
_mm_storeu_pd(&C[(i*35)+20], c18_0);
__m128d c18_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a18_2 = _mm_loadu_pd(&values[261]);
c18_2 = _mm_add_pd(c18_2, _mm_mul_pd(a18_2, b18));
_mm_storeu_pd(&C[(i*35)+22], c18_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_4 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a18_4 = _mm256_loadu_pd(&values[263]);
c18_4 = _mm256_add_pd(c18_4, _mm256_mul_pd(a18_4, b18));
_mm256_storeu_pd(&C[(i*35)+24], c18_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_4 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a18_4 = _mm_loadu_pd(&values[263]);
c18_4 = _mm_add_pd(c18_4, _mm_mul_pd(a18_4, b18));
_mm_storeu_pd(&C[(i*35)+24], c18_4);
__m128d c18_6 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a18_6 = _mm_loadu_pd(&values[265]);
c18_6 = _mm_add_pd(c18_6, _mm_mul_pd(a18_6, b18));
_mm_storeu_pd(&C[(i*35)+26], c18_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_8 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a18_8 = _mm256_loadu_pd(&values[267]);
c18_8 = _mm256_add_pd(c18_8, _mm256_mul_pd(a18_8, b18));
_mm256_storeu_pd(&C[(i*35)+28], c18_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_8 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a18_8 = _mm_loadu_pd(&values[267]);
c18_8 = _mm_add_pd(c18_8, _mm_mul_pd(a18_8, b18));
_mm_storeu_pd(&C[(i*35)+28], c18_8);
__m128d c18_10 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a18_10 = _mm_loadu_pd(&values[269]);
c18_10 = _mm_add_pd(c18_10, _mm_mul_pd(a18_10, b18));
_mm_storeu_pd(&C[(i*35)+30], c18_10);
#endif
__m128d c18_12 = _mm_load_sd(&C[(i*35)+33]);
__m128d a18_12 = _mm_load_sd(&values[271]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_12 = _mm_add_sd(c18_12, _mm_mul_sd(a18_12, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_12 = _mm_add_sd(c18_12, _mm_mul_sd(a18_12, b18));
#endif
_mm_store_sd(&C[(i*35)+33], c18_12);
#else
C[(i*35)+20] += values[259] * B[(i*35)+18];
C[(i*35)+21] += values[260] * B[(i*35)+18];
C[(i*35)+22] += values[261] * B[(i*35)+18];
C[(i*35)+23] += values[262] * B[(i*35)+18];
C[(i*35)+24] += values[263] * B[(i*35)+18];
C[(i*35)+25] += values[264] * B[(i*35)+18];
C[(i*35)+26] += values[265] * B[(i*35)+18];
C[(i*35)+27] += values[266] * B[(i*35)+18];
C[(i*35)+28] += values[267] * B[(i*35)+18];
C[(i*35)+29] += values[268] * B[(i*35)+18];
C[(i*35)+30] += values[269] * B[(i*35)+18];
C[(i*35)+31] += values[270] * B[(i*35)+18];
C[(i*35)+33] += values[271] * B[(i*35)+18];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*35)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*35)+19]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_0 = _mm256_loadu_pd(&C[(i*35)+20]);
__m256d a19_0 = _mm256_loadu_pd(&values[272]);
c19_0 = _mm256_add_pd(c19_0, _mm256_mul_pd(a19_0, b19));
_mm256_storeu_pd(&C[(i*35)+20], c19_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_0 = _mm_loadu_pd(&C[(i*35)+20]);
__m128d a19_0 = _mm_loadu_pd(&values[272]);
c19_0 = _mm_add_pd(c19_0, _mm_mul_pd(a19_0, b19));
_mm_storeu_pd(&C[(i*35)+20], c19_0);
__m128d c19_2 = _mm_loadu_pd(&C[(i*35)+22]);
__m128d a19_2 = _mm_loadu_pd(&values[274]);
c19_2 = _mm_add_pd(c19_2, _mm_mul_pd(a19_2, b19));
_mm_storeu_pd(&C[(i*35)+22], c19_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_4 = _mm256_loadu_pd(&C[(i*35)+24]);
__m256d a19_4 = _mm256_loadu_pd(&values[276]);
c19_4 = _mm256_add_pd(c19_4, _mm256_mul_pd(a19_4, b19));
_mm256_storeu_pd(&C[(i*35)+24], c19_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_4 = _mm_loadu_pd(&C[(i*35)+24]);
__m128d a19_4 = _mm_loadu_pd(&values[276]);
c19_4 = _mm_add_pd(c19_4, _mm_mul_pd(a19_4, b19));
_mm_storeu_pd(&C[(i*35)+24], c19_4);
__m128d c19_6 = _mm_loadu_pd(&C[(i*35)+26]);
__m128d a19_6 = _mm_loadu_pd(&values[278]);
c19_6 = _mm_add_pd(c19_6, _mm_mul_pd(a19_6, b19));
_mm_storeu_pd(&C[(i*35)+26], c19_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_8 = _mm256_loadu_pd(&C[(i*35)+28]);
__m256d a19_8 = _mm256_loadu_pd(&values[280]);
c19_8 = _mm256_add_pd(c19_8, _mm256_mul_pd(a19_8, b19));
_mm256_storeu_pd(&C[(i*35)+28], c19_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_8 = _mm_loadu_pd(&C[(i*35)+28]);
__m128d a19_8 = _mm_loadu_pd(&values[280]);
c19_8 = _mm_add_pd(c19_8, _mm_mul_pd(a19_8, b19));
_mm_storeu_pd(&C[(i*35)+28], c19_8);
__m128d c19_10 = _mm_loadu_pd(&C[(i*35)+30]);
__m128d a19_10 = _mm_loadu_pd(&values[282]);
c19_10 = _mm_add_pd(c19_10, _mm_mul_pd(a19_10, b19));
_mm_storeu_pd(&C[(i*35)+30], c19_10);
#endif
__m128d c19_12 = _mm_loadu_pd(&C[(i*35)+32]);
__m128d a19_12 = _mm_loadu_pd(&values[284]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_12 = _mm_add_pd(c19_12, _mm_mul_pd(a19_12, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_12 = _mm_add_pd(c19_12, _mm_mul_pd(a19_12, b19));
#endif
_mm_storeu_pd(&C[(i*35)+32], c19_12);
__m128d c19_14 = _mm_load_sd(&C[(i*35)+34]);
__m128d a19_14 = _mm_load_sd(&values[286]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_14 = _mm_add_sd(c19_14, _mm_mul_sd(a19_14, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_14 = _mm_add_sd(c19_14, _mm_mul_sd(a19_14, b19));
#endif
_mm_store_sd(&C[(i*35)+34], c19_14);
#else
C[(i*35)+20] += values[272] * B[(i*35)+19];
C[(i*35)+21] += values[273] * B[(i*35)+19];
C[(i*35)+22] += values[274] * B[(i*35)+19];
C[(i*35)+23] += values[275] * B[(i*35)+19];
C[(i*35)+24] += values[276] * B[(i*35)+19];
C[(i*35)+25] += values[277] * B[(i*35)+19];
C[(i*35)+26] += values[278] * B[(i*35)+19];
C[(i*35)+27] += values[279] * B[(i*35)+19];
C[(i*35)+28] += values[280] * B[(i*35)+19];
C[(i*35)+29] += values[281] * B[(i*35)+19];
C[(i*35)+30] += values[282] * B[(i*35)+19];
C[(i*35)+31] += values[283] * B[(i*35)+19];
C[(i*35)+32] += values[284] * B[(i*35)+19];
C[(i*35)+33] += values[285] * B[(i*35)+19];
C[(i*35)+34] += values[286] * B[(i*35)+19];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 5166;
#endif

}

inline void generatedMatrixMultiplication_kXiDivMT_9_35(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*35)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*35)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*35)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*35)+0], c1_0);
#else
C[(i*35)+0] += values[0] * B[(i*35)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*35)+4]);
#endif
__m128d c4_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a4_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, b4));
#endif
_mm_store_sd(&C[(i*35)+1], c4_0);
#else
C[(i*35)+1] += values[1] * B[(i*35)+4];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*35)+5]);
#endif
__m128d c5_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a5_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, b5));
#endif
_mm_store_sd(&C[(i*35)+0], c5_0);
__m128d c5_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a5_1 = _mm_loadu_pd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, b5));
#endif
_mm_storeu_pd(&C[(i*35)+2], c5_1);
#else
C[(i*35)+0] += values[2] * B[(i*35)+5];
C[(i*35)+2] += values[3] * B[(i*35)+5];
C[(i*35)+3] += values[4] * B[(i*35)+5];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*35)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*35)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a7_0 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*35)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a7_1 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*35)+3], c7_1);
#else
C[(i*35)+0] += values[5] * B[(i*35)+7];
C[(i*35)+3] += values[6] * B[(i*35)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*35)+10]);
#endif
__m128d c10_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a10_0 = _mm_load_sd(&values[7]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, b10));
#endif
_mm_store_sd(&C[(i*35)+0], c10_0);
__m128d c10_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a10_1 = _mm_loadu_pd(&values[8]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_1 = _mm_add_pd(c10_1, _mm_mul_pd(a10_1, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_1 = _mm_add_pd(c10_1, _mm_mul_pd(a10_1, b10));
#endif
_mm_storeu_pd(&C[(i*35)+2], c10_1);
__m128d c10_3 = _mm_load_sd(&C[(i*35)+4]);
__m128d a10_3 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_3 = _mm_add_sd(c10_3, _mm_mul_sd(a10_3, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_3 = _mm_add_sd(c10_3, _mm_mul_sd(a10_3, b10));
#endif
_mm_store_sd(&C[(i*35)+4], c10_3);
__m128d c10_4 = _mm_load_sd(&C[(i*35)+6]);
__m128d a10_4 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, b10));
#endif
_mm_store_sd(&C[(i*35)+6], c10_4);
__m128d c10_5 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a10_5 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, b10));
#endif
_mm_storeu_pd(&C[(i*35)+8], c10_5);
#else
C[(i*35)+0] += values[7] * B[(i*35)+10];
C[(i*35)+2] += values[8] * B[(i*35)+10];
C[(i*35)+3] += values[9] * B[(i*35)+10];
C[(i*35)+4] += values[10] * B[(i*35)+10];
C[(i*35)+6] += values[11] * B[(i*35)+10];
C[(i*35)+8] += values[12] * B[(i*35)+10];
C[(i*35)+9] += values[13] * B[(i*35)+10];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*35)+11]);
#endif
__m128d c11_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a11_0 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, b11));
#endif
_mm_store_sd(&C[(i*35)+1], c11_0);
__m128d c11_1 = _mm_load_sd(&C[(i*35)+5]);
__m128d a11_1 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, b11));
#endif
_mm_store_sd(&C[(i*35)+5], c11_1);
__m128d c11_2 = _mm_load_sd(&C[(i*35)+7]);
__m128d a11_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*35)+7], c11_2);
#else
C[(i*35)+1] += values[14] * B[(i*35)+11];
C[(i*35)+5] += values[15] * B[(i*35)+11];
C[(i*35)+7] += values[16] * B[(i*35)+11];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*35)+12]);
#endif
__m128d c12_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a12_0 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, b12));
#endif
_mm_store_sd(&C[(i*35)+0], c12_0);
__m128d c12_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a12_1 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_1 = _mm_add_pd(c12_1, _mm_mul_pd(a12_1, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_1 = _mm_add_pd(c12_1, _mm_mul_pd(a12_1, b12));
#endif
_mm_storeu_pd(&C[(i*35)+2], c12_1);
__m128d c12_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a12_3 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, b12));
#endif
_mm_store_sd(&C[(i*35)+6], c12_3);
__m128d c12_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a12_4 = _mm_loadu_pd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
#endif
_mm_storeu_pd(&C[(i*35)+8], c12_4);
#else
C[(i*35)+0] += values[17] * B[(i*35)+12];
C[(i*35)+2] += values[18] * B[(i*35)+12];
C[(i*35)+3] += values[19] * B[(i*35)+12];
C[(i*35)+6] += values[20] * B[(i*35)+12];
C[(i*35)+8] += values[21] * B[(i*35)+12];
C[(i*35)+9] += values[22] * B[(i*35)+12];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*35)+14]);
#endif
__m128d c14_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a14_0 = _mm_load_sd(&values[23]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, b14));
#endif
_mm_store_sd(&C[(i*35)+1], c14_0);
__m128d c14_1 = _mm_load_sd(&C[(i*35)+7]);
__m128d a14_1 = _mm_load_sd(&values[24]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, b14));
#endif
_mm_store_sd(&C[(i*35)+7], c14_1);
#else
C[(i*35)+1] += values[23] * B[(i*35)+14];
C[(i*35)+7] += values[24] * B[(i*35)+14];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*35)+15]);
#endif
__m128d c15_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a15_0 = _mm_load_sd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, b15));
#endif
_mm_store_sd(&C[(i*35)+0], c15_0);
__m128d c15_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a15_1 = _mm_loadu_pd(&values[26]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_1 = _mm_add_pd(c15_1, _mm_mul_pd(a15_1, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_1 = _mm_add_pd(c15_1, _mm_mul_pd(a15_1, b15));
#endif
_mm_storeu_pd(&C[(i*35)+2], c15_1);
__m128d c15_3 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a15_3 = _mm_loadu_pd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_3 = _mm_add_pd(c15_3, _mm_mul_pd(a15_3, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_3 = _mm_add_pd(c15_3, _mm_mul_pd(a15_3, b15));
#endif
_mm_storeu_pd(&C[(i*35)+8], c15_3);
#else
C[(i*35)+0] += values[25] * B[(i*35)+15];
C[(i*35)+2] += values[26] * B[(i*35)+15];
C[(i*35)+3] += values[27] * B[(i*35)+15];
C[(i*35)+8] += values[28] * B[(i*35)+15];
C[(i*35)+9] += values[29] * B[(i*35)+15];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*35)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a17_0 = _mm_load_sd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*35)+0], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a17_1 = _mm_load_sd(&values[31]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*35)+3], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a17_2 = _mm_load_sd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*35)+9], c17_2);
#else
C[(i*35)+0] += values[30] * B[(i*35)+17];
C[(i*35)+3] += values[31] * B[(i*35)+17];
C[(i*35)+9] += values[32] * B[(i*35)+17];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 20, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*35)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*35)+20]);
#endif
__m128d c20_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a20_0 = _mm_load_sd(&values[33]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_0 = _mm_add_sd(c20_0, _mm_mul_sd(a20_0, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_0 = _mm_add_sd(c20_0, _mm_mul_sd(a20_0, b20));
#endif
_mm_store_sd(&C[(i*35)+1], c20_0);
__m128d c20_1 = _mm_load_sd(&C[(i*35)+5]);
__m128d a20_1 = _mm_load_sd(&values[34]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_1 = _mm_add_sd(c20_1, _mm_mul_sd(a20_1, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_1 = _mm_add_sd(c20_1, _mm_mul_sd(a20_1, b20));
#endif
_mm_store_sd(&C[(i*35)+5], c20_1);
__m128d c20_2 = _mm_load_sd(&C[(i*35)+7]);
__m128d a20_2 = _mm_load_sd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_2 = _mm_add_sd(c20_2, _mm_mul_sd(a20_2, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_2 = _mm_add_sd(c20_2, _mm_mul_sd(a20_2, b20));
#endif
_mm_store_sd(&C[(i*35)+7], c20_2);
__m128d c20_3 = _mm_load_sd(&C[(i*35)+10]);
__m128d a20_3 = _mm_load_sd(&values[36]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_3 = _mm_add_sd(c20_3, _mm_mul_sd(a20_3, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_3 = _mm_add_sd(c20_3, _mm_mul_sd(a20_3, b20));
#endif
_mm_store_sd(&C[(i*35)+10], c20_3);
__m128d c20_4 = _mm_load_sd(&C[(i*35)+12]);
__m128d a20_4 = _mm_load_sd(&values[37]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_4 = _mm_add_sd(c20_4, _mm_mul_sd(a20_4, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_4 = _mm_add_sd(c20_4, _mm_mul_sd(a20_4, b20));
#endif
_mm_store_sd(&C[(i*35)+12], c20_4);
__m128d c20_5 = _mm_load_sd(&C[(i*35)+15]);
__m128d a20_5 = _mm_load_sd(&values[38]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_5 = _mm_add_sd(c20_5, _mm_mul_sd(a20_5, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_5 = _mm_add_sd(c20_5, _mm_mul_sd(a20_5, b20));
#endif
_mm_store_sd(&C[(i*35)+15], c20_5);
__m128d c20_6 = _mm_load_sd(&C[(i*35)+17]);
__m128d a20_6 = _mm_load_sd(&values[39]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_6 = _mm_add_sd(c20_6, _mm_mul_sd(a20_6, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_6 = _mm_add_sd(c20_6, _mm_mul_sd(a20_6, b20));
#endif
_mm_store_sd(&C[(i*35)+17], c20_6);
#else
C[(i*35)+1] += values[33] * B[(i*35)+20];
C[(i*35)+5] += values[34] * B[(i*35)+20];
C[(i*35)+7] += values[35] * B[(i*35)+20];
C[(i*35)+10] += values[36] * B[(i*35)+20];
C[(i*35)+12] += values[37] * B[(i*35)+20];
C[(i*35)+15] += values[38] * B[(i*35)+20];
C[(i*35)+17] += values[39] * B[(i*35)+20];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*35)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*35)+21]);
#endif
__m128d c21_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a21_0 = _mm_load_sd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_0 = _mm_add_sd(c21_0, _mm_mul_sd(a21_0, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_0 = _mm_add_sd(c21_0, _mm_mul_sd(a21_0, b21));
#endif
_mm_store_sd(&C[(i*35)+0], c21_0);
__m128d c21_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a21_1 = _mm_loadu_pd(&values[41]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_1 = _mm_add_pd(c21_1, _mm_mul_pd(a21_1, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_1 = _mm_add_pd(c21_1, _mm_mul_pd(a21_1, b21));
#endif
_mm_storeu_pd(&C[(i*35)+2], c21_1);
__m128d c21_3 = _mm_load_sd(&C[(i*35)+4]);
__m128d a21_3 = _mm_load_sd(&values[43]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_3 = _mm_add_sd(c21_3, _mm_mul_sd(a21_3, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_3 = _mm_add_sd(c21_3, _mm_mul_sd(a21_3, b21));
#endif
_mm_store_sd(&C[(i*35)+4], c21_3);
__m128d c21_4 = _mm_load_sd(&C[(i*35)+6]);
__m128d a21_4 = _mm_load_sd(&values[44]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_4 = _mm_add_sd(c21_4, _mm_mul_sd(a21_4, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_4 = _mm_add_sd(c21_4, _mm_mul_sd(a21_4, b21));
#endif
_mm_store_sd(&C[(i*35)+6], c21_4);
__m128d c21_5 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a21_5 = _mm_loadu_pd(&values[45]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_5 = _mm_add_pd(c21_5, _mm_mul_pd(a21_5, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_5 = _mm_add_pd(c21_5, _mm_mul_pd(a21_5, b21));
#endif
_mm_storeu_pd(&C[(i*35)+8], c21_5);
__m128d c21_7 = _mm_load_sd(&C[(i*35)+11]);
__m128d a21_7 = _mm_load_sd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_7 = _mm_add_sd(c21_7, _mm_mul_sd(a21_7, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_7 = _mm_add_sd(c21_7, _mm_mul_sd(a21_7, b21));
#endif
_mm_store_sd(&C[(i*35)+11], c21_7);
__m128d c21_8 = _mm_loadu_pd(&C[(i*35)+13]);
__m128d a21_8 = _mm_loadu_pd(&values[48]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, b21));
#endif
_mm_storeu_pd(&C[(i*35)+13], c21_8);
__m128d c21_10 = _mm_load_sd(&C[(i*35)+16]);
__m128d a21_10 = _mm_load_sd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_10 = _mm_add_sd(c21_10, _mm_mul_sd(a21_10, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_10 = _mm_add_sd(c21_10, _mm_mul_sd(a21_10, b21));
#endif
_mm_store_sd(&C[(i*35)+16], c21_10);
__m128d c21_11 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a21_11 = _mm_loadu_pd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_11 = _mm_add_pd(c21_11, _mm_mul_pd(a21_11, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_11 = _mm_add_pd(c21_11, _mm_mul_pd(a21_11, b21));
#endif
_mm_storeu_pd(&C[(i*35)+18], c21_11);
#else
C[(i*35)+0] += values[40] * B[(i*35)+21];
C[(i*35)+2] += values[41] * B[(i*35)+21];
C[(i*35)+3] += values[42] * B[(i*35)+21];
C[(i*35)+4] += values[43] * B[(i*35)+21];
C[(i*35)+6] += values[44] * B[(i*35)+21];
C[(i*35)+8] += values[45] * B[(i*35)+21];
C[(i*35)+9] += values[46] * B[(i*35)+21];
C[(i*35)+11] += values[47] * B[(i*35)+21];
C[(i*35)+13] += values[48] * B[(i*35)+21];
C[(i*35)+14] += values[49] * B[(i*35)+21];
C[(i*35)+16] += values[50] * B[(i*35)+21];
C[(i*35)+18] += values[51] * B[(i*35)+21];
C[(i*35)+19] += values[52] * B[(i*35)+21];
#endif
#ifndef NDEBUG
num_flops += 26;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*35)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*35)+22]);
#endif
__m128d c22_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a22_0 = _mm_load_sd(&values[53]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_0 = _mm_add_sd(c22_0, _mm_mul_sd(a22_0, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_0 = _mm_add_sd(c22_0, _mm_mul_sd(a22_0, b22));
#endif
_mm_store_sd(&C[(i*35)+1], c22_0);
__m128d c22_1 = _mm_load_sd(&C[(i*35)+5]);
__m128d a22_1 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_1 = _mm_add_sd(c22_1, _mm_mul_sd(a22_1, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_1 = _mm_add_sd(c22_1, _mm_mul_sd(a22_1, b22));
#endif
_mm_store_sd(&C[(i*35)+5], c22_1);
__m128d c22_2 = _mm_load_sd(&C[(i*35)+7]);
__m128d a22_2 = _mm_load_sd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_2 = _mm_add_sd(c22_2, _mm_mul_sd(a22_2, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_2 = _mm_add_sd(c22_2, _mm_mul_sd(a22_2, b22));
#endif
_mm_store_sd(&C[(i*35)+7], c22_2);
__m128d c22_3 = _mm_load_sd(&C[(i*35)+12]);
__m128d a22_3 = _mm_load_sd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_3 = _mm_add_sd(c22_3, _mm_mul_sd(a22_3, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_3 = _mm_add_sd(c22_3, _mm_mul_sd(a22_3, b22));
#endif
_mm_store_sd(&C[(i*35)+12], c22_3);
__m128d c22_4 = _mm_load_sd(&C[(i*35)+15]);
__m128d a22_4 = _mm_load_sd(&values[57]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_4 = _mm_add_sd(c22_4, _mm_mul_sd(a22_4, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_4 = _mm_add_sd(c22_4, _mm_mul_sd(a22_4, b22));
#endif
_mm_store_sd(&C[(i*35)+15], c22_4);
__m128d c22_5 = _mm_load_sd(&C[(i*35)+17]);
__m128d a22_5 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_5 = _mm_add_sd(c22_5, _mm_mul_sd(a22_5, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_5 = _mm_add_sd(c22_5, _mm_mul_sd(a22_5, b22));
#endif
_mm_store_sd(&C[(i*35)+17], c22_5);
#else
C[(i*35)+1] += values[53] * B[(i*35)+22];
C[(i*35)+5] += values[54] * B[(i*35)+22];
C[(i*35)+7] += values[55] * B[(i*35)+22];
C[(i*35)+12] += values[56] * B[(i*35)+22];
C[(i*35)+15] += values[57] * B[(i*35)+22];
C[(i*35)+17] += values[58] * B[(i*35)+22];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*35)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*35)+23]);
#endif
__m128d c23_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a23_0 = _mm_load_sd(&values[59]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_0 = _mm_add_sd(c23_0, _mm_mul_sd(a23_0, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_0 = _mm_add_sd(c23_0, _mm_mul_sd(a23_0, b23));
#endif
_mm_store_sd(&C[(i*35)+0], c23_0);
__m128d c23_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a23_1 = _mm_loadu_pd(&values[60]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_1 = _mm_add_pd(c23_1, _mm_mul_pd(a23_1, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_1 = _mm_add_pd(c23_1, _mm_mul_pd(a23_1, b23));
#endif
_mm_storeu_pd(&C[(i*35)+2], c23_1);
__m128d c23_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a23_3 = _mm_load_sd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_3 = _mm_add_sd(c23_3, _mm_mul_sd(a23_3, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_3 = _mm_add_sd(c23_3, _mm_mul_sd(a23_3, b23));
#endif
_mm_store_sd(&C[(i*35)+6], c23_3);
__m128d c23_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a23_4 = _mm_loadu_pd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, b23));
#endif
_mm_storeu_pd(&C[(i*35)+8], c23_4);
__m128d c23_6 = _mm_load_sd(&C[(i*35)+13]);
__m128d a23_6 = _mm_load_sd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_6 = _mm_add_sd(c23_6, _mm_mul_sd(a23_6, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_6 = _mm_add_sd(c23_6, _mm_mul_sd(a23_6, b23));
#endif
_mm_store_sd(&C[(i*35)+13], c23_6);
__m128d c23_7 = _mm_load_sd(&C[(i*35)+16]);
__m128d a23_7 = _mm_load_sd(&values[66]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_7 = _mm_add_sd(c23_7, _mm_mul_sd(a23_7, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_7 = _mm_add_sd(c23_7, _mm_mul_sd(a23_7, b23));
#endif
_mm_store_sd(&C[(i*35)+16], c23_7);
__m128d c23_8 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a23_8 = _mm_loadu_pd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_8 = _mm_add_pd(c23_8, _mm_mul_pd(a23_8, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_8 = _mm_add_pd(c23_8, _mm_mul_pd(a23_8, b23));
#endif
_mm_storeu_pd(&C[(i*35)+18], c23_8);
#else
C[(i*35)+0] += values[59] * B[(i*35)+23];
C[(i*35)+2] += values[60] * B[(i*35)+23];
C[(i*35)+3] += values[61] * B[(i*35)+23];
C[(i*35)+6] += values[62] * B[(i*35)+23];
C[(i*35)+8] += values[63] * B[(i*35)+23];
C[(i*35)+9] += values[64] * B[(i*35)+23];
C[(i*35)+13] += values[65] * B[(i*35)+23];
C[(i*35)+16] += values[66] * B[(i*35)+23];
C[(i*35)+18] += values[67] * B[(i*35)+23];
C[(i*35)+19] += values[68] * B[(i*35)+23];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*35)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*35)+25]);
#endif
__m128d c25_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a25_0 = _mm_load_sd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_0 = _mm_add_sd(c25_0, _mm_mul_sd(a25_0, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_0 = _mm_add_sd(c25_0, _mm_mul_sd(a25_0, b25));
#endif
_mm_store_sd(&C[(i*35)+0], c25_0);
__m128d c25_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a25_1 = _mm_loadu_pd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_1 = _mm_add_pd(c25_1, _mm_mul_pd(a25_1, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_1 = _mm_add_pd(c25_1, _mm_mul_pd(a25_1, b25));
#endif
_mm_storeu_pd(&C[(i*35)+2], c25_1);
__m128d c25_3 = _mm_load_sd(&C[(i*35)+4]);
__m128d a25_3 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_3 = _mm_add_sd(c25_3, _mm_mul_sd(a25_3, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_3 = _mm_add_sd(c25_3, _mm_mul_sd(a25_3, b25));
#endif
_mm_store_sd(&C[(i*35)+4], c25_3);
__m128d c25_4 = _mm_load_sd(&C[(i*35)+6]);
__m128d a25_4 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_4 = _mm_add_sd(c25_4, _mm_mul_sd(a25_4, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_4 = _mm_add_sd(c25_4, _mm_mul_sd(a25_4, b25));
#endif
_mm_store_sd(&C[(i*35)+6], c25_4);
__m128d c25_5 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a25_5 = _mm_loadu_pd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_5 = _mm_add_pd(c25_5, _mm_mul_pd(a25_5, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_5 = _mm_add_pd(c25_5, _mm_mul_pd(a25_5, b25));
#endif
_mm_storeu_pd(&C[(i*35)+8], c25_5);
__m128d c25_7 = _mm_load_sd(&C[(i*35)+14]);
__m128d a25_7 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_7 = _mm_add_sd(c25_7, _mm_mul_sd(a25_7, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_7 = _mm_add_sd(c25_7, _mm_mul_sd(a25_7, b25));
#endif
_mm_store_sd(&C[(i*35)+14], c25_7);
__m128d c25_8 = _mm_load_sd(&C[(i*35)+16]);
__m128d a25_8 = _mm_load_sd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_8 = _mm_add_sd(c25_8, _mm_mul_sd(a25_8, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_8 = _mm_add_sd(c25_8, _mm_mul_sd(a25_8, b25));
#endif
_mm_store_sd(&C[(i*35)+16], c25_8);
__m128d c25_9 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a25_9 = _mm_loadu_pd(&values[78]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_9 = _mm_add_pd(c25_9, _mm_mul_pd(a25_9, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_9 = _mm_add_pd(c25_9, _mm_mul_pd(a25_9, b25));
#endif
_mm_storeu_pd(&C[(i*35)+18], c25_9);
#else
C[(i*35)+0] += values[69] * B[(i*35)+25];
C[(i*35)+2] += values[70] * B[(i*35)+25];
C[(i*35)+3] += values[71] * B[(i*35)+25];
C[(i*35)+4] += values[72] * B[(i*35)+25];
C[(i*35)+6] += values[73] * B[(i*35)+25];
C[(i*35)+8] += values[74] * B[(i*35)+25];
C[(i*35)+9] += values[75] * B[(i*35)+25];
C[(i*35)+14] += values[76] * B[(i*35)+25];
C[(i*35)+16] += values[77] * B[(i*35)+25];
C[(i*35)+18] += values[78] * B[(i*35)+25];
C[(i*35)+19] += values[79] * B[(i*35)+25];
#endif
#ifndef NDEBUG
num_flops += 22;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*35)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*35)+26]);
#endif
__m128d c26_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a26_0 = _mm_load_sd(&values[80]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_0 = _mm_add_sd(c26_0, _mm_mul_sd(a26_0, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_0 = _mm_add_sd(c26_0, _mm_mul_sd(a26_0, b26));
#endif
_mm_store_sd(&C[(i*35)+1], c26_0);
__m128d c26_1 = _mm_load_sd(&C[(i*35)+5]);
__m128d a26_1 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_1 = _mm_add_sd(c26_1, _mm_mul_sd(a26_1, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_1 = _mm_add_sd(c26_1, _mm_mul_sd(a26_1, b26));
#endif
_mm_store_sd(&C[(i*35)+5], c26_1);
__m128d c26_2 = _mm_load_sd(&C[(i*35)+7]);
__m128d a26_2 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_2 = _mm_add_sd(c26_2, _mm_mul_sd(a26_2, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_2 = _mm_add_sd(c26_2, _mm_mul_sd(a26_2, b26));
#endif
_mm_store_sd(&C[(i*35)+7], c26_2);
__m128d c26_3 = _mm_load_sd(&C[(i*35)+15]);
__m128d a26_3 = _mm_load_sd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_3 = _mm_add_sd(c26_3, _mm_mul_sd(a26_3, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_3 = _mm_add_sd(c26_3, _mm_mul_sd(a26_3, b26));
#endif
_mm_store_sd(&C[(i*35)+15], c26_3);
__m128d c26_4 = _mm_load_sd(&C[(i*35)+17]);
__m128d a26_4 = _mm_load_sd(&values[84]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_4 = _mm_add_sd(c26_4, _mm_mul_sd(a26_4, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_4 = _mm_add_sd(c26_4, _mm_mul_sd(a26_4, b26));
#endif
_mm_store_sd(&C[(i*35)+17], c26_4);
#else
C[(i*35)+1] += values[80] * B[(i*35)+26];
C[(i*35)+5] += values[81] * B[(i*35)+26];
C[(i*35)+7] += values[82] * B[(i*35)+26];
C[(i*35)+15] += values[83] * B[(i*35)+26];
C[(i*35)+17] += values[84] * B[(i*35)+26];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*35)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*35)+27]);
#endif
__m128d c27_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a27_0 = _mm_load_sd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_0 = _mm_add_sd(c27_0, _mm_mul_sd(a27_0, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_0 = _mm_add_sd(c27_0, _mm_mul_sd(a27_0, b27));
#endif
_mm_store_sd(&C[(i*35)+0], c27_0);
__m128d c27_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a27_1 = _mm_loadu_pd(&values[86]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_1 = _mm_add_pd(c27_1, _mm_mul_pd(a27_1, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_1 = _mm_add_pd(c27_1, _mm_mul_pd(a27_1, b27));
#endif
_mm_storeu_pd(&C[(i*35)+2], c27_1);
__m128d c27_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a27_3 = _mm_load_sd(&values[88]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_3 = _mm_add_sd(c27_3, _mm_mul_sd(a27_3, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_3 = _mm_add_sd(c27_3, _mm_mul_sd(a27_3, b27));
#endif
_mm_store_sd(&C[(i*35)+6], c27_3);
__m128d c27_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a27_4 = _mm_loadu_pd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, b27));
#endif
_mm_storeu_pd(&C[(i*35)+8], c27_4);
__m128d c27_6 = _mm_load_sd(&C[(i*35)+16]);
__m128d a27_6 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_6 = _mm_add_sd(c27_6, _mm_mul_sd(a27_6, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_6 = _mm_add_sd(c27_6, _mm_mul_sd(a27_6, b27));
#endif
_mm_store_sd(&C[(i*35)+16], c27_6);
__m128d c27_7 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a27_7 = _mm_loadu_pd(&values[92]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_7 = _mm_add_pd(c27_7, _mm_mul_pd(a27_7, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_7 = _mm_add_pd(c27_7, _mm_mul_pd(a27_7, b27));
#endif
_mm_storeu_pd(&C[(i*35)+18], c27_7);
#else
C[(i*35)+0] += values[85] * B[(i*35)+27];
C[(i*35)+2] += values[86] * B[(i*35)+27];
C[(i*35)+3] += values[87] * B[(i*35)+27];
C[(i*35)+6] += values[88] * B[(i*35)+27];
C[(i*35)+8] += values[89] * B[(i*35)+27];
C[(i*35)+9] += values[90] * B[(i*35)+27];
C[(i*35)+16] += values[91] * B[(i*35)+27];
C[(i*35)+18] += values[92] * B[(i*35)+27];
C[(i*35)+19] += values[93] * B[(i*35)+27];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*35)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*35)+29]);
#endif
__m128d c29_0 = _mm_load_sd(&C[(i*35)+1]);
__m128d a29_0 = _mm_load_sd(&values[94]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_0 = _mm_add_sd(c29_0, _mm_mul_sd(a29_0, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_0 = _mm_add_sd(c29_0, _mm_mul_sd(a29_0, b29));
#endif
_mm_store_sd(&C[(i*35)+1], c29_0);
__m128d c29_1 = _mm_load_sd(&C[(i*35)+7]);
__m128d a29_1 = _mm_load_sd(&values[95]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_1 = _mm_add_sd(c29_1, _mm_mul_sd(a29_1, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_1 = _mm_add_sd(c29_1, _mm_mul_sd(a29_1, b29));
#endif
_mm_store_sd(&C[(i*35)+7], c29_1);
__m128d c29_2 = _mm_load_sd(&C[(i*35)+17]);
__m128d a29_2 = _mm_load_sd(&values[96]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_2 = _mm_add_sd(c29_2, _mm_mul_sd(a29_2, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_2 = _mm_add_sd(c29_2, _mm_mul_sd(a29_2, b29));
#endif
_mm_store_sd(&C[(i*35)+17], c29_2);
#else
C[(i*35)+1] += values[94] * B[(i*35)+29];
C[(i*35)+7] += values[95] * B[(i*35)+29];
C[(i*35)+17] += values[96] * B[(i*35)+29];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*35)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*35)+30]);
#endif
__m128d c30_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a30_0 = _mm_load_sd(&values[97]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_0 = _mm_add_sd(c30_0, _mm_mul_sd(a30_0, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_0 = _mm_add_sd(c30_0, _mm_mul_sd(a30_0, b30));
#endif
_mm_store_sd(&C[(i*35)+0], c30_0);
__m128d c30_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a30_1 = _mm_loadu_pd(&values[98]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_1 = _mm_add_pd(c30_1, _mm_mul_pd(a30_1, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_1 = _mm_add_pd(c30_1, _mm_mul_pd(a30_1, b30));
#endif
_mm_storeu_pd(&C[(i*35)+2], c30_1);
__m128d c30_3 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a30_3 = _mm_loadu_pd(&values[100]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_3 = _mm_add_pd(c30_3, _mm_mul_pd(a30_3, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_3 = _mm_add_pd(c30_3, _mm_mul_pd(a30_3, b30));
#endif
_mm_storeu_pd(&C[(i*35)+8], c30_3);
__m128d c30_5 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a30_5 = _mm_loadu_pd(&values[102]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, b30));
#endif
_mm_storeu_pd(&C[(i*35)+18], c30_5);
#else
C[(i*35)+0] += values[97] * B[(i*35)+30];
C[(i*35)+2] += values[98] * B[(i*35)+30];
C[(i*35)+3] += values[99] * B[(i*35)+30];
C[(i*35)+8] += values[100] * B[(i*35)+30];
C[(i*35)+9] += values[101] * B[(i*35)+30];
C[(i*35)+18] += values[102] * B[(i*35)+30];
C[(i*35)+19] += values[103] * B[(i*35)+30];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*35)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*35)+32]);
#endif
__m128d c32_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a32_0 = _mm_load_sd(&values[104]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, b32));
#endif
_mm_store_sd(&C[(i*35)+0], c32_0);
__m128d c32_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a32_1 = _mm_load_sd(&values[105]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, b32));
#endif
_mm_store_sd(&C[(i*35)+3], c32_1);
__m128d c32_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a32_2 = _mm_load_sd(&values[106]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, b32));
#endif
_mm_store_sd(&C[(i*35)+9], c32_2);
__m128d c32_3 = _mm_load_sd(&C[(i*35)+19]);
__m128d a32_3 = _mm_load_sd(&values[107]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, b32));
#endif
_mm_store_sd(&C[(i*35)+19], c32_3);
#else
C[(i*35)+0] += values[104] * B[(i*35)+32];
C[(i*35)+3] += values[105] * B[(i*35)+32];
C[(i*35)+9] += values[106] * B[(i*35)+32];
C[(i*35)+19] += values[107] * B[(i*35)+32];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivMT_9_35(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*35)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*35)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*35)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*35)+0], c1_0);
#else
C[(i*35)+0] += values[0] * B[(i*35)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*35)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*35)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*35)+0], c2_0);
#else
C[(i*35)+0] += values[1] * B[(i*35)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[2]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*35)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[2]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*35)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[4]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*35)+2], c4_2);
#endif
#else
C[(i*35)+0] += values[2] * B[(i*35)+4];
C[(i*35)+1] += values[3] * B[(i*35)+4];
C[(i*35)+2] += values[4] * B[(i*35)+4];
C[(i*35)+3] += values[5] * B[(i*35)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[6]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*35)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[6]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*35)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[8]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*35)+2], c5_2);
#endif
#else
C[(i*35)+0] += values[6] * B[(i*35)+5];
C[(i*35)+1] += values[7] * B[(i*35)+5];
C[(i*35)+2] += values[8] * B[(i*35)+5];
C[(i*35)+3] += values[9] * B[(i*35)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*35)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*35)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a6_0 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*35)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*35)+2], c6_1);
#else
C[(i*35)+0] += values[10] * B[(i*35)+6];
C[(i*35)+2] += values[11] * B[(i*35)+6];
C[(i*35)+3] += values[12] * B[(i*35)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*35)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*35)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a7_0 = _mm_load_sd(&values[13]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*35)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a7_1 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*35)+3], c7_1);
#else
C[(i*35)+0] += values[13] * B[(i*35)+7];
C[(i*35)+3] += values[14] * B[(i*35)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*35)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*35)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a8_0 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*35)+0], c8_0);
__m128d c8_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a8_1 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, b8));
#endif
_mm_store_sd(&C[(i*35)+3], c8_1);
#else
C[(i*35)+0] += values[15] * B[(i*35)+8];
C[(i*35)+3] += values[16] * B[(i*35)+8];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a10_0 = _mm256_loadu_pd(&values[17]);
c10_0 = _mm256_add_pd(c10_0, _mm256_mul_pd(a10_0, b10));
_mm256_storeu_pd(&C[(i*35)+0], c10_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a10_0 = _mm_loadu_pd(&values[17]);
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
_mm_storeu_pd(&C[(i*35)+0], c10_0);
__m128d c10_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a10_2 = _mm_loadu_pd(&values[19]);
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, b10));
_mm_storeu_pd(&C[(i*35)+2], c10_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a10_4 = _mm256_loadu_pd(&values[21]);
c10_4 = _mm256_add_pd(c10_4, _mm256_mul_pd(a10_4, b10));
_mm256_storeu_pd(&C[(i*35)+4], c10_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a10_4 = _mm_loadu_pd(&values[21]);
c10_4 = _mm_add_pd(c10_4, _mm_mul_pd(a10_4, b10));
_mm_storeu_pd(&C[(i*35)+4], c10_4);
__m128d c10_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a10_6 = _mm_loadu_pd(&values[23]);
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, b10));
_mm_storeu_pd(&C[(i*35)+6], c10_6);
#endif
__m128d c10_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a10_8 = _mm_loadu_pd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, b10));
#endif
_mm_storeu_pd(&C[(i*35)+8], c10_8);
#else
C[(i*35)+0] += values[17] * B[(i*35)+10];
C[(i*35)+1] += values[18] * B[(i*35)+10];
C[(i*35)+2] += values[19] * B[(i*35)+10];
C[(i*35)+3] += values[20] * B[(i*35)+10];
C[(i*35)+4] += values[21] * B[(i*35)+10];
C[(i*35)+5] += values[22] * B[(i*35)+10];
C[(i*35)+6] += values[23] * B[(i*35)+10];
C[(i*35)+7] += values[24] * B[(i*35)+10];
C[(i*35)+8] += values[25] * B[(i*35)+10];
C[(i*35)+9] += values[26] * B[(i*35)+10];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a11_0 = _mm256_loadu_pd(&values[27]);
c11_0 = _mm256_add_pd(c11_0, _mm256_mul_pd(a11_0, b11));
_mm256_storeu_pd(&C[(i*35)+0], c11_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a11_0 = _mm_loadu_pd(&values[27]);
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
_mm_storeu_pd(&C[(i*35)+0], c11_0);
__m128d c11_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a11_2 = _mm_loadu_pd(&values[29]);
c11_2 = _mm_add_pd(c11_2, _mm_mul_pd(a11_2, b11));
_mm_storeu_pd(&C[(i*35)+2], c11_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a11_4 = _mm256_loadu_pd(&values[31]);
c11_4 = _mm256_add_pd(c11_4, _mm256_mul_pd(a11_4, b11));
_mm256_storeu_pd(&C[(i*35)+4], c11_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a11_4 = _mm_loadu_pd(&values[31]);
c11_4 = _mm_add_pd(c11_4, _mm_mul_pd(a11_4, b11));
_mm_storeu_pd(&C[(i*35)+4], c11_4);
__m128d c11_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a11_6 = _mm_loadu_pd(&values[33]);
c11_6 = _mm_add_pd(c11_6, _mm_mul_pd(a11_6, b11));
_mm_storeu_pd(&C[(i*35)+6], c11_6);
#endif
__m128d c11_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a11_8 = _mm_loadu_pd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, b11));
#endif
_mm_storeu_pd(&C[(i*35)+8], c11_8);
#else
C[(i*35)+0] += values[27] * B[(i*35)+11];
C[(i*35)+1] += values[28] * B[(i*35)+11];
C[(i*35)+2] += values[29] * B[(i*35)+11];
C[(i*35)+3] += values[30] * B[(i*35)+11];
C[(i*35)+4] += values[31] * B[(i*35)+11];
C[(i*35)+5] += values[32] * B[(i*35)+11];
C[(i*35)+6] += values[33] * B[(i*35)+11];
C[(i*35)+7] += values[34] * B[(i*35)+11];
C[(i*35)+8] += values[35] * B[(i*35)+11];
C[(i*35)+9] += values[36] * B[(i*35)+11];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a12_0 = _mm256_loadu_pd(&values[37]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*35)+0], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a12_0 = _mm_loadu_pd(&values[37]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*35)+0], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a12_2 = _mm_loadu_pd(&values[39]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*35)+2], c12_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_4 = _mm256_loadu_pd(&C[(i*35)+5]);
__m256d a12_4 = _mm256_loadu_pd(&values[41]);
c12_4 = _mm256_add_pd(c12_4, _mm256_mul_pd(a12_4, b12));
_mm256_storeu_pd(&C[(i*35)+5], c12_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_4 = _mm_loadu_pd(&C[(i*35)+5]);
__m128d a12_4 = _mm_loadu_pd(&values[41]);
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
_mm_storeu_pd(&C[(i*35)+5], c12_4);
__m128d c12_6 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a12_6 = _mm_loadu_pd(&values[43]);
c12_6 = _mm_add_pd(c12_6, _mm_mul_pd(a12_6, b12));
_mm_storeu_pd(&C[(i*35)+7], c12_6);
#endif
__m128d c12_8 = _mm_load_sd(&C[(i*35)+9]);
__m128d a12_8 = _mm_load_sd(&values[45]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, b12));
#endif
_mm_store_sd(&C[(i*35)+9], c12_8);
#else
C[(i*35)+0] += values[37] * B[(i*35)+12];
C[(i*35)+1] += values[38] * B[(i*35)+12];
C[(i*35)+2] += values[39] * B[(i*35)+12];
C[(i*35)+3] += values[40] * B[(i*35)+12];
C[(i*35)+5] += values[41] * B[(i*35)+12];
C[(i*35)+6] += values[42] * B[(i*35)+12];
C[(i*35)+7] += values[43] * B[(i*35)+12];
C[(i*35)+8] += values[44] * B[(i*35)+12];
C[(i*35)+9] += values[45] * B[(i*35)+12];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*35)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*35)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a13_0 = _mm_load_sd(&values[46]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*35)+0], c13_0);
__m128d c13_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a13_1 = _mm_loadu_pd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, b13));
#endif
_mm_storeu_pd(&C[(i*35)+2], c13_1);
__m128d c13_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a13_3 = _mm_load_sd(&values[49]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, b13));
#endif
_mm_store_sd(&C[(i*35)+6], c13_3);
__m128d c13_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a13_4 = _mm_loadu_pd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, b13));
#endif
_mm_storeu_pd(&C[(i*35)+8], c13_4);
#else
C[(i*35)+0] += values[46] * B[(i*35)+13];
C[(i*35)+2] += values[47] * B[(i*35)+13];
C[(i*35)+3] += values[48] * B[(i*35)+13];
C[(i*35)+6] += values[49] * B[(i*35)+13];
C[(i*35)+8] += values[50] * B[(i*35)+13];
C[(i*35)+9] += values[51] * B[(i*35)+13];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a14_0 = _mm256_loadu_pd(&values[52]);
c14_0 = _mm256_add_pd(c14_0, _mm256_mul_pd(a14_0, b14));
_mm256_storeu_pd(&C[(i*35)+0], c14_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a14_0 = _mm_loadu_pd(&values[52]);
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
_mm_storeu_pd(&C[(i*35)+0], c14_0);
__m128d c14_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a14_2 = _mm_loadu_pd(&values[54]);
c14_2 = _mm_add_pd(c14_2, _mm_mul_pd(a14_2, b14));
_mm_storeu_pd(&C[(i*35)+2], c14_2);
#endif
__m128d c14_4 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a14_4 = _mm_loadu_pd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_4 = _mm_add_pd(c14_4, _mm_mul_pd(a14_4, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_4 = _mm_add_pd(c14_4, _mm_mul_pd(a14_4, b14));
#endif
_mm_storeu_pd(&C[(i*35)+7], c14_4);
__m128d c14_6 = _mm_load_sd(&C[(i*35)+9]);
__m128d a14_6 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_6 = _mm_add_sd(c14_6, _mm_mul_sd(a14_6, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_6 = _mm_add_sd(c14_6, _mm_mul_sd(a14_6, b14));
#endif
_mm_store_sd(&C[(i*35)+9], c14_6);
#else
C[(i*35)+0] += values[52] * B[(i*35)+14];
C[(i*35)+1] += values[53] * B[(i*35)+14];
C[(i*35)+2] += values[54] * B[(i*35)+14];
C[(i*35)+3] += values[55] * B[(i*35)+14];
C[(i*35)+7] += values[56] * B[(i*35)+14];
C[(i*35)+8] += values[57] * B[(i*35)+14];
C[(i*35)+9] += values[58] * B[(i*35)+14];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a15_0 = _mm256_loadu_pd(&values[59]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*35)+0], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a15_0 = _mm_loadu_pd(&values[59]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*35)+0], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a15_2 = _mm_loadu_pd(&values[61]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*35)+2], c15_2);
#endif
__m128d c15_4 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a15_4 = _mm_loadu_pd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, b15));
#endif
_mm_storeu_pd(&C[(i*35)+7], c15_4);
__m128d c15_6 = _mm_load_sd(&C[(i*35)+9]);
__m128d a15_6 = _mm_load_sd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*35)+9], c15_6);
#else
C[(i*35)+0] += values[59] * B[(i*35)+15];
C[(i*35)+1] += values[60] * B[(i*35)+15];
C[(i*35)+2] += values[61] * B[(i*35)+15];
C[(i*35)+3] += values[62] * B[(i*35)+15];
C[(i*35)+7] += values[63] * B[(i*35)+15];
C[(i*35)+8] += values[64] * B[(i*35)+15];
C[(i*35)+9] += values[65] * B[(i*35)+15];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*35)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*35)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a16_0 = _mm_load_sd(&values[66]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*35)+0], c16_0);
__m128d c16_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a16_1 = _mm_loadu_pd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, b16));
#endif
_mm_storeu_pd(&C[(i*35)+2], c16_1);
__m128d c16_3 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a16_3 = _mm_loadu_pd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_pd(c16_3, _mm_mul_pd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_pd(c16_3, _mm_mul_pd(a16_3, b16));
#endif
_mm_storeu_pd(&C[(i*35)+8], c16_3);
#else
C[(i*35)+0] += values[66] * B[(i*35)+16];
C[(i*35)+2] += values[67] * B[(i*35)+16];
C[(i*35)+3] += values[68] * B[(i*35)+16];
C[(i*35)+8] += values[69] * B[(i*35)+16];
C[(i*35)+9] += values[70] * B[(i*35)+16];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*35)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a17_0 = _mm_load_sd(&values[71]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*35)+0], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a17_1 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*35)+3], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a17_2 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*35)+9], c17_2);
#else
C[(i*35)+0] += values[71] * B[(i*35)+17];
C[(i*35)+3] += values[72] * B[(i*35)+17];
C[(i*35)+9] += values[73] * B[(i*35)+17];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*35)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*35)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a18_0 = _mm_load_sd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*35)+0], c18_0);
__m128d c18_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a18_1 = _mm_load_sd(&values[75]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, b18));
#endif
_mm_store_sd(&C[(i*35)+3], c18_1);
__m128d c18_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a18_2 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, b18));
#endif
_mm_store_sd(&C[(i*35)+9], c18_2);
#else
C[(i*35)+0] += values[74] * B[(i*35)+18];
C[(i*35)+3] += values[75] * B[(i*35)+18];
C[(i*35)+9] += values[76] * B[(i*35)+18];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 20, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*35)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*35)+20]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a20_0 = _mm256_loadu_pd(&values[77]);
c20_0 = _mm256_add_pd(c20_0, _mm256_mul_pd(a20_0, b20));
_mm256_storeu_pd(&C[(i*35)+0], c20_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a20_0 = _mm_loadu_pd(&values[77]);
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, b20));
_mm_storeu_pd(&C[(i*35)+0], c20_0);
__m128d c20_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a20_2 = _mm_loadu_pd(&values[79]);
c20_2 = _mm_add_pd(c20_2, _mm_mul_pd(a20_2, b20));
_mm_storeu_pd(&C[(i*35)+2], c20_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a20_4 = _mm256_loadu_pd(&values[81]);
c20_4 = _mm256_add_pd(c20_4, _mm256_mul_pd(a20_4, b20));
_mm256_storeu_pd(&C[(i*35)+4], c20_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a20_4 = _mm_loadu_pd(&values[81]);
c20_4 = _mm_add_pd(c20_4, _mm_mul_pd(a20_4, b20));
_mm_storeu_pd(&C[(i*35)+4], c20_4);
__m128d c20_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a20_6 = _mm_loadu_pd(&values[83]);
c20_6 = _mm_add_pd(c20_6, _mm_mul_pd(a20_6, b20));
_mm_storeu_pd(&C[(i*35)+6], c20_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_8 = _mm256_loadu_pd(&C[(i*35)+8]);
__m256d a20_8 = _mm256_loadu_pd(&values[85]);
c20_8 = _mm256_add_pd(c20_8, _mm256_mul_pd(a20_8, b20));
_mm256_storeu_pd(&C[(i*35)+8], c20_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a20_8 = _mm_loadu_pd(&values[85]);
c20_8 = _mm_add_pd(c20_8, _mm_mul_pd(a20_8, b20));
_mm_storeu_pd(&C[(i*35)+8], c20_8);
__m128d c20_10 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a20_10 = _mm_loadu_pd(&values[87]);
c20_10 = _mm_add_pd(c20_10, _mm_mul_pd(a20_10, b20));
_mm_storeu_pd(&C[(i*35)+10], c20_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_12 = _mm256_loadu_pd(&C[(i*35)+12]);
__m256d a20_12 = _mm256_loadu_pd(&values[89]);
c20_12 = _mm256_add_pd(c20_12, _mm256_mul_pd(a20_12, b20));
_mm256_storeu_pd(&C[(i*35)+12], c20_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_12 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a20_12 = _mm_loadu_pd(&values[89]);
c20_12 = _mm_add_pd(c20_12, _mm_mul_pd(a20_12, b20));
_mm_storeu_pd(&C[(i*35)+12], c20_12);
__m128d c20_14 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a20_14 = _mm_loadu_pd(&values[91]);
c20_14 = _mm_add_pd(c20_14, _mm_mul_pd(a20_14, b20));
_mm_storeu_pd(&C[(i*35)+14], c20_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_16 = _mm256_loadu_pd(&C[(i*35)+16]);
__m256d a20_16 = _mm256_loadu_pd(&values[93]);
c20_16 = _mm256_add_pd(c20_16, _mm256_mul_pd(a20_16, b20));
_mm256_storeu_pd(&C[(i*35)+16], c20_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_16 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a20_16 = _mm_loadu_pd(&values[93]);
c20_16 = _mm_add_pd(c20_16, _mm_mul_pd(a20_16, b20));
_mm_storeu_pd(&C[(i*35)+16], c20_16);
__m128d c20_18 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a20_18 = _mm_loadu_pd(&values[95]);
c20_18 = _mm_add_pd(c20_18, _mm_mul_pd(a20_18, b20));
_mm_storeu_pd(&C[(i*35)+18], c20_18);
#endif
#else
C[(i*35)+0] += values[77] * B[(i*35)+20];
C[(i*35)+1] += values[78] * B[(i*35)+20];
C[(i*35)+2] += values[79] * B[(i*35)+20];
C[(i*35)+3] += values[80] * B[(i*35)+20];
C[(i*35)+4] += values[81] * B[(i*35)+20];
C[(i*35)+5] += values[82] * B[(i*35)+20];
C[(i*35)+6] += values[83] * B[(i*35)+20];
C[(i*35)+7] += values[84] * B[(i*35)+20];
C[(i*35)+8] += values[85] * B[(i*35)+20];
C[(i*35)+9] += values[86] * B[(i*35)+20];
C[(i*35)+10] += values[87] * B[(i*35)+20];
C[(i*35)+11] += values[88] * B[(i*35)+20];
C[(i*35)+12] += values[89] * B[(i*35)+20];
C[(i*35)+13] += values[90] * B[(i*35)+20];
C[(i*35)+14] += values[91] * B[(i*35)+20];
C[(i*35)+15] += values[92] * B[(i*35)+20];
C[(i*35)+16] += values[93] * B[(i*35)+20];
C[(i*35)+17] += values[94] * B[(i*35)+20];
C[(i*35)+18] += values[95] * B[(i*35)+20];
C[(i*35)+19] += values[96] * B[(i*35)+20];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*35)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*35)+21]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a21_0 = _mm256_loadu_pd(&values[97]);
c21_0 = _mm256_add_pd(c21_0, _mm256_mul_pd(a21_0, b21));
_mm256_storeu_pd(&C[(i*35)+0], c21_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a21_0 = _mm_loadu_pd(&values[97]);
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, b21));
_mm_storeu_pd(&C[(i*35)+0], c21_0);
__m128d c21_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a21_2 = _mm_loadu_pd(&values[99]);
c21_2 = _mm_add_pd(c21_2, _mm_mul_pd(a21_2, b21));
_mm_storeu_pd(&C[(i*35)+2], c21_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a21_4 = _mm256_loadu_pd(&values[101]);
c21_4 = _mm256_add_pd(c21_4, _mm256_mul_pd(a21_4, b21));
_mm256_storeu_pd(&C[(i*35)+4], c21_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a21_4 = _mm_loadu_pd(&values[101]);
c21_4 = _mm_add_pd(c21_4, _mm_mul_pd(a21_4, b21));
_mm_storeu_pd(&C[(i*35)+4], c21_4);
__m128d c21_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a21_6 = _mm_loadu_pd(&values[103]);
c21_6 = _mm_add_pd(c21_6, _mm_mul_pd(a21_6, b21));
_mm_storeu_pd(&C[(i*35)+6], c21_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_8 = _mm256_loadu_pd(&C[(i*35)+8]);
__m256d a21_8 = _mm256_loadu_pd(&values[105]);
c21_8 = _mm256_add_pd(c21_8, _mm256_mul_pd(a21_8, b21));
_mm256_storeu_pd(&C[(i*35)+8], c21_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a21_8 = _mm_loadu_pd(&values[105]);
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, b21));
_mm_storeu_pd(&C[(i*35)+8], c21_8);
__m128d c21_10 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a21_10 = _mm_loadu_pd(&values[107]);
c21_10 = _mm_add_pd(c21_10, _mm_mul_pd(a21_10, b21));
_mm_storeu_pd(&C[(i*35)+10], c21_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_12 = _mm256_loadu_pd(&C[(i*35)+12]);
__m256d a21_12 = _mm256_loadu_pd(&values[109]);
c21_12 = _mm256_add_pd(c21_12, _mm256_mul_pd(a21_12, b21));
_mm256_storeu_pd(&C[(i*35)+12], c21_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_12 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a21_12 = _mm_loadu_pd(&values[109]);
c21_12 = _mm_add_pd(c21_12, _mm_mul_pd(a21_12, b21));
_mm_storeu_pd(&C[(i*35)+12], c21_12);
__m128d c21_14 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a21_14 = _mm_loadu_pd(&values[111]);
c21_14 = _mm_add_pd(c21_14, _mm_mul_pd(a21_14, b21));
_mm_storeu_pd(&C[(i*35)+14], c21_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_16 = _mm256_loadu_pd(&C[(i*35)+16]);
__m256d a21_16 = _mm256_loadu_pd(&values[113]);
c21_16 = _mm256_add_pd(c21_16, _mm256_mul_pd(a21_16, b21));
_mm256_storeu_pd(&C[(i*35)+16], c21_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_16 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a21_16 = _mm_loadu_pd(&values[113]);
c21_16 = _mm_add_pd(c21_16, _mm_mul_pd(a21_16, b21));
_mm_storeu_pd(&C[(i*35)+16], c21_16);
__m128d c21_18 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a21_18 = _mm_loadu_pd(&values[115]);
c21_18 = _mm_add_pd(c21_18, _mm_mul_pd(a21_18, b21));
_mm_storeu_pd(&C[(i*35)+18], c21_18);
#endif
#else
C[(i*35)+0] += values[97] * B[(i*35)+21];
C[(i*35)+1] += values[98] * B[(i*35)+21];
C[(i*35)+2] += values[99] * B[(i*35)+21];
C[(i*35)+3] += values[100] * B[(i*35)+21];
C[(i*35)+4] += values[101] * B[(i*35)+21];
C[(i*35)+5] += values[102] * B[(i*35)+21];
C[(i*35)+6] += values[103] * B[(i*35)+21];
C[(i*35)+7] += values[104] * B[(i*35)+21];
C[(i*35)+8] += values[105] * B[(i*35)+21];
C[(i*35)+9] += values[106] * B[(i*35)+21];
C[(i*35)+10] += values[107] * B[(i*35)+21];
C[(i*35)+11] += values[108] * B[(i*35)+21];
C[(i*35)+12] += values[109] * B[(i*35)+21];
C[(i*35)+13] += values[110] * B[(i*35)+21];
C[(i*35)+14] += values[111] * B[(i*35)+21];
C[(i*35)+15] += values[112] * B[(i*35)+21];
C[(i*35)+16] += values[113] * B[(i*35)+21];
C[(i*35)+17] += values[114] * B[(i*35)+21];
C[(i*35)+18] += values[115] * B[(i*35)+21];
C[(i*35)+19] += values[116] * B[(i*35)+21];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*35)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*35)+22]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a22_0 = _mm256_loadu_pd(&values[117]);
c22_0 = _mm256_add_pd(c22_0, _mm256_mul_pd(a22_0, b22));
_mm256_storeu_pd(&C[(i*35)+0], c22_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a22_0 = _mm_loadu_pd(&values[117]);
c22_0 = _mm_add_pd(c22_0, _mm_mul_pd(a22_0, b22));
_mm_storeu_pd(&C[(i*35)+0], c22_0);
__m128d c22_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a22_2 = _mm_loadu_pd(&values[119]);
c22_2 = _mm_add_pd(c22_2, _mm_mul_pd(a22_2, b22));
_mm_storeu_pd(&C[(i*35)+2], c22_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a22_4 = _mm256_loadu_pd(&values[121]);
c22_4 = _mm256_add_pd(c22_4, _mm256_mul_pd(a22_4, b22));
_mm256_storeu_pd(&C[(i*35)+4], c22_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a22_4 = _mm_loadu_pd(&values[121]);
c22_4 = _mm_add_pd(c22_4, _mm_mul_pd(a22_4, b22));
_mm_storeu_pd(&C[(i*35)+4], c22_4);
__m128d c22_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a22_6 = _mm_loadu_pd(&values[123]);
c22_6 = _mm_add_pd(c22_6, _mm_mul_pd(a22_6, b22));
_mm_storeu_pd(&C[(i*35)+6], c22_6);
#endif
__m128d c22_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a22_8 = _mm_loadu_pd(&values[125]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, b22));
#endif
_mm_storeu_pd(&C[(i*35)+8], c22_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_10 = _mm256_loadu_pd(&C[(i*35)+11]);
__m256d a22_10 = _mm256_loadu_pd(&values[127]);
c22_10 = _mm256_add_pd(c22_10, _mm256_mul_pd(a22_10, b22));
_mm256_storeu_pd(&C[(i*35)+11], c22_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_10 = _mm_loadu_pd(&C[(i*35)+11]);
__m128d a22_10 = _mm_loadu_pd(&values[127]);
c22_10 = _mm_add_pd(c22_10, _mm_mul_pd(a22_10, b22));
_mm_storeu_pd(&C[(i*35)+11], c22_10);
__m128d c22_12 = _mm_loadu_pd(&C[(i*35)+13]);
__m128d a22_12 = _mm_loadu_pd(&values[129]);
c22_12 = _mm_add_pd(c22_12, _mm_mul_pd(a22_12, b22));
_mm_storeu_pd(&C[(i*35)+13], c22_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_14 = _mm256_loadu_pd(&C[(i*35)+15]);
__m256d a22_14 = _mm256_loadu_pd(&values[131]);
c22_14 = _mm256_add_pd(c22_14, _mm256_mul_pd(a22_14, b22));
_mm256_storeu_pd(&C[(i*35)+15], c22_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_14 = _mm_loadu_pd(&C[(i*35)+15]);
__m128d a22_14 = _mm_loadu_pd(&values[131]);
c22_14 = _mm_add_pd(c22_14, _mm_mul_pd(a22_14, b22));
_mm_storeu_pd(&C[(i*35)+15], c22_14);
__m128d c22_16 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a22_16 = _mm_loadu_pd(&values[133]);
c22_16 = _mm_add_pd(c22_16, _mm_mul_pd(a22_16, b22));
_mm_storeu_pd(&C[(i*35)+17], c22_16);
#endif
__m128d c22_18 = _mm_load_sd(&C[(i*35)+19]);
__m128d a22_18 = _mm_load_sd(&values[135]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, b22));
#endif
_mm_store_sd(&C[(i*35)+19], c22_18);
#else
C[(i*35)+0] += values[117] * B[(i*35)+22];
C[(i*35)+1] += values[118] * B[(i*35)+22];
C[(i*35)+2] += values[119] * B[(i*35)+22];
C[(i*35)+3] += values[120] * B[(i*35)+22];
C[(i*35)+4] += values[121] * B[(i*35)+22];
C[(i*35)+5] += values[122] * B[(i*35)+22];
C[(i*35)+6] += values[123] * B[(i*35)+22];
C[(i*35)+7] += values[124] * B[(i*35)+22];
C[(i*35)+8] += values[125] * B[(i*35)+22];
C[(i*35)+9] += values[126] * B[(i*35)+22];
C[(i*35)+11] += values[127] * B[(i*35)+22];
C[(i*35)+12] += values[128] * B[(i*35)+22];
C[(i*35)+13] += values[129] * B[(i*35)+22];
C[(i*35)+14] += values[130] * B[(i*35)+22];
C[(i*35)+15] += values[131] * B[(i*35)+22];
C[(i*35)+16] += values[132] * B[(i*35)+22];
C[(i*35)+17] += values[133] * B[(i*35)+22];
C[(i*35)+18] += values[134] * B[(i*35)+22];
C[(i*35)+19] += values[135] * B[(i*35)+22];
#endif
#ifndef NDEBUG
num_flops += 38;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*35)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*35)+23]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a23_0 = _mm256_loadu_pd(&values[136]);
c23_0 = _mm256_add_pd(c23_0, _mm256_mul_pd(a23_0, b23));
_mm256_storeu_pd(&C[(i*35)+0], c23_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a23_0 = _mm_loadu_pd(&values[136]);
c23_0 = _mm_add_pd(c23_0, _mm_mul_pd(a23_0, b23));
_mm_storeu_pd(&C[(i*35)+0], c23_0);
__m128d c23_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a23_2 = _mm_loadu_pd(&values[138]);
c23_2 = _mm_add_pd(c23_2, _mm_mul_pd(a23_2, b23));
_mm_storeu_pd(&C[(i*35)+2], c23_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_4 = _mm256_loadu_pd(&C[(i*35)+5]);
__m256d a23_4 = _mm256_loadu_pd(&values[140]);
c23_4 = _mm256_add_pd(c23_4, _mm256_mul_pd(a23_4, b23));
_mm256_storeu_pd(&C[(i*35)+5], c23_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_4 = _mm_loadu_pd(&C[(i*35)+5]);
__m128d a23_4 = _mm_loadu_pd(&values[140]);
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, b23));
_mm_storeu_pd(&C[(i*35)+5], c23_4);
__m128d c23_6 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a23_6 = _mm_loadu_pd(&values[142]);
c23_6 = _mm_add_pd(c23_6, _mm_mul_pd(a23_6, b23));
_mm_storeu_pd(&C[(i*35)+7], c23_6);
#endif
__m128d c23_8 = _mm_load_sd(&C[(i*35)+9]);
__m128d a23_8 = _mm_load_sd(&values[144]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, b23));
#endif
_mm_store_sd(&C[(i*35)+9], c23_8);
__m128d c23_9 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a23_9 = _mm_loadu_pd(&values[145]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, b23));
#endif
_mm_storeu_pd(&C[(i*35)+12], c23_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_11 = _mm256_loadu_pd(&C[(i*35)+15]);
__m256d a23_11 = _mm256_loadu_pd(&values[147]);
c23_11 = _mm256_add_pd(c23_11, _mm256_mul_pd(a23_11, b23));
_mm256_storeu_pd(&C[(i*35)+15], c23_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_11 = _mm_loadu_pd(&C[(i*35)+15]);
__m128d a23_11 = _mm_loadu_pd(&values[147]);
c23_11 = _mm_add_pd(c23_11, _mm_mul_pd(a23_11, b23));
_mm_storeu_pd(&C[(i*35)+15], c23_11);
__m128d c23_13 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a23_13 = _mm_loadu_pd(&values[149]);
c23_13 = _mm_add_pd(c23_13, _mm_mul_pd(a23_13, b23));
_mm_storeu_pd(&C[(i*35)+17], c23_13);
#endif
__m128d c23_15 = _mm_load_sd(&C[(i*35)+19]);
__m128d a23_15 = _mm_load_sd(&values[151]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, b23));
#endif
_mm_store_sd(&C[(i*35)+19], c23_15);
#else
C[(i*35)+0] += values[136] * B[(i*35)+23];
C[(i*35)+1] += values[137] * B[(i*35)+23];
C[(i*35)+2] += values[138] * B[(i*35)+23];
C[(i*35)+3] += values[139] * B[(i*35)+23];
C[(i*35)+5] += values[140] * B[(i*35)+23];
C[(i*35)+6] += values[141] * B[(i*35)+23];
C[(i*35)+7] += values[142] * B[(i*35)+23];
C[(i*35)+8] += values[143] * B[(i*35)+23];
C[(i*35)+9] += values[144] * B[(i*35)+23];
C[(i*35)+12] += values[145] * B[(i*35)+23];
C[(i*35)+13] += values[146] * B[(i*35)+23];
C[(i*35)+15] += values[147] * B[(i*35)+23];
C[(i*35)+16] += values[148] * B[(i*35)+23];
C[(i*35)+17] += values[149] * B[(i*35)+23];
C[(i*35)+18] += values[150] * B[(i*35)+23];
C[(i*35)+19] += values[151] * B[(i*35)+23];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b24 = _mm256_broadcast_sd(&B[(i*35)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b24 = _mm_loaddup_pd(&B[(i*35)+24]);
#endif
__m128d c24_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a24_0 = _mm_load_sd(&values[152]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, b24));
#endif
_mm_store_sd(&C[(i*35)+0], c24_0);
__m128d c24_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a24_1 = _mm_loadu_pd(&values[153]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, b24));
#endif
_mm_storeu_pd(&C[(i*35)+2], c24_1);
__m128d c24_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a24_3 = _mm_load_sd(&values[155]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, b24));
#endif
_mm_store_sd(&C[(i*35)+6], c24_3);
__m128d c24_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a24_4 = _mm_loadu_pd(&values[156]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, b24));
#endif
_mm_storeu_pd(&C[(i*35)+8], c24_4);
__m128d c24_6 = _mm_load_sd(&C[(i*35)+13]);
__m128d a24_6 = _mm_load_sd(&values[158]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, b24));
#endif
_mm_store_sd(&C[(i*35)+13], c24_6);
__m128d c24_7 = _mm_load_sd(&C[(i*35)+16]);
__m128d a24_7 = _mm_load_sd(&values[159]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, b24));
#endif
_mm_store_sd(&C[(i*35)+16], c24_7);
__m128d c24_8 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a24_8 = _mm_loadu_pd(&values[160]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, b24));
#endif
_mm_storeu_pd(&C[(i*35)+18], c24_8);
#else
C[(i*35)+0] += values[152] * B[(i*35)+24];
C[(i*35)+2] += values[153] * B[(i*35)+24];
C[(i*35)+3] += values[154] * B[(i*35)+24];
C[(i*35)+6] += values[155] * B[(i*35)+24];
C[(i*35)+8] += values[156] * B[(i*35)+24];
C[(i*35)+9] += values[157] * B[(i*35)+24];
C[(i*35)+13] += values[158] * B[(i*35)+24];
C[(i*35)+16] += values[159] * B[(i*35)+24];
C[(i*35)+18] += values[160] * B[(i*35)+24];
C[(i*35)+19] += values[161] * B[(i*35)+24];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*35)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*35)+25]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a25_0 = _mm256_loadu_pd(&values[162]);
c25_0 = _mm256_add_pd(c25_0, _mm256_mul_pd(a25_0, b25));
_mm256_storeu_pd(&C[(i*35)+0], c25_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a25_0 = _mm_loadu_pd(&values[162]);
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, b25));
_mm_storeu_pd(&C[(i*35)+0], c25_0);
__m128d c25_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a25_2 = _mm_loadu_pd(&values[164]);
c25_2 = _mm_add_pd(c25_2, _mm_mul_pd(a25_2, b25));
_mm_storeu_pd(&C[(i*35)+2], c25_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a25_4 = _mm256_loadu_pd(&values[166]);
c25_4 = _mm256_add_pd(c25_4, _mm256_mul_pd(a25_4, b25));
_mm256_storeu_pd(&C[(i*35)+4], c25_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a25_4 = _mm_loadu_pd(&values[166]);
c25_4 = _mm_add_pd(c25_4, _mm_mul_pd(a25_4, b25));
_mm_storeu_pd(&C[(i*35)+4], c25_4);
__m128d c25_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a25_6 = _mm_loadu_pd(&values[168]);
c25_6 = _mm_add_pd(c25_6, _mm_mul_pd(a25_6, b25));
_mm_storeu_pd(&C[(i*35)+6], c25_6);
#endif
__m128d c25_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a25_8 = _mm_loadu_pd(&values[170]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, b25));
#endif
_mm_storeu_pd(&C[(i*35)+8], c25_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_10 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a25_10 = _mm256_loadu_pd(&values[172]);
c25_10 = _mm256_add_pd(c25_10, _mm256_mul_pd(a25_10, b25));
_mm256_storeu_pd(&C[(i*35)+14], c25_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_10 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a25_10 = _mm_loadu_pd(&values[172]);
c25_10 = _mm_add_pd(c25_10, _mm_mul_pd(a25_10, b25));
_mm_storeu_pd(&C[(i*35)+14], c25_10);
__m128d c25_12 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a25_12 = _mm_loadu_pd(&values[174]);
c25_12 = _mm_add_pd(c25_12, _mm_mul_pd(a25_12, b25));
_mm_storeu_pd(&C[(i*35)+16], c25_12);
#endif
__m128d c25_14 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a25_14 = _mm_loadu_pd(&values[176]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_14 = _mm_add_pd(c25_14, _mm_mul_pd(a25_14, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_14 = _mm_add_pd(c25_14, _mm_mul_pd(a25_14, b25));
#endif
_mm_storeu_pd(&C[(i*35)+18], c25_14);
#else
C[(i*35)+0] += values[162] * B[(i*35)+25];
C[(i*35)+1] += values[163] * B[(i*35)+25];
C[(i*35)+2] += values[164] * B[(i*35)+25];
C[(i*35)+3] += values[165] * B[(i*35)+25];
C[(i*35)+4] += values[166] * B[(i*35)+25];
C[(i*35)+5] += values[167] * B[(i*35)+25];
C[(i*35)+6] += values[168] * B[(i*35)+25];
C[(i*35)+7] += values[169] * B[(i*35)+25];
C[(i*35)+8] += values[170] * B[(i*35)+25];
C[(i*35)+9] += values[171] * B[(i*35)+25];
C[(i*35)+14] += values[172] * B[(i*35)+25];
C[(i*35)+15] += values[173] * B[(i*35)+25];
C[(i*35)+16] += values[174] * B[(i*35)+25];
C[(i*35)+17] += values[175] * B[(i*35)+25];
C[(i*35)+18] += values[176] * B[(i*35)+25];
C[(i*35)+19] += values[177] * B[(i*35)+25];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*35)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*35)+26]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a26_0 = _mm256_loadu_pd(&values[178]);
c26_0 = _mm256_add_pd(c26_0, _mm256_mul_pd(a26_0, b26));
_mm256_storeu_pd(&C[(i*35)+0], c26_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a26_0 = _mm_loadu_pd(&values[178]);
c26_0 = _mm_add_pd(c26_0, _mm_mul_pd(a26_0, b26));
_mm_storeu_pd(&C[(i*35)+0], c26_0);
__m128d c26_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a26_2 = _mm_loadu_pd(&values[180]);
c26_2 = _mm_add_pd(c26_2, _mm_mul_pd(a26_2, b26));
_mm_storeu_pd(&C[(i*35)+2], c26_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a26_4 = _mm256_loadu_pd(&values[182]);
c26_4 = _mm256_add_pd(c26_4, _mm256_mul_pd(a26_4, b26));
_mm256_storeu_pd(&C[(i*35)+4], c26_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a26_4 = _mm_loadu_pd(&values[182]);
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, b26));
_mm_storeu_pd(&C[(i*35)+4], c26_4);
__m128d c26_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a26_6 = _mm_loadu_pd(&values[184]);
c26_6 = _mm_add_pd(c26_6, _mm_mul_pd(a26_6, b26));
_mm_storeu_pd(&C[(i*35)+6], c26_6);
#endif
__m128d c26_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a26_8 = _mm_loadu_pd(&values[186]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, b26));
#endif
_mm_storeu_pd(&C[(i*35)+8], c26_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_10 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a26_10 = _mm256_loadu_pd(&values[188]);
c26_10 = _mm256_add_pd(c26_10, _mm256_mul_pd(a26_10, b26));
_mm256_storeu_pd(&C[(i*35)+14], c26_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_10 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a26_10 = _mm_loadu_pd(&values[188]);
c26_10 = _mm_add_pd(c26_10, _mm_mul_pd(a26_10, b26));
_mm_storeu_pd(&C[(i*35)+14], c26_10);
__m128d c26_12 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a26_12 = _mm_loadu_pd(&values[190]);
c26_12 = _mm_add_pd(c26_12, _mm_mul_pd(a26_12, b26));
_mm_storeu_pd(&C[(i*35)+16], c26_12);
#endif
__m128d c26_14 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a26_14 = _mm_loadu_pd(&values[192]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_14 = _mm_add_pd(c26_14, _mm_mul_pd(a26_14, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_14 = _mm_add_pd(c26_14, _mm_mul_pd(a26_14, b26));
#endif
_mm_storeu_pd(&C[(i*35)+18], c26_14);
#else
C[(i*35)+0] += values[178] * B[(i*35)+26];
C[(i*35)+1] += values[179] * B[(i*35)+26];
C[(i*35)+2] += values[180] * B[(i*35)+26];
C[(i*35)+3] += values[181] * B[(i*35)+26];
C[(i*35)+4] += values[182] * B[(i*35)+26];
C[(i*35)+5] += values[183] * B[(i*35)+26];
C[(i*35)+6] += values[184] * B[(i*35)+26];
C[(i*35)+7] += values[185] * B[(i*35)+26];
C[(i*35)+8] += values[186] * B[(i*35)+26];
C[(i*35)+9] += values[187] * B[(i*35)+26];
C[(i*35)+14] += values[188] * B[(i*35)+26];
C[(i*35)+15] += values[189] * B[(i*35)+26];
C[(i*35)+16] += values[190] * B[(i*35)+26];
C[(i*35)+17] += values[191] * B[(i*35)+26];
C[(i*35)+18] += values[192] * B[(i*35)+26];
C[(i*35)+19] += values[193] * B[(i*35)+26];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*35)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*35)+27]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a27_0 = _mm256_loadu_pd(&values[194]);
c27_0 = _mm256_add_pd(c27_0, _mm256_mul_pd(a27_0, b27));
_mm256_storeu_pd(&C[(i*35)+0], c27_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a27_0 = _mm_loadu_pd(&values[194]);
c27_0 = _mm_add_pd(c27_0, _mm_mul_pd(a27_0, b27));
_mm_storeu_pd(&C[(i*35)+0], c27_0);
__m128d c27_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a27_2 = _mm_loadu_pd(&values[196]);
c27_2 = _mm_add_pd(c27_2, _mm_mul_pd(a27_2, b27));
_mm_storeu_pd(&C[(i*35)+2], c27_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_4 = _mm256_loadu_pd(&C[(i*35)+5]);
__m256d a27_4 = _mm256_loadu_pd(&values[198]);
c27_4 = _mm256_add_pd(c27_4, _mm256_mul_pd(a27_4, b27));
_mm256_storeu_pd(&C[(i*35)+5], c27_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_4 = _mm_loadu_pd(&C[(i*35)+5]);
__m128d a27_4 = _mm_loadu_pd(&values[198]);
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, b27));
_mm_storeu_pd(&C[(i*35)+5], c27_4);
__m128d c27_6 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a27_6 = _mm_loadu_pd(&values[200]);
c27_6 = _mm_add_pd(c27_6, _mm_mul_pd(a27_6, b27));
_mm_storeu_pd(&C[(i*35)+7], c27_6);
#endif
__m128d c27_8 = _mm_load_sd(&C[(i*35)+9]);
__m128d a27_8 = _mm_load_sd(&values[202]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, b27));
#endif
_mm_store_sd(&C[(i*35)+9], c27_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_9 = _mm256_loadu_pd(&C[(i*35)+15]);
__m256d a27_9 = _mm256_loadu_pd(&values[203]);
c27_9 = _mm256_add_pd(c27_9, _mm256_mul_pd(a27_9, b27));
_mm256_storeu_pd(&C[(i*35)+15], c27_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_9 = _mm_loadu_pd(&C[(i*35)+15]);
__m128d a27_9 = _mm_loadu_pd(&values[203]);
c27_9 = _mm_add_pd(c27_9, _mm_mul_pd(a27_9, b27));
_mm_storeu_pd(&C[(i*35)+15], c27_9);
__m128d c27_11 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a27_11 = _mm_loadu_pd(&values[205]);
c27_11 = _mm_add_pd(c27_11, _mm_mul_pd(a27_11, b27));
_mm_storeu_pd(&C[(i*35)+17], c27_11);
#endif
__m128d c27_13 = _mm_load_sd(&C[(i*35)+19]);
__m128d a27_13 = _mm_load_sd(&values[207]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_13 = _mm_add_sd(c27_13, _mm_mul_sd(a27_13, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_13 = _mm_add_sd(c27_13, _mm_mul_sd(a27_13, b27));
#endif
_mm_store_sd(&C[(i*35)+19], c27_13);
#else
C[(i*35)+0] += values[194] * B[(i*35)+27];
C[(i*35)+1] += values[195] * B[(i*35)+27];
C[(i*35)+2] += values[196] * B[(i*35)+27];
C[(i*35)+3] += values[197] * B[(i*35)+27];
C[(i*35)+5] += values[198] * B[(i*35)+27];
C[(i*35)+6] += values[199] * B[(i*35)+27];
C[(i*35)+7] += values[200] * B[(i*35)+27];
C[(i*35)+8] += values[201] * B[(i*35)+27];
C[(i*35)+9] += values[202] * B[(i*35)+27];
C[(i*35)+15] += values[203] * B[(i*35)+27];
C[(i*35)+16] += values[204] * B[(i*35)+27];
C[(i*35)+17] += values[205] * B[(i*35)+27];
C[(i*35)+18] += values[206] * B[(i*35)+27];
C[(i*35)+19] += values[207] * B[(i*35)+27];
#endif
#ifndef NDEBUG
num_flops += 28;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b28 = _mm256_broadcast_sd(&B[(i*35)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b28 = _mm_loaddup_pd(&B[(i*35)+28]);
#endif
__m128d c28_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a28_0 = _mm_load_sd(&values[208]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, b28));
#endif
_mm_store_sd(&C[(i*35)+0], c28_0);
__m128d c28_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a28_1 = _mm_loadu_pd(&values[209]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, b28));
#endif
_mm_storeu_pd(&C[(i*35)+2], c28_1);
__m128d c28_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a28_3 = _mm_load_sd(&values[211]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, b28));
#endif
_mm_store_sd(&C[(i*35)+6], c28_3);
__m128d c28_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a28_4 = _mm_loadu_pd(&values[212]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, b28));
#endif
_mm_storeu_pd(&C[(i*35)+8], c28_4);
__m128d c28_6 = _mm_load_sd(&C[(i*35)+16]);
__m128d a28_6 = _mm_load_sd(&values[214]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, b28));
#endif
_mm_store_sd(&C[(i*35)+16], c28_6);
__m128d c28_7 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a28_7 = _mm_loadu_pd(&values[215]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_7 = _mm_add_pd(c28_7, _mm_mul_pd(a28_7, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_7 = _mm_add_pd(c28_7, _mm_mul_pd(a28_7, b28));
#endif
_mm_storeu_pd(&C[(i*35)+18], c28_7);
#else
C[(i*35)+0] += values[208] * B[(i*35)+28];
C[(i*35)+2] += values[209] * B[(i*35)+28];
C[(i*35)+3] += values[210] * B[(i*35)+28];
C[(i*35)+6] += values[211] * B[(i*35)+28];
C[(i*35)+8] += values[212] * B[(i*35)+28];
C[(i*35)+9] += values[213] * B[(i*35)+28];
C[(i*35)+16] += values[214] * B[(i*35)+28];
C[(i*35)+18] += values[215] * B[(i*35)+28];
C[(i*35)+19] += values[216] * B[(i*35)+28];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*35)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*35)+29]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c29_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a29_0 = _mm256_loadu_pd(&values[217]);
c29_0 = _mm256_add_pd(c29_0, _mm256_mul_pd(a29_0, b29));
_mm256_storeu_pd(&C[(i*35)+0], c29_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c29_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a29_0 = _mm_loadu_pd(&values[217]);
c29_0 = _mm_add_pd(c29_0, _mm_mul_pd(a29_0, b29));
_mm_storeu_pd(&C[(i*35)+0], c29_0);
__m128d c29_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a29_2 = _mm_loadu_pd(&values[219]);
c29_2 = _mm_add_pd(c29_2, _mm_mul_pd(a29_2, b29));
_mm_storeu_pd(&C[(i*35)+2], c29_2);
#endif
__m128d c29_4 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a29_4 = _mm_loadu_pd(&values[221]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, b29));
#endif
_mm_storeu_pd(&C[(i*35)+7], c29_4);
__m128d c29_6 = _mm_load_sd(&C[(i*35)+9]);
__m128d a29_6 = _mm_load_sd(&values[223]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, b29));
#endif
_mm_store_sd(&C[(i*35)+9], c29_6);
__m128d c29_7 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a29_7 = _mm_loadu_pd(&values[224]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, b29));
#endif
_mm_storeu_pd(&C[(i*35)+17], c29_7);
__m128d c29_9 = _mm_load_sd(&C[(i*35)+19]);
__m128d a29_9 = _mm_load_sd(&values[226]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_9 = _mm_add_sd(c29_9, _mm_mul_sd(a29_9, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_9 = _mm_add_sd(c29_9, _mm_mul_sd(a29_9, b29));
#endif
_mm_store_sd(&C[(i*35)+19], c29_9);
#else
C[(i*35)+0] += values[217] * B[(i*35)+29];
C[(i*35)+1] += values[218] * B[(i*35)+29];
C[(i*35)+2] += values[219] * B[(i*35)+29];
C[(i*35)+3] += values[220] * B[(i*35)+29];
C[(i*35)+7] += values[221] * B[(i*35)+29];
C[(i*35)+8] += values[222] * B[(i*35)+29];
C[(i*35)+9] += values[223] * B[(i*35)+29];
C[(i*35)+17] += values[224] * B[(i*35)+29];
C[(i*35)+18] += values[225] * B[(i*35)+29];
C[(i*35)+19] += values[226] * B[(i*35)+29];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*35)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*35)+30]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a30_0 = _mm256_loadu_pd(&values[227]);
c30_0 = _mm256_add_pd(c30_0, _mm256_mul_pd(a30_0, b30));
_mm256_storeu_pd(&C[(i*35)+0], c30_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a30_0 = _mm_loadu_pd(&values[227]);
c30_0 = _mm_add_pd(c30_0, _mm_mul_pd(a30_0, b30));
_mm_storeu_pd(&C[(i*35)+0], c30_0);
__m128d c30_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a30_2 = _mm_loadu_pd(&values[229]);
c30_2 = _mm_add_pd(c30_2, _mm_mul_pd(a30_2, b30));
_mm_storeu_pd(&C[(i*35)+2], c30_2);
#endif
__m128d c30_4 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a30_4 = _mm_loadu_pd(&values[231]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_4 = _mm_add_pd(c30_4, _mm_mul_pd(a30_4, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_4 = _mm_add_pd(c30_4, _mm_mul_pd(a30_4, b30));
#endif
_mm_storeu_pd(&C[(i*35)+7], c30_4);
__m128d c30_6 = _mm_load_sd(&C[(i*35)+9]);
__m128d a30_6 = _mm_load_sd(&values[233]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_6 = _mm_add_sd(c30_6, _mm_mul_sd(a30_6, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_6 = _mm_add_sd(c30_6, _mm_mul_sd(a30_6, b30));
#endif
_mm_store_sd(&C[(i*35)+9], c30_6);
__m128d c30_7 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a30_7 = _mm_loadu_pd(&values[234]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_7 = _mm_add_pd(c30_7, _mm_mul_pd(a30_7, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_7 = _mm_add_pd(c30_7, _mm_mul_pd(a30_7, b30));
#endif
_mm_storeu_pd(&C[(i*35)+17], c30_7);
__m128d c30_9 = _mm_load_sd(&C[(i*35)+19]);
__m128d a30_9 = _mm_load_sd(&values[236]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_9 = _mm_add_sd(c30_9, _mm_mul_sd(a30_9, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_9 = _mm_add_sd(c30_9, _mm_mul_sd(a30_9, b30));
#endif
_mm_store_sd(&C[(i*35)+19], c30_9);
#else
C[(i*35)+0] += values[227] * B[(i*35)+30];
C[(i*35)+1] += values[228] * B[(i*35)+30];
C[(i*35)+2] += values[229] * B[(i*35)+30];
C[(i*35)+3] += values[230] * B[(i*35)+30];
C[(i*35)+7] += values[231] * B[(i*35)+30];
C[(i*35)+8] += values[232] * B[(i*35)+30];
C[(i*35)+9] += values[233] * B[(i*35)+30];
C[(i*35)+17] += values[234] * B[(i*35)+30];
C[(i*35)+18] += values[235] * B[(i*35)+30];
C[(i*35)+19] += values[236] * B[(i*35)+30];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b31 = _mm256_broadcast_sd(&B[(i*35)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b31 = _mm_loaddup_pd(&B[(i*35)+31]);
#endif
__m128d c31_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a31_0 = _mm_load_sd(&values[237]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, b31));
#endif
_mm_store_sd(&C[(i*35)+0], c31_0);
__m128d c31_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a31_1 = _mm_loadu_pd(&values[238]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, b31));
#endif
_mm_storeu_pd(&C[(i*35)+2], c31_1);
__m128d c31_3 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a31_3 = _mm_loadu_pd(&values[240]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_3 = _mm_add_pd(c31_3, _mm_mul_pd(a31_3, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_3 = _mm_add_pd(c31_3, _mm_mul_pd(a31_3, b31));
#endif
_mm_storeu_pd(&C[(i*35)+8], c31_3);
__m128d c31_5 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a31_5 = _mm_loadu_pd(&values[242]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_5 = _mm_add_pd(c31_5, _mm_mul_pd(a31_5, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_5 = _mm_add_pd(c31_5, _mm_mul_pd(a31_5, b31));
#endif
_mm_storeu_pd(&C[(i*35)+18], c31_5);
#else
C[(i*35)+0] += values[237] * B[(i*35)+31];
C[(i*35)+2] += values[238] * B[(i*35)+31];
C[(i*35)+3] += values[239] * B[(i*35)+31];
C[(i*35)+8] += values[240] * B[(i*35)+31];
C[(i*35)+9] += values[241] * B[(i*35)+31];
C[(i*35)+18] += values[242] * B[(i*35)+31];
C[(i*35)+19] += values[243] * B[(i*35)+31];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*35)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*35)+32]);
#endif
__m128d c32_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a32_0 = _mm_load_sd(&values[244]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, b32));
#endif
_mm_store_sd(&C[(i*35)+0], c32_0);
__m128d c32_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a32_1 = _mm_load_sd(&values[245]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, b32));
#endif
_mm_store_sd(&C[(i*35)+3], c32_1);
__m128d c32_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a32_2 = _mm_load_sd(&values[246]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, b32));
#endif
_mm_store_sd(&C[(i*35)+9], c32_2);
__m128d c32_3 = _mm_load_sd(&C[(i*35)+19]);
__m128d a32_3 = _mm_load_sd(&values[247]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, b32));
#endif
_mm_store_sd(&C[(i*35)+19], c32_3);
#else
C[(i*35)+0] += values[244] * B[(i*35)+32];
C[(i*35)+3] += values[245] * B[(i*35)+32];
C[(i*35)+9] += values[246] * B[(i*35)+32];
C[(i*35)+19] += values[247] * B[(i*35)+32];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b33 = _mm256_broadcast_sd(&B[(i*35)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b33 = _mm_loaddup_pd(&B[(i*35)+33]);
#endif
__m128d c33_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a33_0 = _mm_load_sd(&values[248]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, b33));
#endif
_mm_store_sd(&C[(i*35)+0], c33_0);
__m128d c33_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a33_1 = _mm_load_sd(&values[249]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_1 = _mm_add_sd(c33_1, _mm_mul_sd(a33_1, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_1 = _mm_add_sd(c33_1, _mm_mul_sd(a33_1, b33));
#endif
_mm_store_sd(&C[(i*35)+3], c33_1);
__m128d c33_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a33_2 = _mm_load_sd(&values[250]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_2 = _mm_add_sd(c33_2, _mm_mul_sd(a33_2, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_2 = _mm_add_sd(c33_2, _mm_mul_sd(a33_2, b33));
#endif
_mm_store_sd(&C[(i*35)+9], c33_2);
__m128d c33_3 = _mm_load_sd(&C[(i*35)+19]);
__m128d a33_3 = _mm_load_sd(&values[251]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_3 = _mm_add_sd(c33_3, _mm_mul_sd(a33_3, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_3 = _mm_add_sd(c33_3, _mm_mul_sd(a33_3, b33));
#endif
_mm_store_sd(&C[(i*35)+19], c33_3);
#else
C[(i*35)+0] += values[248] * B[(i*35)+33];
C[(i*35)+3] += values[249] * B[(i*35)+33];
C[(i*35)+9] += values[250] * B[(i*35)+33];
C[(i*35)+19] += values[251] * B[(i*35)+33];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivMT_9_35(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*35)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*35)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*35)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*35)+0], c1_0);
#else
C[(i*35)+0] += values[0] * B[(i*35)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*35)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*35)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*35)+0], c2_0);
#else
C[(i*35)+0] += values[1] * B[(i*35)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*35)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*35)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a3_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*35)+0], c3_0);
#else
C[(i*35)+0] += values[2] * B[(i*35)+3];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*35)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[3]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*35)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[3]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*35)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[5]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*35)+2], c4_2);
#endif
#else
C[(i*35)+0] += values[3] * B[(i*35)+4];
C[(i*35)+1] += values[4] * B[(i*35)+4];
C[(i*35)+2] += values[5] * B[(i*35)+4];
C[(i*35)+3] += values[6] * B[(i*35)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*35)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[7]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*35)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[7]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*35)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[9]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*35)+2], c5_2);
#endif
#else
C[(i*35)+0] += values[7] * B[(i*35)+5];
C[(i*35)+1] += values[8] * B[(i*35)+5];
C[(i*35)+2] += values[9] * B[(i*35)+5];
C[(i*35)+3] += values[10] * B[(i*35)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*35)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*35)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a6_0 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*35)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*35)+2], c6_1);
#else
C[(i*35)+0] += values[11] * B[(i*35)+6];
C[(i*35)+2] += values[12] * B[(i*35)+6];
C[(i*35)+3] += values[13] * B[(i*35)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*35)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*35)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a7_0 = _mm_loadu_pd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*35)+0], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*35)+3]);
__m128d a7_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*35)+3], c7_2);
#else
C[(i*35)+0] += values[14] * B[(i*35)+7];
C[(i*35)+1] += values[15] * B[(i*35)+7];
C[(i*35)+3] += values[16] * B[(i*35)+7];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*35)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*35)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a8_0 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*35)+0], c8_0);
__m128d c8_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a8_1 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, b8));
#endif
_mm_storeu_pd(&C[(i*35)+2], c8_1);
#else
C[(i*35)+0] += values[17] * B[(i*35)+8];
C[(i*35)+2] += values[18] * B[(i*35)+8];
C[(i*35)+3] += values[19] * B[(i*35)+8];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*35)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*35)+9]);
#endif
__m128d c9_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a9_0 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, b9));
#endif
_mm_store_sd(&C[(i*35)+0], c9_0);
__m128d c9_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a9_1 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, b9));
#endif
_mm_store_sd(&C[(i*35)+3], c9_1);
#else
C[(i*35)+0] += values[20] * B[(i*35)+9];
C[(i*35)+3] += values[21] * B[(i*35)+9];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*35)+10]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a10_0 = _mm256_loadu_pd(&values[22]);
c10_0 = _mm256_add_pd(c10_0, _mm256_mul_pd(a10_0, b10));
_mm256_storeu_pd(&C[(i*35)+0], c10_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a10_0 = _mm_loadu_pd(&values[22]);
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
_mm_storeu_pd(&C[(i*35)+0], c10_0);
__m128d c10_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a10_2 = _mm_loadu_pd(&values[24]);
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, b10));
_mm_storeu_pd(&C[(i*35)+2], c10_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a10_4 = _mm256_loadu_pd(&values[26]);
c10_4 = _mm256_add_pd(c10_4, _mm256_mul_pd(a10_4, b10));
_mm256_storeu_pd(&C[(i*35)+4], c10_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a10_4 = _mm_loadu_pd(&values[26]);
c10_4 = _mm_add_pd(c10_4, _mm_mul_pd(a10_4, b10));
_mm_storeu_pd(&C[(i*35)+4], c10_4);
__m128d c10_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a10_6 = _mm_loadu_pd(&values[28]);
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, b10));
_mm_storeu_pd(&C[(i*35)+6], c10_6);
#endif
__m128d c10_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a10_8 = _mm_loadu_pd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, b10));
#endif
_mm_storeu_pd(&C[(i*35)+8], c10_8);
#else
C[(i*35)+0] += values[22] * B[(i*35)+10];
C[(i*35)+1] += values[23] * B[(i*35)+10];
C[(i*35)+2] += values[24] * B[(i*35)+10];
C[(i*35)+3] += values[25] * B[(i*35)+10];
C[(i*35)+4] += values[26] * B[(i*35)+10];
C[(i*35)+5] += values[27] * B[(i*35)+10];
C[(i*35)+6] += values[28] * B[(i*35)+10];
C[(i*35)+7] += values[29] * B[(i*35)+10];
C[(i*35)+8] += values[30] * B[(i*35)+10];
C[(i*35)+9] += values[31] * B[(i*35)+10];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*35)+11]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a11_0 = _mm256_loadu_pd(&values[32]);
c11_0 = _mm256_add_pd(c11_0, _mm256_mul_pd(a11_0, b11));
_mm256_storeu_pd(&C[(i*35)+0], c11_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a11_0 = _mm_loadu_pd(&values[32]);
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
_mm_storeu_pd(&C[(i*35)+0], c11_0);
__m128d c11_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a11_2 = _mm_loadu_pd(&values[34]);
c11_2 = _mm_add_pd(c11_2, _mm_mul_pd(a11_2, b11));
_mm_storeu_pd(&C[(i*35)+2], c11_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a11_4 = _mm256_loadu_pd(&values[36]);
c11_4 = _mm256_add_pd(c11_4, _mm256_mul_pd(a11_4, b11));
_mm256_storeu_pd(&C[(i*35)+4], c11_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a11_4 = _mm_loadu_pd(&values[36]);
c11_4 = _mm_add_pd(c11_4, _mm_mul_pd(a11_4, b11));
_mm_storeu_pd(&C[(i*35)+4], c11_4);
__m128d c11_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a11_6 = _mm_loadu_pd(&values[38]);
c11_6 = _mm_add_pd(c11_6, _mm_mul_pd(a11_6, b11));
_mm_storeu_pd(&C[(i*35)+6], c11_6);
#endif
__m128d c11_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a11_8 = _mm_loadu_pd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, b11));
#endif
_mm_storeu_pd(&C[(i*35)+8], c11_8);
#else
C[(i*35)+0] += values[32] * B[(i*35)+11];
C[(i*35)+1] += values[33] * B[(i*35)+11];
C[(i*35)+2] += values[34] * B[(i*35)+11];
C[(i*35)+3] += values[35] * B[(i*35)+11];
C[(i*35)+4] += values[36] * B[(i*35)+11];
C[(i*35)+5] += values[37] * B[(i*35)+11];
C[(i*35)+6] += values[38] * B[(i*35)+11];
C[(i*35)+7] += values[39] * B[(i*35)+11];
C[(i*35)+8] += values[40] * B[(i*35)+11];
C[(i*35)+9] += values[41] * B[(i*35)+11];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*35)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a12_0 = _mm256_loadu_pd(&values[42]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*35)+0], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a12_0 = _mm_loadu_pd(&values[42]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*35)+0], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a12_2 = _mm_loadu_pd(&values[44]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*35)+2], c12_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_4 = _mm256_loadu_pd(&C[(i*35)+5]);
__m256d a12_4 = _mm256_loadu_pd(&values[46]);
c12_4 = _mm256_add_pd(c12_4, _mm256_mul_pd(a12_4, b12));
_mm256_storeu_pd(&C[(i*35)+5], c12_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_4 = _mm_loadu_pd(&C[(i*35)+5]);
__m128d a12_4 = _mm_loadu_pd(&values[46]);
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
_mm_storeu_pd(&C[(i*35)+5], c12_4);
__m128d c12_6 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a12_6 = _mm_loadu_pd(&values[48]);
c12_6 = _mm_add_pd(c12_6, _mm_mul_pd(a12_6, b12));
_mm_storeu_pd(&C[(i*35)+7], c12_6);
#endif
__m128d c12_8 = _mm_load_sd(&C[(i*35)+9]);
__m128d a12_8 = _mm_load_sd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, b12));
#endif
_mm_store_sd(&C[(i*35)+9], c12_8);
#else
C[(i*35)+0] += values[42] * B[(i*35)+12];
C[(i*35)+1] += values[43] * B[(i*35)+12];
C[(i*35)+2] += values[44] * B[(i*35)+12];
C[(i*35)+3] += values[45] * B[(i*35)+12];
C[(i*35)+5] += values[46] * B[(i*35)+12];
C[(i*35)+6] += values[47] * B[(i*35)+12];
C[(i*35)+7] += values[48] * B[(i*35)+12];
C[(i*35)+8] += values[49] * B[(i*35)+12];
C[(i*35)+9] += values[50] * B[(i*35)+12];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*35)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*35)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a13_0 = _mm_load_sd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*35)+0], c13_0);
__m128d c13_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a13_1 = _mm_loadu_pd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, b13));
#endif
_mm_storeu_pd(&C[(i*35)+2], c13_1);
__m128d c13_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a13_3 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, b13));
#endif
_mm_store_sd(&C[(i*35)+6], c13_3);
__m128d c13_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a13_4 = _mm_loadu_pd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, b13));
#endif
_mm_storeu_pd(&C[(i*35)+8], c13_4);
#else
C[(i*35)+0] += values[51] * B[(i*35)+13];
C[(i*35)+2] += values[52] * B[(i*35)+13];
C[(i*35)+3] += values[53] * B[(i*35)+13];
C[(i*35)+6] += values[54] * B[(i*35)+13];
C[(i*35)+8] += values[55] * B[(i*35)+13];
C[(i*35)+9] += values[56] * B[(i*35)+13];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*35)+14]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a14_0 = _mm256_loadu_pd(&values[57]);
c14_0 = _mm256_add_pd(c14_0, _mm256_mul_pd(a14_0, b14));
_mm256_storeu_pd(&C[(i*35)+0], c14_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a14_0 = _mm_loadu_pd(&values[57]);
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
_mm_storeu_pd(&C[(i*35)+0], c14_0);
__m128d c14_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a14_2 = _mm_loadu_pd(&values[59]);
c14_2 = _mm_add_pd(c14_2, _mm_mul_pd(a14_2, b14));
_mm_storeu_pd(&C[(i*35)+2], c14_2);
#endif
__m128d c14_4 = _mm_load_sd(&C[(i*35)+4]);
__m128d a14_4 = _mm_load_sd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, b14));
#endif
_mm_store_sd(&C[(i*35)+4], c14_4);
__m128d c14_5 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a14_5 = _mm_loadu_pd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_5 = _mm_add_pd(c14_5, _mm_mul_pd(a14_5, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_5 = _mm_add_pd(c14_5, _mm_mul_pd(a14_5, b14));
#endif
_mm_storeu_pd(&C[(i*35)+7], c14_5);
__m128d c14_7 = _mm_load_sd(&C[(i*35)+9]);
__m128d a14_7 = _mm_load_sd(&values[64]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_7 = _mm_add_sd(c14_7, _mm_mul_sd(a14_7, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_7 = _mm_add_sd(c14_7, _mm_mul_sd(a14_7, b14));
#endif
_mm_store_sd(&C[(i*35)+9], c14_7);
#else
C[(i*35)+0] += values[57] * B[(i*35)+14];
C[(i*35)+1] += values[58] * B[(i*35)+14];
C[(i*35)+2] += values[59] * B[(i*35)+14];
C[(i*35)+3] += values[60] * B[(i*35)+14];
C[(i*35)+4] += values[61] * B[(i*35)+14];
C[(i*35)+7] += values[62] * B[(i*35)+14];
C[(i*35)+8] += values[63] * B[(i*35)+14];
C[(i*35)+9] += values[64] * B[(i*35)+14];
#endif
#ifndef NDEBUG
num_flops += 16;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*35)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a15_0 = _mm256_loadu_pd(&values[65]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*35)+0], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a15_0 = _mm_loadu_pd(&values[65]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*35)+0], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a15_2 = _mm_loadu_pd(&values[67]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*35)+2], c15_2);
#endif
__m128d c15_4 = _mm_load_sd(&C[(i*35)+5]);
__m128d a15_4 = _mm_load_sd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, b15));
#endif
_mm_store_sd(&C[(i*35)+5], c15_4);
__m128d c15_5 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a15_5 = _mm_loadu_pd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_5 = _mm_add_pd(c15_5, _mm_mul_pd(a15_5, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_5 = _mm_add_pd(c15_5, _mm_mul_pd(a15_5, b15));
#endif
_mm_storeu_pd(&C[(i*35)+7], c15_5);
__m128d c15_7 = _mm_load_sd(&C[(i*35)+9]);
__m128d a15_7 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, b15));
#endif
_mm_store_sd(&C[(i*35)+9], c15_7);
#else
C[(i*35)+0] += values[65] * B[(i*35)+15];
C[(i*35)+1] += values[66] * B[(i*35)+15];
C[(i*35)+2] += values[67] * B[(i*35)+15];
C[(i*35)+3] += values[68] * B[(i*35)+15];
C[(i*35)+5] += values[69] * B[(i*35)+15];
C[(i*35)+7] += values[70] * B[(i*35)+15];
C[(i*35)+8] += values[71] * B[(i*35)+15];
C[(i*35)+9] += values[72] * B[(i*35)+15];
#endif
#ifndef NDEBUG
num_flops += 16;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*35)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*35)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a16_0 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*35)+0], c16_0);
__m128d c16_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a16_1 = _mm_loadu_pd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, b16));
#endif
_mm_storeu_pd(&C[(i*35)+2], c16_1);
__m128d c16_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a16_3 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, b16));
#endif
_mm_store_sd(&C[(i*35)+6], c16_3);
__m128d c16_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a16_4 = _mm_loadu_pd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, b16));
#endif
_mm_storeu_pd(&C[(i*35)+8], c16_4);
#else
C[(i*35)+0] += values[73] * B[(i*35)+16];
C[(i*35)+2] += values[74] * B[(i*35)+16];
C[(i*35)+3] += values[75] * B[(i*35)+16];
C[(i*35)+6] += values[76] * B[(i*35)+16];
C[(i*35)+8] += values[77] * B[(i*35)+16];
C[(i*35)+9] += values[78] * B[(i*35)+16];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*35)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*35)+17]);
#endif
__m128d c17_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a17_0 = _mm_loadu_pd(&values[79]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, b17));
#endif
_mm_storeu_pd(&C[(i*35)+0], c17_0);
__m128d c17_2 = _mm_load_sd(&C[(i*35)+3]);
__m128d a17_2 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*35)+3], c17_2);
__m128d c17_3 = _mm_load_sd(&C[(i*35)+7]);
__m128d a17_3 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, b17));
#endif
_mm_store_sd(&C[(i*35)+7], c17_3);
__m128d c17_4 = _mm_load_sd(&C[(i*35)+9]);
__m128d a17_4 = _mm_load_sd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, b17));
#endif
_mm_store_sd(&C[(i*35)+9], c17_4);
#else
C[(i*35)+0] += values[79] * B[(i*35)+17];
C[(i*35)+1] += values[80] * B[(i*35)+17];
C[(i*35)+3] += values[81] * B[(i*35)+17];
C[(i*35)+7] += values[82] * B[(i*35)+17];
C[(i*35)+9] += values[83] * B[(i*35)+17];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*35)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*35)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a18_0 = _mm_load_sd(&values[84]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*35)+0], c18_0);
__m128d c18_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a18_1 = _mm_loadu_pd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_pd(c18_1, _mm_mul_pd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_pd(c18_1, _mm_mul_pd(a18_1, b18));
#endif
_mm_storeu_pd(&C[(i*35)+2], c18_1);
__m128d c18_3 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a18_3 = _mm_loadu_pd(&values[87]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_3 = _mm_add_pd(c18_3, _mm_mul_pd(a18_3, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_3 = _mm_add_pd(c18_3, _mm_mul_pd(a18_3, b18));
#endif
_mm_storeu_pd(&C[(i*35)+8], c18_3);
#else
C[(i*35)+0] += values[84] * B[(i*35)+18];
C[(i*35)+2] += values[85] * B[(i*35)+18];
C[(i*35)+3] += values[86] * B[(i*35)+18];
C[(i*35)+8] += values[87] * B[(i*35)+18];
C[(i*35)+9] += values[88] * B[(i*35)+18];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*35)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*35)+19]);
#endif
__m128d c19_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a19_0 = _mm_load_sd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, b19));
#endif
_mm_store_sd(&C[(i*35)+0], c19_0);
__m128d c19_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a19_1 = _mm_load_sd(&values[90]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, b19));
#endif
_mm_store_sd(&C[(i*35)+3], c19_1);
__m128d c19_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a19_2 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, b19));
#endif
_mm_store_sd(&C[(i*35)+9], c19_2);
#else
C[(i*35)+0] += values[89] * B[(i*35)+19];
C[(i*35)+3] += values[90] * B[(i*35)+19];
C[(i*35)+9] += values[91] * B[(i*35)+19];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 20, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*35)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*35)+20]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a20_0 = _mm256_loadu_pd(&values[92]);
c20_0 = _mm256_add_pd(c20_0, _mm256_mul_pd(a20_0, b20));
_mm256_storeu_pd(&C[(i*35)+0], c20_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a20_0 = _mm_loadu_pd(&values[92]);
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, b20));
_mm_storeu_pd(&C[(i*35)+0], c20_0);
__m128d c20_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a20_2 = _mm_loadu_pd(&values[94]);
c20_2 = _mm_add_pd(c20_2, _mm_mul_pd(a20_2, b20));
_mm_storeu_pd(&C[(i*35)+2], c20_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a20_4 = _mm256_loadu_pd(&values[96]);
c20_4 = _mm256_add_pd(c20_4, _mm256_mul_pd(a20_4, b20));
_mm256_storeu_pd(&C[(i*35)+4], c20_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a20_4 = _mm_loadu_pd(&values[96]);
c20_4 = _mm_add_pd(c20_4, _mm_mul_pd(a20_4, b20));
_mm_storeu_pd(&C[(i*35)+4], c20_4);
__m128d c20_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a20_6 = _mm_loadu_pd(&values[98]);
c20_6 = _mm_add_pd(c20_6, _mm_mul_pd(a20_6, b20));
_mm_storeu_pd(&C[(i*35)+6], c20_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_8 = _mm256_loadu_pd(&C[(i*35)+8]);
__m256d a20_8 = _mm256_loadu_pd(&values[100]);
c20_8 = _mm256_add_pd(c20_8, _mm256_mul_pd(a20_8, b20));
_mm256_storeu_pd(&C[(i*35)+8], c20_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a20_8 = _mm_loadu_pd(&values[100]);
c20_8 = _mm_add_pd(c20_8, _mm_mul_pd(a20_8, b20));
_mm_storeu_pd(&C[(i*35)+8], c20_8);
__m128d c20_10 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a20_10 = _mm_loadu_pd(&values[102]);
c20_10 = _mm_add_pd(c20_10, _mm_mul_pd(a20_10, b20));
_mm_storeu_pd(&C[(i*35)+10], c20_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_12 = _mm256_loadu_pd(&C[(i*35)+12]);
__m256d a20_12 = _mm256_loadu_pd(&values[104]);
c20_12 = _mm256_add_pd(c20_12, _mm256_mul_pd(a20_12, b20));
_mm256_storeu_pd(&C[(i*35)+12], c20_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_12 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a20_12 = _mm_loadu_pd(&values[104]);
c20_12 = _mm_add_pd(c20_12, _mm_mul_pd(a20_12, b20));
_mm_storeu_pd(&C[(i*35)+12], c20_12);
__m128d c20_14 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a20_14 = _mm_loadu_pd(&values[106]);
c20_14 = _mm_add_pd(c20_14, _mm_mul_pd(a20_14, b20));
_mm_storeu_pd(&C[(i*35)+14], c20_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_16 = _mm256_loadu_pd(&C[(i*35)+16]);
__m256d a20_16 = _mm256_loadu_pd(&values[108]);
c20_16 = _mm256_add_pd(c20_16, _mm256_mul_pd(a20_16, b20));
_mm256_storeu_pd(&C[(i*35)+16], c20_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_16 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a20_16 = _mm_loadu_pd(&values[108]);
c20_16 = _mm_add_pd(c20_16, _mm_mul_pd(a20_16, b20));
_mm_storeu_pd(&C[(i*35)+16], c20_16);
__m128d c20_18 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a20_18 = _mm_loadu_pd(&values[110]);
c20_18 = _mm_add_pd(c20_18, _mm_mul_pd(a20_18, b20));
_mm_storeu_pd(&C[(i*35)+18], c20_18);
#endif
#else
C[(i*35)+0] += values[92] * B[(i*35)+20];
C[(i*35)+1] += values[93] * B[(i*35)+20];
C[(i*35)+2] += values[94] * B[(i*35)+20];
C[(i*35)+3] += values[95] * B[(i*35)+20];
C[(i*35)+4] += values[96] * B[(i*35)+20];
C[(i*35)+5] += values[97] * B[(i*35)+20];
C[(i*35)+6] += values[98] * B[(i*35)+20];
C[(i*35)+7] += values[99] * B[(i*35)+20];
C[(i*35)+8] += values[100] * B[(i*35)+20];
C[(i*35)+9] += values[101] * B[(i*35)+20];
C[(i*35)+10] += values[102] * B[(i*35)+20];
C[(i*35)+11] += values[103] * B[(i*35)+20];
C[(i*35)+12] += values[104] * B[(i*35)+20];
C[(i*35)+13] += values[105] * B[(i*35)+20];
C[(i*35)+14] += values[106] * B[(i*35)+20];
C[(i*35)+15] += values[107] * B[(i*35)+20];
C[(i*35)+16] += values[108] * B[(i*35)+20];
C[(i*35)+17] += values[109] * B[(i*35)+20];
C[(i*35)+18] += values[110] * B[(i*35)+20];
C[(i*35)+19] += values[111] * B[(i*35)+20];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*35)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*35)+21]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a21_0 = _mm256_loadu_pd(&values[112]);
c21_0 = _mm256_add_pd(c21_0, _mm256_mul_pd(a21_0, b21));
_mm256_storeu_pd(&C[(i*35)+0], c21_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a21_0 = _mm_loadu_pd(&values[112]);
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, b21));
_mm_storeu_pd(&C[(i*35)+0], c21_0);
__m128d c21_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a21_2 = _mm_loadu_pd(&values[114]);
c21_2 = _mm_add_pd(c21_2, _mm_mul_pd(a21_2, b21));
_mm_storeu_pd(&C[(i*35)+2], c21_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a21_4 = _mm256_loadu_pd(&values[116]);
c21_4 = _mm256_add_pd(c21_4, _mm256_mul_pd(a21_4, b21));
_mm256_storeu_pd(&C[(i*35)+4], c21_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a21_4 = _mm_loadu_pd(&values[116]);
c21_4 = _mm_add_pd(c21_4, _mm_mul_pd(a21_4, b21));
_mm_storeu_pd(&C[(i*35)+4], c21_4);
__m128d c21_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a21_6 = _mm_loadu_pd(&values[118]);
c21_6 = _mm_add_pd(c21_6, _mm_mul_pd(a21_6, b21));
_mm_storeu_pd(&C[(i*35)+6], c21_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_8 = _mm256_loadu_pd(&C[(i*35)+8]);
__m256d a21_8 = _mm256_loadu_pd(&values[120]);
c21_8 = _mm256_add_pd(c21_8, _mm256_mul_pd(a21_8, b21));
_mm256_storeu_pd(&C[(i*35)+8], c21_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a21_8 = _mm_loadu_pd(&values[120]);
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, b21));
_mm_storeu_pd(&C[(i*35)+8], c21_8);
__m128d c21_10 = _mm_loadu_pd(&C[(i*35)+10]);
__m128d a21_10 = _mm_loadu_pd(&values[122]);
c21_10 = _mm_add_pd(c21_10, _mm_mul_pd(a21_10, b21));
_mm_storeu_pd(&C[(i*35)+10], c21_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_12 = _mm256_loadu_pd(&C[(i*35)+12]);
__m256d a21_12 = _mm256_loadu_pd(&values[124]);
c21_12 = _mm256_add_pd(c21_12, _mm256_mul_pd(a21_12, b21));
_mm256_storeu_pd(&C[(i*35)+12], c21_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_12 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a21_12 = _mm_loadu_pd(&values[124]);
c21_12 = _mm_add_pd(c21_12, _mm_mul_pd(a21_12, b21));
_mm_storeu_pd(&C[(i*35)+12], c21_12);
__m128d c21_14 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a21_14 = _mm_loadu_pd(&values[126]);
c21_14 = _mm_add_pd(c21_14, _mm_mul_pd(a21_14, b21));
_mm_storeu_pd(&C[(i*35)+14], c21_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_16 = _mm256_loadu_pd(&C[(i*35)+16]);
__m256d a21_16 = _mm256_loadu_pd(&values[128]);
c21_16 = _mm256_add_pd(c21_16, _mm256_mul_pd(a21_16, b21));
_mm256_storeu_pd(&C[(i*35)+16], c21_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_16 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a21_16 = _mm_loadu_pd(&values[128]);
c21_16 = _mm_add_pd(c21_16, _mm_mul_pd(a21_16, b21));
_mm_storeu_pd(&C[(i*35)+16], c21_16);
__m128d c21_18 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a21_18 = _mm_loadu_pd(&values[130]);
c21_18 = _mm_add_pd(c21_18, _mm_mul_pd(a21_18, b21));
_mm_storeu_pd(&C[(i*35)+18], c21_18);
#endif
#else
C[(i*35)+0] += values[112] * B[(i*35)+21];
C[(i*35)+1] += values[113] * B[(i*35)+21];
C[(i*35)+2] += values[114] * B[(i*35)+21];
C[(i*35)+3] += values[115] * B[(i*35)+21];
C[(i*35)+4] += values[116] * B[(i*35)+21];
C[(i*35)+5] += values[117] * B[(i*35)+21];
C[(i*35)+6] += values[118] * B[(i*35)+21];
C[(i*35)+7] += values[119] * B[(i*35)+21];
C[(i*35)+8] += values[120] * B[(i*35)+21];
C[(i*35)+9] += values[121] * B[(i*35)+21];
C[(i*35)+10] += values[122] * B[(i*35)+21];
C[(i*35)+11] += values[123] * B[(i*35)+21];
C[(i*35)+12] += values[124] * B[(i*35)+21];
C[(i*35)+13] += values[125] * B[(i*35)+21];
C[(i*35)+14] += values[126] * B[(i*35)+21];
C[(i*35)+15] += values[127] * B[(i*35)+21];
C[(i*35)+16] += values[128] * B[(i*35)+21];
C[(i*35)+17] += values[129] * B[(i*35)+21];
C[(i*35)+18] += values[130] * B[(i*35)+21];
C[(i*35)+19] += values[131] * B[(i*35)+21];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*35)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*35)+22]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a22_0 = _mm256_loadu_pd(&values[132]);
c22_0 = _mm256_add_pd(c22_0, _mm256_mul_pd(a22_0, b22));
_mm256_storeu_pd(&C[(i*35)+0], c22_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a22_0 = _mm_loadu_pd(&values[132]);
c22_0 = _mm_add_pd(c22_0, _mm_mul_pd(a22_0, b22));
_mm_storeu_pd(&C[(i*35)+0], c22_0);
__m128d c22_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a22_2 = _mm_loadu_pd(&values[134]);
c22_2 = _mm_add_pd(c22_2, _mm_mul_pd(a22_2, b22));
_mm_storeu_pd(&C[(i*35)+2], c22_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a22_4 = _mm256_loadu_pd(&values[136]);
c22_4 = _mm256_add_pd(c22_4, _mm256_mul_pd(a22_4, b22));
_mm256_storeu_pd(&C[(i*35)+4], c22_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a22_4 = _mm_loadu_pd(&values[136]);
c22_4 = _mm_add_pd(c22_4, _mm_mul_pd(a22_4, b22));
_mm_storeu_pd(&C[(i*35)+4], c22_4);
__m128d c22_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a22_6 = _mm_loadu_pd(&values[138]);
c22_6 = _mm_add_pd(c22_6, _mm_mul_pd(a22_6, b22));
_mm_storeu_pd(&C[(i*35)+6], c22_6);
#endif
__m128d c22_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a22_8 = _mm_loadu_pd(&values[140]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, b22));
#endif
_mm_storeu_pd(&C[(i*35)+8], c22_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_10 = _mm256_loadu_pd(&C[(i*35)+11]);
__m256d a22_10 = _mm256_loadu_pd(&values[142]);
c22_10 = _mm256_add_pd(c22_10, _mm256_mul_pd(a22_10, b22));
_mm256_storeu_pd(&C[(i*35)+11], c22_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_10 = _mm_loadu_pd(&C[(i*35)+11]);
__m128d a22_10 = _mm_loadu_pd(&values[142]);
c22_10 = _mm_add_pd(c22_10, _mm_mul_pd(a22_10, b22));
_mm_storeu_pd(&C[(i*35)+11], c22_10);
__m128d c22_12 = _mm_loadu_pd(&C[(i*35)+13]);
__m128d a22_12 = _mm_loadu_pd(&values[144]);
c22_12 = _mm_add_pd(c22_12, _mm_mul_pd(a22_12, b22));
_mm_storeu_pd(&C[(i*35)+13], c22_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_14 = _mm256_loadu_pd(&C[(i*35)+15]);
__m256d a22_14 = _mm256_loadu_pd(&values[146]);
c22_14 = _mm256_add_pd(c22_14, _mm256_mul_pd(a22_14, b22));
_mm256_storeu_pd(&C[(i*35)+15], c22_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_14 = _mm_loadu_pd(&C[(i*35)+15]);
__m128d a22_14 = _mm_loadu_pd(&values[146]);
c22_14 = _mm_add_pd(c22_14, _mm_mul_pd(a22_14, b22));
_mm_storeu_pd(&C[(i*35)+15], c22_14);
__m128d c22_16 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a22_16 = _mm_loadu_pd(&values[148]);
c22_16 = _mm_add_pd(c22_16, _mm_mul_pd(a22_16, b22));
_mm_storeu_pd(&C[(i*35)+17], c22_16);
#endif
__m128d c22_18 = _mm_load_sd(&C[(i*35)+19]);
__m128d a22_18 = _mm_load_sd(&values[150]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, b22));
#endif
_mm_store_sd(&C[(i*35)+19], c22_18);
#else
C[(i*35)+0] += values[132] * B[(i*35)+22];
C[(i*35)+1] += values[133] * B[(i*35)+22];
C[(i*35)+2] += values[134] * B[(i*35)+22];
C[(i*35)+3] += values[135] * B[(i*35)+22];
C[(i*35)+4] += values[136] * B[(i*35)+22];
C[(i*35)+5] += values[137] * B[(i*35)+22];
C[(i*35)+6] += values[138] * B[(i*35)+22];
C[(i*35)+7] += values[139] * B[(i*35)+22];
C[(i*35)+8] += values[140] * B[(i*35)+22];
C[(i*35)+9] += values[141] * B[(i*35)+22];
C[(i*35)+11] += values[142] * B[(i*35)+22];
C[(i*35)+12] += values[143] * B[(i*35)+22];
C[(i*35)+13] += values[144] * B[(i*35)+22];
C[(i*35)+14] += values[145] * B[(i*35)+22];
C[(i*35)+15] += values[146] * B[(i*35)+22];
C[(i*35)+16] += values[147] * B[(i*35)+22];
C[(i*35)+17] += values[148] * B[(i*35)+22];
C[(i*35)+18] += values[149] * B[(i*35)+22];
C[(i*35)+19] += values[150] * B[(i*35)+22];
#endif
#ifndef NDEBUG
num_flops += 38;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*35)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*35)+23]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a23_0 = _mm256_loadu_pd(&values[151]);
c23_0 = _mm256_add_pd(c23_0, _mm256_mul_pd(a23_0, b23));
_mm256_storeu_pd(&C[(i*35)+0], c23_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a23_0 = _mm_loadu_pd(&values[151]);
c23_0 = _mm_add_pd(c23_0, _mm_mul_pd(a23_0, b23));
_mm_storeu_pd(&C[(i*35)+0], c23_0);
__m128d c23_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a23_2 = _mm_loadu_pd(&values[153]);
c23_2 = _mm_add_pd(c23_2, _mm_mul_pd(a23_2, b23));
_mm_storeu_pd(&C[(i*35)+2], c23_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_4 = _mm256_loadu_pd(&C[(i*35)+5]);
__m256d a23_4 = _mm256_loadu_pd(&values[155]);
c23_4 = _mm256_add_pd(c23_4, _mm256_mul_pd(a23_4, b23));
_mm256_storeu_pd(&C[(i*35)+5], c23_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_4 = _mm_loadu_pd(&C[(i*35)+5]);
__m128d a23_4 = _mm_loadu_pd(&values[155]);
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, b23));
_mm_storeu_pd(&C[(i*35)+5], c23_4);
__m128d c23_6 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a23_6 = _mm_loadu_pd(&values[157]);
c23_6 = _mm_add_pd(c23_6, _mm_mul_pd(a23_6, b23));
_mm_storeu_pd(&C[(i*35)+7], c23_6);
#endif
__m128d c23_8 = _mm_load_sd(&C[(i*35)+9]);
__m128d a23_8 = _mm_load_sd(&values[159]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, b23));
#endif
_mm_store_sd(&C[(i*35)+9], c23_8);
__m128d c23_9 = _mm_loadu_pd(&C[(i*35)+12]);
__m128d a23_9 = _mm_loadu_pd(&values[160]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, b23));
#endif
_mm_storeu_pd(&C[(i*35)+12], c23_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_11 = _mm256_loadu_pd(&C[(i*35)+15]);
__m256d a23_11 = _mm256_loadu_pd(&values[162]);
c23_11 = _mm256_add_pd(c23_11, _mm256_mul_pd(a23_11, b23));
_mm256_storeu_pd(&C[(i*35)+15], c23_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_11 = _mm_loadu_pd(&C[(i*35)+15]);
__m128d a23_11 = _mm_loadu_pd(&values[162]);
c23_11 = _mm_add_pd(c23_11, _mm_mul_pd(a23_11, b23));
_mm_storeu_pd(&C[(i*35)+15], c23_11);
__m128d c23_13 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a23_13 = _mm_loadu_pd(&values[164]);
c23_13 = _mm_add_pd(c23_13, _mm_mul_pd(a23_13, b23));
_mm_storeu_pd(&C[(i*35)+17], c23_13);
#endif
__m128d c23_15 = _mm_load_sd(&C[(i*35)+19]);
__m128d a23_15 = _mm_load_sd(&values[166]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, b23));
#endif
_mm_store_sd(&C[(i*35)+19], c23_15);
#else
C[(i*35)+0] += values[151] * B[(i*35)+23];
C[(i*35)+1] += values[152] * B[(i*35)+23];
C[(i*35)+2] += values[153] * B[(i*35)+23];
C[(i*35)+3] += values[154] * B[(i*35)+23];
C[(i*35)+5] += values[155] * B[(i*35)+23];
C[(i*35)+6] += values[156] * B[(i*35)+23];
C[(i*35)+7] += values[157] * B[(i*35)+23];
C[(i*35)+8] += values[158] * B[(i*35)+23];
C[(i*35)+9] += values[159] * B[(i*35)+23];
C[(i*35)+12] += values[160] * B[(i*35)+23];
C[(i*35)+13] += values[161] * B[(i*35)+23];
C[(i*35)+15] += values[162] * B[(i*35)+23];
C[(i*35)+16] += values[163] * B[(i*35)+23];
C[(i*35)+17] += values[164] * B[(i*35)+23];
C[(i*35)+18] += values[165] * B[(i*35)+23];
C[(i*35)+19] += values[166] * B[(i*35)+23];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b24 = _mm256_broadcast_sd(&B[(i*35)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b24 = _mm_loaddup_pd(&B[(i*35)+24]);
#endif
__m128d c24_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a24_0 = _mm_load_sd(&values[167]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, b24));
#endif
_mm_store_sd(&C[(i*35)+0], c24_0);
__m128d c24_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a24_1 = _mm_loadu_pd(&values[168]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, b24));
#endif
_mm_storeu_pd(&C[(i*35)+2], c24_1);
__m128d c24_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a24_3 = _mm_load_sd(&values[170]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, b24));
#endif
_mm_store_sd(&C[(i*35)+6], c24_3);
__m128d c24_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a24_4 = _mm_loadu_pd(&values[171]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, b24));
#endif
_mm_storeu_pd(&C[(i*35)+8], c24_4);
__m128d c24_6 = _mm_load_sd(&C[(i*35)+13]);
__m128d a24_6 = _mm_load_sd(&values[173]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, b24));
#endif
_mm_store_sd(&C[(i*35)+13], c24_6);
__m128d c24_7 = _mm_load_sd(&C[(i*35)+16]);
__m128d a24_7 = _mm_load_sd(&values[174]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, b24));
#endif
_mm_store_sd(&C[(i*35)+16], c24_7);
__m128d c24_8 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a24_8 = _mm_loadu_pd(&values[175]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, b24));
#endif
_mm_storeu_pd(&C[(i*35)+18], c24_8);
#else
C[(i*35)+0] += values[167] * B[(i*35)+24];
C[(i*35)+2] += values[168] * B[(i*35)+24];
C[(i*35)+3] += values[169] * B[(i*35)+24];
C[(i*35)+6] += values[170] * B[(i*35)+24];
C[(i*35)+8] += values[171] * B[(i*35)+24];
C[(i*35)+9] += values[172] * B[(i*35)+24];
C[(i*35)+13] += values[173] * B[(i*35)+24];
C[(i*35)+16] += values[174] * B[(i*35)+24];
C[(i*35)+18] += values[175] * B[(i*35)+24];
C[(i*35)+19] += values[176] * B[(i*35)+24];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*35)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*35)+25]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a25_0 = _mm256_loadu_pd(&values[177]);
c25_0 = _mm256_add_pd(c25_0, _mm256_mul_pd(a25_0, b25));
_mm256_storeu_pd(&C[(i*35)+0], c25_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a25_0 = _mm_loadu_pd(&values[177]);
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, b25));
_mm_storeu_pd(&C[(i*35)+0], c25_0);
__m128d c25_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a25_2 = _mm_loadu_pd(&values[179]);
c25_2 = _mm_add_pd(c25_2, _mm_mul_pd(a25_2, b25));
_mm_storeu_pd(&C[(i*35)+2], c25_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a25_4 = _mm256_loadu_pd(&values[181]);
c25_4 = _mm256_add_pd(c25_4, _mm256_mul_pd(a25_4, b25));
_mm256_storeu_pd(&C[(i*35)+4], c25_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a25_4 = _mm_loadu_pd(&values[181]);
c25_4 = _mm_add_pd(c25_4, _mm_mul_pd(a25_4, b25));
_mm_storeu_pd(&C[(i*35)+4], c25_4);
__m128d c25_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a25_6 = _mm_loadu_pd(&values[183]);
c25_6 = _mm_add_pd(c25_6, _mm_mul_pd(a25_6, b25));
_mm_storeu_pd(&C[(i*35)+6], c25_6);
#endif
__m128d c25_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a25_8 = _mm_loadu_pd(&values[185]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, b25));
#endif
_mm_storeu_pd(&C[(i*35)+8], c25_8);
__m128d c25_10 = _mm_load_sd(&C[(i*35)+10]);
__m128d a25_10 = _mm_load_sd(&values[187]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_10 = _mm_add_sd(c25_10, _mm_mul_sd(a25_10, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_10 = _mm_add_sd(c25_10, _mm_mul_sd(a25_10, b25));
#endif
_mm_store_sd(&C[(i*35)+10], c25_10);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_11 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a25_11 = _mm256_loadu_pd(&values[188]);
c25_11 = _mm256_add_pd(c25_11, _mm256_mul_pd(a25_11, b25));
_mm256_storeu_pd(&C[(i*35)+14], c25_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_11 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a25_11 = _mm_loadu_pd(&values[188]);
c25_11 = _mm_add_pd(c25_11, _mm_mul_pd(a25_11, b25));
_mm_storeu_pd(&C[(i*35)+14], c25_11);
__m128d c25_13 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a25_13 = _mm_loadu_pd(&values[190]);
c25_13 = _mm_add_pd(c25_13, _mm_mul_pd(a25_13, b25));
_mm_storeu_pd(&C[(i*35)+16], c25_13);
#endif
__m128d c25_15 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a25_15 = _mm_loadu_pd(&values[192]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_15 = _mm_add_pd(c25_15, _mm_mul_pd(a25_15, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_15 = _mm_add_pd(c25_15, _mm_mul_pd(a25_15, b25));
#endif
_mm_storeu_pd(&C[(i*35)+18], c25_15);
#else
C[(i*35)+0] += values[177] * B[(i*35)+25];
C[(i*35)+1] += values[178] * B[(i*35)+25];
C[(i*35)+2] += values[179] * B[(i*35)+25];
C[(i*35)+3] += values[180] * B[(i*35)+25];
C[(i*35)+4] += values[181] * B[(i*35)+25];
C[(i*35)+5] += values[182] * B[(i*35)+25];
C[(i*35)+6] += values[183] * B[(i*35)+25];
C[(i*35)+7] += values[184] * B[(i*35)+25];
C[(i*35)+8] += values[185] * B[(i*35)+25];
C[(i*35)+9] += values[186] * B[(i*35)+25];
C[(i*35)+10] += values[187] * B[(i*35)+25];
C[(i*35)+14] += values[188] * B[(i*35)+25];
C[(i*35)+15] += values[189] * B[(i*35)+25];
C[(i*35)+16] += values[190] * B[(i*35)+25];
C[(i*35)+17] += values[191] * B[(i*35)+25];
C[(i*35)+18] += values[192] * B[(i*35)+25];
C[(i*35)+19] += values[193] * B[(i*35)+25];
#endif
#ifndef NDEBUG
num_flops += 34;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*35)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*35)+26]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a26_0 = _mm256_loadu_pd(&values[194]);
c26_0 = _mm256_add_pd(c26_0, _mm256_mul_pd(a26_0, b26));
_mm256_storeu_pd(&C[(i*35)+0], c26_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a26_0 = _mm_loadu_pd(&values[194]);
c26_0 = _mm_add_pd(c26_0, _mm_mul_pd(a26_0, b26));
_mm_storeu_pd(&C[(i*35)+0], c26_0);
__m128d c26_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a26_2 = _mm_loadu_pd(&values[196]);
c26_2 = _mm_add_pd(c26_2, _mm_mul_pd(a26_2, b26));
_mm_storeu_pd(&C[(i*35)+2], c26_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_4 = _mm256_loadu_pd(&C[(i*35)+4]);
__m256d a26_4 = _mm256_loadu_pd(&values[198]);
c26_4 = _mm256_add_pd(c26_4, _mm256_mul_pd(a26_4, b26));
_mm256_storeu_pd(&C[(i*35)+4], c26_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_4 = _mm_loadu_pd(&C[(i*35)+4]);
__m128d a26_4 = _mm_loadu_pd(&values[198]);
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, b26));
_mm_storeu_pd(&C[(i*35)+4], c26_4);
__m128d c26_6 = _mm_loadu_pd(&C[(i*35)+6]);
__m128d a26_6 = _mm_loadu_pd(&values[200]);
c26_6 = _mm_add_pd(c26_6, _mm_mul_pd(a26_6, b26));
_mm_storeu_pd(&C[(i*35)+6], c26_6);
#endif
__m128d c26_8 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a26_8 = _mm_loadu_pd(&values[202]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, b26));
#endif
_mm_storeu_pd(&C[(i*35)+8], c26_8);
__m128d c26_10 = _mm_load_sd(&C[(i*35)+11]);
__m128d a26_10 = _mm_load_sd(&values[204]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_10 = _mm_add_sd(c26_10, _mm_mul_sd(a26_10, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_10 = _mm_add_sd(c26_10, _mm_mul_sd(a26_10, b26));
#endif
_mm_store_sd(&C[(i*35)+11], c26_10);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_11 = _mm256_loadu_pd(&C[(i*35)+14]);
__m256d a26_11 = _mm256_loadu_pd(&values[205]);
c26_11 = _mm256_add_pd(c26_11, _mm256_mul_pd(a26_11, b26));
_mm256_storeu_pd(&C[(i*35)+14], c26_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_11 = _mm_loadu_pd(&C[(i*35)+14]);
__m128d a26_11 = _mm_loadu_pd(&values[205]);
c26_11 = _mm_add_pd(c26_11, _mm_mul_pd(a26_11, b26));
_mm_storeu_pd(&C[(i*35)+14], c26_11);
__m128d c26_13 = _mm_loadu_pd(&C[(i*35)+16]);
__m128d a26_13 = _mm_loadu_pd(&values[207]);
c26_13 = _mm_add_pd(c26_13, _mm_mul_pd(a26_13, b26));
_mm_storeu_pd(&C[(i*35)+16], c26_13);
#endif
__m128d c26_15 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a26_15 = _mm_loadu_pd(&values[209]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_15 = _mm_add_pd(c26_15, _mm_mul_pd(a26_15, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_15 = _mm_add_pd(c26_15, _mm_mul_pd(a26_15, b26));
#endif
_mm_storeu_pd(&C[(i*35)+18], c26_15);
#else
C[(i*35)+0] += values[194] * B[(i*35)+26];
C[(i*35)+1] += values[195] * B[(i*35)+26];
C[(i*35)+2] += values[196] * B[(i*35)+26];
C[(i*35)+3] += values[197] * B[(i*35)+26];
C[(i*35)+4] += values[198] * B[(i*35)+26];
C[(i*35)+5] += values[199] * B[(i*35)+26];
C[(i*35)+6] += values[200] * B[(i*35)+26];
C[(i*35)+7] += values[201] * B[(i*35)+26];
C[(i*35)+8] += values[202] * B[(i*35)+26];
C[(i*35)+9] += values[203] * B[(i*35)+26];
C[(i*35)+11] += values[204] * B[(i*35)+26];
C[(i*35)+14] += values[205] * B[(i*35)+26];
C[(i*35)+15] += values[206] * B[(i*35)+26];
C[(i*35)+16] += values[207] * B[(i*35)+26];
C[(i*35)+17] += values[208] * B[(i*35)+26];
C[(i*35)+18] += values[209] * B[(i*35)+26];
C[(i*35)+19] += values[210] * B[(i*35)+26];
#endif
#ifndef NDEBUG
num_flops += 34;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*35)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*35)+27]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a27_0 = _mm256_loadu_pd(&values[211]);
c27_0 = _mm256_add_pd(c27_0, _mm256_mul_pd(a27_0, b27));
_mm256_storeu_pd(&C[(i*35)+0], c27_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a27_0 = _mm_loadu_pd(&values[211]);
c27_0 = _mm_add_pd(c27_0, _mm_mul_pd(a27_0, b27));
_mm_storeu_pd(&C[(i*35)+0], c27_0);
__m128d c27_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a27_2 = _mm_loadu_pd(&values[213]);
c27_2 = _mm_add_pd(c27_2, _mm_mul_pd(a27_2, b27));
_mm_storeu_pd(&C[(i*35)+2], c27_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_4 = _mm256_loadu_pd(&C[(i*35)+5]);
__m256d a27_4 = _mm256_loadu_pd(&values[215]);
c27_4 = _mm256_add_pd(c27_4, _mm256_mul_pd(a27_4, b27));
_mm256_storeu_pd(&C[(i*35)+5], c27_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_4 = _mm_loadu_pd(&C[(i*35)+5]);
__m128d a27_4 = _mm_loadu_pd(&values[215]);
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, b27));
_mm_storeu_pd(&C[(i*35)+5], c27_4);
__m128d c27_6 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a27_6 = _mm_loadu_pd(&values[217]);
c27_6 = _mm_add_pd(c27_6, _mm_mul_pd(a27_6, b27));
_mm_storeu_pd(&C[(i*35)+7], c27_6);
#endif
__m128d c27_8 = _mm_load_sd(&C[(i*35)+9]);
__m128d a27_8 = _mm_load_sd(&values[219]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, b27));
#endif
_mm_store_sd(&C[(i*35)+9], c27_8);
__m128d c27_9 = _mm_load_sd(&C[(i*35)+12]);
__m128d a27_9 = _mm_load_sd(&values[220]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_9 = _mm_add_sd(c27_9, _mm_mul_sd(a27_9, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_9 = _mm_add_sd(c27_9, _mm_mul_sd(a27_9, b27));
#endif
_mm_store_sd(&C[(i*35)+12], c27_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_10 = _mm256_loadu_pd(&C[(i*35)+15]);
__m256d a27_10 = _mm256_loadu_pd(&values[221]);
c27_10 = _mm256_add_pd(c27_10, _mm256_mul_pd(a27_10, b27));
_mm256_storeu_pd(&C[(i*35)+15], c27_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_10 = _mm_loadu_pd(&C[(i*35)+15]);
__m128d a27_10 = _mm_loadu_pd(&values[221]);
c27_10 = _mm_add_pd(c27_10, _mm_mul_pd(a27_10, b27));
_mm_storeu_pd(&C[(i*35)+15], c27_10);
__m128d c27_12 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a27_12 = _mm_loadu_pd(&values[223]);
c27_12 = _mm_add_pd(c27_12, _mm_mul_pd(a27_12, b27));
_mm_storeu_pd(&C[(i*35)+17], c27_12);
#endif
__m128d c27_14 = _mm_load_sd(&C[(i*35)+19]);
__m128d a27_14 = _mm_load_sd(&values[225]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_14 = _mm_add_sd(c27_14, _mm_mul_sd(a27_14, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_14 = _mm_add_sd(c27_14, _mm_mul_sd(a27_14, b27));
#endif
_mm_store_sd(&C[(i*35)+19], c27_14);
#else
C[(i*35)+0] += values[211] * B[(i*35)+27];
C[(i*35)+1] += values[212] * B[(i*35)+27];
C[(i*35)+2] += values[213] * B[(i*35)+27];
C[(i*35)+3] += values[214] * B[(i*35)+27];
C[(i*35)+5] += values[215] * B[(i*35)+27];
C[(i*35)+6] += values[216] * B[(i*35)+27];
C[(i*35)+7] += values[217] * B[(i*35)+27];
C[(i*35)+8] += values[218] * B[(i*35)+27];
C[(i*35)+9] += values[219] * B[(i*35)+27];
C[(i*35)+12] += values[220] * B[(i*35)+27];
C[(i*35)+15] += values[221] * B[(i*35)+27];
C[(i*35)+16] += values[222] * B[(i*35)+27];
C[(i*35)+17] += values[223] * B[(i*35)+27];
C[(i*35)+18] += values[224] * B[(i*35)+27];
C[(i*35)+19] += values[225] * B[(i*35)+27];
#endif
#ifndef NDEBUG
num_flops += 30;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b28 = _mm256_broadcast_sd(&B[(i*35)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b28 = _mm_loaddup_pd(&B[(i*35)+28]);
#endif
__m128d c28_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a28_0 = _mm_load_sd(&values[226]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, b28));
#endif
_mm_store_sd(&C[(i*35)+0], c28_0);
__m128d c28_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a28_1 = _mm_loadu_pd(&values[227]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, b28));
#endif
_mm_storeu_pd(&C[(i*35)+2], c28_1);
__m128d c28_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a28_3 = _mm_load_sd(&values[229]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, b28));
#endif
_mm_store_sd(&C[(i*35)+6], c28_3);
__m128d c28_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a28_4 = _mm_loadu_pd(&values[230]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, b28));
#endif
_mm_storeu_pd(&C[(i*35)+8], c28_4);
__m128d c28_6 = _mm_load_sd(&C[(i*35)+13]);
__m128d a28_6 = _mm_load_sd(&values[232]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, b28));
#endif
_mm_store_sd(&C[(i*35)+13], c28_6);
__m128d c28_7 = _mm_load_sd(&C[(i*35)+16]);
__m128d a28_7 = _mm_load_sd(&values[233]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_7 = _mm_add_sd(c28_7, _mm_mul_sd(a28_7, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_7 = _mm_add_sd(c28_7, _mm_mul_sd(a28_7, b28));
#endif
_mm_store_sd(&C[(i*35)+16], c28_7);
__m128d c28_8 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a28_8 = _mm_loadu_pd(&values[234]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, b28));
#endif
_mm_storeu_pd(&C[(i*35)+18], c28_8);
#else
C[(i*35)+0] += values[226] * B[(i*35)+28];
C[(i*35)+2] += values[227] * B[(i*35)+28];
C[(i*35)+3] += values[228] * B[(i*35)+28];
C[(i*35)+6] += values[229] * B[(i*35)+28];
C[(i*35)+8] += values[230] * B[(i*35)+28];
C[(i*35)+9] += values[231] * B[(i*35)+28];
C[(i*35)+13] += values[232] * B[(i*35)+28];
C[(i*35)+16] += values[233] * B[(i*35)+28];
C[(i*35)+18] += values[234] * B[(i*35)+28];
C[(i*35)+19] += values[235] * B[(i*35)+28];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*35)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*35)+29]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c29_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a29_0 = _mm256_loadu_pd(&values[236]);
c29_0 = _mm256_add_pd(c29_0, _mm256_mul_pd(a29_0, b29));
_mm256_storeu_pd(&C[(i*35)+0], c29_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c29_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a29_0 = _mm_loadu_pd(&values[236]);
c29_0 = _mm_add_pd(c29_0, _mm_mul_pd(a29_0, b29));
_mm_storeu_pd(&C[(i*35)+0], c29_0);
__m128d c29_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a29_2 = _mm_loadu_pd(&values[238]);
c29_2 = _mm_add_pd(c29_2, _mm_mul_pd(a29_2, b29));
_mm_storeu_pd(&C[(i*35)+2], c29_2);
#endif
__m128d c29_4 = _mm_load_sd(&C[(i*35)+4]);
__m128d a29_4 = _mm_load_sd(&values[240]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_4 = _mm_add_sd(c29_4, _mm_mul_sd(a29_4, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_4 = _mm_add_sd(c29_4, _mm_mul_sd(a29_4, b29));
#endif
_mm_store_sd(&C[(i*35)+4], c29_4);
__m128d c29_5 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a29_5 = _mm_loadu_pd(&values[241]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_5 = _mm_add_pd(c29_5, _mm_mul_pd(a29_5, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_5 = _mm_add_pd(c29_5, _mm_mul_pd(a29_5, b29));
#endif
_mm_storeu_pd(&C[(i*35)+7], c29_5);
__m128d c29_7 = _mm_load_sd(&C[(i*35)+9]);
__m128d a29_7 = _mm_load_sd(&values[243]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_7 = _mm_add_sd(c29_7, _mm_mul_sd(a29_7, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_7 = _mm_add_sd(c29_7, _mm_mul_sd(a29_7, b29));
#endif
_mm_store_sd(&C[(i*35)+9], c29_7);
__m128d c29_8 = _mm_load_sd(&C[(i*35)+14]);
__m128d a29_8 = _mm_load_sd(&values[244]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_8 = _mm_add_sd(c29_8, _mm_mul_sd(a29_8, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_8 = _mm_add_sd(c29_8, _mm_mul_sd(a29_8, b29));
#endif
_mm_store_sd(&C[(i*35)+14], c29_8);
__m128d c29_9 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a29_9 = _mm_loadu_pd(&values[245]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_9 = _mm_add_pd(c29_9, _mm_mul_pd(a29_9, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_9 = _mm_add_pd(c29_9, _mm_mul_pd(a29_9, b29));
#endif
_mm_storeu_pd(&C[(i*35)+17], c29_9);
__m128d c29_11 = _mm_load_sd(&C[(i*35)+19]);
__m128d a29_11 = _mm_load_sd(&values[247]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_11 = _mm_add_sd(c29_11, _mm_mul_sd(a29_11, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_11 = _mm_add_sd(c29_11, _mm_mul_sd(a29_11, b29));
#endif
_mm_store_sd(&C[(i*35)+19], c29_11);
#else
C[(i*35)+0] += values[236] * B[(i*35)+29];
C[(i*35)+1] += values[237] * B[(i*35)+29];
C[(i*35)+2] += values[238] * B[(i*35)+29];
C[(i*35)+3] += values[239] * B[(i*35)+29];
C[(i*35)+4] += values[240] * B[(i*35)+29];
C[(i*35)+7] += values[241] * B[(i*35)+29];
C[(i*35)+8] += values[242] * B[(i*35)+29];
C[(i*35)+9] += values[243] * B[(i*35)+29];
C[(i*35)+14] += values[244] * B[(i*35)+29];
C[(i*35)+17] += values[245] * B[(i*35)+29];
C[(i*35)+18] += values[246] * B[(i*35)+29];
C[(i*35)+19] += values[247] * B[(i*35)+29];
#endif
#ifndef NDEBUG
num_flops += 24;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*35)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*35)+30]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_0 = _mm256_loadu_pd(&C[(i*35)+0]);
__m256d a30_0 = _mm256_loadu_pd(&values[248]);
c30_0 = _mm256_add_pd(c30_0, _mm256_mul_pd(a30_0, b30));
_mm256_storeu_pd(&C[(i*35)+0], c30_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a30_0 = _mm_loadu_pd(&values[248]);
c30_0 = _mm_add_pd(c30_0, _mm_mul_pd(a30_0, b30));
_mm_storeu_pd(&C[(i*35)+0], c30_0);
__m128d c30_2 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a30_2 = _mm_loadu_pd(&values[250]);
c30_2 = _mm_add_pd(c30_2, _mm_mul_pd(a30_2, b30));
_mm_storeu_pd(&C[(i*35)+2], c30_2);
#endif
__m128d c30_4 = _mm_load_sd(&C[(i*35)+5]);
__m128d a30_4 = _mm_load_sd(&values[252]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, b30));
#endif
_mm_store_sd(&C[(i*35)+5], c30_4);
__m128d c30_5 = _mm_loadu_pd(&C[(i*35)+7]);
__m128d a30_5 = _mm_loadu_pd(&values[253]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, b30));
#endif
_mm_storeu_pd(&C[(i*35)+7], c30_5);
__m128d c30_7 = _mm_load_sd(&C[(i*35)+9]);
__m128d a30_7 = _mm_load_sd(&values[255]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_7 = _mm_add_sd(c30_7, _mm_mul_sd(a30_7, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_7 = _mm_add_sd(c30_7, _mm_mul_sd(a30_7, b30));
#endif
_mm_store_sd(&C[(i*35)+9], c30_7);
__m128d c30_8 = _mm_load_sd(&C[(i*35)+15]);
__m128d a30_8 = _mm_load_sd(&values[256]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_8 = _mm_add_sd(c30_8, _mm_mul_sd(a30_8, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_8 = _mm_add_sd(c30_8, _mm_mul_sd(a30_8, b30));
#endif
_mm_store_sd(&C[(i*35)+15], c30_8);
__m128d c30_9 = _mm_loadu_pd(&C[(i*35)+17]);
__m128d a30_9 = _mm_loadu_pd(&values[257]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, b30));
#endif
_mm_storeu_pd(&C[(i*35)+17], c30_9);
__m128d c30_11 = _mm_load_sd(&C[(i*35)+19]);
__m128d a30_11 = _mm_load_sd(&values[259]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, b30));
#endif
_mm_store_sd(&C[(i*35)+19], c30_11);
#else
C[(i*35)+0] += values[248] * B[(i*35)+30];
C[(i*35)+1] += values[249] * B[(i*35)+30];
C[(i*35)+2] += values[250] * B[(i*35)+30];
C[(i*35)+3] += values[251] * B[(i*35)+30];
C[(i*35)+5] += values[252] * B[(i*35)+30];
C[(i*35)+7] += values[253] * B[(i*35)+30];
C[(i*35)+8] += values[254] * B[(i*35)+30];
C[(i*35)+9] += values[255] * B[(i*35)+30];
C[(i*35)+15] += values[256] * B[(i*35)+30];
C[(i*35)+17] += values[257] * B[(i*35)+30];
C[(i*35)+18] += values[258] * B[(i*35)+30];
C[(i*35)+19] += values[259] * B[(i*35)+30];
#endif
#ifndef NDEBUG
num_flops += 24;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b31 = _mm256_broadcast_sd(&B[(i*35)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b31 = _mm_loaddup_pd(&B[(i*35)+31]);
#endif
__m128d c31_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a31_0 = _mm_load_sd(&values[260]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, b31));
#endif
_mm_store_sd(&C[(i*35)+0], c31_0);
__m128d c31_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a31_1 = _mm_loadu_pd(&values[261]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, b31));
#endif
_mm_storeu_pd(&C[(i*35)+2], c31_1);
__m128d c31_3 = _mm_load_sd(&C[(i*35)+6]);
__m128d a31_3 = _mm_load_sd(&values[263]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_3 = _mm_add_sd(c31_3, _mm_mul_sd(a31_3, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_3 = _mm_add_sd(c31_3, _mm_mul_sd(a31_3, b31));
#endif
_mm_store_sd(&C[(i*35)+6], c31_3);
__m128d c31_4 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a31_4 = _mm_loadu_pd(&values[264]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_4 = _mm_add_pd(c31_4, _mm_mul_pd(a31_4, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_4 = _mm_add_pd(c31_4, _mm_mul_pd(a31_4, b31));
#endif
_mm_storeu_pd(&C[(i*35)+8], c31_4);
__m128d c31_6 = _mm_load_sd(&C[(i*35)+16]);
__m128d a31_6 = _mm_load_sd(&values[266]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_6 = _mm_add_sd(c31_6, _mm_mul_sd(a31_6, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_6 = _mm_add_sd(c31_6, _mm_mul_sd(a31_6, b31));
#endif
_mm_store_sd(&C[(i*35)+16], c31_6);
__m128d c31_7 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a31_7 = _mm_loadu_pd(&values[267]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_7 = _mm_add_pd(c31_7, _mm_mul_pd(a31_7, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_7 = _mm_add_pd(c31_7, _mm_mul_pd(a31_7, b31));
#endif
_mm_storeu_pd(&C[(i*35)+18], c31_7);
#else
C[(i*35)+0] += values[260] * B[(i*35)+31];
C[(i*35)+2] += values[261] * B[(i*35)+31];
C[(i*35)+3] += values[262] * B[(i*35)+31];
C[(i*35)+6] += values[263] * B[(i*35)+31];
C[(i*35)+8] += values[264] * B[(i*35)+31];
C[(i*35)+9] += values[265] * B[(i*35)+31];
C[(i*35)+16] += values[266] * B[(i*35)+31];
C[(i*35)+18] += values[267] * B[(i*35)+31];
C[(i*35)+19] += values[268] * B[(i*35)+31];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*35)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*35)+32]);
#endif
__m128d c32_0 = _mm_loadu_pd(&C[(i*35)+0]);
__m128d a32_0 = _mm_loadu_pd(&values[269]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_0 = _mm_add_pd(c32_0, _mm_mul_pd(a32_0, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_0 = _mm_add_pd(c32_0, _mm_mul_pd(a32_0, b32));
#endif
_mm_storeu_pd(&C[(i*35)+0], c32_0);
__m128d c32_2 = _mm_load_sd(&C[(i*35)+3]);
__m128d a32_2 = _mm_load_sd(&values[271]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, b32));
#endif
_mm_store_sd(&C[(i*35)+3], c32_2);
__m128d c32_3 = _mm_load_sd(&C[(i*35)+7]);
__m128d a32_3 = _mm_load_sd(&values[272]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, b32));
#endif
_mm_store_sd(&C[(i*35)+7], c32_3);
__m128d c32_4 = _mm_load_sd(&C[(i*35)+9]);
__m128d a32_4 = _mm_load_sd(&values[273]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, b32));
#endif
_mm_store_sd(&C[(i*35)+9], c32_4);
__m128d c32_5 = _mm_load_sd(&C[(i*35)+17]);
__m128d a32_5 = _mm_load_sd(&values[274]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_5 = _mm_add_sd(c32_5, _mm_mul_sd(a32_5, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_5 = _mm_add_sd(c32_5, _mm_mul_sd(a32_5, b32));
#endif
_mm_store_sd(&C[(i*35)+17], c32_5);
__m128d c32_6 = _mm_load_sd(&C[(i*35)+19]);
__m128d a32_6 = _mm_load_sd(&values[275]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_6 = _mm_add_sd(c32_6, _mm_mul_sd(a32_6, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_6 = _mm_add_sd(c32_6, _mm_mul_sd(a32_6, b32));
#endif
_mm_store_sd(&C[(i*35)+19], c32_6);
#else
C[(i*35)+0] += values[269] * B[(i*35)+32];
C[(i*35)+1] += values[270] * B[(i*35)+32];
C[(i*35)+3] += values[271] * B[(i*35)+32];
C[(i*35)+7] += values[272] * B[(i*35)+32];
C[(i*35)+9] += values[273] * B[(i*35)+32];
C[(i*35)+17] += values[274] * B[(i*35)+32];
C[(i*35)+19] += values[275] * B[(i*35)+32];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b33 = _mm256_broadcast_sd(&B[(i*35)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b33 = _mm_loaddup_pd(&B[(i*35)+33]);
#endif
__m128d c33_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a33_0 = _mm_load_sd(&values[276]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, b33));
#endif
_mm_store_sd(&C[(i*35)+0], c33_0);
__m128d c33_1 = _mm_loadu_pd(&C[(i*35)+2]);
__m128d a33_1 = _mm_loadu_pd(&values[277]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_1 = _mm_add_pd(c33_1, _mm_mul_pd(a33_1, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_1 = _mm_add_pd(c33_1, _mm_mul_pd(a33_1, b33));
#endif
_mm_storeu_pd(&C[(i*35)+2], c33_1);
__m128d c33_3 = _mm_loadu_pd(&C[(i*35)+8]);
__m128d a33_3 = _mm_loadu_pd(&values[279]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_3 = _mm_add_pd(c33_3, _mm_mul_pd(a33_3, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_3 = _mm_add_pd(c33_3, _mm_mul_pd(a33_3, b33));
#endif
_mm_storeu_pd(&C[(i*35)+8], c33_3);
__m128d c33_5 = _mm_loadu_pd(&C[(i*35)+18]);
__m128d a33_5 = _mm_loadu_pd(&values[281]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_5 = _mm_add_pd(c33_5, _mm_mul_pd(a33_5, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_5 = _mm_add_pd(c33_5, _mm_mul_pd(a33_5, b33));
#endif
_mm_storeu_pd(&C[(i*35)+18], c33_5);
#else
C[(i*35)+0] += values[276] * B[(i*35)+33];
C[(i*35)+2] += values[277] * B[(i*35)+33];
C[(i*35)+3] += values[278] * B[(i*35)+33];
C[(i*35)+8] += values[279] * B[(i*35)+33];
C[(i*35)+9] += values[280] * B[(i*35)+33];
C[(i*35)+18] += values[281] * B[(i*35)+33];
C[(i*35)+19] += values[282] * B[(i*35)+33];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b34 = _mm256_broadcast_sd(&B[(i*35)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b34 = _mm_loaddup_pd(&B[(i*35)+34]);
#endif
__m128d c34_0 = _mm_load_sd(&C[(i*35)+0]);
__m128d a34_0 = _mm_load_sd(&values[283]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_0 = _mm_add_sd(c34_0, _mm_mul_sd(a34_0, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_0 = _mm_add_sd(c34_0, _mm_mul_sd(a34_0, b34));
#endif
_mm_store_sd(&C[(i*35)+0], c34_0);
__m128d c34_1 = _mm_load_sd(&C[(i*35)+3]);
__m128d a34_1 = _mm_load_sd(&values[284]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_1 = _mm_add_sd(c34_1, _mm_mul_sd(a34_1, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_1 = _mm_add_sd(c34_1, _mm_mul_sd(a34_1, b34));
#endif
_mm_store_sd(&C[(i*35)+3], c34_1);
__m128d c34_2 = _mm_load_sd(&C[(i*35)+9]);
__m128d a34_2 = _mm_load_sd(&values[285]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_2 = _mm_add_sd(c34_2, _mm_mul_sd(a34_2, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_2 = _mm_add_sd(c34_2, _mm_mul_sd(a34_2, b34));
#endif
_mm_store_sd(&C[(i*35)+9], c34_2);
__m128d c34_3 = _mm_load_sd(&C[(i*35)+19]);
__m128d a34_3 = _mm_load_sd(&values[286]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_3 = _mm_add_sd(c34_3, _mm_mul_sd(a34_3, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_3 = _mm_add_sd(c34_3, _mm_mul_sd(a34_3, b34));
#endif
_mm_store_sd(&C[(i*35)+19], c34_3);
#else
C[(i*35)+0] += values[283] * B[(i*35)+34];
C[(i*35)+3] += values[284] * B[(i*35)+34];
C[(i*35)+9] += values[285] * B[(i*35)+34];
C[(i*35)+19] += values[286] * B[(i*35)+34];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kXiDivM_9_56(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 56; m++) {
    C[(i*56)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*56)+0]);
#endif
__m128d c0_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a0_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_sd(c0_0, _mm_mul_sd(a0_0, b0));
#endif
_mm_store_sd(&C[(i*56)+1], c0_0);
__m128d c0_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a0_1 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_1 = _mm_add_sd(c0_1, _mm_mul_sd(a0_1, b0));
#endif
_mm_store_sd(&C[(i*56)+5], c0_1);
__m128d c0_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a0_2 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_2 = _mm_add_sd(c0_2, _mm_mul_sd(a0_2, b0));
#endif
_mm_store_sd(&C[(i*56)+7], c0_2);
__m128d c0_3 = _mm_load_sd(&C[(i*56)+10]);
__m128d a0_3 = _mm_load_sd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_3 = _mm_add_sd(c0_3, _mm_mul_sd(a0_3, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_3 = _mm_add_sd(c0_3, _mm_mul_sd(a0_3, b0));
#endif
_mm_store_sd(&C[(i*56)+10], c0_3);
__m128d c0_4 = _mm_load_sd(&C[(i*56)+12]);
__m128d a0_4 = _mm_load_sd(&values[4]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_4 = _mm_add_sd(c0_4, _mm_mul_sd(a0_4, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_4 = _mm_add_sd(c0_4, _mm_mul_sd(a0_4, b0));
#endif
_mm_store_sd(&C[(i*56)+12], c0_4);
__m128d c0_5 = _mm_load_sd(&C[(i*56)+15]);
__m128d a0_5 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_5 = _mm_add_sd(c0_5, _mm_mul_sd(a0_5, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_5 = _mm_add_sd(c0_5, _mm_mul_sd(a0_5, b0));
#endif
_mm_store_sd(&C[(i*56)+15], c0_5);
__m128d c0_6 = _mm_load_sd(&C[(i*56)+17]);
__m128d a0_6 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, b0));
#endif
_mm_store_sd(&C[(i*56)+17], c0_6);
__m128d c0_7 = _mm_load_sd(&C[(i*56)+21]);
__m128d a0_7 = _mm_load_sd(&values[7]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_7 = _mm_add_sd(c0_7, _mm_mul_sd(a0_7, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_7 = _mm_add_sd(c0_7, _mm_mul_sd(a0_7, b0));
#endif
_mm_store_sd(&C[(i*56)+21], c0_7);
__m128d c0_8 = _mm_load_sd(&C[(i*56)+23]);
__m128d a0_8 = _mm_load_sd(&values[8]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_8 = _mm_add_sd(c0_8, _mm_mul_sd(a0_8, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_8 = _mm_add_sd(c0_8, _mm_mul_sd(a0_8, b0));
#endif
_mm_store_sd(&C[(i*56)+23], c0_8);
__m128d c0_9 = _mm_load_sd(&C[(i*56)+25]);
__m128d a0_9 = _mm_load_sd(&values[9]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_9 = _mm_add_sd(c0_9, _mm_mul_sd(a0_9, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_9 = _mm_add_sd(c0_9, _mm_mul_sd(a0_9, b0));
#endif
_mm_store_sd(&C[(i*56)+25], c0_9);
__m128d c0_10 = _mm_load_sd(&C[(i*56)+27]);
__m128d a0_10 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_10 = _mm_add_sd(c0_10, _mm_mul_sd(a0_10, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_10 = _mm_add_sd(c0_10, _mm_mul_sd(a0_10, b0));
#endif
_mm_store_sd(&C[(i*56)+27], c0_10);
__m128d c0_11 = _mm_load_sd(&C[(i*56)+30]);
__m128d a0_11 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_11 = _mm_add_sd(c0_11, _mm_mul_sd(a0_11, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_11 = _mm_add_sd(c0_11, _mm_mul_sd(a0_11, b0));
#endif
_mm_store_sd(&C[(i*56)+30], c0_11);
__m128d c0_12 = _mm_load_sd(&C[(i*56)+32]);
__m128d a0_12 = _mm_load_sd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_12 = _mm_add_sd(c0_12, _mm_mul_sd(a0_12, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_12 = _mm_add_sd(c0_12, _mm_mul_sd(a0_12, b0));
#endif
_mm_store_sd(&C[(i*56)+32], c0_12);
__m128d c0_13 = _mm_load_sd(&C[(i*56)+35]);
__m128d a0_13 = _mm_load_sd(&values[13]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_13 = _mm_add_sd(c0_13, _mm_mul_sd(a0_13, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_13 = _mm_add_sd(c0_13, _mm_mul_sd(a0_13, b0));
#endif
_mm_store_sd(&C[(i*56)+35], c0_13);
__m128d c0_14 = _mm_load_sd(&C[(i*56)+37]);
__m128d a0_14 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_14 = _mm_add_sd(c0_14, _mm_mul_sd(a0_14, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_14 = _mm_add_sd(c0_14, _mm_mul_sd(a0_14, b0));
#endif
_mm_store_sd(&C[(i*56)+37], c0_14);
__m128d c0_15 = _mm_load_sd(&C[(i*56)+39]);
__m128d a0_15 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, b0));
#endif
_mm_store_sd(&C[(i*56)+39], c0_15);
__m128d c0_16 = _mm_load_sd(&C[(i*56)+42]);
__m128d a0_16 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_16 = _mm_add_sd(c0_16, _mm_mul_sd(a0_16, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_16 = _mm_add_sd(c0_16, _mm_mul_sd(a0_16, b0));
#endif
_mm_store_sd(&C[(i*56)+42], c0_16);
__m128d c0_17 = _mm_load_sd(&C[(i*56)+44]);
__m128d a0_17 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_17 = _mm_add_sd(c0_17, _mm_mul_sd(a0_17, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_17 = _mm_add_sd(c0_17, _mm_mul_sd(a0_17, b0));
#endif
_mm_store_sd(&C[(i*56)+44], c0_17);
__m128d c0_18 = _mm_load_sd(&C[(i*56)+46]);
__m128d a0_18 = _mm_load_sd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_18 = _mm_add_sd(c0_18, _mm_mul_sd(a0_18, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_18 = _mm_add_sd(c0_18, _mm_mul_sd(a0_18, b0));
#endif
_mm_store_sd(&C[(i*56)+46], c0_18);
__m128d c0_19 = _mm_load_sd(&C[(i*56)+48]);
__m128d a0_19 = _mm_load_sd(&values[19]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_19 = _mm_add_sd(c0_19, _mm_mul_sd(a0_19, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_19 = _mm_add_sd(c0_19, _mm_mul_sd(a0_19, b0));
#endif
_mm_store_sd(&C[(i*56)+48], c0_19);
__m128d c0_20 = _mm_load_sd(&C[(i*56)+51]);
__m128d a0_20 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_20 = _mm_add_sd(c0_20, _mm_mul_sd(a0_20, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_20 = _mm_add_sd(c0_20, _mm_mul_sd(a0_20, b0));
#endif
_mm_store_sd(&C[(i*56)+51], c0_20);
__m128d c0_21 = _mm_load_sd(&C[(i*56)+53]);
__m128d a0_21 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_21 = _mm_add_sd(c0_21, _mm_mul_sd(a0_21, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_21 = _mm_add_sd(c0_21, _mm_mul_sd(a0_21, b0));
#endif
_mm_store_sd(&C[(i*56)+53], c0_21);
#else
C[(i*56)+1] += values[0] * B[(i*56)+0];
C[(i*56)+5] += values[1] * B[(i*56)+0];
C[(i*56)+7] += values[2] * B[(i*56)+0];
C[(i*56)+10] += values[3] * B[(i*56)+0];
C[(i*56)+12] += values[4] * B[(i*56)+0];
C[(i*56)+15] += values[5] * B[(i*56)+0];
C[(i*56)+17] += values[6] * B[(i*56)+0];
C[(i*56)+21] += values[7] * B[(i*56)+0];
C[(i*56)+23] += values[8] * B[(i*56)+0];
C[(i*56)+25] += values[9] * B[(i*56)+0];
C[(i*56)+27] += values[10] * B[(i*56)+0];
C[(i*56)+30] += values[11] * B[(i*56)+0];
C[(i*56)+32] += values[12] * B[(i*56)+0];
C[(i*56)+35] += values[13] * B[(i*56)+0];
C[(i*56)+37] += values[14] * B[(i*56)+0];
C[(i*56)+39] += values[15] * B[(i*56)+0];
C[(i*56)+42] += values[16] * B[(i*56)+0];
C[(i*56)+44] += values[17] * B[(i*56)+0];
C[(i*56)+46] += values[18] * B[(i*56)+0];
C[(i*56)+48] += values[19] * B[(i*56)+0];
C[(i*56)+51] += values[20] * B[(i*56)+0];
C[(i*56)+53] += values[21] * B[(i*56)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*56)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*56)+4]);
__m128d a1_0 = _mm_load_sd(&values[22]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*56)+4], c1_0);
__m128d c1_1 = _mm_load_sd(&C[(i*56)+11]);
__m128d a1_1 = _mm_load_sd(&values[23]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_1 = _mm_add_sd(c1_1, _mm_mul_sd(a1_1, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_1 = _mm_add_sd(c1_1, _mm_mul_sd(a1_1, b1));
#endif
_mm_store_sd(&C[(i*56)+11], c1_1);
__m128d c1_2 = _mm_load_sd(&C[(i*56)+14]);
__m128d a1_2 = _mm_load_sd(&values[24]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, b1));
#endif
_mm_store_sd(&C[(i*56)+14], c1_2);
__m128d c1_3 = _mm_load_sd(&C[(i*56)+20]);
__m128d a1_3 = _mm_load_sd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_3 = _mm_add_sd(c1_3, _mm_mul_sd(a1_3, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_3 = _mm_add_sd(c1_3, _mm_mul_sd(a1_3, b1));
#endif
_mm_store_sd(&C[(i*56)+20], c1_3);
__m128d c1_4 = _mm_load_sd(&C[(i*56)+22]);
__m128d a1_4 = _mm_load_sd(&values[26]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, b1));
#endif
_mm_store_sd(&C[(i*56)+22], c1_4);
__m128d c1_5 = _mm_load_sd(&C[(i*56)+26]);
__m128d a1_5 = _mm_load_sd(&values[27]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, b1));
#endif
_mm_store_sd(&C[(i*56)+26], c1_5);
__m128d c1_6 = _mm_load_sd(&C[(i*56)+29]);
__m128d a1_6 = _mm_load_sd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_6 = _mm_add_sd(c1_6, _mm_mul_sd(a1_6, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_6 = _mm_add_sd(c1_6, _mm_mul_sd(a1_6, b1));
#endif
_mm_store_sd(&C[(i*56)+29], c1_6);
__m128d c1_7 = _mm_load_sd(&C[(i*56)+36]);
__m128d a1_7 = _mm_load_sd(&values[29]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_7 = _mm_add_sd(c1_7, _mm_mul_sd(a1_7, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_7 = _mm_add_sd(c1_7, _mm_mul_sd(a1_7, b1));
#endif
_mm_store_sd(&C[(i*56)+36], c1_7);
__m128d c1_8 = _mm_load_sd(&C[(i*56)+38]);
__m128d a1_8 = _mm_load_sd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, b1));
#endif
_mm_store_sd(&C[(i*56)+38], c1_8);
__m128d c1_9 = _mm_load_sd(&C[(i*56)+41]);
__m128d a1_9 = _mm_load_sd(&values[31]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_9 = _mm_add_sd(c1_9, _mm_mul_sd(a1_9, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_9 = _mm_add_sd(c1_9, _mm_mul_sd(a1_9, b1));
#endif
_mm_store_sd(&C[(i*56)+41], c1_9);
__m128d c1_10 = _mm_load_sd(&C[(i*56)+43]);
__m128d a1_10 = _mm_load_sd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_10 = _mm_add_sd(c1_10, _mm_mul_sd(a1_10, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_10 = _mm_add_sd(c1_10, _mm_mul_sd(a1_10, b1));
#endif
_mm_store_sd(&C[(i*56)+43], c1_10);
__m128d c1_11 = _mm_load_sd(&C[(i*56)+47]);
__m128d a1_11 = _mm_load_sd(&values[33]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_11 = _mm_add_sd(c1_11, _mm_mul_sd(a1_11, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_11 = _mm_add_sd(c1_11, _mm_mul_sd(a1_11, b1));
#endif
_mm_store_sd(&C[(i*56)+47], c1_11);
__m128d c1_12 = _mm_load_sd(&C[(i*56)+50]);
__m128d a1_12 = _mm_load_sd(&values[34]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_12 = _mm_add_sd(c1_12, _mm_mul_sd(a1_12, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_12 = _mm_add_sd(c1_12, _mm_mul_sd(a1_12, b1));
#endif
_mm_store_sd(&C[(i*56)+50], c1_12);
#else
C[(i*56)+4] += values[22] * B[(i*56)+1];
C[(i*56)+11] += values[23] * B[(i*56)+1];
C[(i*56)+14] += values[24] * B[(i*56)+1];
C[(i*56)+20] += values[25] * B[(i*56)+1];
C[(i*56)+22] += values[26] * B[(i*56)+1];
C[(i*56)+26] += values[27] * B[(i*56)+1];
C[(i*56)+29] += values[28] * B[(i*56)+1];
C[(i*56)+36] += values[29] * B[(i*56)+1];
C[(i*56)+38] += values[30] * B[(i*56)+1];
C[(i*56)+41] += values[31] * B[(i*56)+1];
C[(i*56)+43] += values[32] * B[(i*56)+1];
C[(i*56)+47] += values[33] * B[(i*56)+1];
C[(i*56)+50] += values[34] * B[(i*56)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*56)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*56)+5]);
__m128d a2_0 = _mm_load_sd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*56)+5], c2_0);
__m128d c2_1 = _mm_load_sd(&C[(i*56)+10]);
__m128d a2_1 = _mm_load_sd(&values[36]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_1 = _mm_add_sd(c2_1, _mm_mul_sd(a2_1, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_1 = _mm_add_sd(c2_1, _mm_mul_sd(a2_1, b2));
#endif
_mm_store_sd(&C[(i*56)+10], c2_1);
__m128d c2_2 = _mm_load_sd(&C[(i*56)+12]);
__m128d a2_2 = _mm_load_sd(&values[37]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*56)+12], c2_2);
__m128d c2_3 = _mm_load_sd(&C[(i*56)+15]);
__m128d a2_3 = _mm_load_sd(&values[38]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, b2));
#endif
_mm_store_sd(&C[(i*56)+15], c2_3);
__m128d c2_4 = _mm_load_sd(&C[(i*56)+21]);
__m128d a2_4 = _mm_load_sd(&values[39]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_4 = _mm_add_sd(c2_4, _mm_mul_sd(a2_4, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_4 = _mm_add_sd(c2_4, _mm_mul_sd(a2_4, b2));
#endif
_mm_store_sd(&C[(i*56)+21], c2_4);
__m128d c2_5 = _mm_load_sd(&C[(i*56)+23]);
__m128d a2_5 = _mm_load_sd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_5 = _mm_add_sd(c2_5, _mm_mul_sd(a2_5, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_5 = _mm_add_sd(c2_5, _mm_mul_sd(a2_5, b2));
#endif
_mm_store_sd(&C[(i*56)+23], c2_5);
__m128d c2_6 = _mm_load_sd(&C[(i*56)+25]);
__m128d a2_6 = _mm_load_sd(&values[41]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_6 = _mm_add_sd(c2_6, _mm_mul_sd(a2_6, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_6 = _mm_add_sd(c2_6, _mm_mul_sd(a2_6, b2));
#endif
_mm_store_sd(&C[(i*56)+25], c2_6);
__m128d c2_7 = _mm_load_sd(&C[(i*56)+27]);
__m128d a2_7 = _mm_load_sd(&values[42]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_7 = _mm_add_sd(c2_7, _mm_mul_sd(a2_7, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_7 = _mm_add_sd(c2_7, _mm_mul_sd(a2_7, b2));
#endif
_mm_store_sd(&C[(i*56)+27], c2_7);
__m128d c2_8 = _mm_load_sd(&C[(i*56)+30]);
__m128d a2_8 = _mm_load_sd(&values[43]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_8 = _mm_add_sd(c2_8, _mm_mul_sd(a2_8, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_8 = _mm_add_sd(c2_8, _mm_mul_sd(a2_8, b2));
#endif
_mm_store_sd(&C[(i*56)+30], c2_8);
__m128d c2_9 = _mm_load_sd(&C[(i*56)+35]);
__m128d a2_9 = _mm_load_sd(&values[44]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, b2));
#endif
_mm_store_sd(&C[(i*56)+35], c2_9);
__m128d c2_10 = _mm_load_sd(&C[(i*56)+37]);
__m128d a2_10 = _mm_load_sd(&values[45]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, b2));
#endif
_mm_store_sd(&C[(i*56)+37], c2_10);
__m128d c2_11 = _mm_load_sd(&C[(i*56)+39]);
__m128d a2_11 = _mm_load_sd(&values[46]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, b2));
#endif
_mm_store_sd(&C[(i*56)+39], c2_11);
__m128d c2_12 = _mm_load_sd(&C[(i*56)+42]);
__m128d a2_12 = _mm_load_sd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_12 = _mm_add_sd(c2_12, _mm_mul_sd(a2_12, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_12 = _mm_add_sd(c2_12, _mm_mul_sd(a2_12, b2));
#endif
_mm_store_sd(&C[(i*56)+42], c2_12);
__m128d c2_13 = _mm_load_sd(&C[(i*56)+44]);
__m128d a2_13 = _mm_load_sd(&values[48]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_13 = _mm_add_sd(c2_13, _mm_mul_sd(a2_13, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_13 = _mm_add_sd(c2_13, _mm_mul_sd(a2_13, b2));
#endif
_mm_store_sd(&C[(i*56)+44], c2_13);
__m128d c2_14 = _mm_load_sd(&C[(i*56)+46]);
__m128d a2_14 = _mm_load_sd(&values[49]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_14 = _mm_add_sd(c2_14, _mm_mul_sd(a2_14, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_14 = _mm_add_sd(c2_14, _mm_mul_sd(a2_14, b2));
#endif
_mm_store_sd(&C[(i*56)+46], c2_14);
__m128d c2_15 = _mm_load_sd(&C[(i*56)+48]);
__m128d a2_15 = _mm_load_sd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_15 = _mm_add_sd(c2_15, _mm_mul_sd(a2_15, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_15 = _mm_add_sd(c2_15, _mm_mul_sd(a2_15, b2));
#endif
_mm_store_sd(&C[(i*56)+48], c2_15);
__m128d c2_16 = _mm_load_sd(&C[(i*56)+51]);
__m128d a2_16 = _mm_load_sd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_16 = _mm_add_sd(c2_16, _mm_mul_sd(a2_16, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_16 = _mm_add_sd(c2_16, _mm_mul_sd(a2_16, b2));
#endif
_mm_store_sd(&C[(i*56)+51], c2_16);
#else
C[(i*56)+5] += values[35] * B[(i*56)+2];
C[(i*56)+10] += values[36] * B[(i*56)+2];
C[(i*56)+12] += values[37] * B[(i*56)+2];
C[(i*56)+15] += values[38] * B[(i*56)+2];
C[(i*56)+21] += values[39] * B[(i*56)+2];
C[(i*56)+23] += values[40] * B[(i*56)+2];
C[(i*56)+25] += values[41] * B[(i*56)+2];
C[(i*56)+27] += values[42] * B[(i*56)+2];
C[(i*56)+30] += values[43] * B[(i*56)+2];
C[(i*56)+35] += values[44] * B[(i*56)+2];
C[(i*56)+37] += values[45] * B[(i*56)+2];
C[(i*56)+39] += values[46] * B[(i*56)+2];
C[(i*56)+42] += values[47] * B[(i*56)+2];
C[(i*56)+44] += values[48] * B[(i*56)+2];
C[(i*56)+46] += values[49] * B[(i*56)+2];
C[(i*56)+48] += values[50] * B[(i*56)+2];
C[(i*56)+51] += values[51] * B[(i*56)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*56)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*56)+5]);
__m128d a3_0 = _mm_load_sd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*56)+5], c3_0);
__m128d c3_1 = _mm_load_sd(&C[(i*56)+7]);
__m128d a3_1 = _mm_load_sd(&values[53]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_1 = _mm_add_sd(c3_1, _mm_mul_sd(a3_1, b3));
#endif
_mm_store_sd(&C[(i*56)+7], c3_1);
__m128d c3_2 = _mm_load_sd(&C[(i*56)+10]);
__m128d a3_2 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_2 = _mm_add_sd(c3_2, _mm_mul_sd(a3_2, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_2 = _mm_add_sd(c3_2, _mm_mul_sd(a3_2, b3));
#endif
_mm_store_sd(&C[(i*56)+10], c3_2);
__m128d c3_3 = _mm_load_sd(&C[(i*56)+12]);
__m128d a3_3 = _mm_load_sd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_3 = _mm_add_sd(c3_3, _mm_mul_sd(a3_3, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_3 = _mm_add_sd(c3_3, _mm_mul_sd(a3_3, b3));
#endif
_mm_store_sd(&C[(i*56)+12], c3_3);
__m128d c3_4 = _mm_load_sd(&C[(i*56)+15]);
__m128d a3_4 = _mm_load_sd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, b3));
#endif
_mm_store_sd(&C[(i*56)+15], c3_4);
__m128d c3_5 = _mm_load_sd(&C[(i*56)+17]);
__m128d a3_5 = _mm_load_sd(&values[57]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_5 = _mm_add_sd(c3_5, _mm_mul_sd(a3_5, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_5 = _mm_add_sd(c3_5, _mm_mul_sd(a3_5, b3));
#endif
_mm_store_sd(&C[(i*56)+17], c3_5);
__m128d c3_6 = _mm_load_sd(&C[(i*56)+21]);
__m128d a3_6 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_6 = _mm_add_sd(c3_6, _mm_mul_sd(a3_6, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_6 = _mm_add_sd(c3_6, _mm_mul_sd(a3_6, b3));
#endif
_mm_store_sd(&C[(i*56)+21], c3_6);
__m128d c3_7 = _mm_load_sd(&C[(i*56)+23]);
__m128d a3_7 = _mm_load_sd(&values[59]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_7 = _mm_add_sd(c3_7, _mm_mul_sd(a3_7, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_7 = _mm_add_sd(c3_7, _mm_mul_sd(a3_7, b3));
#endif
_mm_store_sd(&C[(i*56)+23], c3_7);
__m128d c3_8 = _mm_load_sd(&C[(i*56)+25]);
__m128d a3_8 = _mm_load_sd(&values[60]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_8 = _mm_add_sd(c3_8, _mm_mul_sd(a3_8, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_8 = _mm_add_sd(c3_8, _mm_mul_sd(a3_8, b3));
#endif
_mm_store_sd(&C[(i*56)+25], c3_8);
__m128d c3_9 = _mm_load_sd(&C[(i*56)+27]);
__m128d a3_9 = _mm_load_sd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_9 = _mm_add_sd(c3_9, _mm_mul_sd(a3_9, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_9 = _mm_add_sd(c3_9, _mm_mul_sd(a3_9, b3));
#endif
_mm_store_sd(&C[(i*56)+27], c3_9);
__m128d c3_10 = _mm_load_sd(&C[(i*56)+30]);
__m128d a3_10 = _mm_load_sd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_10 = _mm_add_sd(c3_10, _mm_mul_sd(a3_10, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_10 = _mm_add_sd(c3_10, _mm_mul_sd(a3_10, b3));
#endif
_mm_store_sd(&C[(i*56)+30], c3_10);
__m128d c3_11 = _mm_load_sd(&C[(i*56)+32]);
__m128d a3_11 = _mm_load_sd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_11 = _mm_add_sd(c3_11, _mm_mul_sd(a3_11, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_11 = _mm_add_sd(c3_11, _mm_mul_sd(a3_11, b3));
#endif
_mm_store_sd(&C[(i*56)+32], c3_11);
__m128d c3_12 = _mm_load_sd(&C[(i*56)+35]);
__m128d a3_12 = _mm_load_sd(&values[64]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_12 = _mm_add_sd(c3_12, _mm_mul_sd(a3_12, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_12 = _mm_add_sd(c3_12, _mm_mul_sd(a3_12, b3));
#endif
_mm_store_sd(&C[(i*56)+35], c3_12);
__m128d c3_13 = _mm_load_sd(&C[(i*56)+37]);
__m128d a3_13 = _mm_load_sd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, b3));
#endif
_mm_store_sd(&C[(i*56)+37], c3_13);
__m128d c3_14 = _mm_load_sd(&C[(i*56)+39]);
__m128d a3_14 = _mm_load_sd(&values[66]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_14 = _mm_add_sd(c3_14, _mm_mul_sd(a3_14, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_14 = _mm_add_sd(c3_14, _mm_mul_sd(a3_14, b3));
#endif
_mm_store_sd(&C[(i*56)+39], c3_14);
__m128d c3_15 = _mm_load_sd(&C[(i*56)+42]);
__m128d a3_15 = _mm_load_sd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_15 = _mm_add_sd(c3_15, _mm_mul_sd(a3_15, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_15 = _mm_add_sd(c3_15, _mm_mul_sd(a3_15, b3));
#endif
_mm_store_sd(&C[(i*56)+42], c3_15);
__m128d c3_16 = _mm_load_sd(&C[(i*56)+44]);
__m128d a3_16 = _mm_load_sd(&values[68]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_16 = _mm_add_sd(c3_16, _mm_mul_sd(a3_16, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_16 = _mm_add_sd(c3_16, _mm_mul_sd(a3_16, b3));
#endif
_mm_store_sd(&C[(i*56)+44], c3_16);
__m128d c3_17 = _mm_load_sd(&C[(i*56)+46]);
__m128d a3_17 = _mm_load_sd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_17 = _mm_add_sd(c3_17, _mm_mul_sd(a3_17, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_17 = _mm_add_sd(c3_17, _mm_mul_sd(a3_17, b3));
#endif
_mm_store_sd(&C[(i*56)+46], c3_17);
__m128d c3_18 = _mm_load_sd(&C[(i*56)+48]);
__m128d a3_18 = _mm_load_sd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_18 = _mm_add_sd(c3_18, _mm_mul_sd(a3_18, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_18 = _mm_add_sd(c3_18, _mm_mul_sd(a3_18, b3));
#endif
_mm_store_sd(&C[(i*56)+48], c3_18);
__m128d c3_19 = _mm_load_sd(&C[(i*56)+51]);
__m128d a3_19 = _mm_load_sd(&values[71]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_19 = _mm_add_sd(c3_19, _mm_mul_sd(a3_19, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_19 = _mm_add_sd(c3_19, _mm_mul_sd(a3_19, b3));
#endif
_mm_store_sd(&C[(i*56)+51], c3_19);
__m128d c3_20 = _mm_load_sd(&C[(i*56)+53]);
__m128d a3_20 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_20 = _mm_add_sd(c3_20, _mm_mul_sd(a3_20, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_20 = _mm_add_sd(c3_20, _mm_mul_sd(a3_20, b3));
#endif
_mm_store_sd(&C[(i*56)+53], c3_20);
#else
C[(i*56)+5] += values[52] * B[(i*56)+3];
C[(i*56)+7] += values[53] * B[(i*56)+3];
C[(i*56)+10] += values[54] * B[(i*56)+3];
C[(i*56)+12] += values[55] * B[(i*56)+3];
C[(i*56)+15] += values[56] * B[(i*56)+3];
C[(i*56)+17] += values[57] * B[(i*56)+3];
C[(i*56)+21] += values[58] * B[(i*56)+3];
C[(i*56)+23] += values[59] * B[(i*56)+3];
C[(i*56)+25] += values[60] * B[(i*56)+3];
C[(i*56)+27] += values[61] * B[(i*56)+3];
C[(i*56)+30] += values[62] * B[(i*56)+3];
C[(i*56)+32] += values[63] * B[(i*56)+3];
C[(i*56)+35] += values[64] * B[(i*56)+3];
C[(i*56)+37] += values[65] * B[(i*56)+3];
C[(i*56)+39] += values[66] * B[(i*56)+3];
C[(i*56)+42] += values[67] * B[(i*56)+3];
C[(i*56)+44] += values[68] * B[(i*56)+3];
C[(i*56)+46] += values[69] * B[(i*56)+3];
C[(i*56)+48] += values[70] * B[(i*56)+3];
C[(i*56)+51] += values[71] * B[(i*56)+3];
C[(i*56)+53] += values[72] * B[(i*56)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*56)+4]);
#endif
__m128d c4_0 = _mm_load_sd(&C[(i*56)+10]);
__m128d a4_0 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, b4));
#endif
_mm_store_sd(&C[(i*56)+10], c4_0);
__m128d c4_1 = _mm_load_sd(&C[(i*56)+21]);
__m128d a4_1 = _mm_load_sd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_1 = _mm_add_sd(c4_1, _mm_mul_sd(a4_1, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_1 = _mm_add_sd(c4_1, _mm_mul_sd(a4_1, b4));
#endif
_mm_store_sd(&C[(i*56)+21], c4_1);
__m128d c4_2 = _mm_load_sd(&C[(i*56)+25]);
__m128d a4_2 = _mm_load_sd(&values[75]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, b4));
#endif
_mm_store_sd(&C[(i*56)+25], c4_2);
__m128d c4_3 = _mm_load_sd(&C[(i*56)+35]);
__m128d a4_3 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_3 = _mm_add_sd(c4_3, _mm_mul_sd(a4_3, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_3 = _mm_add_sd(c4_3, _mm_mul_sd(a4_3, b4));
#endif
_mm_store_sd(&C[(i*56)+35], c4_3);
__m128d c4_4 = _mm_load_sd(&C[(i*56)+37]);
__m128d a4_4 = _mm_load_sd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_4 = _mm_add_sd(c4_4, _mm_mul_sd(a4_4, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_4 = _mm_add_sd(c4_4, _mm_mul_sd(a4_4, b4));
#endif
_mm_store_sd(&C[(i*56)+37], c4_4);
__m128d c4_5 = _mm_load_sd(&C[(i*56)+42]);
__m128d a4_5 = _mm_load_sd(&values[78]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_5 = _mm_add_sd(c4_5, _mm_mul_sd(a4_5, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_5 = _mm_add_sd(c4_5, _mm_mul_sd(a4_5, b4));
#endif
_mm_store_sd(&C[(i*56)+42], c4_5);
__m128d c4_6 = _mm_load_sd(&C[(i*56)+46]);
__m128d a4_6 = _mm_load_sd(&values[79]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_6 = _mm_add_sd(c4_6, _mm_mul_sd(a4_6, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_6 = _mm_add_sd(c4_6, _mm_mul_sd(a4_6, b4));
#endif
_mm_store_sd(&C[(i*56)+46], c4_6);
#else
C[(i*56)+10] += values[73] * B[(i*56)+4];
C[(i*56)+21] += values[74] * B[(i*56)+4];
C[(i*56)+25] += values[75] * B[(i*56)+4];
C[(i*56)+35] += values[76] * B[(i*56)+4];
C[(i*56)+37] += values[77] * B[(i*56)+4];
C[(i*56)+42] += values[78] * B[(i*56)+4];
C[(i*56)+46] += values[79] * B[(i*56)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*56)+5]);
#endif
__m128d c5_0 = _mm_load_sd(&C[(i*56)+11]);
__m128d a5_0 = _mm_load_sd(&values[80]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, b5));
#endif
_mm_store_sd(&C[(i*56)+11], c5_0);
__m128d c5_1 = _mm_load_sd(&C[(i*56)+20]);
__m128d a5_1 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_1 = _mm_add_sd(c5_1, _mm_mul_sd(a5_1, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_1 = _mm_add_sd(c5_1, _mm_mul_sd(a5_1, b5));
#endif
_mm_store_sd(&C[(i*56)+20], c5_1);
__m128d c5_2 = _mm_load_sd(&C[(i*56)+22]);
__m128d a5_2 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*56)+22], c5_2);
__m128d c5_3 = _mm_load_sd(&C[(i*56)+26]);
__m128d a5_3 = _mm_load_sd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, b5));
#endif
_mm_store_sd(&C[(i*56)+26], c5_3);
__m128d c5_4 = _mm_load_sd(&C[(i*56)+36]);
__m128d a5_4 = _mm_load_sd(&values[84]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_4 = _mm_add_sd(c5_4, _mm_mul_sd(a5_4, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_4 = _mm_add_sd(c5_4, _mm_mul_sd(a5_4, b5));
#endif
_mm_store_sd(&C[(i*56)+36], c5_4);
__m128d c5_5 = _mm_load_sd(&C[(i*56)+38]);
__m128d a5_5 = _mm_load_sd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_5 = _mm_add_sd(c5_5, _mm_mul_sd(a5_5, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_5 = _mm_add_sd(c5_5, _mm_mul_sd(a5_5, b5));
#endif
_mm_store_sd(&C[(i*56)+38], c5_5);
__m128d c5_6 = _mm_load_sd(&C[(i*56)+41]);
__m128d a5_6 = _mm_load_sd(&values[86]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_6 = _mm_add_sd(c5_6, _mm_mul_sd(a5_6, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_6 = _mm_add_sd(c5_6, _mm_mul_sd(a5_6, b5));
#endif
_mm_store_sd(&C[(i*56)+41], c5_6);
__m128d c5_7 = _mm_load_sd(&C[(i*56)+43]);
__m128d a5_7 = _mm_load_sd(&values[87]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_7 = _mm_add_sd(c5_7, _mm_mul_sd(a5_7, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_7 = _mm_add_sd(c5_7, _mm_mul_sd(a5_7, b5));
#endif
_mm_store_sd(&C[(i*56)+43], c5_7);
__m128d c5_8 = _mm_load_sd(&C[(i*56)+47]);
__m128d a5_8 = _mm_load_sd(&values[88]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_8 = _mm_add_sd(c5_8, _mm_mul_sd(a5_8, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_8 = _mm_add_sd(c5_8, _mm_mul_sd(a5_8, b5));
#endif
_mm_store_sd(&C[(i*56)+47], c5_8);
#else
C[(i*56)+11] += values[80] * B[(i*56)+5];
C[(i*56)+20] += values[81] * B[(i*56)+5];
C[(i*56)+22] += values[82] * B[(i*56)+5];
C[(i*56)+26] += values[83] * B[(i*56)+5];
C[(i*56)+36] += values[84] * B[(i*56)+5];
C[(i*56)+38] += values[85] * B[(i*56)+5];
C[(i*56)+41] += values[86] * B[(i*56)+5];
C[(i*56)+43] += values[87] * B[(i*56)+5];
C[(i*56)+47] += values[88] * B[(i*56)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*56)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*56)+10]);
__m128d a6_0 = _mm_load_sd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*56)+10], c6_0);
__m128d c6_1 = _mm_load_sd(&C[(i*56)+12]);
__m128d a6_1 = _mm_load_sd(&values[90]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_sd(c6_1, _mm_mul_sd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_sd(c6_1, _mm_mul_sd(a6_1, b6));
#endif
_mm_store_sd(&C[(i*56)+12], c6_1);
__m128d c6_2 = _mm_load_sd(&C[(i*56)+21]);
__m128d a6_2 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_2 = _mm_add_sd(c6_2, _mm_mul_sd(a6_2, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_2 = _mm_add_sd(c6_2, _mm_mul_sd(a6_2, b6));
#endif
_mm_store_sd(&C[(i*56)+21], c6_2);
__m128d c6_3 = _mm_load_sd(&C[(i*56)+23]);
__m128d a6_3 = _mm_load_sd(&values[92]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_3 = _mm_add_sd(c6_3, _mm_mul_sd(a6_3, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_3 = _mm_add_sd(c6_3, _mm_mul_sd(a6_3, b6));
#endif
_mm_store_sd(&C[(i*56)+23], c6_3);
__m128d c6_4 = _mm_load_sd(&C[(i*56)+25]);
__m128d a6_4 = _mm_load_sd(&values[93]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, b6));
#endif
_mm_store_sd(&C[(i*56)+25], c6_4);
__m128d c6_5 = _mm_load_sd(&C[(i*56)+27]);
__m128d a6_5 = _mm_load_sd(&values[94]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_5 = _mm_add_sd(c6_5, _mm_mul_sd(a6_5, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_5 = _mm_add_sd(c6_5, _mm_mul_sd(a6_5, b6));
#endif
_mm_store_sd(&C[(i*56)+27], c6_5);
__m128d c6_6 = _mm_load_sd(&C[(i*56)+35]);
__m128d a6_6 = _mm_load_sd(&values[95]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_6 = _mm_add_sd(c6_6, _mm_mul_sd(a6_6, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_6 = _mm_add_sd(c6_6, _mm_mul_sd(a6_6, b6));
#endif
_mm_store_sd(&C[(i*56)+35], c6_6);
__m128d c6_7 = _mm_load_sd(&C[(i*56)+37]);
__m128d a6_7 = _mm_load_sd(&values[96]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_7 = _mm_add_sd(c6_7, _mm_mul_sd(a6_7, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_7 = _mm_add_sd(c6_7, _mm_mul_sd(a6_7, b6));
#endif
_mm_store_sd(&C[(i*56)+37], c6_7);
__m128d c6_8 = _mm_load_sd(&C[(i*56)+39]);
__m128d a6_8 = _mm_load_sd(&values[97]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_8 = _mm_add_sd(c6_8, _mm_mul_sd(a6_8, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_8 = _mm_add_sd(c6_8, _mm_mul_sd(a6_8, b6));
#endif
_mm_store_sd(&C[(i*56)+39], c6_8);
__m128d c6_9 = _mm_load_sd(&C[(i*56)+42]);
__m128d a6_9 = _mm_load_sd(&values[98]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_9 = _mm_add_sd(c6_9, _mm_mul_sd(a6_9, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_9 = _mm_add_sd(c6_9, _mm_mul_sd(a6_9, b6));
#endif
_mm_store_sd(&C[(i*56)+42], c6_9);
__m128d c6_10 = _mm_load_sd(&C[(i*56)+44]);
__m128d a6_10 = _mm_load_sd(&values[99]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_10 = _mm_add_sd(c6_10, _mm_mul_sd(a6_10, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_10 = _mm_add_sd(c6_10, _mm_mul_sd(a6_10, b6));
#endif
_mm_store_sd(&C[(i*56)+44], c6_10);
__m128d c6_11 = _mm_load_sd(&C[(i*56)+46]);
__m128d a6_11 = _mm_load_sd(&values[100]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_11 = _mm_add_sd(c6_11, _mm_mul_sd(a6_11, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_11 = _mm_add_sd(c6_11, _mm_mul_sd(a6_11, b6));
#endif
_mm_store_sd(&C[(i*56)+46], c6_11);
__m128d c6_12 = _mm_load_sd(&C[(i*56)+48]);
__m128d a6_12 = _mm_load_sd(&values[101]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_12 = _mm_add_sd(c6_12, _mm_mul_sd(a6_12, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_12 = _mm_add_sd(c6_12, _mm_mul_sd(a6_12, b6));
#endif
_mm_store_sd(&C[(i*56)+48], c6_12);
#else
C[(i*56)+10] += values[89] * B[(i*56)+6];
C[(i*56)+12] += values[90] * B[(i*56)+6];
C[(i*56)+21] += values[91] * B[(i*56)+6];
C[(i*56)+23] += values[92] * B[(i*56)+6];
C[(i*56)+25] += values[93] * B[(i*56)+6];
C[(i*56)+27] += values[94] * B[(i*56)+6];
C[(i*56)+35] += values[95] * B[(i*56)+6];
C[(i*56)+37] += values[96] * B[(i*56)+6];
C[(i*56)+39] += values[97] * B[(i*56)+6];
C[(i*56)+42] += values[98] * B[(i*56)+6];
C[(i*56)+44] += values[99] * B[(i*56)+6];
C[(i*56)+46] += values[100] * B[(i*56)+6];
C[(i*56)+48] += values[101] * B[(i*56)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*56)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*56)+11]);
__m128d a7_0 = _mm_load_sd(&values[102]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*56)+11], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*56)+14]);
__m128d a7_1 = _mm_load_sd(&values[103]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*56)+14], c7_1);
__m128d c7_2 = _mm_load_sd(&C[(i*56)+20]);
__m128d a7_2 = _mm_load_sd(&values[104]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*56)+20], c7_2);
__m128d c7_3 = _mm_load_sd(&C[(i*56)+22]);
__m128d a7_3 = _mm_load_sd(&values[105]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_sd(c7_3, _mm_mul_sd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_sd(c7_3, _mm_mul_sd(a7_3, b7));
#endif
_mm_store_sd(&C[(i*56)+22], c7_3);
__m128d c7_4 = _mm_load_sd(&C[(i*56)+26]);
__m128d a7_4 = _mm_load_sd(&values[106]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_4 = _mm_add_sd(c7_4, _mm_mul_sd(a7_4, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_4 = _mm_add_sd(c7_4, _mm_mul_sd(a7_4, b7));
#endif
_mm_store_sd(&C[(i*56)+26], c7_4);
__m128d c7_5 = _mm_load_sd(&C[(i*56)+29]);
__m128d a7_5 = _mm_load_sd(&values[107]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, b7));
#endif
_mm_store_sd(&C[(i*56)+29], c7_5);
__m128d c7_6 = _mm_load_sd(&C[(i*56)+36]);
__m128d a7_6 = _mm_load_sd(&values[108]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_6 = _mm_add_sd(c7_6, _mm_mul_sd(a7_6, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_6 = _mm_add_sd(c7_6, _mm_mul_sd(a7_6, b7));
#endif
_mm_store_sd(&C[(i*56)+36], c7_6);
__m128d c7_7 = _mm_load_sd(&C[(i*56)+38]);
__m128d a7_7 = _mm_load_sd(&values[109]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_7 = _mm_add_sd(c7_7, _mm_mul_sd(a7_7, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_7 = _mm_add_sd(c7_7, _mm_mul_sd(a7_7, b7));
#endif
_mm_store_sd(&C[(i*56)+38], c7_7);
__m128d c7_8 = _mm_load_sd(&C[(i*56)+41]);
__m128d a7_8 = _mm_load_sd(&values[110]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_8 = _mm_add_sd(c7_8, _mm_mul_sd(a7_8, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_8 = _mm_add_sd(c7_8, _mm_mul_sd(a7_8, b7));
#endif
_mm_store_sd(&C[(i*56)+41], c7_8);
__m128d c7_9 = _mm_load_sd(&C[(i*56)+43]);
__m128d a7_9 = _mm_load_sd(&values[111]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_9 = _mm_add_sd(c7_9, _mm_mul_sd(a7_9, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_9 = _mm_add_sd(c7_9, _mm_mul_sd(a7_9, b7));
#endif
_mm_store_sd(&C[(i*56)+43], c7_9);
__m128d c7_10 = _mm_load_sd(&C[(i*56)+47]);
__m128d a7_10 = _mm_load_sd(&values[112]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_10 = _mm_add_sd(c7_10, _mm_mul_sd(a7_10, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_10 = _mm_add_sd(c7_10, _mm_mul_sd(a7_10, b7));
#endif
_mm_store_sd(&C[(i*56)+47], c7_10);
__m128d c7_11 = _mm_load_sd(&C[(i*56)+50]);
__m128d a7_11 = _mm_load_sd(&values[113]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_11 = _mm_add_sd(c7_11, _mm_mul_sd(a7_11, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_11 = _mm_add_sd(c7_11, _mm_mul_sd(a7_11, b7));
#endif
_mm_store_sd(&C[(i*56)+50], c7_11);
#else
C[(i*56)+11] += values[102] * B[(i*56)+7];
C[(i*56)+14] += values[103] * B[(i*56)+7];
C[(i*56)+20] += values[104] * B[(i*56)+7];
C[(i*56)+22] += values[105] * B[(i*56)+7];
C[(i*56)+26] += values[106] * B[(i*56)+7];
C[(i*56)+29] += values[107] * B[(i*56)+7];
C[(i*56)+36] += values[108] * B[(i*56)+7];
C[(i*56)+38] += values[109] * B[(i*56)+7];
C[(i*56)+41] += values[110] * B[(i*56)+7];
C[(i*56)+43] += values[111] * B[(i*56)+7];
C[(i*56)+47] += values[112] * B[(i*56)+7];
C[(i*56)+50] += values[113] * B[(i*56)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*56)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*56)+10]);
__m128d a8_0 = _mm_load_sd(&values[114]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*56)+10], c8_0);
__m128d c8_1 = _mm_load_sd(&C[(i*56)+12]);
__m128d a8_1 = _mm_load_sd(&values[115]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, b8));
#endif
_mm_store_sd(&C[(i*56)+12], c8_1);
__m128d c8_2 = _mm_load_sd(&C[(i*56)+15]);
__m128d a8_2 = _mm_load_sd(&values[116]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_2 = _mm_add_sd(c8_2, _mm_mul_sd(a8_2, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_2 = _mm_add_sd(c8_2, _mm_mul_sd(a8_2, b8));
#endif
_mm_store_sd(&C[(i*56)+15], c8_2);
__m128d c8_3 = _mm_load_sd(&C[(i*56)+21]);
__m128d a8_3 = _mm_load_sd(&values[117]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_3 = _mm_add_sd(c8_3, _mm_mul_sd(a8_3, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_3 = _mm_add_sd(c8_3, _mm_mul_sd(a8_3, b8));
#endif
_mm_store_sd(&C[(i*56)+21], c8_3);
__m128d c8_4 = _mm_load_sd(&C[(i*56)+23]);
__m128d a8_4 = _mm_load_sd(&values[118]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_sd(c8_4, _mm_mul_sd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_sd(c8_4, _mm_mul_sd(a8_4, b8));
#endif
_mm_store_sd(&C[(i*56)+23], c8_4);
__m128d c8_5 = _mm_load_sd(&C[(i*56)+25]);
__m128d a8_5 = _mm_load_sd(&values[119]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_5 = _mm_add_sd(c8_5, _mm_mul_sd(a8_5, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_5 = _mm_add_sd(c8_5, _mm_mul_sd(a8_5, b8));
#endif
_mm_store_sd(&C[(i*56)+25], c8_5);
__m128d c8_6 = _mm_load_sd(&C[(i*56)+27]);
__m128d a8_6 = _mm_load_sd(&values[120]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*56)+27], c8_6);
__m128d c8_7 = _mm_load_sd(&C[(i*56)+30]);
__m128d a8_7 = _mm_load_sd(&values[121]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, b8));
#endif
_mm_store_sd(&C[(i*56)+30], c8_7);
__m128d c8_8 = _mm_load_sd(&C[(i*56)+35]);
__m128d a8_8 = _mm_load_sd(&values[122]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_8 = _mm_add_sd(c8_8, _mm_mul_sd(a8_8, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_8 = _mm_add_sd(c8_8, _mm_mul_sd(a8_8, b8));
#endif
_mm_store_sd(&C[(i*56)+35], c8_8);
__m128d c8_9 = _mm_load_sd(&C[(i*56)+37]);
__m128d a8_9 = _mm_load_sd(&values[123]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_9 = _mm_add_sd(c8_9, _mm_mul_sd(a8_9, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_9 = _mm_add_sd(c8_9, _mm_mul_sd(a8_9, b8));
#endif
_mm_store_sd(&C[(i*56)+37], c8_9);
__m128d c8_10 = _mm_load_sd(&C[(i*56)+39]);
__m128d a8_10 = _mm_load_sd(&values[124]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_10 = _mm_add_sd(c8_10, _mm_mul_sd(a8_10, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_10 = _mm_add_sd(c8_10, _mm_mul_sd(a8_10, b8));
#endif
_mm_store_sd(&C[(i*56)+39], c8_10);
__m128d c8_11 = _mm_load_sd(&C[(i*56)+42]);
__m128d a8_11 = _mm_load_sd(&values[125]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_11 = _mm_add_sd(c8_11, _mm_mul_sd(a8_11, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_11 = _mm_add_sd(c8_11, _mm_mul_sd(a8_11, b8));
#endif
_mm_store_sd(&C[(i*56)+42], c8_11);
__m128d c8_12 = _mm_load_sd(&C[(i*56)+44]);
__m128d a8_12 = _mm_load_sd(&values[126]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_12 = _mm_add_sd(c8_12, _mm_mul_sd(a8_12, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_12 = _mm_add_sd(c8_12, _mm_mul_sd(a8_12, b8));
#endif
_mm_store_sd(&C[(i*56)+44], c8_12);
__m128d c8_13 = _mm_load_sd(&C[(i*56)+46]);
__m128d a8_13 = _mm_load_sd(&values[127]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_13 = _mm_add_sd(c8_13, _mm_mul_sd(a8_13, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_13 = _mm_add_sd(c8_13, _mm_mul_sd(a8_13, b8));
#endif
_mm_store_sd(&C[(i*56)+46], c8_13);
__m128d c8_14 = _mm_load_sd(&C[(i*56)+48]);
__m128d a8_14 = _mm_load_sd(&values[128]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_14 = _mm_add_sd(c8_14, _mm_mul_sd(a8_14, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_14 = _mm_add_sd(c8_14, _mm_mul_sd(a8_14, b8));
#endif
_mm_store_sd(&C[(i*56)+48], c8_14);
__m128d c8_15 = _mm_load_sd(&C[(i*56)+51]);
__m128d a8_15 = _mm_load_sd(&values[129]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_15 = _mm_add_sd(c8_15, _mm_mul_sd(a8_15, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_15 = _mm_add_sd(c8_15, _mm_mul_sd(a8_15, b8));
#endif
_mm_store_sd(&C[(i*56)+51], c8_15);
#else
C[(i*56)+10] += values[114] * B[(i*56)+8];
C[(i*56)+12] += values[115] * B[(i*56)+8];
C[(i*56)+15] += values[116] * B[(i*56)+8];
C[(i*56)+21] += values[117] * B[(i*56)+8];
C[(i*56)+23] += values[118] * B[(i*56)+8];
C[(i*56)+25] += values[119] * B[(i*56)+8];
C[(i*56)+27] += values[120] * B[(i*56)+8];
C[(i*56)+30] += values[121] * B[(i*56)+8];
C[(i*56)+35] += values[122] * B[(i*56)+8];
C[(i*56)+37] += values[123] * B[(i*56)+8];
C[(i*56)+39] += values[124] * B[(i*56)+8];
C[(i*56)+42] += values[125] * B[(i*56)+8];
C[(i*56)+44] += values[126] * B[(i*56)+8];
C[(i*56)+46] += values[127] * B[(i*56)+8];
C[(i*56)+48] += values[128] * B[(i*56)+8];
C[(i*56)+51] += values[129] * B[(i*56)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*56)+9]);
#endif
__m128d c9_0 = _mm_load_sd(&C[(i*56)+10]);
__m128d a9_0 = _mm_load_sd(&values[130]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, b9));
#endif
_mm_store_sd(&C[(i*56)+10], c9_0);
__m128d c9_1 = _mm_load_sd(&C[(i*56)+12]);
__m128d a9_1 = _mm_load_sd(&values[131]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, b9));
#endif
_mm_store_sd(&C[(i*56)+12], c9_1);
__m128d c9_2 = _mm_load_sd(&C[(i*56)+15]);
__m128d a9_2 = _mm_load_sd(&values[132]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_2 = _mm_add_sd(c9_2, _mm_mul_sd(a9_2, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_2 = _mm_add_sd(c9_2, _mm_mul_sd(a9_2, b9));
#endif
_mm_store_sd(&C[(i*56)+15], c9_2);
__m128d c9_3 = _mm_load_sd(&C[(i*56)+17]);
__m128d a9_3 = _mm_load_sd(&values[133]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_3 = _mm_add_sd(c9_3, _mm_mul_sd(a9_3, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_3 = _mm_add_sd(c9_3, _mm_mul_sd(a9_3, b9));
#endif
_mm_store_sd(&C[(i*56)+17], c9_3);
__m128d c9_4 = _mm_load_sd(&C[(i*56)+21]);
__m128d a9_4 = _mm_load_sd(&values[134]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_4 = _mm_add_sd(c9_4, _mm_mul_sd(a9_4, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_4 = _mm_add_sd(c9_4, _mm_mul_sd(a9_4, b9));
#endif
_mm_store_sd(&C[(i*56)+21], c9_4);
__m128d c9_5 = _mm_load_sd(&C[(i*56)+23]);
__m128d a9_5 = _mm_load_sd(&values[135]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_5 = _mm_add_sd(c9_5, _mm_mul_sd(a9_5, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_5 = _mm_add_sd(c9_5, _mm_mul_sd(a9_5, b9));
#endif
_mm_store_sd(&C[(i*56)+23], c9_5);
__m128d c9_6 = _mm_load_sd(&C[(i*56)+25]);
__m128d a9_6 = _mm_load_sd(&values[136]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_6 = _mm_add_sd(c9_6, _mm_mul_sd(a9_6, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_6 = _mm_add_sd(c9_6, _mm_mul_sd(a9_6, b9));
#endif
_mm_store_sd(&C[(i*56)+25], c9_6);
__m128d c9_7 = _mm_load_sd(&C[(i*56)+27]);
__m128d a9_7 = _mm_load_sd(&values[137]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_7 = _mm_add_sd(c9_7, _mm_mul_sd(a9_7, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_7 = _mm_add_sd(c9_7, _mm_mul_sd(a9_7, b9));
#endif
_mm_store_sd(&C[(i*56)+27], c9_7);
__m128d c9_8 = _mm_load_sd(&C[(i*56)+30]);
__m128d a9_8 = _mm_load_sd(&values[138]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, b9));
#endif
_mm_store_sd(&C[(i*56)+30], c9_8);
__m128d c9_9 = _mm_load_sd(&C[(i*56)+32]);
__m128d a9_9 = _mm_load_sd(&values[139]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_9 = _mm_add_sd(c9_9, _mm_mul_sd(a9_9, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_9 = _mm_add_sd(c9_9, _mm_mul_sd(a9_9, b9));
#endif
_mm_store_sd(&C[(i*56)+32], c9_9);
__m128d c9_10 = _mm_load_sd(&C[(i*56)+35]);
__m128d a9_10 = _mm_load_sd(&values[140]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_10 = _mm_add_sd(c9_10, _mm_mul_sd(a9_10, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_10 = _mm_add_sd(c9_10, _mm_mul_sd(a9_10, b9));
#endif
_mm_store_sd(&C[(i*56)+35], c9_10);
__m128d c9_11 = _mm_load_sd(&C[(i*56)+37]);
__m128d a9_11 = _mm_load_sd(&values[141]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_11 = _mm_add_sd(c9_11, _mm_mul_sd(a9_11, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_11 = _mm_add_sd(c9_11, _mm_mul_sd(a9_11, b9));
#endif
_mm_store_sd(&C[(i*56)+37], c9_11);
__m128d c9_12 = _mm_load_sd(&C[(i*56)+39]);
__m128d a9_12 = _mm_load_sd(&values[142]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_12 = _mm_add_sd(c9_12, _mm_mul_sd(a9_12, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_12 = _mm_add_sd(c9_12, _mm_mul_sd(a9_12, b9));
#endif
_mm_store_sd(&C[(i*56)+39], c9_12);
__m128d c9_13 = _mm_load_sd(&C[(i*56)+42]);
__m128d a9_13 = _mm_load_sd(&values[143]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_13 = _mm_add_sd(c9_13, _mm_mul_sd(a9_13, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_13 = _mm_add_sd(c9_13, _mm_mul_sd(a9_13, b9));
#endif
_mm_store_sd(&C[(i*56)+42], c9_13);
__m128d c9_14 = _mm_load_sd(&C[(i*56)+44]);
__m128d a9_14 = _mm_load_sd(&values[144]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_14 = _mm_add_sd(c9_14, _mm_mul_sd(a9_14, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_14 = _mm_add_sd(c9_14, _mm_mul_sd(a9_14, b9));
#endif
_mm_store_sd(&C[(i*56)+44], c9_14);
__m128d c9_15 = _mm_load_sd(&C[(i*56)+46]);
__m128d a9_15 = _mm_load_sd(&values[145]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_15 = _mm_add_sd(c9_15, _mm_mul_sd(a9_15, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_15 = _mm_add_sd(c9_15, _mm_mul_sd(a9_15, b9));
#endif
_mm_store_sd(&C[(i*56)+46], c9_15);
__m128d c9_16 = _mm_load_sd(&C[(i*56)+48]);
__m128d a9_16 = _mm_load_sd(&values[146]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_16 = _mm_add_sd(c9_16, _mm_mul_sd(a9_16, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_16 = _mm_add_sd(c9_16, _mm_mul_sd(a9_16, b9));
#endif
_mm_store_sd(&C[(i*56)+48], c9_16);
__m128d c9_17 = _mm_load_sd(&C[(i*56)+51]);
__m128d a9_17 = _mm_load_sd(&values[147]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_17 = _mm_add_sd(c9_17, _mm_mul_sd(a9_17, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_17 = _mm_add_sd(c9_17, _mm_mul_sd(a9_17, b9));
#endif
_mm_store_sd(&C[(i*56)+51], c9_17);
__m128d c9_18 = _mm_load_sd(&C[(i*56)+53]);
__m128d a9_18 = _mm_load_sd(&values[148]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_18 = _mm_add_sd(c9_18, _mm_mul_sd(a9_18, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_18 = _mm_add_sd(c9_18, _mm_mul_sd(a9_18, b9));
#endif
_mm_store_sd(&C[(i*56)+53], c9_18);
#else
C[(i*56)+10] += values[130] * B[(i*56)+9];
C[(i*56)+12] += values[131] * B[(i*56)+9];
C[(i*56)+15] += values[132] * B[(i*56)+9];
C[(i*56)+17] += values[133] * B[(i*56)+9];
C[(i*56)+21] += values[134] * B[(i*56)+9];
C[(i*56)+23] += values[135] * B[(i*56)+9];
C[(i*56)+25] += values[136] * B[(i*56)+9];
C[(i*56)+27] += values[137] * B[(i*56)+9];
C[(i*56)+30] += values[138] * B[(i*56)+9];
C[(i*56)+32] += values[139] * B[(i*56)+9];
C[(i*56)+35] += values[140] * B[(i*56)+9];
C[(i*56)+37] += values[141] * B[(i*56)+9];
C[(i*56)+39] += values[142] * B[(i*56)+9];
C[(i*56)+42] += values[143] * B[(i*56)+9];
C[(i*56)+44] += values[144] * B[(i*56)+9];
C[(i*56)+46] += values[145] * B[(i*56)+9];
C[(i*56)+48] += values[146] * B[(i*56)+9];
C[(i*56)+51] += values[147] * B[(i*56)+9];
C[(i*56)+53] += values[148] * B[(i*56)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*56)+10]);
#endif
__m128d c10_0 = _mm_load_sd(&C[(i*56)+20]);
__m128d a10_0 = _mm_load_sd(&values[149]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, b10));
#endif
_mm_store_sd(&C[(i*56)+20], c10_0);
__m128d c10_1 = _mm_load_sd(&C[(i*56)+36]);
__m128d a10_1 = _mm_load_sd(&values[150]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_1 = _mm_add_sd(c10_1, _mm_mul_sd(a10_1, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_1 = _mm_add_sd(c10_1, _mm_mul_sd(a10_1, b10));
#endif
_mm_store_sd(&C[(i*56)+36], c10_1);
__m128d c10_2 = _mm_load_sd(&C[(i*56)+41]);
__m128d a10_2 = _mm_load_sd(&values[151]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_2 = _mm_add_sd(c10_2, _mm_mul_sd(a10_2, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_2 = _mm_add_sd(c10_2, _mm_mul_sd(a10_2, b10));
#endif
_mm_store_sd(&C[(i*56)+41], c10_2);
#else
C[(i*56)+20] += values[149] * B[(i*56)+10];
C[(i*56)+36] += values[150] * B[(i*56)+10];
C[(i*56)+41] += values[151] * B[(i*56)+10];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*56)+11]);
#endif
__m128d c11_0 = _mm_load_sd(&C[(i*56)+21]);
__m128d a11_0 = _mm_load_sd(&values[152]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, b11));
#endif
_mm_store_sd(&C[(i*56)+21], c11_0);
__m128d c11_1 = _mm_load_sd(&C[(i*56)+35]);
__m128d a11_1 = _mm_load_sd(&values[153]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, b11));
#endif
_mm_store_sd(&C[(i*56)+35], c11_1);
__m128d c11_2 = _mm_load_sd(&C[(i*56)+37]);
__m128d a11_2 = _mm_load_sd(&values[154]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*56)+37], c11_2);
__m128d c11_3 = _mm_load_sd(&C[(i*56)+42]);
__m128d a11_3 = _mm_load_sd(&values[155]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_3 = _mm_add_sd(c11_3, _mm_mul_sd(a11_3, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_3 = _mm_add_sd(c11_3, _mm_mul_sd(a11_3, b11));
#endif
_mm_store_sd(&C[(i*56)+42], c11_3);
#else
C[(i*56)+21] += values[152] * B[(i*56)+11];
C[(i*56)+35] += values[153] * B[(i*56)+11];
C[(i*56)+37] += values[154] * B[(i*56)+11];
C[(i*56)+42] += values[155] * B[(i*56)+11];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*56)+12]);
#endif
__m128d c12_0 = _mm_load_sd(&C[(i*56)+20]);
__m128d a12_0 = _mm_load_sd(&values[156]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, b12));
#endif
_mm_store_sd(&C[(i*56)+20], c12_0);
__m128d c12_1 = _mm_load_sd(&C[(i*56)+22]);
__m128d a12_1 = _mm_load_sd(&values[157]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_1 = _mm_add_sd(c12_1, _mm_mul_sd(a12_1, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_1 = _mm_add_sd(c12_1, _mm_mul_sd(a12_1, b12));
#endif
_mm_store_sd(&C[(i*56)+22], c12_1);
__m128d c12_2 = _mm_load_sd(&C[(i*56)+36]);
__m128d a12_2 = _mm_load_sd(&values[158]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_2 = _mm_add_sd(c12_2, _mm_mul_sd(a12_2, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_2 = _mm_add_sd(c12_2, _mm_mul_sd(a12_2, b12));
#endif
_mm_store_sd(&C[(i*56)+36], c12_2);
__m128d c12_3 = _mm_load_sd(&C[(i*56)+38]);
__m128d a12_3 = _mm_load_sd(&values[159]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, b12));
#endif
_mm_store_sd(&C[(i*56)+38], c12_3);
__m128d c12_4 = _mm_load_sd(&C[(i*56)+41]);
__m128d a12_4 = _mm_load_sd(&values[160]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_4 = _mm_add_sd(c12_4, _mm_mul_sd(a12_4, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_4 = _mm_add_sd(c12_4, _mm_mul_sd(a12_4, b12));
#endif
_mm_store_sd(&C[(i*56)+41], c12_4);
__m128d c12_5 = _mm_load_sd(&C[(i*56)+43]);
__m128d a12_5 = _mm_load_sd(&values[161]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_5 = _mm_add_sd(c12_5, _mm_mul_sd(a12_5, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_5 = _mm_add_sd(c12_5, _mm_mul_sd(a12_5, b12));
#endif
_mm_store_sd(&C[(i*56)+43], c12_5);
#else
C[(i*56)+20] += values[156] * B[(i*56)+12];
C[(i*56)+22] += values[157] * B[(i*56)+12];
C[(i*56)+36] += values[158] * B[(i*56)+12];
C[(i*56)+38] += values[159] * B[(i*56)+12];
C[(i*56)+41] += values[160] * B[(i*56)+12];
C[(i*56)+43] += values[161] * B[(i*56)+12];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*56)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*56)+21]);
__m128d a13_0 = _mm_load_sd(&values[162]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*56)+21], c13_0);
__m128d c13_1 = _mm_load_sd(&C[(i*56)+23]);
__m128d a13_1 = _mm_load_sd(&values[163]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_sd(c13_1, _mm_mul_sd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_sd(c13_1, _mm_mul_sd(a13_1, b13));
#endif
_mm_store_sd(&C[(i*56)+23], c13_1);
__m128d c13_2 = _mm_load_sd(&C[(i*56)+35]);
__m128d a13_2 = _mm_load_sd(&values[164]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_2 = _mm_add_sd(c13_2, _mm_mul_sd(a13_2, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_2 = _mm_add_sd(c13_2, _mm_mul_sd(a13_2, b13));
#endif
_mm_store_sd(&C[(i*56)+35], c13_2);
__m128d c13_3 = _mm_load_sd(&C[(i*56)+37]);
__m128d a13_3 = _mm_load_sd(&values[165]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, b13));
#endif
_mm_store_sd(&C[(i*56)+37], c13_3);
__m128d c13_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a13_4 = _mm_load_sd(&values[166]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, b13));
#endif
_mm_store_sd(&C[(i*56)+39], c13_4);
__m128d c13_5 = _mm_load_sd(&C[(i*56)+42]);
__m128d a13_5 = _mm_load_sd(&values[167]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_5 = _mm_add_sd(c13_5, _mm_mul_sd(a13_5, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_5 = _mm_add_sd(c13_5, _mm_mul_sd(a13_5, b13));
#endif
_mm_store_sd(&C[(i*56)+42], c13_5);
__m128d c13_6 = _mm_load_sd(&C[(i*56)+44]);
__m128d a13_6 = _mm_load_sd(&values[168]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_6 = _mm_add_sd(c13_6, _mm_mul_sd(a13_6, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_6 = _mm_add_sd(c13_6, _mm_mul_sd(a13_6, b13));
#endif
_mm_store_sd(&C[(i*56)+44], c13_6);
#else
C[(i*56)+21] += values[162] * B[(i*56)+13];
C[(i*56)+23] += values[163] * B[(i*56)+13];
C[(i*56)+35] += values[164] * B[(i*56)+13];
C[(i*56)+37] += values[165] * B[(i*56)+13];
C[(i*56)+39] += values[166] * B[(i*56)+13];
C[(i*56)+42] += values[167] * B[(i*56)+13];
C[(i*56)+44] += values[168] * B[(i*56)+13];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*56)+14]);
#endif
__m128d c14_0 = _mm_load_sd(&C[(i*56)+21]);
__m128d a14_0 = _mm_load_sd(&values[169]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, b14));
#endif
_mm_store_sd(&C[(i*56)+21], c14_0);
__m128d c14_1 = _mm_load_sd(&C[(i*56)+25]);
__m128d a14_1 = _mm_load_sd(&values[170]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, b14));
#endif
_mm_store_sd(&C[(i*56)+25], c14_1);
__m128d c14_2 = _mm_load_sd(&C[(i*56)+35]);
__m128d a14_2 = _mm_load_sd(&values[171]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, b14));
#endif
_mm_store_sd(&C[(i*56)+35], c14_2);
__m128d c14_3 = _mm_load_sd(&C[(i*56)+37]);
__m128d a14_3 = _mm_load_sd(&values[172]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_3 = _mm_add_sd(c14_3, _mm_mul_sd(a14_3, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_3 = _mm_add_sd(c14_3, _mm_mul_sd(a14_3, b14));
#endif
_mm_store_sd(&C[(i*56)+37], c14_3);
__m128d c14_4 = _mm_load_sd(&C[(i*56)+42]);
__m128d a14_4 = _mm_load_sd(&values[173]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, b14));
#endif
_mm_store_sd(&C[(i*56)+42], c14_4);
__m128d c14_5 = _mm_load_sd(&C[(i*56)+46]);
__m128d a14_5 = _mm_load_sd(&values[174]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_5 = _mm_add_sd(c14_5, _mm_mul_sd(a14_5, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_5 = _mm_add_sd(c14_5, _mm_mul_sd(a14_5, b14));
#endif
_mm_store_sd(&C[(i*56)+46], c14_5);
#else
C[(i*56)+21] += values[169] * B[(i*56)+14];
C[(i*56)+25] += values[170] * B[(i*56)+14];
C[(i*56)+35] += values[171] * B[(i*56)+14];
C[(i*56)+37] += values[172] * B[(i*56)+14];
C[(i*56)+42] += values[173] * B[(i*56)+14];
C[(i*56)+46] += values[174] * B[(i*56)+14];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*56)+15]);
#endif
__m128d c15_0 = _mm_load_sd(&C[(i*56)+20]);
__m128d a15_0 = _mm_load_sd(&values[175]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, b15));
#endif
_mm_store_sd(&C[(i*56)+20], c15_0);
__m128d c15_1 = _mm_load_sd(&C[(i*56)+22]);
__m128d a15_1 = _mm_load_sd(&values[176]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_1 = _mm_add_sd(c15_1, _mm_mul_sd(a15_1, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_1 = _mm_add_sd(c15_1, _mm_mul_sd(a15_1, b15));
#endif
_mm_store_sd(&C[(i*56)+22], c15_1);
__m128d c15_2 = _mm_load_sd(&C[(i*56)+26]);
__m128d a15_2 = _mm_load_sd(&values[177]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_2 = _mm_add_sd(c15_2, _mm_mul_sd(a15_2, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_2 = _mm_add_sd(c15_2, _mm_mul_sd(a15_2, b15));
#endif
_mm_store_sd(&C[(i*56)+26], c15_2);
__m128d c15_3 = _mm_load_sd(&C[(i*56)+36]);
__m128d a15_3 = _mm_load_sd(&values[178]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_3 = _mm_add_sd(c15_3, _mm_mul_sd(a15_3, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_3 = _mm_add_sd(c15_3, _mm_mul_sd(a15_3, b15));
#endif
_mm_store_sd(&C[(i*56)+36], c15_3);
__m128d c15_4 = _mm_load_sd(&C[(i*56)+38]);
__m128d a15_4 = _mm_load_sd(&values[179]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, b15));
#endif
_mm_store_sd(&C[(i*56)+38], c15_4);
__m128d c15_5 = _mm_load_sd(&C[(i*56)+41]);
__m128d a15_5 = _mm_load_sd(&values[180]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_5 = _mm_add_sd(c15_5, _mm_mul_sd(a15_5, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_5 = _mm_add_sd(c15_5, _mm_mul_sd(a15_5, b15));
#endif
_mm_store_sd(&C[(i*56)+41], c15_5);
__m128d c15_6 = _mm_load_sd(&C[(i*56)+43]);
__m128d a15_6 = _mm_load_sd(&values[181]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*56)+43], c15_6);
__m128d c15_7 = _mm_load_sd(&C[(i*56)+47]);
__m128d a15_7 = _mm_load_sd(&values[182]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, b15));
#endif
_mm_store_sd(&C[(i*56)+47], c15_7);
#else
C[(i*56)+20] += values[175] * B[(i*56)+15];
C[(i*56)+22] += values[176] * B[(i*56)+15];
C[(i*56)+26] += values[177] * B[(i*56)+15];
C[(i*56)+36] += values[178] * B[(i*56)+15];
C[(i*56)+38] += values[179] * B[(i*56)+15];
C[(i*56)+41] += values[180] * B[(i*56)+15];
C[(i*56)+43] += values[181] * B[(i*56)+15];
C[(i*56)+47] += values[182] * B[(i*56)+15];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*56)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*56)+21]);
__m128d a16_0 = _mm_load_sd(&values[183]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*56)+21], c16_0);
__m128d c16_1 = _mm_load_sd(&C[(i*56)+23]);
__m128d a16_1 = _mm_load_sd(&values[184]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_sd(c16_1, _mm_mul_sd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_sd(c16_1, _mm_mul_sd(a16_1, b16));
#endif
_mm_store_sd(&C[(i*56)+23], c16_1);
__m128d c16_2 = _mm_load_sd(&C[(i*56)+25]);
__m128d a16_2 = _mm_load_sd(&values[185]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_2 = _mm_add_sd(c16_2, _mm_mul_sd(a16_2, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_2 = _mm_add_sd(c16_2, _mm_mul_sd(a16_2, b16));
#endif
_mm_store_sd(&C[(i*56)+25], c16_2);
__m128d c16_3 = _mm_load_sd(&C[(i*56)+27]);
__m128d a16_3 = _mm_load_sd(&values[186]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, b16));
#endif
_mm_store_sd(&C[(i*56)+27], c16_3);
__m128d c16_4 = _mm_load_sd(&C[(i*56)+35]);
__m128d a16_4 = _mm_load_sd(&values[187]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_4 = _mm_add_sd(c16_4, _mm_mul_sd(a16_4, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_4 = _mm_add_sd(c16_4, _mm_mul_sd(a16_4, b16));
#endif
_mm_store_sd(&C[(i*56)+35], c16_4);
__m128d c16_5 = _mm_load_sd(&C[(i*56)+37]);
__m128d a16_5 = _mm_load_sd(&values[188]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_5 = _mm_add_sd(c16_5, _mm_mul_sd(a16_5, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_5 = _mm_add_sd(c16_5, _mm_mul_sd(a16_5, b16));
#endif
_mm_store_sd(&C[(i*56)+37], c16_5);
__m128d c16_6 = _mm_load_sd(&C[(i*56)+39]);
__m128d a16_6 = _mm_load_sd(&values[189]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_6 = _mm_add_sd(c16_6, _mm_mul_sd(a16_6, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_6 = _mm_add_sd(c16_6, _mm_mul_sd(a16_6, b16));
#endif
_mm_store_sd(&C[(i*56)+39], c16_6);
__m128d c16_7 = _mm_load_sd(&C[(i*56)+42]);
__m128d a16_7 = _mm_load_sd(&values[190]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_7 = _mm_add_sd(c16_7, _mm_mul_sd(a16_7, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_7 = _mm_add_sd(c16_7, _mm_mul_sd(a16_7, b16));
#endif
_mm_store_sd(&C[(i*56)+42], c16_7);
__m128d c16_8 = _mm_load_sd(&C[(i*56)+44]);
__m128d a16_8 = _mm_load_sd(&values[191]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, b16));
#endif
_mm_store_sd(&C[(i*56)+44], c16_8);
__m128d c16_9 = _mm_load_sd(&C[(i*56)+46]);
__m128d a16_9 = _mm_load_sd(&values[192]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_9 = _mm_add_sd(c16_9, _mm_mul_sd(a16_9, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_9 = _mm_add_sd(c16_9, _mm_mul_sd(a16_9, b16));
#endif
_mm_store_sd(&C[(i*56)+46], c16_9);
__m128d c16_10 = _mm_load_sd(&C[(i*56)+48]);
__m128d a16_10 = _mm_load_sd(&values[193]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_10 = _mm_add_sd(c16_10, _mm_mul_sd(a16_10, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_10 = _mm_add_sd(c16_10, _mm_mul_sd(a16_10, b16));
#endif
_mm_store_sd(&C[(i*56)+48], c16_10);
#else
C[(i*56)+21] += values[183] * B[(i*56)+16];
C[(i*56)+23] += values[184] * B[(i*56)+16];
C[(i*56)+25] += values[185] * B[(i*56)+16];
C[(i*56)+27] += values[186] * B[(i*56)+16];
C[(i*56)+35] += values[187] * B[(i*56)+16];
C[(i*56)+37] += values[188] * B[(i*56)+16];
C[(i*56)+39] += values[189] * B[(i*56)+16];
C[(i*56)+42] += values[190] * B[(i*56)+16];
C[(i*56)+44] += values[191] * B[(i*56)+16];
C[(i*56)+46] += values[192] * B[(i*56)+16];
C[(i*56)+48] += values[193] * B[(i*56)+16];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*56)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*56)+20]);
__m128d a17_0 = _mm_load_sd(&values[194]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*56)+20], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*56)+22]);
__m128d a17_1 = _mm_load_sd(&values[195]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*56)+22], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*56)+26]);
__m128d a17_2 = _mm_load_sd(&values[196]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*56)+26], c17_2);
__m128d c17_3 = _mm_load_sd(&C[(i*56)+29]);
__m128d a17_3 = _mm_load_sd(&values[197]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, b17));
#endif
_mm_store_sd(&C[(i*56)+29], c17_3);
__m128d c17_4 = _mm_load_sd(&C[(i*56)+36]);
__m128d a17_4 = _mm_load_sd(&values[198]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, b17));
#endif
_mm_store_sd(&C[(i*56)+36], c17_4);
__m128d c17_5 = _mm_load_sd(&C[(i*56)+38]);
__m128d a17_5 = _mm_load_sd(&values[199]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_5 = _mm_add_sd(c17_5, _mm_mul_sd(a17_5, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_5 = _mm_add_sd(c17_5, _mm_mul_sd(a17_5, b17));
#endif
_mm_store_sd(&C[(i*56)+38], c17_5);
__m128d c17_6 = _mm_load_sd(&C[(i*56)+41]);
__m128d a17_6 = _mm_load_sd(&values[200]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, b17));
#endif
_mm_store_sd(&C[(i*56)+41], c17_6);
__m128d c17_7 = _mm_load_sd(&C[(i*56)+43]);
__m128d a17_7 = _mm_load_sd(&values[201]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_7 = _mm_add_sd(c17_7, _mm_mul_sd(a17_7, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_7 = _mm_add_sd(c17_7, _mm_mul_sd(a17_7, b17));
#endif
_mm_store_sd(&C[(i*56)+43], c17_7);
__m128d c17_8 = _mm_load_sd(&C[(i*56)+47]);
__m128d a17_8 = _mm_load_sd(&values[202]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_8 = _mm_add_sd(c17_8, _mm_mul_sd(a17_8, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_8 = _mm_add_sd(c17_8, _mm_mul_sd(a17_8, b17));
#endif
_mm_store_sd(&C[(i*56)+47], c17_8);
__m128d c17_9 = _mm_load_sd(&C[(i*56)+50]);
__m128d a17_9 = _mm_load_sd(&values[203]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_9 = _mm_add_sd(c17_9, _mm_mul_sd(a17_9, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_9 = _mm_add_sd(c17_9, _mm_mul_sd(a17_9, b17));
#endif
_mm_store_sd(&C[(i*56)+50], c17_9);
#else
C[(i*56)+20] += values[194] * B[(i*56)+17];
C[(i*56)+22] += values[195] * B[(i*56)+17];
C[(i*56)+26] += values[196] * B[(i*56)+17];
C[(i*56)+29] += values[197] * B[(i*56)+17];
C[(i*56)+36] += values[198] * B[(i*56)+17];
C[(i*56)+38] += values[199] * B[(i*56)+17];
C[(i*56)+41] += values[200] * B[(i*56)+17];
C[(i*56)+43] += values[201] * B[(i*56)+17];
C[(i*56)+47] += values[202] * B[(i*56)+17];
C[(i*56)+50] += values[203] * B[(i*56)+17];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*56)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*56)+21]);
__m128d a18_0 = _mm_load_sd(&values[204]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*56)+21], c18_0);
__m128d c18_1 = _mm_load_sd(&C[(i*56)+23]);
__m128d a18_1 = _mm_load_sd(&values[205]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, b18));
#endif
_mm_store_sd(&C[(i*56)+23], c18_1);
__m128d c18_2 = _mm_load_sd(&C[(i*56)+25]);
__m128d a18_2 = _mm_load_sd(&values[206]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, b18));
#endif
_mm_store_sd(&C[(i*56)+25], c18_2);
__m128d c18_3 = _mm_load_sd(&C[(i*56)+27]);
__m128d a18_3 = _mm_load_sd(&values[207]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_3 = _mm_add_sd(c18_3, _mm_mul_sd(a18_3, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_3 = _mm_add_sd(c18_3, _mm_mul_sd(a18_3, b18));
#endif
_mm_store_sd(&C[(i*56)+27], c18_3);
__m128d c18_4 = _mm_load_sd(&C[(i*56)+30]);
__m128d a18_4 = _mm_load_sd(&values[208]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_4 = _mm_add_sd(c18_4, _mm_mul_sd(a18_4, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_4 = _mm_add_sd(c18_4, _mm_mul_sd(a18_4, b18));
#endif
_mm_store_sd(&C[(i*56)+30], c18_4);
__m128d c18_5 = _mm_load_sd(&C[(i*56)+35]);
__m128d a18_5 = _mm_load_sd(&values[209]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_5 = _mm_add_sd(c18_5, _mm_mul_sd(a18_5, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_5 = _mm_add_sd(c18_5, _mm_mul_sd(a18_5, b18));
#endif
_mm_store_sd(&C[(i*56)+35], c18_5);
__m128d c18_6 = _mm_load_sd(&C[(i*56)+37]);
__m128d a18_6 = _mm_load_sd(&values[210]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_6 = _mm_add_sd(c18_6, _mm_mul_sd(a18_6, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_6 = _mm_add_sd(c18_6, _mm_mul_sd(a18_6, b18));
#endif
_mm_store_sd(&C[(i*56)+37], c18_6);
__m128d c18_7 = _mm_load_sd(&C[(i*56)+39]);
__m128d a18_7 = _mm_load_sd(&values[211]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_7 = _mm_add_sd(c18_7, _mm_mul_sd(a18_7, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_7 = _mm_add_sd(c18_7, _mm_mul_sd(a18_7, b18));
#endif
_mm_store_sd(&C[(i*56)+39], c18_7);
__m128d c18_8 = _mm_load_sd(&C[(i*56)+42]);
__m128d a18_8 = _mm_load_sd(&values[212]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_8 = _mm_add_sd(c18_8, _mm_mul_sd(a18_8, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_8 = _mm_add_sd(c18_8, _mm_mul_sd(a18_8, b18));
#endif
_mm_store_sd(&C[(i*56)+42], c18_8);
__m128d c18_9 = _mm_load_sd(&C[(i*56)+44]);
__m128d a18_9 = _mm_load_sd(&values[213]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_9 = _mm_add_sd(c18_9, _mm_mul_sd(a18_9, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_9 = _mm_add_sd(c18_9, _mm_mul_sd(a18_9, b18));
#endif
_mm_store_sd(&C[(i*56)+44], c18_9);
__m128d c18_10 = _mm_load_sd(&C[(i*56)+46]);
__m128d a18_10 = _mm_load_sd(&values[214]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_10 = _mm_add_sd(c18_10, _mm_mul_sd(a18_10, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_10 = _mm_add_sd(c18_10, _mm_mul_sd(a18_10, b18));
#endif
_mm_store_sd(&C[(i*56)+46], c18_10);
__m128d c18_11 = _mm_load_sd(&C[(i*56)+48]);
__m128d a18_11 = _mm_load_sd(&values[215]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_11 = _mm_add_sd(c18_11, _mm_mul_sd(a18_11, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_11 = _mm_add_sd(c18_11, _mm_mul_sd(a18_11, b18));
#endif
_mm_store_sd(&C[(i*56)+48], c18_11);
__m128d c18_12 = _mm_load_sd(&C[(i*56)+51]);
__m128d a18_12 = _mm_load_sd(&values[216]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_12 = _mm_add_sd(c18_12, _mm_mul_sd(a18_12, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_12 = _mm_add_sd(c18_12, _mm_mul_sd(a18_12, b18));
#endif
_mm_store_sd(&C[(i*56)+51], c18_12);
#else
C[(i*56)+21] += values[204] * B[(i*56)+18];
C[(i*56)+23] += values[205] * B[(i*56)+18];
C[(i*56)+25] += values[206] * B[(i*56)+18];
C[(i*56)+27] += values[207] * B[(i*56)+18];
C[(i*56)+30] += values[208] * B[(i*56)+18];
C[(i*56)+35] += values[209] * B[(i*56)+18];
C[(i*56)+37] += values[210] * B[(i*56)+18];
C[(i*56)+39] += values[211] * B[(i*56)+18];
C[(i*56)+42] += values[212] * B[(i*56)+18];
C[(i*56)+44] += values[213] * B[(i*56)+18];
C[(i*56)+46] += values[214] * B[(i*56)+18];
C[(i*56)+48] += values[215] * B[(i*56)+18];
C[(i*56)+51] += values[216] * B[(i*56)+18];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*56)+19]);
#endif
__m128d c19_0 = _mm_load_sd(&C[(i*56)+21]);
__m128d a19_0 = _mm_load_sd(&values[217]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, b19));
#endif
_mm_store_sd(&C[(i*56)+21], c19_0);
__m128d c19_1 = _mm_load_sd(&C[(i*56)+23]);
__m128d a19_1 = _mm_load_sd(&values[218]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, b19));
#endif
_mm_store_sd(&C[(i*56)+23], c19_1);
__m128d c19_2 = _mm_load_sd(&C[(i*56)+25]);
__m128d a19_2 = _mm_load_sd(&values[219]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, b19));
#endif
_mm_store_sd(&C[(i*56)+25], c19_2);
__m128d c19_3 = _mm_load_sd(&C[(i*56)+27]);
__m128d a19_3 = _mm_load_sd(&values[220]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_3 = _mm_add_sd(c19_3, _mm_mul_sd(a19_3, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_3 = _mm_add_sd(c19_3, _mm_mul_sd(a19_3, b19));
#endif
_mm_store_sd(&C[(i*56)+27], c19_3);
__m128d c19_4 = _mm_load_sd(&C[(i*56)+30]);
__m128d a19_4 = _mm_load_sd(&values[221]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_4 = _mm_add_sd(c19_4, _mm_mul_sd(a19_4, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_4 = _mm_add_sd(c19_4, _mm_mul_sd(a19_4, b19));
#endif
_mm_store_sd(&C[(i*56)+30], c19_4);
__m128d c19_5 = _mm_load_sd(&C[(i*56)+32]);
__m128d a19_5 = _mm_load_sd(&values[222]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_5 = _mm_add_sd(c19_5, _mm_mul_sd(a19_5, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_5 = _mm_add_sd(c19_5, _mm_mul_sd(a19_5, b19));
#endif
_mm_store_sd(&C[(i*56)+32], c19_5);
__m128d c19_6 = _mm_load_sd(&C[(i*56)+35]);
__m128d a19_6 = _mm_load_sd(&values[223]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_6 = _mm_add_sd(c19_6, _mm_mul_sd(a19_6, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_6 = _mm_add_sd(c19_6, _mm_mul_sd(a19_6, b19));
#endif
_mm_store_sd(&C[(i*56)+35], c19_6);
__m128d c19_7 = _mm_load_sd(&C[(i*56)+37]);
__m128d a19_7 = _mm_load_sd(&values[224]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_7 = _mm_add_sd(c19_7, _mm_mul_sd(a19_7, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_7 = _mm_add_sd(c19_7, _mm_mul_sd(a19_7, b19));
#endif
_mm_store_sd(&C[(i*56)+37], c19_7);
__m128d c19_8 = _mm_load_sd(&C[(i*56)+39]);
__m128d a19_8 = _mm_load_sd(&values[225]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_8 = _mm_add_sd(c19_8, _mm_mul_sd(a19_8, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_8 = _mm_add_sd(c19_8, _mm_mul_sd(a19_8, b19));
#endif
_mm_store_sd(&C[(i*56)+39], c19_8);
__m128d c19_9 = _mm_load_sd(&C[(i*56)+42]);
__m128d a19_9 = _mm_load_sd(&values[226]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_9 = _mm_add_sd(c19_9, _mm_mul_sd(a19_9, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_9 = _mm_add_sd(c19_9, _mm_mul_sd(a19_9, b19));
#endif
_mm_store_sd(&C[(i*56)+42], c19_9);
__m128d c19_10 = _mm_load_sd(&C[(i*56)+44]);
__m128d a19_10 = _mm_load_sd(&values[227]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_10 = _mm_add_sd(c19_10, _mm_mul_sd(a19_10, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_10 = _mm_add_sd(c19_10, _mm_mul_sd(a19_10, b19));
#endif
_mm_store_sd(&C[(i*56)+44], c19_10);
__m128d c19_11 = _mm_load_sd(&C[(i*56)+46]);
__m128d a19_11 = _mm_load_sd(&values[228]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_11 = _mm_add_sd(c19_11, _mm_mul_sd(a19_11, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_11 = _mm_add_sd(c19_11, _mm_mul_sd(a19_11, b19));
#endif
_mm_store_sd(&C[(i*56)+46], c19_11);
__m128d c19_12 = _mm_load_sd(&C[(i*56)+48]);
__m128d a19_12 = _mm_load_sd(&values[229]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_12 = _mm_add_sd(c19_12, _mm_mul_sd(a19_12, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_12 = _mm_add_sd(c19_12, _mm_mul_sd(a19_12, b19));
#endif
_mm_store_sd(&C[(i*56)+48], c19_12);
__m128d c19_13 = _mm_load_sd(&C[(i*56)+51]);
__m128d a19_13 = _mm_load_sd(&values[230]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_13 = _mm_add_sd(c19_13, _mm_mul_sd(a19_13, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_13 = _mm_add_sd(c19_13, _mm_mul_sd(a19_13, b19));
#endif
_mm_store_sd(&C[(i*56)+51], c19_13);
__m128d c19_14 = _mm_load_sd(&C[(i*56)+53]);
__m128d a19_14 = _mm_load_sd(&values[231]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_14 = _mm_add_sd(c19_14, _mm_mul_sd(a19_14, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_14 = _mm_add_sd(c19_14, _mm_mul_sd(a19_14, b19));
#endif
_mm_store_sd(&C[(i*56)+53], c19_14);
#else
C[(i*56)+21] += values[217] * B[(i*56)+19];
C[(i*56)+23] += values[218] * B[(i*56)+19];
C[(i*56)+25] += values[219] * B[(i*56)+19];
C[(i*56)+27] += values[220] * B[(i*56)+19];
C[(i*56)+30] += values[221] * B[(i*56)+19];
C[(i*56)+32] += values[222] * B[(i*56)+19];
C[(i*56)+35] += values[223] * B[(i*56)+19];
C[(i*56)+37] += values[224] * B[(i*56)+19];
C[(i*56)+39] += values[225] * B[(i*56)+19];
C[(i*56)+42] += values[226] * B[(i*56)+19];
C[(i*56)+44] += values[227] * B[(i*56)+19];
C[(i*56)+46] += values[228] * B[(i*56)+19];
C[(i*56)+48] += values[229] * B[(i*56)+19];
C[(i*56)+51] += values[230] * B[(i*56)+19];
C[(i*56)+53] += values[231] * B[(i*56)+19];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*56)+20]);
#endif
__m128d c20_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a20_0 = _mm_load_sd(&values[232]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_0 = _mm_add_sd(c20_0, _mm_mul_sd(a20_0, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_0 = _mm_add_sd(c20_0, _mm_mul_sd(a20_0, b20));
#endif
_mm_store_sd(&C[(i*56)+35], c20_0);
#else
C[(i*56)+35] += values[232] * B[(i*56)+20];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*56)+21]);
#endif
__m128d c21_0 = _mm_load_sd(&C[(i*56)+36]);
__m128d a21_0 = _mm_load_sd(&values[233]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_0 = _mm_add_sd(c21_0, _mm_mul_sd(a21_0, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_0 = _mm_add_sd(c21_0, _mm_mul_sd(a21_0, b21));
#endif
_mm_store_sd(&C[(i*56)+36], c21_0);
#else
C[(i*56)+36] += values[233] * B[(i*56)+21];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*56)+22]);
#endif
__m128d c22_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a22_0 = _mm_load_sd(&values[234]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_0 = _mm_add_sd(c22_0, _mm_mul_sd(a22_0, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_0 = _mm_add_sd(c22_0, _mm_mul_sd(a22_0, b22));
#endif
_mm_store_sd(&C[(i*56)+35], c22_0);
__m128d c22_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a22_1 = _mm_load_sd(&values[235]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_1 = _mm_add_sd(c22_1, _mm_mul_sd(a22_1, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_1 = _mm_add_sd(c22_1, _mm_mul_sd(a22_1, b22));
#endif
_mm_store_sd(&C[(i*56)+37], c22_1);
#else
C[(i*56)+35] += values[234] * B[(i*56)+22];
C[(i*56)+37] += values[235] * B[(i*56)+22];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*56)+23]);
#endif
__m128d c23_0 = _mm_load_sd(&C[(i*56)+36]);
__m128d a23_0 = _mm_load_sd(&values[236]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_0 = _mm_add_sd(c23_0, _mm_mul_sd(a23_0, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_0 = _mm_add_sd(c23_0, _mm_mul_sd(a23_0, b23));
#endif
_mm_store_sd(&C[(i*56)+36], c23_0);
__m128d c23_1 = _mm_load_sd(&C[(i*56)+38]);
__m128d a23_1 = _mm_load_sd(&values[237]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_1 = _mm_add_sd(c23_1, _mm_mul_sd(a23_1, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_1 = _mm_add_sd(c23_1, _mm_mul_sd(a23_1, b23));
#endif
_mm_store_sd(&C[(i*56)+38], c23_1);
#else
C[(i*56)+36] += values[236] * B[(i*56)+23];
C[(i*56)+38] += values[237] * B[(i*56)+23];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b24 = _mm256_broadcast_sd(&B[(i*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b24 = _mm_loaddup_pd(&B[(i*56)+24]);
#endif
__m128d c24_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a24_0 = _mm_load_sd(&values[238]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, b24));
#endif
_mm_store_sd(&C[(i*56)+35], c24_0);
__m128d c24_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a24_1 = _mm_load_sd(&values[239]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_1 = _mm_add_sd(c24_1, _mm_mul_sd(a24_1, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_1 = _mm_add_sd(c24_1, _mm_mul_sd(a24_1, b24));
#endif
_mm_store_sd(&C[(i*56)+37], c24_1);
__m128d c24_2 = _mm_load_sd(&C[(i*56)+39]);
__m128d a24_2 = _mm_load_sd(&values[240]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_2 = _mm_add_sd(c24_2, _mm_mul_sd(a24_2, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_2 = _mm_add_sd(c24_2, _mm_mul_sd(a24_2, b24));
#endif
_mm_store_sd(&C[(i*56)+39], c24_2);
#else
C[(i*56)+35] += values[238] * B[(i*56)+24];
C[(i*56)+37] += values[239] * B[(i*56)+24];
C[(i*56)+39] += values[240] * B[(i*56)+24];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*56)+25]);
#endif
__m128d c25_0 = _mm_load_sd(&C[(i*56)+36]);
__m128d a25_0 = _mm_load_sd(&values[241]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_0 = _mm_add_sd(c25_0, _mm_mul_sd(a25_0, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_0 = _mm_add_sd(c25_0, _mm_mul_sd(a25_0, b25));
#endif
_mm_store_sd(&C[(i*56)+36], c25_0);
__m128d c25_1 = _mm_load_sd(&C[(i*56)+41]);
__m128d a25_1 = _mm_load_sd(&values[242]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_1 = _mm_add_sd(c25_1, _mm_mul_sd(a25_1, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_1 = _mm_add_sd(c25_1, _mm_mul_sd(a25_1, b25));
#endif
_mm_store_sd(&C[(i*56)+41], c25_1);
#else
C[(i*56)+36] += values[241] * B[(i*56)+25];
C[(i*56)+41] += values[242] * B[(i*56)+25];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*56)+26]);
#endif
__m128d c26_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a26_0 = _mm_load_sd(&values[243]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_0 = _mm_add_sd(c26_0, _mm_mul_sd(a26_0, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_0 = _mm_add_sd(c26_0, _mm_mul_sd(a26_0, b26));
#endif
_mm_store_sd(&C[(i*56)+35], c26_0);
__m128d c26_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a26_1 = _mm_load_sd(&values[244]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_1 = _mm_add_sd(c26_1, _mm_mul_sd(a26_1, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_1 = _mm_add_sd(c26_1, _mm_mul_sd(a26_1, b26));
#endif
_mm_store_sd(&C[(i*56)+37], c26_1);
__m128d c26_2 = _mm_load_sd(&C[(i*56)+42]);
__m128d a26_2 = _mm_load_sd(&values[245]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_2 = _mm_add_sd(c26_2, _mm_mul_sd(a26_2, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_2 = _mm_add_sd(c26_2, _mm_mul_sd(a26_2, b26));
#endif
_mm_store_sd(&C[(i*56)+42], c26_2);
#else
C[(i*56)+35] += values[243] * B[(i*56)+26];
C[(i*56)+37] += values[244] * B[(i*56)+26];
C[(i*56)+42] += values[245] * B[(i*56)+26];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*56)+27]);
#endif
__m128d c27_0 = _mm_load_sd(&C[(i*56)+36]);
__m128d a27_0 = _mm_load_sd(&values[246]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_0 = _mm_add_sd(c27_0, _mm_mul_sd(a27_0, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_0 = _mm_add_sd(c27_0, _mm_mul_sd(a27_0, b27));
#endif
_mm_store_sd(&C[(i*56)+36], c27_0);
__m128d c27_1 = _mm_load_sd(&C[(i*56)+38]);
__m128d a27_1 = _mm_load_sd(&values[247]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_1 = _mm_add_sd(c27_1, _mm_mul_sd(a27_1, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_1 = _mm_add_sd(c27_1, _mm_mul_sd(a27_1, b27));
#endif
_mm_store_sd(&C[(i*56)+38], c27_1);
__m128d c27_2 = _mm_load_sd(&C[(i*56)+41]);
__m128d a27_2 = _mm_load_sd(&values[248]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_2 = _mm_add_sd(c27_2, _mm_mul_sd(a27_2, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_2 = _mm_add_sd(c27_2, _mm_mul_sd(a27_2, b27));
#endif
_mm_store_sd(&C[(i*56)+41], c27_2);
__m128d c27_3 = _mm_load_sd(&C[(i*56)+43]);
__m128d a27_3 = _mm_load_sd(&values[249]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_3 = _mm_add_sd(c27_3, _mm_mul_sd(a27_3, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_3 = _mm_add_sd(c27_3, _mm_mul_sd(a27_3, b27));
#endif
_mm_store_sd(&C[(i*56)+43], c27_3);
#else
C[(i*56)+36] += values[246] * B[(i*56)+27];
C[(i*56)+38] += values[247] * B[(i*56)+27];
C[(i*56)+41] += values[248] * B[(i*56)+27];
C[(i*56)+43] += values[249] * B[(i*56)+27];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b28 = _mm256_broadcast_sd(&B[(i*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b28 = _mm_loaddup_pd(&B[(i*56)+28]);
#endif
__m128d c28_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a28_0 = _mm_load_sd(&values[250]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, b28));
#endif
_mm_store_sd(&C[(i*56)+35], c28_0);
__m128d c28_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a28_1 = _mm_load_sd(&values[251]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_1 = _mm_add_sd(c28_1, _mm_mul_sd(a28_1, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_1 = _mm_add_sd(c28_1, _mm_mul_sd(a28_1, b28));
#endif
_mm_store_sd(&C[(i*56)+37], c28_1);
__m128d c28_2 = _mm_load_sd(&C[(i*56)+39]);
__m128d a28_2 = _mm_load_sd(&values[252]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_2 = _mm_add_sd(c28_2, _mm_mul_sd(a28_2, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_2 = _mm_add_sd(c28_2, _mm_mul_sd(a28_2, b28));
#endif
_mm_store_sd(&C[(i*56)+39], c28_2);
__m128d c28_3 = _mm_load_sd(&C[(i*56)+42]);
__m128d a28_3 = _mm_load_sd(&values[253]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, b28));
#endif
_mm_store_sd(&C[(i*56)+42], c28_3);
__m128d c28_4 = _mm_load_sd(&C[(i*56)+44]);
__m128d a28_4 = _mm_load_sd(&values[254]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_4 = _mm_add_sd(c28_4, _mm_mul_sd(a28_4, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_4 = _mm_add_sd(c28_4, _mm_mul_sd(a28_4, b28));
#endif
_mm_store_sd(&C[(i*56)+44], c28_4);
#else
C[(i*56)+35] += values[250] * B[(i*56)+28];
C[(i*56)+37] += values[251] * B[(i*56)+28];
C[(i*56)+39] += values[252] * B[(i*56)+28];
C[(i*56)+42] += values[253] * B[(i*56)+28];
C[(i*56)+44] += values[254] * B[(i*56)+28];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*56)+29]);
#endif
__m128d c29_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a29_0 = _mm_load_sd(&values[255]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_0 = _mm_add_sd(c29_0, _mm_mul_sd(a29_0, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_0 = _mm_add_sd(c29_0, _mm_mul_sd(a29_0, b29));
#endif
_mm_store_sd(&C[(i*56)+35], c29_0);
__m128d c29_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a29_1 = _mm_load_sd(&values[256]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_1 = _mm_add_sd(c29_1, _mm_mul_sd(a29_1, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_1 = _mm_add_sd(c29_1, _mm_mul_sd(a29_1, b29));
#endif
_mm_store_sd(&C[(i*56)+37], c29_1);
__m128d c29_2 = _mm_load_sd(&C[(i*56)+42]);
__m128d a29_2 = _mm_load_sd(&values[257]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_2 = _mm_add_sd(c29_2, _mm_mul_sd(a29_2, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_2 = _mm_add_sd(c29_2, _mm_mul_sd(a29_2, b29));
#endif
_mm_store_sd(&C[(i*56)+42], c29_2);
__m128d c29_3 = _mm_load_sd(&C[(i*56)+46]);
__m128d a29_3 = _mm_load_sd(&values[258]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_3 = _mm_add_sd(c29_3, _mm_mul_sd(a29_3, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_3 = _mm_add_sd(c29_3, _mm_mul_sd(a29_3, b29));
#endif
_mm_store_sd(&C[(i*56)+46], c29_3);
#else
C[(i*56)+35] += values[255] * B[(i*56)+29];
C[(i*56)+37] += values[256] * B[(i*56)+29];
C[(i*56)+42] += values[257] * B[(i*56)+29];
C[(i*56)+46] += values[258] * B[(i*56)+29];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*56)+30]);
#endif
__m128d c30_0 = _mm_load_sd(&C[(i*56)+36]);
__m128d a30_0 = _mm_load_sd(&values[259]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_0 = _mm_add_sd(c30_0, _mm_mul_sd(a30_0, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_0 = _mm_add_sd(c30_0, _mm_mul_sd(a30_0, b30));
#endif
_mm_store_sd(&C[(i*56)+36], c30_0);
__m128d c30_1 = _mm_load_sd(&C[(i*56)+38]);
__m128d a30_1 = _mm_load_sd(&values[260]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_1 = _mm_add_sd(c30_1, _mm_mul_sd(a30_1, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_1 = _mm_add_sd(c30_1, _mm_mul_sd(a30_1, b30));
#endif
_mm_store_sd(&C[(i*56)+38], c30_1);
__m128d c30_2 = _mm_load_sd(&C[(i*56)+41]);
__m128d a30_2 = _mm_load_sd(&values[261]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_2 = _mm_add_sd(c30_2, _mm_mul_sd(a30_2, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_2 = _mm_add_sd(c30_2, _mm_mul_sd(a30_2, b30));
#endif
_mm_store_sd(&C[(i*56)+41], c30_2);
__m128d c30_3 = _mm_load_sd(&C[(i*56)+43]);
__m128d a30_3 = _mm_load_sd(&values[262]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_3 = _mm_add_sd(c30_3, _mm_mul_sd(a30_3, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_3 = _mm_add_sd(c30_3, _mm_mul_sd(a30_3, b30));
#endif
_mm_store_sd(&C[(i*56)+43], c30_3);
__m128d c30_4 = _mm_load_sd(&C[(i*56)+47]);
__m128d a30_4 = _mm_load_sd(&values[263]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, b30));
#endif
_mm_store_sd(&C[(i*56)+47], c30_4);
#else
C[(i*56)+36] += values[259] * B[(i*56)+30];
C[(i*56)+38] += values[260] * B[(i*56)+30];
C[(i*56)+41] += values[261] * B[(i*56)+30];
C[(i*56)+43] += values[262] * B[(i*56)+30];
C[(i*56)+47] += values[263] * B[(i*56)+30];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b31 = _mm256_broadcast_sd(&B[(i*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b31 = _mm_loaddup_pd(&B[(i*56)+31]);
#endif
__m128d c31_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a31_0 = _mm_load_sd(&values[264]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, b31));
#endif
_mm_store_sd(&C[(i*56)+35], c31_0);
__m128d c31_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a31_1 = _mm_load_sd(&values[265]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_1 = _mm_add_sd(c31_1, _mm_mul_sd(a31_1, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_1 = _mm_add_sd(c31_1, _mm_mul_sd(a31_1, b31));
#endif
_mm_store_sd(&C[(i*56)+37], c31_1);
__m128d c31_2 = _mm_load_sd(&C[(i*56)+39]);
__m128d a31_2 = _mm_load_sd(&values[266]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_2 = _mm_add_sd(c31_2, _mm_mul_sd(a31_2, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_2 = _mm_add_sd(c31_2, _mm_mul_sd(a31_2, b31));
#endif
_mm_store_sd(&C[(i*56)+39], c31_2);
__m128d c31_3 = _mm_load_sd(&C[(i*56)+42]);
__m128d a31_3 = _mm_load_sd(&values[267]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_3 = _mm_add_sd(c31_3, _mm_mul_sd(a31_3, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_3 = _mm_add_sd(c31_3, _mm_mul_sd(a31_3, b31));
#endif
_mm_store_sd(&C[(i*56)+42], c31_3);
__m128d c31_4 = _mm_load_sd(&C[(i*56)+44]);
__m128d a31_4 = _mm_load_sd(&values[268]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_4 = _mm_add_sd(c31_4, _mm_mul_sd(a31_4, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_4 = _mm_add_sd(c31_4, _mm_mul_sd(a31_4, b31));
#endif
_mm_store_sd(&C[(i*56)+44], c31_4);
__m128d c31_5 = _mm_load_sd(&C[(i*56)+46]);
__m128d a31_5 = _mm_load_sd(&values[269]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_5 = _mm_add_sd(c31_5, _mm_mul_sd(a31_5, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_5 = _mm_add_sd(c31_5, _mm_mul_sd(a31_5, b31));
#endif
_mm_store_sd(&C[(i*56)+46], c31_5);
__m128d c31_6 = _mm_load_sd(&C[(i*56)+48]);
__m128d a31_6 = _mm_load_sd(&values[270]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_6 = _mm_add_sd(c31_6, _mm_mul_sd(a31_6, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_6 = _mm_add_sd(c31_6, _mm_mul_sd(a31_6, b31));
#endif
_mm_store_sd(&C[(i*56)+48], c31_6);
#else
C[(i*56)+35] += values[264] * B[(i*56)+31];
C[(i*56)+37] += values[265] * B[(i*56)+31];
C[(i*56)+39] += values[266] * B[(i*56)+31];
C[(i*56)+42] += values[267] * B[(i*56)+31];
C[(i*56)+44] += values[268] * B[(i*56)+31];
C[(i*56)+46] += values[269] * B[(i*56)+31];
C[(i*56)+48] += values[270] * B[(i*56)+31];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*56)+32]);
#endif
__m128d c32_0 = _mm_load_sd(&C[(i*56)+36]);
__m128d a32_0 = _mm_load_sd(&values[271]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, b32));
#endif
_mm_store_sd(&C[(i*56)+36], c32_0);
__m128d c32_1 = _mm_load_sd(&C[(i*56)+38]);
__m128d a32_1 = _mm_load_sd(&values[272]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, b32));
#endif
_mm_store_sd(&C[(i*56)+38], c32_1);
__m128d c32_2 = _mm_load_sd(&C[(i*56)+41]);
__m128d a32_2 = _mm_load_sd(&values[273]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, b32));
#endif
_mm_store_sd(&C[(i*56)+41], c32_2);
__m128d c32_3 = _mm_load_sd(&C[(i*56)+43]);
__m128d a32_3 = _mm_load_sd(&values[274]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, b32));
#endif
_mm_store_sd(&C[(i*56)+43], c32_3);
__m128d c32_4 = _mm_load_sd(&C[(i*56)+47]);
__m128d a32_4 = _mm_load_sd(&values[275]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, b32));
#endif
_mm_store_sd(&C[(i*56)+47], c32_4);
__m128d c32_5 = _mm_load_sd(&C[(i*56)+50]);
__m128d a32_5 = _mm_load_sd(&values[276]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_5 = _mm_add_sd(c32_5, _mm_mul_sd(a32_5, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_5 = _mm_add_sd(c32_5, _mm_mul_sd(a32_5, b32));
#endif
_mm_store_sd(&C[(i*56)+50], c32_5);
#else
C[(i*56)+36] += values[271] * B[(i*56)+32];
C[(i*56)+38] += values[272] * B[(i*56)+32];
C[(i*56)+41] += values[273] * B[(i*56)+32];
C[(i*56)+43] += values[274] * B[(i*56)+32];
C[(i*56)+47] += values[275] * B[(i*56)+32];
C[(i*56)+50] += values[276] * B[(i*56)+32];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b33 = _mm256_broadcast_sd(&B[(i*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b33 = _mm_loaddup_pd(&B[(i*56)+33]);
#endif
__m128d c33_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a33_0 = _mm_load_sd(&values[277]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, b33));
#endif
_mm_store_sd(&C[(i*56)+35], c33_0);
__m128d c33_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a33_1 = _mm_load_sd(&values[278]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_1 = _mm_add_sd(c33_1, _mm_mul_sd(a33_1, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_1 = _mm_add_sd(c33_1, _mm_mul_sd(a33_1, b33));
#endif
_mm_store_sd(&C[(i*56)+37], c33_1);
__m128d c33_2 = _mm_load_sd(&C[(i*56)+39]);
__m128d a33_2 = _mm_load_sd(&values[279]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_2 = _mm_add_sd(c33_2, _mm_mul_sd(a33_2, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_2 = _mm_add_sd(c33_2, _mm_mul_sd(a33_2, b33));
#endif
_mm_store_sd(&C[(i*56)+39], c33_2);
__m128d c33_3 = _mm_load_sd(&C[(i*56)+42]);
__m128d a33_3 = _mm_load_sd(&values[280]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_3 = _mm_add_sd(c33_3, _mm_mul_sd(a33_3, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_3 = _mm_add_sd(c33_3, _mm_mul_sd(a33_3, b33));
#endif
_mm_store_sd(&C[(i*56)+42], c33_3);
__m128d c33_4 = _mm_load_sd(&C[(i*56)+44]);
__m128d a33_4 = _mm_load_sd(&values[281]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_4 = _mm_add_sd(c33_4, _mm_mul_sd(a33_4, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_4 = _mm_add_sd(c33_4, _mm_mul_sd(a33_4, b33));
#endif
_mm_store_sd(&C[(i*56)+44], c33_4);
__m128d c33_5 = _mm_load_sd(&C[(i*56)+46]);
__m128d a33_5 = _mm_load_sd(&values[282]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_5 = _mm_add_sd(c33_5, _mm_mul_sd(a33_5, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_5 = _mm_add_sd(c33_5, _mm_mul_sd(a33_5, b33));
#endif
_mm_store_sd(&C[(i*56)+46], c33_5);
__m128d c33_6 = _mm_load_sd(&C[(i*56)+48]);
__m128d a33_6 = _mm_load_sd(&values[283]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_6 = _mm_add_sd(c33_6, _mm_mul_sd(a33_6, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_6 = _mm_add_sd(c33_6, _mm_mul_sd(a33_6, b33));
#endif
_mm_store_sd(&C[(i*56)+48], c33_6);
__m128d c33_7 = _mm_load_sd(&C[(i*56)+51]);
__m128d a33_7 = _mm_load_sd(&values[284]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_7 = _mm_add_sd(c33_7, _mm_mul_sd(a33_7, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_7 = _mm_add_sd(c33_7, _mm_mul_sd(a33_7, b33));
#endif
_mm_store_sd(&C[(i*56)+51], c33_7);
#else
C[(i*56)+35] += values[277] * B[(i*56)+33];
C[(i*56)+37] += values[278] * B[(i*56)+33];
C[(i*56)+39] += values[279] * B[(i*56)+33];
C[(i*56)+42] += values[280] * B[(i*56)+33];
C[(i*56)+44] += values[281] * B[(i*56)+33];
C[(i*56)+46] += values[282] * B[(i*56)+33];
C[(i*56)+48] += values[283] * B[(i*56)+33];
C[(i*56)+51] += values[284] * B[(i*56)+33];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b34 = _mm256_broadcast_sd(&B[(i*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b34 = _mm_loaddup_pd(&B[(i*56)+34]);
#endif
__m128d c34_0 = _mm_load_sd(&C[(i*56)+35]);
__m128d a34_0 = _mm_load_sd(&values[285]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_0 = _mm_add_sd(c34_0, _mm_mul_sd(a34_0, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_0 = _mm_add_sd(c34_0, _mm_mul_sd(a34_0, b34));
#endif
_mm_store_sd(&C[(i*56)+35], c34_0);
__m128d c34_1 = _mm_load_sd(&C[(i*56)+37]);
__m128d a34_1 = _mm_load_sd(&values[286]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_1 = _mm_add_sd(c34_1, _mm_mul_sd(a34_1, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_1 = _mm_add_sd(c34_1, _mm_mul_sd(a34_1, b34));
#endif
_mm_store_sd(&C[(i*56)+37], c34_1);
__m128d c34_2 = _mm_load_sd(&C[(i*56)+39]);
__m128d a34_2 = _mm_load_sd(&values[287]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_2 = _mm_add_sd(c34_2, _mm_mul_sd(a34_2, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_2 = _mm_add_sd(c34_2, _mm_mul_sd(a34_2, b34));
#endif
_mm_store_sd(&C[(i*56)+39], c34_2);
__m128d c34_3 = _mm_load_sd(&C[(i*56)+42]);
__m128d a34_3 = _mm_load_sd(&values[288]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_3 = _mm_add_sd(c34_3, _mm_mul_sd(a34_3, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_3 = _mm_add_sd(c34_3, _mm_mul_sd(a34_3, b34));
#endif
_mm_store_sd(&C[(i*56)+42], c34_3);
__m128d c34_4 = _mm_load_sd(&C[(i*56)+44]);
__m128d a34_4 = _mm_load_sd(&values[289]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_4 = _mm_add_sd(c34_4, _mm_mul_sd(a34_4, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_4 = _mm_add_sd(c34_4, _mm_mul_sd(a34_4, b34));
#endif
_mm_store_sd(&C[(i*56)+44], c34_4);
__m128d c34_5 = _mm_load_sd(&C[(i*56)+46]);
__m128d a34_5 = _mm_load_sd(&values[290]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_5 = _mm_add_sd(c34_5, _mm_mul_sd(a34_5, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_5 = _mm_add_sd(c34_5, _mm_mul_sd(a34_5, b34));
#endif
_mm_store_sd(&C[(i*56)+46], c34_5);
__m128d c34_6 = _mm_load_sd(&C[(i*56)+48]);
__m128d a34_6 = _mm_load_sd(&values[291]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_6 = _mm_add_sd(c34_6, _mm_mul_sd(a34_6, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_6 = _mm_add_sd(c34_6, _mm_mul_sd(a34_6, b34));
#endif
_mm_store_sd(&C[(i*56)+48], c34_6);
__m128d c34_7 = _mm_load_sd(&C[(i*56)+51]);
__m128d a34_7 = _mm_load_sd(&values[292]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_7 = _mm_add_sd(c34_7, _mm_mul_sd(a34_7, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_7 = _mm_add_sd(c34_7, _mm_mul_sd(a34_7, b34));
#endif
_mm_store_sd(&C[(i*56)+51], c34_7);
__m128d c34_8 = _mm_load_sd(&C[(i*56)+53]);
__m128d a34_8 = _mm_load_sd(&values[293]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_8 = _mm_add_sd(c34_8, _mm_mul_sd(a34_8, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_8 = _mm_add_sd(c34_8, _mm_mul_sd(a34_8, b34));
#endif
_mm_store_sd(&C[(i*56)+53], c34_8);
#else
C[(i*56)+35] += values[285] * B[(i*56)+34];
C[(i*56)+37] += values[286] * B[(i*56)+34];
C[(i*56)+39] += values[287] * B[(i*56)+34];
C[(i*56)+42] += values[288] * B[(i*56)+34];
C[(i*56)+44] += values[289] * B[(i*56)+34];
C[(i*56)+46] += values[290] * B[(i*56)+34];
C[(i*56)+48] += values[291] * B[(i*56)+34];
C[(i*56)+51] += values[292] * B[(i*56)+34];
C[(i*56)+53] += values[293] * B[(i*56)+34];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 5292;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivM_9_56(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 56; m++) {
    C[(i*56)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*56)+0]);
#endif
__m128d c0_0 = _mm_loadu_pd(&C[(i*56)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
#endif
_mm_storeu_pd(&C[(i*56)+1], c0_0);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_2 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a0_2 = _mm256_loadu_pd(&values[2]);
c0_2 = _mm256_add_pd(c0_2, _mm256_mul_pd(a0_2, b0));
_mm256_storeu_pd(&C[(i*56)+4], c0_2);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_2 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*56)+4], c0_2);
__m128d c0_4 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*56)+6], c0_4);
#endif
__m128d c0_6 = _mm_load_sd(&C[(i*56)+8]);
__m128d a0_6 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_6 = _mm_add_sd(c0_6, _mm_mul_sd(a0_6, b0));
#endif
_mm_store_sd(&C[(i*56)+8], c0_6);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_7 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a0_7 = _mm256_loadu_pd(&values[7]);
c0_7 = _mm256_add_pd(c0_7, _mm256_mul_pd(a0_7, b0));
_mm256_storeu_pd(&C[(i*56)+10], c0_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_7 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a0_7 = _mm_loadu_pd(&values[7]);
c0_7 = _mm_add_pd(c0_7, _mm_mul_pd(a0_7, b0));
_mm_storeu_pd(&C[(i*56)+10], c0_7);
__m128d c0_9 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a0_9 = _mm_loadu_pd(&values[9]);
c0_9 = _mm_add_pd(c0_9, _mm_mul_pd(a0_9, b0));
_mm_storeu_pd(&C[(i*56)+12], c0_9);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_11 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a0_11 = _mm256_loadu_pd(&values[11]);
c0_11 = _mm256_add_pd(c0_11, _mm256_mul_pd(a0_11, b0));
_mm256_storeu_pd(&C[(i*56)+14], c0_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_11 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a0_11 = _mm_loadu_pd(&values[11]);
c0_11 = _mm_add_pd(c0_11, _mm_mul_pd(a0_11, b0));
_mm_storeu_pd(&C[(i*56)+14], c0_11);
__m128d c0_13 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a0_13 = _mm_loadu_pd(&values[13]);
c0_13 = _mm_add_pd(c0_13, _mm_mul_pd(a0_13, b0));
_mm_storeu_pd(&C[(i*56)+16], c0_13);
#endif
__m128d c0_15 = _mm_load_sd(&C[(i*56)+18]);
__m128d a0_15 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_15 = _mm_add_sd(c0_15, _mm_mul_sd(a0_15, b0));
#endif
_mm_store_sd(&C[(i*56)+18], c0_15);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_16 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a0_16 = _mm256_loadu_pd(&values[16]);
c0_16 = _mm256_add_pd(c0_16, _mm256_mul_pd(a0_16, b0));
_mm256_storeu_pd(&C[(i*56)+20], c0_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_16 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a0_16 = _mm_loadu_pd(&values[16]);
c0_16 = _mm_add_pd(c0_16, _mm_mul_pd(a0_16, b0));
_mm_storeu_pd(&C[(i*56)+20], c0_16);
__m128d c0_18 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a0_18 = _mm_loadu_pd(&values[18]);
c0_18 = _mm_add_pd(c0_18, _mm_mul_pd(a0_18, b0));
_mm_storeu_pd(&C[(i*56)+22], c0_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_20 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a0_20 = _mm256_loadu_pd(&values[20]);
c0_20 = _mm256_add_pd(c0_20, _mm256_mul_pd(a0_20, b0));
_mm256_storeu_pd(&C[(i*56)+24], c0_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_20 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a0_20 = _mm_loadu_pd(&values[20]);
c0_20 = _mm_add_pd(c0_20, _mm_mul_pd(a0_20, b0));
_mm_storeu_pd(&C[(i*56)+24], c0_20);
__m128d c0_22 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a0_22 = _mm_loadu_pd(&values[22]);
c0_22 = _mm_add_pd(c0_22, _mm_mul_pd(a0_22, b0));
_mm_storeu_pd(&C[(i*56)+26], c0_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_24 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a0_24 = _mm256_loadu_pd(&values[24]);
c0_24 = _mm256_add_pd(c0_24, _mm256_mul_pd(a0_24, b0));
_mm256_storeu_pd(&C[(i*56)+28], c0_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_24 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a0_24 = _mm_loadu_pd(&values[24]);
c0_24 = _mm_add_pd(c0_24, _mm_mul_pd(a0_24, b0));
_mm_storeu_pd(&C[(i*56)+28], c0_24);
__m128d c0_26 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a0_26 = _mm_loadu_pd(&values[26]);
c0_26 = _mm_add_pd(c0_26, _mm_mul_pd(a0_26, b0));
_mm_storeu_pd(&C[(i*56)+30], c0_26);
#endif
__m128d c0_28 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a0_28 = _mm_loadu_pd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_28 = _mm_add_pd(c0_28, _mm_mul_pd(a0_28, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_28 = _mm_add_pd(c0_28, _mm_mul_pd(a0_28, b0));
#endif
_mm_storeu_pd(&C[(i*56)+32], c0_28);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_30 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a0_30 = _mm256_loadu_pd(&values[30]);
c0_30 = _mm256_add_pd(c0_30, _mm256_mul_pd(a0_30, b0));
_mm256_storeu_pd(&C[(i*56)+35], c0_30);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_30 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a0_30 = _mm_loadu_pd(&values[30]);
c0_30 = _mm_add_pd(c0_30, _mm_mul_pd(a0_30, b0));
_mm_storeu_pd(&C[(i*56)+35], c0_30);
__m128d c0_32 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a0_32 = _mm_loadu_pd(&values[32]);
c0_32 = _mm_add_pd(c0_32, _mm_mul_pd(a0_32, b0));
_mm_storeu_pd(&C[(i*56)+37], c0_32);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_34 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a0_34 = _mm256_loadu_pd(&values[34]);
c0_34 = _mm256_add_pd(c0_34, _mm256_mul_pd(a0_34, b0));
_mm256_storeu_pd(&C[(i*56)+39], c0_34);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_34 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a0_34 = _mm_loadu_pd(&values[34]);
c0_34 = _mm_add_pd(c0_34, _mm_mul_pd(a0_34, b0));
_mm_storeu_pd(&C[(i*56)+39], c0_34);
__m128d c0_36 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a0_36 = _mm_loadu_pd(&values[36]);
c0_36 = _mm_add_pd(c0_36, _mm_mul_pd(a0_36, b0));
_mm_storeu_pd(&C[(i*56)+41], c0_36);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_38 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a0_38 = _mm256_loadu_pd(&values[38]);
c0_38 = _mm256_add_pd(c0_38, _mm256_mul_pd(a0_38, b0));
_mm256_storeu_pd(&C[(i*56)+43], c0_38);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_38 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a0_38 = _mm_loadu_pd(&values[38]);
c0_38 = _mm_add_pd(c0_38, _mm_mul_pd(a0_38, b0));
_mm_storeu_pd(&C[(i*56)+43], c0_38);
__m128d c0_40 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a0_40 = _mm_loadu_pd(&values[40]);
c0_40 = _mm_add_pd(c0_40, _mm_mul_pd(a0_40, b0));
_mm_storeu_pd(&C[(i*56)+45], c0_40);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_42 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a0_42 = _mm256_loadu_pd(&values[42]);
c0_42 = _mm256_add_pd(c0_42, _mm256_mul_pd(a0_42, b0));
_mm256_storeu_pd(&C[(i*56)+47], c0_42);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_42 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a0_42 = _mm_loadu_pd(&values[42]);
c0_42 = _mm_add_pd(c0_42, _mm_mul_pd(a0_42, b0));
_mm_storeu_pd(&C[(i*56)+47], c0_42);
__m128d c0_44 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a0_44 = _mm_loadu_pd(&values[44]);
c0_44 = _mm_add_pd(c0_44, _mm_mul_pd(a0_44, b0));
_mm_storeu_pd(&C[(i*56)+49], c0_44);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_46 = _mm256_loadu_pd(&C[(i*56)+51]);
__m256d a0_46 = _mm256_loadu_pd(&values[46]);
c0_46 = _mm256_add_pd(c0_46, _mm256_mul_pd(a0_46, b0));
_mm256_storeu_pd(&C[(i*56)+51], c0_46);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_46 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a0_46 = _mm_loadu_pd(&values[46]);
c0_46 = _mm_add_pd(c0_46, _mm_mul_pd(a0_46, b0));
_mm_storeu_pd(&C[(i*56)+51], c0_46);
__m128d c0_48 = _mm_loadu_pd(&C[(i*56)+53]);
__m128d a0_48 = _mm_loadu_pd(&values[48]);
c0_48 = _mm_add_pd(c0_48, _mm_mul_pd(a0_48, b0));
_mm_storeu_pd(&C[(i*56)+53], c0_48);
#endif
#else
C[(i*56)+1] += values[0] * B[(i*56)+0];
C[(i*56)+2] += values[1] * B[(i*56)+0];
C[(i*56)+4] += values[2] * B[(i*56)+0];
C[(i*56)+5] += values[3] * B[(i*56)+0];
C[(i*56)+6] += values[4] * B[(i*56)+0];
C[(i*56)+7] += values[5] * B[(i*56)+0];
C[(i*56)+8] += values[6] * B[(i*56)+0];
C[(i*56)+10] += values[7] * B[(i*56)+0];
C[(i*56)+11] += values[8] * B[(i*56)+0];
C[(i*56)+12] += values[9] * B[(i*56)+0];
C[(i*56)+13] += values[10] * B[(i*56)+0];
C[(i*56)+14] += values[11] * B[(i*56)+0];
C[(i*56)+15] += values[12] * B[(i*56)+0];
C[(i*56)+16] += values[13] * B[(i*56)+0];
C[(i*56)+17] += values[14] * B[(i*56)+0];
C[(i*56)+18] += values[15] * B[(i*56)+0];
C[(i*56)+20] += values[16] * B[(i*56)+0];
C[(i*56)+21] += values[17] * B[(i*56)+0];
C[(i*56)+22] += values[18] * B[(i*56)+0];
C[(i*56)+23] += values[19] * B[(i*56)+0];
C[(i*56)+24] += values[20] * B[(i*56)+0];
C[(i*56)+25] += values[21] * B[(i*56)+0];
C[(i*56)+26] += values[22] * B[(i*56)+0];
C[(i*56)+27] += values[23] * B[(i*56)+0];
C[(i*56)+28] += values[24] * B[(i*56)+0];
C[(i*56)+29] += values[25] * B[(i*56)+0];
C[(i*56)+30] += values[26] * B[(i*56)+0];
C[(i*56)+31] += values[27] * B[(i*56)+0];
C[(i*56)+32] += values[28] * B[(i*56)+0];
C[(i*56)+33] += values[29] * B[(i*56)+0];
C[(i*56)+35] += values[30] * B[(i*56)+0];
C[(i*56)+36] += values[31] * B[(i*56)+0];
C[(i*56)+37] += values[32] * B[(i*56)+0];
C[(i*56)+38] += values[33] * B[(i*56)+0];
C[(i*56)+39] += values[34] * B[(i*56)+0];
C[(i*56)+40] += values[35] * B[(i*56)+0];
C[(i*56)+41] += values[36] * B[(i*56)+0];
C[(i*56)+42] += values[37] * B[(i*56)+0];
C[(i*56)+43] += values[38] * B[(i*56)+0];
C[(i*56)+44] += values[39] * B[(i*56)+0];
C[(i*56)+45] += values[40] * B[(i*56)+0];
C[(i*56)+46] += values[41] * B[(i*56)+0];
C[(i*56)+47] += values[42] * B[(i*56)+0];
C[(i*56)+48] += values[43] * B[(i*56)+0];
C[(i*56)+49] += values[44] * B[(i*56)+0];
C[(i*56)+50] += values[45] * B[(i*56)+0];
C[(i*56)+51] += values[46] * B[(i*56)+0];
C[(i*56)+52] += values[47] * B[(i*56)+0];
C[(i*56)+53] += values[48] * B[(i*56)+0];
C[(i*56)+54] += values[49] * B[(i*56)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*56)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*56)+4], c1_0);
__m128d c1_2 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a1_2 = _mm_loadu_pd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_pd(c1_2, _mm_mul_pd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_pd(c1_2, _mm_mul_pd(a1_2, b1));
#endif
_mm_storeu_pd(&C[(i*56)+10], c1_2);
__m128d c1_4 = _mm_load_sd(&C[(i*56)+12]);
__m128d a1_4 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_4 = _mm_add_sd(c1_4, _mm_mul_sd(a1_4, b1));
#endif
_mm_store_sd(&C[(i*56)+12], c1_4);
__m128d c1_5 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a1_5 = _mm_loadu_pd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_pd(c1_5, _mm_mul_pd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_pd(c1_5, _mm_mul_pd(a1_5, b1));
#endif
_mm_storeu_pd(&C[(i*56)+14], c1_5);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_7 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a1_7 = _mm256_loadu_pd(&values[57]);
c1_7 = _mm256_add_pd(c1_7, _mm256_mul_pd(a1_7, b1));
_mm256_storeu_pd(&C[(i*56)+20], c1_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_7 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a1_7 = _mm_loadu_pd(&values[57]);
c1_7 = _mm_add_pd(c1_7, _mm_mul_pd(a1_7, b1));
_mm_storeu_pd(&C[(i*56)+20], c1_7);
__m128d c1_9 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a1_9 = _mm_loadu_pd(&values[59]);
c1_9 = _mm_add_pd(c1_9, _mm_mul_pd(a1_9, b1));
_mm_storeu_pd(&C[(i*56)+22], c1_9);
#endif
__m128d c1_11 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a1_11 = _mm_loadu_pd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_11 = _mm_add_pd(c1_11, _mm_mul_pd(a1_11, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_11 = _mm_add_pd(c1_11, _mm_mul_pd(a1_11, b1));
#endif
_mm_storeu_pd(&C[(i*56)+25], c1_11);
__m128d c1_13 = _mm_load_sd(&C[(i*56)+27]);
__m128d a1_13 = _mm_load_sd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_13 = _mm_add_sd(c1_13, _mm_mul_sd(a1_13, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_13 = _mm_add_sd(c1_13, _mm_mul_sd(a1_13, b1));
#endif
_mm_store_sd(&C[(i*56)+27], c1_13);
__m128d c1_14 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a1_14 = _mm_loadu_pd(&values[64]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_14 = _mm_add_pd(c1_14, _mm_mul_pd(a1_14, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_14 = _mm_add_pd(c1_14, _mm_mul_pd(a1_14, b1));
#endif
_mm_storeu_pd(&C[(i*56)+29], c1_14);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_16 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a1_16 = _mm256_loadu_pd(&values[66]);
c1_16 = _mm256_add_pd(c1_16, _mm256_mul_pd(a1_16, b1));
_mm256_storeu_pd(&C[(i*56)+35], c1_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_16 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a1_16 = _mm_loadu_pd(&values[66]);
c1_16 = _mm_add_pd(c1_16, _mm_mul_pd(a1_16, b1));
_mm_storeu_pd(&C[(i*56)+35], c1_16);
__m128d c1_18 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a1_18 = _mm_loadu_pd(&values[68]);
c1_18 = _mm_add_pd(c1_18, _mm_mul_pd(a1_18, b1));
_mm_storeu_pd(&C[(i*56)+37], c1_18);
#endif
__m128d c1_20 = _mm_load_sd(&C[(i*56)+39]);
__m128d a1_20 = _mm_load_sd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_20 = _mm_add_sd(c1_20, _mm_mul_sd(a1_20, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_20 = _mm_add_sd(c1_20, _mm_mul_sd(a1_20, b1));
#endif
_mm_store_sd(&C[(i*56)+39], c1_20);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_21 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a1_21 = _mm256_loadu_pd(&values[71]);
c1_21 = _mm256_add_pd(c1_21, _mm256_mul_pd(a1_21, b1));
_mm256_storeu_pd(&C[(i*56)+41], c1_21);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_21 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a1_21 = _mm_loadu_pd(&values[71]);
c1_21 = _mm_add_pd(c1_21, _mm_mul_pd(a1_21, b1));
_mm_storeu_pd(&C[(i*56)+41], c1_21);
__m128d c1_23 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a1_23 = _mm_loadu_pd(&values[73]);
c1_23 = _mm_add_pd(c1_23, _mm_mul_pd(a1_23, b1));
_mm_storeu_pd(&C[(i*56)+43], c1_23);
#endif
__m128d c1_25 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a1_25 = _mm_loadu_pd(&values[75]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_25 = _mm_add_pd(c1_25, _mm_mul_pd(a1_25, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_25 = _mm_add_pd(c1_25, _mm_mul_pd(a1_25, b1));
#endif
_mm_storeu_pd(&C[(i*56)+46], c1_25);
__m128d c1_27 = _mm_load_sd(&C[(i*56)+48]);
__m128d a1_27 = _mm_load_sd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_27 = _mm_add_sd(c1_27, _mm_mul_sd(a1_27, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_27 = _mm_add_sd(c1_27, _mm_mul_sd(a1_27, b1));
#endif
_mm_store_sd(&C[(i*56)+48], c1_27);
__m128d c1_28 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a1_28 = _mm_loadu_pd(&values[78]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_28 = _mm_add_pd(c1_28, _mm_mul_pd(a1_28, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_28 = _mm_add_pd(c1_28, _mm_mul_pd(a1_28, b1));
#endif
_mm_storeu_pd(&C[(i*56)+50], c1_28);
#else
C[(i*56)+4] += values[50] * B[(i*56)+1];
C[(i*56)+5] += values[51] * B[(i*56)+1];
C[(i*56)+10] += values[52] * B[(i*56)+1];
C[(i*56)+11] += values[53] * B[(i*56)+1];
C[(i*56)+12] += values[54] * B[(i*56)+1];
C[(i*56)+14] += values[55] * B[(i*56)+1];
C[(i*56)+15] += values[56] * B[(i*56)+1];
C[(i*56)+20] += values[57] * B[(i*56)+1];
C[(i*56)+21] += values[58] * B[(i*56)+1];
C[(i*56)+22] += values[59] * B[(i*56)+1];
C[(i*56)+23] += values[60] * B[(i*56)+1];
C[(i*56)+25] += values[61] * B[(i*56)+1];
C[(i*56)+26] += values[62] * B[(i*56)+1];
C[(i*56)+27] += values[63] * B[(i*56)+1];
C[(i*56)+29] += values[64] * B[(i*56)+1];
C[(i*56)+30] += values[65] * B[(i*56)+1];
C[(i*56)+35] += values[66] * B[(i*56)+1];
C[(i*56)+36] += values[67] * B[(i*56)+1];
C[(i*56)+37] += values[68] * B[(i*56)+1];
C[(i*56)+38] += values[69] * B[(i*56)+1];
C[(i*56)+39] += values[70] * B[(i*56)+1];
C[(i*56)+41] += values[71] * B[(i*56)+1];
C[(i*56)+42] += values[72] * B[(i*56)+1];
C[(i*56)+43] += values[73] * B[(i*56)+1];
C[(i*56)+44] += values[74] * B[(i*56)+1];
C[(i*56)+46] += values[75] * B[(i*56)+1];
C[(i*56)+47] += values[76] * B[(i*56)+1];
C[(i*56)+48] += values[77] * B[(i*56)+1];
C[(i*56)+50] += values[78] * B[(i*56)+1];
C[(i*56)+51] += values[79] * B[(i*56)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*56)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[80]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*56)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*56)+6]);
__m128d a2_2 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*56)+6], c2_2);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_3 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a2_3 = _mm256_loadu_pd(&values[83]);
c2_3 = _mm256_add_pd(c2_3, _mm256_mul_pd(a2_3, b2));
_mm256_storeu_pd(&C[(i*56)+10], c2_3);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_3 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a2_3 = _mm_loadu_pd(&values[83]);
c2_3 = _mm_add_pd(c2_3, _mm_mul_pd(a2_3, b2));
_mm_storeu_pd(&C[(i*56)+10], c2_3);
__m128d c2_5 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a2_5 = _mm_loadu_pd(&values[85]);
c2_5 = _mm_add_pd(c2_5, _mm_mul_pd(a2_5, b2));
_mm_storeu_pd(&C[(i*56)+12], c2_5);
#endif
__m128d c2_7 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a2_7 = _mm_loadu_pd(&values[87]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_7 = _mm_add_pd(c2_7, _mm_mul_pd(a2_7, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_7 = _mm_add_pd(c2_7, _mm_mul_pd(a2_7, b2));
#endif
_mm_storeu_pd(&C[(i*56)+14], c2_7);
__m128d c2_9 = _mm_load_sd(&C[(i*56)+16]);
__m128d a2_9 = _mm_load_sd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_9 = _mm_add_sd(c2_9, _mm_mul_sd(a2_9, b2));
#endif
_mm_store_sd(&C[(i*56)+16], c2_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_10 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a2_10 = _mm256_loadu_pd(&values[90]);
c2_10 = _mm256_add_pd(c2_10, _mm256_mul_pd(a2_10, b2));
_mm256_storeu_pd(&C[(i*56)+20], c2_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_10 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a2_10 = _mm_loadu_pd(&values[90]);
c2_10 = _mm_add_pd(c2_10, _mm_mul_pd(a2_10, b2));
_mm_storeu_pd(&C[(i*56)+20], c2_10);
__m128d c2_12 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a2_12 = _mm_loadu_pd(&values[92]);
c2_12 = _mm_add_pd(c2_12, _mm_mul_pd(a2_12, b2));
_mm_storeu_pd(&C[(i*56)+22], c2_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_14 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a2_14 = _mm256_loadu_pd(&values[94]);
c2_14 = _mm256_add_pd(c2_14, _mm256_mul_pd(a2_14, b2));
_mm256_storeu_pd(&C[(i*56)+24], c2_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_14 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a2_14 = _mm_loadu_pd(&values[94]);
c2_14 = _mm_add_pd(c2_14, _mm_mul_pd(a2_14, b2));
_mm_storeu_pd(&C[(i*56)+24], c2_14);
__m128d c2_16 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a2_16 = _mm_loadu_pd(&values[96]);
c2_16 = _mm_add_pd(c2_16, _mm_mul_pd(a2_16, b2));
_mm_storeu_pd(&C[(i*56)+26], c2_16);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_18 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a2_18 = _mm256_loadu_pd(&values[98]);
c2_18 = _mm256_add_pd(c2_18, _mm256_mul_pd(a2_18, b2));
_mm256_storeu_pd(&C[(i*56)+28], c2_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_18 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a2_18 = _mm_loadu_pd(&values[98]);
c2_18 = _mm_add_pd(c2_18, _mm_mul_pd(a2_18, b2));
_mm_storeu_pd(&C[(i*56)+28], c2_18);
__m128d c2_20 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a2_20 = _mm_loadu_pd(&values[100]);
c2_20 = _mm_add_pd(c2_20, _mm_mul_pd(a2_20, b2));
_mm_storeu_pd(&C[(i*56)+30], c2_20);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_22 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a2_22 = _mm256_loadu_pd(&values[102]);
c2_22 = _mm256_add_pd(c2_22, _mm256_mul_pd(a2_22, b2));
_mm256_storeu_pd(&C[(i*56)+35], c2_22);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_22 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a2_22 = _mm_loadu_pd(&values[102]);
c2_22 = _mm_add_pd(c2_22, _mm_mul_pd(a2_22, b2));
_mm_storeu_pd(&C[(i*56)+35], c2_22);
__m128d c2_24 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a2_24 = _mm_loadu_pd(&values[104]);
c2_24 = _mm_add_pd(c2_24, _mm_mul_pd(a2_24, b2));
_mm_storeu_pd(&C[(i*56)+37], c2_24);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_26 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a2_26 = _mm256_loadu_pd(&values[106]);
c2_26 = _mm256_add_pd(c2_26, _mm256_mul_pd(a2_26, b2));
_mm256_storeu_pd(&C[(i*56)+39], c2_26);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_26 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a2_26 = _mm_loadu_pd(&values[106]);
c2_26 = _mm_add_pd(c2_26, _mm_mul_pd(a2_26, b2));
_mm_storeu_pd(&C[(i*56)+39], c2_26);
__m128d c2_28 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a2_28 = _mm_loadu_pd(&values[108]);
c2_28 = _mm_add_pd(c2_28, _mm_mul_pd(a2_28, b2));
_mm_storeu_pd(&C[(i*56)+41], c2_28);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_30 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a2_30 = _mm256_loadu_pd(&values[110]);
c2_30 = _mm256_add_pd(c2_30, _mm256_mul_pd(a2_30, b2));
_mm256_storeu_pd(&C[(i*56)+43], c2_30);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_30 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a2_30 = _mm_loadu_pd(&values[110]);
c2_30 = _mm_add_pd(c2_30, _mm_mul_pd(a2_30, b2));
_mm_storeu_pd(&C[(i*56)+43], c2_30);
__m128d c2_32 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a2_32 = _mm_loadu_pd(&values[112]);
c2_32 = _mm_add_pd(c2_32, _mm_mul_pd(a2_32, b2));
_mm_storeu_pd(&C[(i*56)+45], c2_32);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_34 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a2_34 = _mm256_loadu_pd(&values[114]);
c2_34 = _mm256_add_pd(c2_34, _mm256_mul_pd(a2_34, b2));
_mm256_storeu_pd(&C[(i*56)+47], c2_34);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_34 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a2_34 = _mm_loadu_pd(&values[114]);
c2_34 = _mm_add_pd(c2_34, _mm_mul_pd(a2_34, b2));
_mm_storeu_pd(&C[(i*56)+47], c2_34);
__m128d c2_36 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a2_36 = _mm_loadu_pd(&values[116]);
c2_36 = _mm_add_pd(c2_36, _mm_mul_pd(a2_36, b2));
_mm_storeu_pd(&C[(i*56)+49], c2_36);
#endif
__m128d c2_38 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a2_38 = _mm_loadu_pd(&values[118]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_38 = _mm_add_pd(c2_38, _mm_mul_pd(a2_38, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_38 = _mm_add_pd(c2_38, _mm_mul_pd(a2_38, b2));
#endif
_mm_storeu_pd(&C[(i*56)+51], c2_38);
#else
C[(i*56)+4] += values[80] * B[(i*56)+2];
C[(i*56)+5] += values[81] * B[(i*56)+2];
C[(i*56)+6] += values[82] * B[(i*56)+2];
C[(i*56)+10] += values[83] * B[(i*56)+2];
C[(i*56)+11] += values[84] * B[(i*56)+2];
C[(i*56)+12] += values[85] * B[(i*56)+2];
C[(i*56)+13] += values[86] * B[(i*56)+2];
C[(i*56)+14] += values[87] * B[(i*56)+2];
C[(i*56)+15] += values[88] * B[(i*56)+2];
C[(i*56)+16] += values[89] * B[(i*56)+2];
C[(i*56)+20] += values[90] * B[(i*56)+2];
C[(i*56)+21] += values[91] * B[(i*56)+2];
C[(i*56)+22] += values[92] * B[(i*56)+2];
C[(i*56)+23] += values[93] * B[(i*56)+2];
C[(i*56)+24] += values[94] * B[(i*56)+2];
C[(i*56)+25] += values[95] * B[(i*56)+2];
C[(i*56)+26] += values[96] * B[(i*56)+2];
C[(i*56)+27] += values[97] * B[(i*56)+2];
C[(i*56)+28] += values[98] * B[(i*56)+2];
C[(i*56)+29] += values[99] * B[(i*56)+2];
C[(i*56)+30] += values[100] * B[(i*56)+2];
C[(i*56)+31] += values[101] * B[(i*56)+2];
C[(i*56)+35] += values[102] * B[(i*56)+2];
C[(i*56)+36] += values[103] * B[(i*56)+2];
C[(i*56)+37] += values[104] * B[(i*56)+2];
C[(i*56)+38] += values[105] * B[(i*56)+2];
C[(i*56)+39] += values[106] * B[(i*56)+2];
C[(i*56)+40] += values[107] * B[(i*56)+2];
C[(i*56)+41] += values[108] * B[(i*56)+2];
C[(i*56)+42] += values[109] * B[(i*56)+2];
C[(i*56)+43] += values[110] * B[(i*56)+2];
C[(i*56)+44] += values[111] * B[(i*56)+2];
C[(i*56)+45] += values[112] * B[(i*56)+2];
C[(i*56)+46] += values[113] * B[(i*56)+2];
C[(i*56)+47] += values[114] * B[(i*56)+2];
C[(i*56)+48] += values[115] * B[(i*56)+2];
C[(i*56)+49] += values[116] * B[(i*56)+2];
C[(i*56)+50] += values[117] * B[(i*56)+2];
C[(i*56)+51] += values[118] * B[(i*56)+2];
C[(i*56)+52] += values[119] * B[(i*56)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*56)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[120]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*56)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[120]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*56)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[122]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*56)+6], c3_2);
#endif
__m128d c3_4 = _mm_load_sd(&C[(i*56)+8]);
__m128d a3_4 = _mm_load_sd(&values[124]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_4 = _mm_add_sd(c3_4, _mm_mul_sd(a3_4, b3));
#endif
_mm_store_sd(&C[(i*56)+8], c3_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_5 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a3_5 = _mm256_loadu_pd(&values[125]);
c3_5 = _mm256_add_pd(c3_5, _mm256_mul_pd(a3_5, b3));
_mm256_storeu_pd(&C[(i*56)+10], c3_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_5 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a3_5 = _mm_loadu_pd(&values[125]);
c3_5 = _mm_add_pd(c3_5, _mm_mul_pd(a3_5, b3));
_mm_storeu_pd(&C[(i*56)+10], c3_5);
__m128d c3_7 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a3_7 = _mm_loadu_pd(&values[127]);
c3_7 = _mm_add_pd(c3_7, _mm_mul_pd(a3_7, b3));
_mm_storeu_pd(&C[(i*56)+12], c3_7);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_9 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a3_9 = _mm256_loadu_pd(&values[129]);
c3_9 = _mm256_add_pd(c3_9, _mm256_mul_pd(a3_9, b3));
_mm256_storeu_pd(&C[(i*56)+14], c3_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_9 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a3_9 = _mm_loadu_pd(&values[129]);
c3_9 = _mm_add_pd(c3_9, _mm_mul_pd(a3_9, b3));
_mm_storeu_pd(&C[(i*56)+14], c3_9);
__m128d c3_11 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a3_11 = _mm_loadu_pd(&values[131]);
c3_11 = _mm_add_pd(c3_11, _mm_mul_pd(a3_11, b3));
_mm_storeu_pd(&C[(i*56)+16], c3_11);
#endif
__m128d c3_13 = _mm_load_sd(&C[(i*56)+18]);
__m128d a3_13 = _mm_load_sd(&values[133]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_13 = _mm_add_sd(c3_13, _mm_mul_sd(a3_13, b3));
#endif
_mm_store_sd(&C[(i*56)+18], c3_13);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_14 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a3_14 = _mm256_loadu_pd(&values[134]);
c3_14 = _mm256_add_pd(c3_14, _mm256_mul_pd(a3_14, b3));
_mm256_storeu_pd(&C[(i*56)+20], c3_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_14 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a3_14 = _mm_loadu_pd(&values[134]);
c3_14 = _mm_add_pd(c3_14, _mm_mul_pd(a3_14, b3));
_mm_storeu_pd(&C[(i*56)+20], c3_14);
__m128d c3_16 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a3_16 = _mm_loadu_pd(&values[136]);
c3_16 = _mm_add_pd(c3_16, _mm_mul_pd(a3_16, b3));
_mm_storeu_pd(&C[(i*56)+22], c3_16);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_18 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a3_18 = _mm256_loadu_pd(&values[138]);
c3_18 = _mm256_add_pd(c3_18, _mm256_mul_pd(a3_18, b3));
_mm256_storeu_pd(&C[(i*56)+24], c3_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_18 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a3_18 = _mm_loadu_pd(&values[138]);
c3_18 = _mm_add_pd(c3_18, _mm_mul_pd(a3_18, b3));
_mm_storeu_pd(&C[(i*56)+24], c3_18);
__m128d c3_20 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a3_20 = _mm_loadu_pd(&values[140]);
c3_20 = _mm_add_pd(c3_20, _mm_mul_pd(a3_20, b3));
_mm_storeu_pd(&C[(i*56)+26], c3_20);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_22 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a3_22 = _mm256_loadu_pd(&values[142]);
c3_22 = _mm256_add_pd(c3_22, _mm256_mul_pd(a3_22, b3));
_mm256_storeu_pd(&C[(i*56)+28], c3_22);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_22 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a3_22 = _mm_loadu_pd(&values[142]);
c3_22 = _mm_add_pd(c3_22, _mm_mul_pd(a3_22, b3));
_mm_storeu_pd(&C[(i*56)+28], c3_22);
__m128d c3_24 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a3_24 = _mm_loadu_pd(&values[144]);
c3_24 = _mm_add_pd(c3_24, _mm_mul_pd(a3_24, b3));
_mm_storeu_pd(&C[(i*56)+30], c3_24);
#endif
__m128d c3_26 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a3_26 = _mm_loadu_pd(&values[146]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_26 = _mm_add_pd(c3_26, _mm_mul_pd(a3_26, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_26 = _mm_add_pd(c3_26, _mm_mul_pd(a3_26, b3));
#endif
_mm_storeu_pd(&C[(i*56)+32], c3_26);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_28 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a3_28 = _mm256_loadu_pd(&values[148]);
c3_28 = _mm256_add_pd(c3_28, _mm256_mul_pd(a3_28, b3));
_mm256_storeu_pd(&C[(i*56)+35], c3_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_28 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a3_28 = _mm_loadu_pd(&values[148]);
c3_28 = _mm_add_pd(c3_28, _mm_mul_pd(a3_28, b3));
_mm_storeu_pd(&C[(i*56)+35], c3_28);
__m128d c3_30 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a3_30 = _mm_loadu_pd(&values[150]);
c3_30 = _mm_add_pd(c3_30, _mm_mul_pd(a3_30, b3));
_mm_storeu_pd(&C[(i*56)+37], c3_30);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_32 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a3_32 = _mm256_loadu_pd(&values[152]);
c3_32 = _mm256_add_pd(c3_32, _mm256_mul_pd(a3_32, b3));
_mm256_storeu_pd(&C[(i*56)+39], c3_32);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_32 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a3_32 = _mm_loadu_pd(&values[152]);
c3_32 = _mm_add_pd(c3_32, _mm_mul_pd(a3_32, b3));
_mm_storeu_pd(&C[(i*56)+39], c3_32);
__m128d c3_34 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a3_34 = _mm_loadu_pd(&values[154]);
c3_34 = _mm_add_pd(c3_34, _mm_mul_pd(a3_34, b3));
_mm_storeu_pd(&C[(i*56)+41], c3_34);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_36 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a3_36 = _mm256_loadu_pd(&values[156]);
c3_36 = _mm256_add_pd(c3_36, _mm256_mul_pd(a3_36, b3));
_mm256_storeu_pd(&C[(i*56)+43], c3_36);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_36 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a3_36 = _mm_loadu_pd(&values[156]);
c3_36 = _mm_add_pd(c3_36, _mm_mul_pd(a3_36, b3));
_mm_storeu_pd(&C[(i*56)+43], c3_36);
__m128d c3_38 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a3_38 = _mm_loadu_pd(&values[158]);
c3_38 = _mm_add_pd(c3_38, _mm_mul_pd(a3_38, b3));
_mm_storeu_pd(&C[(i*56)+45], c3_38);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_40 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a3_40 = _mm256_loadu_pd(&values[160]);
c3_40 = _mm256_add_pd(c3_40, _mm256_mul_pd(a3_40, b3));
_mm256_storeu_pd(&C[(i*56)+47], c3_40);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_40 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a3_40 = _mm_loadu_pd(&values[160]);
c3_40 = _mm_add_pd(c3_40, _mm_mul_pd(a3_40, b3));
_mm_storeu_pd(&C[(i*56)+47], c3_40);
__m128d c3_42 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a3_42 = _mm_loadu_pd(&values[162]);
c3_42 = _mm_add_pd(c3_42, _mm_mul_pd(a3_42, b3));
_mm_storeu_pd(&C[(i*56)+49], c3_42);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_44 = _mm256_loadu_pd(&C[(i*56)+51]);
__m256d a3_44 = _mm256_loadu_pd(&values[164]);
c3_44 = _mm256_add_pd(c3_44, _mm256_mul_pd(a3_44, b3));
_mm256_storeu_pd(&C[(i*56)+51], c3_44);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_44 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a3_44 = _mm_loadu_pd(&values[164]);
c3_44 = _mm_add_pd(c3_44, _mm_mul_pd(a3_44, b3));
_mm_storeu_pd(&C[(i*56)+51], c3_44);
__m128d c3_46 = _mm_loadu_pd(&C[(i*56)+53]);
__m128d a3_46 = _mm_loadu_pd(&values[166]);
c3_46 = _mm_add_pd(c3_46, _mm_mul_pd(a3_46, b3));
_mm_storeu_pd(&C[(i*56)+53], c3_46);
#endif
#else
C[(i*56)+4] += values[120] * B[(i*56)+3];
C[(i*56)+5] += values[121] * B[(i*56)+3];
C[(i*56)+6] += values[122] * B[(i*56)+3];
C[(i*56)+7] += values[123] * B[(i*56)+3];
C[(i*56)+8] += values[124] * B[(i*56)+3];
C[(i*56)+10] += values[125] * B[(i*56)+3];
C[(i*56)+11] += values[126] * B[(i*56)+3];
C[(i*56)+12] += values[127] * B[(i*56)+3];
C[(i*56)+13] += values[128] * B[(i*56)+3];
C[(i*56)+14] += values[129] * B[(i*56)+3];
C[(i*56)+15] += values[130] * B[(i*56)+3];
C[(i*56)+16] += values[131] * B[(i*56)+3];
C[(i*56)+17] += values[132] * B[(i*56)+3];
C[(i*56)+18] += values[133] * B[(i*56)+3];
C[(i*56)+20] += values[134] * B[(i*56)+3];
C[(i*56)+21] += values[135] * B[(i*56)+3];
C[(i*56)+22] += values[136] * B[(i*56)+3];
C[(i*56)+23] += values[137] * B[(i*56)+3];
C[(i*56)+24] += values[138] * B[(i*56)+3];
C[(i*56)+25] += values[139] * B[(i*56)+3];
C[(i*56)+26] += values[140] * B[(i*56)+3];
C[(i*56)+27] += values[141] * B[(i*56)+3];
C[(i*56)+28] += values[142] * B[(i*56)+3];
C[(i*56)+29] += values[143] * B[(i*56)+3];
C[(i*56)+30] += values[144] * B[(i*56)+3];
C[(i*56)+31] += values[145] * B[(i*56)+3];
C[(i*56)+32] += values[146] * B[(i*56)+3];
C[(i*56)+33] += values[147] * B[(i*56)+3];
C[(i*56)+35] += values[148] * B[(i*56)+3];
C[(i*56)+36] += values[149] * B[(i*56)+3];
C[(i*56)+37] += values[150] * B[(i*56)+3];
C[(i*56)+38] += values[151] * B[(i*56)+3];
C[(i*56)+39] += values[152] * B[(i*56)+3];
C[(i*56)+40] += values[153] * B[(i*56)+3];
C[(i*56)+41] += values[154] * B[(i*56)+3];
C[(i*56)+42] += values[155] * B[(i*56)+3];
C[(i*56)+43] += values[156] * B[(i*56)+3];
C[(i*56)+44] += values[157] * B[(i*56)+3];
C[(i*56)+45] += values[158] * B[(i*56)+3];
C[(i*56)+46] += values[159] * B[(i*56)+3];
C[(i*56)+47] += values[160] * B[(i*56)+3];
C[(i*56)+48] += values[161] * B[(i*56)+3];
C[(i*56)+49] += values[162] * B[(i*56)+3];
C[(i*56)+50] += values[163] * B[(i*56)+3];
C[(i*56)+51] += values[164] * B[(i*56)+3];
C[(i*56)+52] += values[165] * B[(i*56)+3];
C[(i*56)+53] += values[166] * B[(i*56)+3];
C[(i*56)+54] += values[167] * B[(i*56)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*56)+4]);
#endif
__m128d c4_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a4_0 = _mm_loadu_pd(&values[168]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
#endif
_mm_storeu_pd(&C[(i*56)+10], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a4_2 = _mm_loadu_pd(&values[170]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
#endif
_mm_storeu_pd(&C[(i*56)+20], c4_2);
__m128d c4_4 = _mm_load_sd(&C[(i*56)+22]);
__m128d a4_4 = _mm_load_sd(&values[172]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_4 = _mm_add_sd(c4_4, _mm_mul_sd(a4_4, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_4 = _mm_add_sd(c4_4, _mm_mul_sd(a4_4, b4));
#endif
_mm_store_sd(&C[(i*56)+22], c4_4);
__m128d c4_5 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a4_5 = _mm_loadu_pd(&values[173]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_5 = _mm_add_pd(c4_5, _mm_mul_pd(a4_5, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_5 = _mm_add_pd(c4_5, _mm_mul_pd(a4_5, b4));
#endif
_mm_storeu_pd(&C[(i*56)+25], c4_5);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_7 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a4_7 = _mm256_loadu_pd(&values[175]);
c4_7 = _mm256_add_pd(c4_7, _mm256_mul_pd(a4_7, b4));
_mm256_storeu_pd(&C[(i*56)+35], c4_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_7 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a4_7 = _mm_loadu_pd(&values[175]);
c4_7 = _mm_add_pd(c4_7, _mm_mul_pd(a4_7, b4));
_mm_storeu_pd(&C[(i*56)+35], c4_7);
__m128d c4_9 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a4_9 = _mm_loadu_pd(&values[177]);
c4_9 = _mm_add_pd(c4_9, _mm_mul_pd(a4_9, b4));
_mm_storeu_pd(&C[(i*56)+37], c4_9);
#endif
__m128d c4_11 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a4_11 = _mm_loadu_pd(&values[179]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_11 = _mm_add_pd(c4_11, _mm_mul_pd(a4_11, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_11 = _mm_add_pd(c4_11, _mm_mul_pd(a4_11, b4));
#endif
_mm_storeu_pd(&C[(i*56)+41], c4_11);
__m128d c4_13 = _mm_load_sd(&C[(i*56)+43]);
__m128d a4_13 = _mm_load_sd(&values[181]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_13 = _mm_add_sd(c4_13, _mm_mul_sd(a4_13, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_13 = _mm_add_sd(c4_13, _mm_mul_sd(a4_13, b4));
#endif
_mm_store_sd(&C[(i*56)+43], c4_13);
__m128d c4_14 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a4_14 = _mm_loadu_pd(&values[182]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_14 = _mm_add_pd(c4_14, _mm_mul_pd(a4_14, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_14 = _mm_add_pd(c4_14, _mm_mul_pd(a4_14, b4));
#endif
_mm_storeu_pd(&C[(i*56)+46], c4_14);
#else
C[(i*56)+10] += values[168] * B[(i*56)+4];
C[(i*56)+11] += values[169] * B[(i*56)+4];
C[(i*56)+20] += values[170] * B[(i*56)+4];
C[(i*56)+21] += values[171] * B[(i*56)+4];
C[(i*56)+22] += values[172] * B[(i*56)+4];
C[(i*56)+25] += values[173] * B[(i*56)+4];
C[(i*56)+26] += values[174] * B[(i*56)+4];
C[(i*56)+35] += values[175] * B[(i*56)+4];
C[(i*56)+36] += values[176] * B[(i*56)+4];
C[(i*56)+37] += values[177] * B[(i*56)+4];
C[(i*56)+38] += values[178] * B[(i*56)+4];
C[(i*56)+41] += values[179] * B[(i*56)+4];
C[(i*56)+42] += values[180] * B[(i*56)+4];
C[(i*56)+43] += values[181] * B[(i*56)+4];
C[(i*56)+46] += values[182] * B[(i*56)+4];
C[(i*56)+47] += values[183] * B[(i*56)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*56)+5]);
#endif
__m128d c5_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a5_0 = _mm_loadu_pd(&values[184]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
#endif
_mm_storeu_pd(&C[(i*56)+10], c5_0);
__m128d c5_2 = _mm_load_sd(&C[(i*56)+12]);
__m128d a5_2 = _mm_load_sd(&values[186]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*56)+12], c5_2);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_3 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a5_3 = _mm256_loadu_pd(&values[187]);
c5_3 = _mm256_add_pd(c5_3, _mm256_mul_pd(a5_3, b5));
_mm256_storeu_pd(&C[(i*56)+20], c5_3);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_3 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a5_3 = _mm_loadu_pd(&values[187]);
c5_3 = _mm_add_pd(c5_3, _mm_mul_pd(a5_3, b5));
_mm_storeu_pd(&C[(i*56)+20], c5_3);
__m128d c5_5 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a5_5 = _mm_loadu_pd(&values[189]);
c5_5 = _mm_add_pd(c5_5, _mm_mul_pd(a5_5, b5));
_mm_storeu_pd(&C[(i*56)+22], c5_5);
#endif
__m128d c5_7 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a5_7 = _mm_loadu_pd(&values[191]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_7 = _mm_add_pd(c5_7, _mm_mul_pd(a5_7, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_7 = _mm_add_pd(c5_7, _mm_mul_pd(a5_7, b5));
#endif
_mm_storeu_pd(&C[(i*56)+25], c5_7);
__m128d c5_9 = _mm_load_sd(&C[(i*56)+27]);
__m128d a5_9 = _mm_load_sd(&values[193]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_9 = _mm_add_sd(c5_9, _mm_mul_sd(a5_9, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_9 = _mm_add_sd(c5_9, _mm_mul_sd(a5_9, b5));
#endif
_mm_store_sd(&C[(i*56)+27], c5_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_10 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a5_10 = _mm256_loadu_pd(&values[194]);
c5_10 = _mm256_add_pd(c5_10, _mm256_mul_pd(a5_10, b5));
_mm256_storeu_pd(&C[(i*56)+35], c5_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_10 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a5_10 = _mm_loadu_pd(&values[194]);
c5_10 = _mm_add_pd(c5_10, _mm_mul_pd(a5_10, b5));
_mm_storeu_pd(&C[(i*56)+35], c5_10);
__m128d c5_12 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a5_12 = _mm_loadu_pd(&values[196]);
c5_12 = _mm_add_pd(c5_12, _mm_mul_pd(a5_12, b5));
_mm_storeu_pd(&C[(i*56)+37], c5_12);
#endif
__m128d c5_14 = _mm_load_sd(&C[(i*56)+39]);
__m128d a5_14 = _mm_load_sd(&values[198]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_14 = _mm_add_sd(c5_14, _mm_mul_sd(a5_14, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_14 = _mm_add_sd(c5_14, _mm_mul_sd(a5_14, b5));
#endif
_mm_store_sd(&C[(i*56)+39], c5_14);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_15 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a5_15 = _mm256_loadu_pd(&values[199]);
c5_15 = _mm256_add_pd(c5_15, _mm256_mul_pd(a5_15, b5));
_mm256_storeu_pd(&C[(i*56)+41], c5_15);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_15 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a5_15 = _mm_loadu_pd(&values[199]);
c5_15 = _mm_add_pd(c5_15, _mm_mul_pd(a5_15, b5));
_mm_storeu_pd(&C[(i*56)+41], c5_15);
__m128d c5_17 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a5_17 = _mm_loadu_pd(&values[201]);
c5_17 = _mm_add_pd(c5_17, _mm_mul_pd(a5_17, b5));
_mm_storeu_pd(&C[(i*56)+43], c5_17);
#endif
__m128d c5_19 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a5_19 = _mm_loadu_pd(&values[203]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_19 = _mm_add_pd(c5_19, _mm_mul_pd(a5_19, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_19 = _mm_add_pd(c5_19, _mm_mul_pd(a5_19, b5));
#endif
_mm_storeu_pd(&C[(i*56)+46], c5_19);
__m128d c5_21 = _mm_load_sd(&C[(i*56)+48]);
__m128d a5_21 = _mm_load_sd(&values[205]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_21 = _mm_add_sd(c5_21, _mm_mul_sd(a5_21, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_21 = _mm_add_sd(c5_21, _mm_mul_sd(a5_21, b5));
#endif
_mm_store_sd(&C[(i*56)+48], c5_21);
#else
C[(i*56)+10] += values[184] * B[(i*56)+5];
C[(i*56)+11] += values[185] * B[(i*56)+5];
C[(i*56)+12] += values[186] * B[(i*56)+5];
C[(i*56)+20] += values[187] * B[(i*56)+5];
C[(i*56)+21] += values[188] * B[(i*56)+5];
C[(i*56)+22] += values[189] * B[(i*56)+5];
C[(i*56)+23] += values[190] * B[(i*56)+5];
C[(i*56)+25] += values[191] * B[(i*56)+5];
C[(i*56)+26] += values[192] * B[(i*56)+5];
C[(i*56)+27] += values[193] * B[(i*56)+5];
C[(i*56)+35] += values[194] * B[(i*56)+5];
C[(i*56)+36] += values[195] * B[(i*56)+5];
C[(i*56)+37] += values[196] * B[(i*56)+5];
C[(i*56)+38] += values[197] * B[(i*56)+5];
C[(i*56)+39] += values[198] * B[(i*56)+5];
C[(i*56)+41] += values[199] * B[(i*56)+5];
C[(i*56)+42] += values[200] * B[(i*56)+5];
C[(i*56)+43] += values[201] * B[(i*56)+5];
C[(i*56)+44] += values[202] * B[(i*56)+5];
C[(i*56)+46] += values[203] * B[(i*56)+5];
C[(i*56)+47] += values[204] * B[(i*56)+5];
C[(i*56)+48] += values[205] * B[(i*56)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*56)+6]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_0 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a6_0 = _mm256_loadu_pd(&values[206]);
c6_0 = _mm256_add_pd(c6_0, _mm256_mul_pd(a6_0, b6));
_mm256_storeu_pd(&C[(i*56)+10], c6_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a6_0 = _mm_loadu_pd(&values[206]);
c6_0 = _mm_add_pd(c6_0, _mm_mul_pd(a6_0, b6));
_mm_storeu_pd(&C[(i*56)+10], c6_0);
__m128d c6_2 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a6_2 = _mm_loadu_pd(&values[208]);
c6_2 = _mm_add_pd(c6_2, _mm_mul_pd(a6_2, b6));
_mm_storeu_pd(&C[(i*56)+12], c6_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_4 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a6_4 = _mm256_loadu_pd(&values[210]);
c6_4 = _mm256_add_pd(c6_4, _mm256_mul_pd(a6_4, b6));
_mm256_storeu_pd(&C[(i*56)+20], c6_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_4 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a6_4 = _mm_loadu_pd(&values[210]);
c6_4 = _mm_add_pd(c6_4, _mm_mul_pd(a6_4, b6));
_mm_storeu_pd(&C[(i*56)+20], c6_4);
__m128d c6_6 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a6_6 = _mm_loadu_pd(&values[212]);
c6_6 = _mm_add_pd(c6_6, _mm_mul_pd(a6_6, b6));
_mm_storeu_pd(&C[(i*56)+22], c6_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_8 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a6_8 = _mm256_loadu_pd(&values[214]);
c6_8 = _mm256_add_pd(c6_8, _mm256_mul_pd(a6_8, b6));
_mm256_storeu_pd(&C[(i*56)+24], c6_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_8 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a6_8 = _mm_loadu_pd(&values[214]);
c6_8 = _mm_add_pd(c6_8, _mm_mul_pd(a6_8, b6));
_mm_storeu_pd(&C[(i*56)+24], c6_8);
__m128d c6_10 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a6_10 = _mm_loadu_pd(&values[216]);
c6_10 = _mm_add_pd(c6_10, _mm_mul_pd(a6_10, b6));
_mm_storeu_pd(&C[(i*56)+26], c6_10);
#endif
__m128d c6_12 = _mm_load_sd(&C[(i*56)+28]);
__m128d a6_12 = _mm_load_sd(&values[218]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_12 = _mm_add_sd(c6_12, _mm_mul_sd(a6_12, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_12 = _mm_add_sd(c6_12, _mm_mul_sd(a6_12, b6));
#endif
_mm_store_sd(&C[(i*56)+28], c6_12);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_13 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a6_13 = _mm256_loadu_pd(&values[219]);
c6_13 = _mm256_add_pd(c6_13, _mm256_mul_pd(a6_13, b6));
_mm256_storeu_pd(&C[(i*56)+35], c6_13);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_13 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a6_13 = _mm_loadu_pd(&values[219]);
c6_13 = _mm_add_pd(c6_13, _mm_mul_pd(a6_13, b6));
_mm_storeu_pd(&C[(i*56)+35], c6_13);
__m128d c6_15 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a6_15 = _mm_loadu_pd(&values[221]);
c6_15 = _mm_add_pd(c6_15, _mm_mul_pd(a6_15, b6));
_mm_storeu_pd(&C[(i*56)+37], c6_15);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_17 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a6_17 = _mm256_loadu_pd(&values[223]);
c6_17 = _mm256_add_pd(c6_17, _mm256_mul_pd(a6_17, b6));
_mm256_storeu_pd(&C[(i*56)+39], c6_17);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_17 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a6_17 = _mm_loadu_pd(&values[223]);
c6_17 = _mm_add_pd(c6_17, _mm_mul_pd(a6_17, b6));
_mm_storeu_pd(&C[(i*56)+39], c6_17);
__m128d c6_19 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a6_19 = _mm_loadu_pd(&values[225]);
c6_19 = _mm_add_pd(c6_19, _mm_mul_pd(a6_19, b6));
_mm_storeu_pd(&C[(i*56)+41], c6_19);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_21 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a6_21 = _mm256_loadu_pd(&values[227]);
c6_21 = _mm256_add_pd(c6_21, _mm256_mul_pd(a6_21, b6));
_mm256_storeu_pd(&C[(i*56)+43], c6_21);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_21 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a6_21 = _mm_loadu_pd(&values[227]);
c6_21 = _mm_add_pd(c6_21, _mm_mul_pd(a6_21, b6));
_mm_storeu_pd(&C[(i*56)+43], c6_21);
__m128d c6_23 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a6_23 = _mm_loadu_pd(&values[229]);
c6_23 = _mm_add_pd(c6_23, _mm_mul_pd(a6_23, b6));
_mm_storeu_pd(&C[(i*56)+45], c6_23);
#endif
__m128d c6_25 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a6_25 = _mm_loadu_pd(&values[231]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_25 = _mm_add_pd(c6_25, _mm_mul_pd(a6_25, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_25 = _mm_add_pd(c6_25, _mm_mul_pd(a6_25, b6));
#endif
_mm_storeu_pd(&C[(i*56)+47], c6_25);
__m128d c6_27 = _mm_load_sd(&C[(i*56)+49]);
__m128d a6_27 = _mm_load_sd(&values[233]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_27 = _mm_add_sd(c6_27, _mm_mul_sd(a6_27, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_27 = _mm_add_sd(c6_27, _mm_mul_sd(a6_27, b6));
#endif
_mm_store_sd(&C[(i*56)+49], c6_27);
#else
C[(i*56)+10] += values[206] * B[(i*56)+6];
C[(i*56)+11] += values[207] * B[(i*56)+6];
C[(i*56)+12] += values[208] * B[(i*56)+6];
C[(i*56)+13] += values[209] * B[(i*56)+6];
C[(i*56)+20] += values[210] * B[(i*56)+6];
C[(i*56)+21] += values[211] * B[(i*56)+6];
C[(i*56)+22] += values[212] * B[(i*56)+6];
C[(i*56)+23] += values[213] * B[(i*56)+6];
C[(i*56)+24] += values[214] * B[(i*56)+6];
C[(i*56)+25] += values[215] * B[(i*56)+6];
C[(i*56)+26] += values[216] * B[(i*56)+6];
C[(i*56)+27] += values[217] * B[(i*56)+6];
C[(i*56)+28] += values[218] * B[(i*56)+6];
C[(i*56)+35] += values[219] * B[(i*56)+6];
C[(i*56)+36] += values[220] * B[(i*56)+6];
C[(i*56)+37] += values[221] * B[(i*56)+6];
C[(i*56)+38] += values[222] * B[(i*56)+6];
C[(i*56)+39] += values[223] * B[(i*56)+6];
C[(i*56)+40] += values[224] * B[(i*56)+6];
C[(i*56)+41] += values[225] * B[(i*56)+6];
C[(i*56)+42] += values[226] * B[(i*56)+6];
C[(i*56)+43] += values[227] * B[(i*56)+6];
C[(i*56)+44] += values[228] * B[(i*56)+6];
C[(i*56)+45] += values[229] * B[(i*56)+6];
C[(i*56)+46] += values[230] * B[(i*56)+6];
C[(i*56)+47] += values[231] * B[(i*56)+6];
C[(i*56)+48] += values[232] * B[(i*56)+6];
C[(i*56)+49] += values[233] * B[(i*56)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*56)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a7_0 = _mm_loadu_pd(&values[234]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*56)+10], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*56)+12]);
__m128d a7_2 = _mm_load_sd(&values[236]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*56)+12], c7_2);
__m128d c7_3 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a7_3 = _mm_loadu_pd(&values[237]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, b7));
#endif
_mm_storeu_pd(&C[(i*56)+14], c7_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_5 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a7_5 = _mm256_loadu_pd(&values[239]);
c7_5 = _mm256_add_pd(c7_5, _mm256_mul_pd(a7_5, b7));
_mm256_storeu_pd(&C[(i*56)+20], c7_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_5 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a7_5 = _mm_loadu_pd(&values[239]);
c7_5 = _mm_add_pd(c7_5, _mm_mul_pd(a7_5, b7));
_mm_storeu_pd(&C[(i*56)+20], c7_5);
__m128d c7_7 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a7_7 = _mm_loadu_pd(&values[241]);
c7_7 = _mm_add_pd(c7_7, _mm_mul_pd(a7_7, b7));
_mm_storeu_pd(&C[(i*56)+22], c7_7);
#endif
__m128d c7_9 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a7_9 = _mm_loadu_pd(&values[243]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_9 = _mm_add_pd(c7_9, _mm_mul_pd(a7_9, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_9 = _mm_add_pd(c7_9, _mm_mul_pd(a7_9, b7));
#endif
_mm_storeu_pd(&C[(i*56)+25], c7_9);
__m128d c7_11 = _mm_load_sd(&C[(i*56)+27]);
__m128d a7_11 = _mm_load_sd(&values[245]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_11 = _mm_add_sd(c7_11, _mm_mul_sd(a7_11, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_11 = _mm_add_sd(c7_11, _mm_mul_sd(a7_11, b7));
#endif
_mm_store_sd(&C[(i*56)+27], c7_11);
__m128d c7_12 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a7_12 = _mm_loadu_pd(&values[246]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_12 = _mm_add_pd(c7_12, _mm_mul_pd(a7_12, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_12 = _mm_add_pd(c7_12, _mm_mul_pd(a7_12, b7));
#endif
_mm_storeu_pd(&C[(i*56)+29], c7_12);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_14 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a7_14 = _mm256_loadu_pd(&values[248]);
c7_14 = _mm256_add_pd(c7_14, _mm256_mul_pd(a7_14, b7));
_mm256_storeu_pd(&C[(i*56)+35], c7_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_14 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a7_14 = _mm_loadu_pd(&values[248]);
c7_14 = _mm_add_pd(c7_14, _mm_mul_pd(a7_14, b7));
_mm_storeu_pd(&C[(i*56)+35], c7_14);
__m128d c7_16 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a7_16 = _mm_loadu_pd(&values[250]);
c7_16 = _mm_add_pd(c7_16, _mm_mul_pd(a7_16, b7));
_mm_storeu_pd(&C[(i*56)+37], c7_16);
#endif
__m128d c7_18 = _mm_load_sd(&C[(i*56)+39]);
__m128d a7_18 = _mm_load_sd(&values[252]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_18 = _mm_add_sd(c7_18, _mm_mul_sd(a7_18, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_18 = _mm_add_sd(c7_18, _mm_mul_sd(a7_18, b7));
#endif
_mm_store_sd(&C[(i*56)+39], c7_18);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_19 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a7_19 = _mm256_loadu_pd(&values[253]);
c7_19 = _mm256_add_pd(c7_19, _mm256_mul_pd(a7_19, b7));
_mm256_storeu_pd(&C[(i*56)+41], c7_19);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_19 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a7_19 = _mm_loadu_pd(&values[253]);
c7_19 = _mm_add_pd(c7_19, _mm_mul_pd(a7_19, b7));
_mm_storeu_pd(&C[(i*56)+41], c7_19);
__m128d c7_21 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a7_21 = _mm_loadu_pd(&values[255]);
c7_21 = _mm_add_pd(c7_21, _mm_mul_pd(a7_21, b7));
_mm_storeu_pd(&C[(i*56)+43], c7_21);
#endif
__m128d c7_23 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a7_23 = _mm_loadu_pd(&values[257]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_23 = _mm_add_pd(c7_23, _mm_mul_pd(a7_23, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_23 = _mm_add_pd(c7_23, _mm_mul_pd(a7_23, b7));
#endif
_mm_storeu_pd(&C[(i*56)+46], c7_23);
__m128d c7_25 = _mm_load_sd(&C[(i*56)+48]);
__m128d a7_25 = _mm_load_sd(&values[259]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_25 = _mm_add_sd(c7_25, _mm_mul_sd(a7_25, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_25 = _mm_add_sd(c7_25, _mm_mul_sd(a7_25, b7));
#endif
_mm_store_sd(&C[(i*56)+48], c7_25);
__m128d c7_26 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a7_26 = _mm_loadu_pd(&values[260]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_26 = _mm_add_pd(c7_26, _mm_mul_pd(a7_26, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_26 = _mm_add_pd(c7_26, _mm_mul_pd(a7_26, b7));
#endif
_mm_storeu_pd(&C[(i*56)+50], c7_26);
#else
C[(i*56)+10] += values[234] * B[(i*56)+7];
C[(i*56)+11] += values[235] * B[(i*56)+7];
C[(i*56)+12] += values[236] * B[(i*56)+7];
C[(i*56)+14] += values[237] * B[(i*56)+7];
C[(i*56)+15] += values[238] * B[(i*56)+7];
C[(i*56)+20] += values[239] * B[(i*56)+7];
C[(i*56)+21] += values[240] * B[(i*56)+7];
C[(i*56)+22] += values[241] * B[(i*56)+7];
C[(i*56)+23] += values[242] * B[(i*56)+7];
C[(i*56)+25] += values[243] * B[(i*56)+7];
C[(i*56)+26] += values[244] * B[(i*56)+7];
C[(i*56)+27] += values[245] * B[(i*56)+7];
C[(i*56)+29] += values[246] * B[(i*56)+7];
C[(i*56)+30] += values[247] * B[(i*56)+7];
C[(i*56)+35] += values[248] * B[(i*56)+7];
C[(i*56)+36] += values[249] * B[(i*56)+7];
C[(i*56)+37] += values[250] * B[(i*56)+7];
C[(i*56)+38] += values[251] * B[(i*56)+7];
C[(i*56)+39] += values[252] * B[(i*56)+7];
C[(i*56)+41] += values[253] * B[(i*56)+7];
C[(i*56)+42] += values[254] * B[(i*56)+7];
C[(i*56)+43] += values[255] * B[(i*56)+7];
C[(i*56)+44] += values[256] * B[(i*56)+7];
C[(i*56)+46] += values[257] * B[(i*56)+7];
C[(i*56)+47] += values[258] * B[(i*56)+7];
C[(i*56)+48] += values[259] * B[(i*56)+7];
C[(i*56)+50] += values[260] * B[(i*56)+7];
C[(i*56)+51] += values[261] * B[(i*56)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*56)+8]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_0 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a8_0 = _mm256_loadu_pd(&values[262]);
c8_0 = _mm256_add_pd(c8_0, _mm256_mul_pd(a8_0, b8));
_mm256_storeu_pd(&C[(i*56)+10], c8_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a8_0 = _mm_loadu_pd(&values[262]);
c8_0 = _mm_add_pd(c8_0, _mm_mul_pd(a8_0, b8));
_mm_storeu_pd(&C[(i*56)+10], c8_0);
__m128d c8_2 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a8_2 = _mm_loadu_pd(&values[264]);
c8_2 = _mm_add_pd(c8_2, _mm_mul_pd(a8_2, b8));
_mm_storeu_pd(&C[(i*56)+12], c8_2);
#endif
__m128d c8_4 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a8_4 = _mm_loadu_pd(&values[266]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, b8));
#endif
_mm_storeu_pd(&C[(i*56)+14], c8_4);
__m128d c8_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a8_6 = _mm_load_sd(&values[268]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*56)+16], c8_6);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_7 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a8_7 = _mm256_loadu_pd(&values[269]);
c8_7 = _mm256_add_pd(c8_7, _mm256_mul_pd(a8_7, b8));
_mm256_storeu_pd(&C[(i*56)+20], c8_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_7 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a8_7 = _mm_loadu_pd(&values[269]);
c8_7 = _mm_add_pd(c8_7, _mm_mul_pd(a8_7, b8));
_mm_storeu_pd(&C[(i*56)+20], c8_7);
__m128d c8_9 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a8_9 = _mm_loadu_pd(&values[271]);
c8_9 = _mm_add_pd(c8_9, _mm_mul_pd(a8_9, b8));
_mm_storeu_pd(&C[(i*56)+22], c8_9);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_11 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a8_11 = _mm256_loadu_pd(&values[273]);
c8_11 = _mm256_add_pd(c8_11, _mm256_mul_pd(a8_11, b8));
_mm256_storeu_pd(&C[(i*56)+24], c8_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_11 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a8_11 = _mm_loadu_pd(&values[273]);
c8_11 = _mm_add_pd(c8_11, _mm_mul_pd(a8_11, b8));
_mm_storeu_pd(&C[(i*56)+24], c8_11);
__m128d c8_13 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a8_13 = _mm_loadu_pd(&values[275]);
c8_13 = _mm_add_pd(c8_13, _mm_mul_pd(a8_13, b8));
_mm_storeu_pd(&C[(i*56)+26], c8_13);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_15 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a8_15 = _mm256_loadu_pd(&values[277]);
c8_15 = _mm256_add_pd(c8_15, _mm256_mul_pd(a8_15, b8));
_mm256_storeu_pd(&C[(i*56)+28], c8_15);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_15 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a8_15 = _mm_loadu_pd(&values[277]);
c8_15 = _mm_add_pd(c8_15, _mm_mul_pd(a8_15, b8));
_mm_storeu_pd(&C[(i*56)+28], c8_15);
__m128d c8_17 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a8_17 = _mm_loadu_pd(&values[279]);
c8_17 = _mm_add_pd(c8_17, _mm_mul_pd(a8_17, b8));
_mm_storeu_pd(&C[(i*56)+30], c8_17);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_19 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a8_19 = _mm256_loadu_pd(&values[281]);
c8_19 = _mm256_add_pd(c8_19, _mm256_mul_pd(a8_19, b8));
_mm256_storeu_pd(&C[(i*56)+35], c8_19);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_19 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a8_19 = _mm_loadu_pd(&values[281]);
c8_19 = _mm_add_pd(c8_19, _mm_mul_pd(a8_19, b8));
_mm_storeu_pd(&C[(i*56)+35], c8_19);
__m128d c8_21 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a8_21 = _mm_loadu_pd(&values[283]);
c8_21 = _mm_add_pd(c8_21, _mm_mul_pd(a8_21, b8));
_mm_storeu_pd(&C[(i*56)+37], c8_21);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_23 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a8_23 = _mm256_loadu_pd(&values[285]);
c8_23 = _mm256_add_pd(c8_23, _mm256_mul_pd(a8_23, b8));
_mm256_storeu_pd(&C[(i*56)+39], c8_23);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_23 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a8_23 = _mm_loadu_pd(&values[285]);
c8_23 = _mm_add_pd(c8_23, _mm_mul_pd(a8_23, b8));
_mm_storeu_pd(&C[(i*56)+39], c8_23);
__m128d c8_25 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a8_25 = _mm_loadu_pd(&values[287]);
c8_25 = _mm_add_pd(c8_25, _mm_mul_pd(a8_25, b8));
_mm_storeu_pd(&C[(i*56)+41], c8_25);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_27 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a8_27 = _mm256_loadu_pd(&values[289]);
c8_27 = _mm256_add_pd(c8_27, _mm256_mul_pd(a8_27, b8));
_mm256_storeu_pd(&C[(i*56)+43], c8_27);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_27 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a8_27 = _mm_loadu_pd(&values[289]);
c8_27 = _mm_add_pd(c8_27, _mm_mul_pd(a8_27, b8));
_mm_storeu_pd(&C[(i*56)+43], c8_27);
__m128d c8_29 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a8_29 = _mm_loadu_pd(&values[291]);
c8_29 = _mm_add_pd(c8_29, _mm_mul_pd(a8_29, b8));
_mm_storeu_pd(&C[(i*56)+45], c8_29);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_31 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a8_31 = _mm256_loadu_pd(&values[293]);
c8_31 = _mm256_add_pd(c8_31, _mm256_mul_pd(a8_31, b8));
_mm256_storeu_pd(&C[(i*56)+47], c8_31);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_31 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a8_31 = _mm_loadu_pd(&values[293]);
c8_31 = _mm_add_pd(c8_31, _mm_mul_pd(a8_31, b8));
_mm_storeu_pd(&C[(i*56)+47], c8_31);
__m128d c8_33 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a8_33 = _mm_loadu_pd(&values[295]);
c8_33 = _mm_add_pd(c8_33, _mm_mul_pd(a8_33, b8));
_mm_storeu_pd(&C[(i*56)+49], c8_33);
#endif
__m128d c8_35 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a8_35 = _mm_loadu_pd(&values[297]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_35 = _mm_add_pd(c8_35, _mm_mul_pd(a8_35, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_35 = _mm_add_pd(c8_35, _mm_mul_pd(a8_35, b8));
#endif
_mm_storeu_pd(&C[(i*56)+51], c8_35);
#else
C[(i*56)+10] += values[262] * B[(i*56)+8];
C[(i*56)+11] += values[263] * B[(i*56)+8];
C[(i*56)+12] += values[264] * B[(i*56)+8];
C[(i*56)+13] += values[265] * B[(i*56)+8];
C[(i*56)+14] += values[266] * B[(i*56)+8];
C[(i*56)+15] += values[267] * B[(i*56)+8];
C[(i*56)+16] += values[268] * B[(i*56)+8];
C[(i*56)+20] += values[269] * B[(i*56)+8];
C[(i*56)+21] += values[270] * B[(i*56)+8];
C[(i*56)+22] += values[271] * B[(i*56)+8];
C[(i*56)+23] += values[272] * B[(i*56)+8];
C[(i*56)+24] += values[273] * B[(i*56)+8];
C[(i*56)+25] += values[274] * B[(i*56)+8];
C[(i*56)+26] += values[275] * B[(i*56)+8];
C[(i*56)+27] += values[276] * B[(i*56)+8];
C[(i*56)+28] += values[277] * B[(i*56)+8];
C[(i*56)+29] += values[278] * B[(i*56)+8];
C[(i*56)+30] += values[279] * B[(i*56)+8];
C[(i*56)+31] += values[280] * B[(i*56)+8];
C[(i*56)+35] += values[281] * B[(i*56)+8];
C[(i*56)+36] += values[282] * B[(i*56)+8];
C[(i*56)+37] += values[283] * B[(i*56)+8];
C[(i*56)+38] += values[284] * B[(i*56)+8];
C[(i*56)+39] += values[285] * B[(i*56)+8];
C[(i*56)+40] += values[286] * B[(i*56)+8];
C[(i*56)+41] += values[287] * B[(i*56)+8];
C[(i*56)+42] += values[288] * B[(i*56)+8];
C[(i*56)+43] += values[289] * B[(i*56)+8];
C[(i*56)+44] += values[290] * B[(i*56)+8];
C[(i*56)+45] += values[291] * B[(i*56)+8];
C[(i*56)+46] += values[292] * B[(i*56)+8];
C[(i*56)+47] += values[293] * B[(i*56)+8];
C[(i*56)+48] += values[294] * B[(i*56)+8];
C[(i*56)+49] += values[295] * B[(i*56)+8];
C[(i*56)+50] += values[296] * B[(i*56)+8];
C[(i*56)+51] += values[297] * B[(i*56)+8];
C[(i*56)+52] += values[298] * B[(i*56)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*56)+9]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_0 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a9_0 = _mm256_loadu_pd(&values[299]);
c9_0 = _mm256_add_pd(c9_0, _mm256_mul_pd(a9_0, b9));
_mm256_storeu_pd(&C[(i*56)+10], c9_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a9_0 = _mm_loadu_pd(&values[299]);
c9_0 = _mm_add_pd(c9_0, _mm_mul_pd(a9_0, b9));
_mm_storeu_pd(&C[(i*56)+10], c9_0);
__m128d c9_2 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a9_2 = _mm_loadu_pd(&values[301]);
c9_2 = _mm_add_pd(c9_2, _mm_mul_pd(a9_2, b9));
_mm_storeu_pd(&C[(i*56)+12], c9_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_4 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a9_4 = _mm256_loadu_pd(&values[303]);
c9_4 = _mm256_add_pd(c9_4, _mm256_mul_pd(a9_4, b9));
_mm256_storeu_pd(&C[(i*56)+14], c9_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_4 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a9_4 = _mm_loadu_pd(&values[303]);
c9_4 = _mm_add_pd(c9_4, _mm_mul_pd(a9_4, b9));
_mm_storeu_pd(&C[(i*56)+14], c9_4);
__m128d c9_6 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a9_6 = _mm_loadu_pd(&values[305]);
c9_6 = _mm_add_pd(c9_6, _mm_mul_pd(a9_6, b9));
_mm_storeu_pd(&C[(i*56)+16], c9_6);
#endif
__m128d c9_8 = _mm_load_sd(&C[(i*56)+18]);
__m128d a9_8 = _mm_load_sd(&values[307]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_8 = _mm_add_sd(c9_8, _mm_mul_sd(a9_8, b9));
#endif
_mm_store_sd(&C[(i*56)+18], c9_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_9 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a9_9 = _mm256_loadu_pd(&values[308]);
c9_9 = _mm256_add_pd(c9_9, _mm256_mul_pd(a9_9, b9));
_mm256_storeu_pd(&C[(i*56)+20], c9_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_9 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a9_9 = _mm_loadu_pd(&values[308]);
c9_9 = _mm_add_pd(c9_9, _mm_mul_pd(a9_9, b9));
_mm_storeu_pd(&C[(i*56)+20], c9_9);
__m128d c9_11 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a9_11 = _mm_loadu_pd(&values[310]);
c9_11 = _mm_add_pd(c9_11, _mm_mul_pd(a9_11, b9));
_mm_storeu_pd(&C[(i*56)+22], c9_11);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_13 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a9_13 = _mm256_loadu_pd(&values[312]);
c9_13 = _mm256_add_pd(c9_13, _mm256_mul_pd(a9_13, b9));
_mm256_storeu_pd(&C[(i*56)+24], c9_13);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_13 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a9_13 = _mm_loadu_pd(&values[312]);
c9_13 = _mm_add_pd(c9_13, _mm_mul_pd(a9_13, b9));
_mm_storeu_pd(&C[(i*56)+24], c9_13);
__m128d c9_15 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a9_15 = _mm_loadu_pd(&values[314]);
c9_15 = _mm_add_pd(c9_15, _mm_mul_pd(a9_15, b9));
_mm_storeu_pd(&C[(i*56)+26], c9_15);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_17 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a9_17 = _mm256_loadu_pd(&values[316]);
c9_17 = _mm256_add_pd(c9_17, _mm256_mul_pd(a9_17, b9));
_mm256_storeu_pd(&C[(i*56)+28], c9_17);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_17 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a9_17 = _mm_loadu_pd(&values[316]);
c9_17 = _mm_add_pd(c9_17, _mm_mul_pd(a9_17, b9));
_mm_storeu_pd(&C[(i*56)+28], c9_17);
__m128d c9_19 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a9_19 = _mm_loadu_pd(&values[318]);
c9_19 = _mm_add_pd(c9_19, _mm_mul_pd(a9_19, b9));
_mm_storeu_pd(&C[(i*56)+30], c9_19);
#endif
__m128d c9_21 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a9_21 = _mm_loadu_pd(&values[320]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_21 = _mm_add_pd(c9_21, _mm_mul_pd(a9_21, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_21 = _mm_add_pd(c9_21, _mm_mul_pd(a9_21, b9));
#endif
_mm_storeu_pd(&C[(i*56)+32], c9_21);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_23 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a9_23 = _mm256_loadu_pd(&values[322]);
c9_23 = _mm256_add_pd(c9_23, _mm256_mul_pd(a9_23, b9));
_mm256_storeu_pd(&C[(i*56)+35], c9_23);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_23 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a9_23 = _mm_loadu_pd(&values[322]);
c9_23 = _mm_add_pd(c9_23, _mm_mul_pd(a9_23, b9));
_mm_storeu_pd(&C[(i*56)+35], c9_23);
__m128d c9_25 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a9_25 = _mm_loadu_pd(&values[324]);
c9_25 = _mm_add_pd(c9_25, _mm_mul_pd(a9_25, b9));
_mm_storeu_pd(&C[(i*56)+37], c9_25);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_27 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a9_27 = _mm256_loadu_pd(&values[326]);
c9_27 = _mm256_add_pd(c9_27, _mm256_mul_pd(a9_27, b9));
_mm256_storeu_pd(&C[(i*56)+39], c9_27);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_27 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a9_27 = _mm_loadu_pd(&values[326]);
c9_27 = _mm_add_pd(c9_27, _mm_mul_pd(a9_27, b9));
_mm_storeu_pd(&C[(i*56)+39], c9_27);
__m128d c9_29 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a9_29 = _mm_loadu_pd(&values[328]);
c9_29 = _mm_add_pd(c9_29, _mm_mul_pd(a9_29, b9));
_mm_storeu_pd(&C[(i*56)+41], c9_29);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_31 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a9_31 = _mm256_loadu_pd(&values[330]);
c9_31 = _mm256_add_pd(c9_31, _mm256_mul_pd(a9_31, b9));
_mm256_storeu_pd(&C[(i*56)+43], c9_31);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_31 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a9_31 = _mm_loadu_pd(&values[330]);
c9_31 = _mm_add_pd(c9_31, _mm_mul_pd(a9_31, b9));
_mm_storeu_pd(&C[(i*56)+43], c9_31);
__m128d c9_33 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a9_33 = _mm_loadu_pd(&values[332]);
c9_33 = _mm_add_pd(c9_33, _mm_mul_pd(a9_33, b9));
_mm_storeu_pd(&C[(i*56)+45], c9_33);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_35 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a9_35 = _mm256_loadu_pd(&values[334]);
c9_35 = _mm256_add_pd(c9_35, _mm256_mul_pd(a9_35, b9));
_mm256_storeu_pd(&C[(i*56)+47], c9_35);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_35 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a9_35 = _mm_loadu_pd(&values[334]);
c9_35 = _mm_add_pd(c9_35, _mm_mul_pd(a9_35, b9));
_mm_storeu_pd(&C[(i*56)+47], c9_35);
__m128d c9_37 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a9_37 = _mm_loadu_pd(&values[336]);
c9_37 = _mm_add_pd(c9_37, _mm_mul_pd(a9_37, b9));
_mm_storeu_pd(&C[(i*56)+49], c9_37);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_39 = _mm256_loadu_pd(&C[(i*56)+51]);
__m256d a9_39 = _mm256_loadu_pd(&values[338]);
c9_39 = _mm256_add_pd(c9_39, _mm256_mul_pd(a9_39, b9));
_mm256_storeu_pd(&C[(i*56)+51], c9_39);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_39 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a9_39 = _mm_loadu_pd(&values[338]);
c9_39 = _mm_add_pd(c9_39, _mm_mul_pd(a9_39, b9));
_mm_storeu_pd(&C[(i*56)+51], c9_39);
__m128d c9_41 = _mm_loadu_pd(&C[(i*56)+53]);
__m128d a9_41 = _mm_loadu_pd(&values[340]);
c9_41 = _mm_add_pd(c9_41, _mm_mul_pd(a9_41, b9));
_mm_storeu_pd(&C[(i*56)+53], c9_41);
#endif
#else
C[(i*56)+10] += values[299] * B[(i*56)+9];
C[(i*56)+11] += values[300] * B[(i*56)+9];
C[(i*56)+12] += values[301] * B[(i*56)+9];
C[(i*56)+13] += values[302] * B[(i*56)+9];
C[(i*56)+14] += values[303] * B[(i*56)+9];
C[(i*56)+15] += values[304] * B[(i*56)+9];
C[(i*56)+16] += values[305] * B[(i*56)+9];
C[(i*56)+17] += values[306] * B[(i*56)+9];
C[(i*56)+18] += values[307] * B[(i*56)+9];
C[(i*56)+20] += values[308] * B[(i*56)+9];
C[(i*56)+21] += values[309] * B[(i*56)+9];
C[(i*56)+22] += values[310] * B[(i*56)+9];
C[(i*56)+23] += values[311] * B[(i*56)+9];
C[(i*56)+24] += values[312] * B[(i*56)+9];
C[(i*56)+25] += values[313] * B[(i*56)+9];
C[(i*56)+26] += values[314] * B[(i*56)+9];
C[(i*56)+27] += values[315] * B[(i*56)+9];
C[(i*56)+28] += values[316] * B[(i*56)+9];
C[(i*56)+29] += values[317] * B[(i*56)+9];
C[(i*56)+30] += values[318] * B[(i*56)+9];
C[(i*56)+31] += values[319] * B[(i*56)+9];
C[(i*56)+32] += values[320] * B[(i*56)+9];
C[(i*56)+33] += values[321] * B[(i*56)+9];
C[(i*56)+35] += values[322] * B[(i*56)+9];
C[(i*56)+36] += values[323] * B[(i*56)+9];
C[(i*56)+37] += values[324] * B[(i*56)+9];
C[(i*56)+38] += values[325] * B[(i*56)+9];
C[(i*56)+39] += values[326] * B[(i*56)+9];
C[(i*56)+40] += values[327] * B[(i*56)+9];
C[(i*56)+41] += values[328] * B[(i*56)+9];
C[(i*56)+42] += values[329] * B[(i*56)+9];
C[(i*56)+43] += values[330] * B[(i*56)+9];
C[(i*56)+44] += values[331] * B[(i*56)+9];
C[(i*56)+45] += values[332] * B[(i*56)+9];
C[(i*56)+46] += values[333] * B[(i*56)+9];
C[(i*56)+47] += values[334] * B[(i*56)+9];
C[(i*56)+48] += values[335] * B[(i*56)+9];
C[(i*56)+49] += values[336] * B[(i*56)+9];
C[(i*56)+50] += values[337] * B[(i*56)+9];
C[(i*56)+51] += values[338] * B[(i*56)+9];
C[(i*56)+52] += values[339] * B[(i*56)+9];
C[(i*56)+53] += values[340] * B[(i*56)+9];
C[(i*56)+54] += values[341] * B[(i*56)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*56)+10]);
#endif
__m128d c10_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a10_0 = _mm_loadu_pd(&values[342]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
#endif
_mm_storeu_pd(&C[(i*56)+20], c10_0);
__m128d c10_2 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a10_2 = _mm_loadu_pd(&values[344]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, b10));
#endif
_mm_storeu_pd(&C[(i*56)+35], c10_2);
__m128d c10_4 = _mm_load_sd(&C[(i*56)+37]);
__m128d a10_4 = _mm_load_sd(&values[346]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, b10));
#endif
_mm_store_sd(&C[(i*56)+37], c10_4);
__m128d c10_5 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a10_5 = _mm_loadu_pd(&values[347]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, b10));
#endif
_mm_storeu_pd(&C[(i*56)+41], c10_5);
#else
C[(i*56)+20] += values[342] * B[(i*56)+10];
C[(i*56)+21] += values[343] * B[(i*56)+10];
C[(i*56)+35] += values[344] * B[(i*56)+10];
C[(i*56)+36] += values[345] * B[(i*56)+10];
C[(i*56)+37] += values[346] * B[(i*56)+10];
C[(i*56)+41] += values[347] * B[(i*56)+10];
C[(i*56)+42] += values[348] * B[(i*56)+10];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*56)+11]);
#endif
__m128d c11_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a11_0 = _mm_loadu_pd(&values[349]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
#endif
_mm_storeu_pd(&C[(i*56)+20], c11_0);
__m128d c11_2 = _mm_load_sd(&C[(i*56)+22]);
__m128d a11_2 = _mm_load_sd(&values[351]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*56)+22], c11_2);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_3 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a11_3 = _mm256_loadu_pd(&values[352]);
c11_3 = _mm256_add_pd(c11_3, _mm256_mul_pd(a11_3, b11));
_mm256_storeu_pd(&C[(i*56)+35], c11_3);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_3 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a11_3 = _mm_loadu_pd(&values[352]);
c11_3 = _mm_add_pd(c11_3, _mm_mul_pd(a11_3, b11));
_mm_storeu_pd(&C[(i*56)+35], c11_3);
__m128d c11_5 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a11_5 = _mm_loadu_pd(&values[354]);
c11_5 = _mm_add_pd(c11_5, _mm_mul_pd(a11_5, b11));
_mm_storeu_pd(&C[(i*56)+37], c11_5);
#endif
__m128d c11_7 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a11_7 = _mm_loadu_pd(&values[356]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_7 = _mm_add_pd(c11_7, _mm_mul_pd(a11_7, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_7 = _mm_add_pd(c11_7, _mm_mul_pd(a11_7, b11));
#endif
_mm_storeu_pd(&C[(i*56)+41], c11_7);
__m128d c11_9 = _mm_load_sd(&C[(i*56)+43]);
__m128d a11_9 = _mm_load_sd(&values[358]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_9 = _mm_add_sd(c11_9, _mm_mul_sd(a11_9, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_9 = _mm_add_sd(c11_9, _mm_mul_sd(a11_9, b11));
#endif
_mm_store_sd(&C[(i*56)+43], c11_9);
#else
C[(i*56)+20] += values[349] * B[(i*56)+11];
C[(i*56)+21] += values[350] * B[(i*56)+11];
C[(i*56)+22] += values[351] * B[(i*56)+11];
C[(i*56)+35] += values[352] * B[(i*56)+11];
C[(i*56)+36] += values[353] * B[(i*56)+11];
C[(i*56)+37] += values[354] * B[(i*56)+11];
C[(i*56)+38] += values[355] * B[(i*56)+11];
C[(i*56)+41] += values[356] * B[(i*56)+11];
C[(i*56)+42] += values[357] * B[(i*56)+11];
C[(i*56)+43] += values[358] * B[(i*56)+11];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a12_0 = _mm256_loadu_pd(&values[359]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*56)+20], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a12_0 = _mm_loadu_pd(&values[359]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*56)+20], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a12_2 = _mm_loadu_pd(&values[361]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*56)+22], c12_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_4 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a12_4 = _mm256_loadu_pd(&values[363]);
c12_4 = _mm256_add_pd(c12_4, _mm256_mul_pd(a12_4, b12));
_mm256_storeu_pd(&C[(i*56)+35], c12_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_4 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a12_4 = _mm_loadu_pd(&values[363]);
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
_mm_storeu_pd(&C[(i*56)+35], c12_4);
__m128d c12_6 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a12_6 = _mm_loadu_pd(&values[365]);
c12_6 = _mm_add_pd(c12_6, _mm_mul_pd(a12_6, b12));
_mm_storeu_pd(&C[(i*56)+37], c12_6);
#endif
__m128d c12_8 = _mm_load_sd(&C[(i*56)+39]);
__m128d a12_8 = _mm_load_sd(&values[367]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, b12));
#endif
_mm_store_sd(&C[(i*56)+39], c12_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_9 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a12_9 = _mm256_loadu_pd(&values[368]);
c12_9 = _mm256_add_pd(c12_9, _mm256_mul_pd(a12_9, b12));
_mm256_storeu_pd(&C[(i*56)+41], c12_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_9 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a12_9 = _mm_loadu_pd(&values[368]);
c12_9 = _mm_add_pd(c12_9, _mm_mul_pd(a12_9, b12));
_mm_storeu_pd(&C[(i*56)+41], c12_9);
__m128d c12_11 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a12_11 = _mm_loadu_pd(&values[370]);
c12_11 = _mm_add_pd(c12_11, _mm_mul_pd(a12_11, b12));
_mm_storeu_pd(&C[(i*56)+43], c12_11);
#endif
#else
C[(i*56)+20] += values[359] * B[(i*56)+12];
C[(i*56)+21] += values[360] * B[(i*56)+12];
C[(i*56)+22] += values[361] * B[(i*56)+12];
C[(i*56)+23] += values[362] * B[(i*56)+12];
C[(i*56)+35] += values[363] * B[(i*56)+12];
C[(i*56)+36] += values[364] * B[(i*56)+12];
C[(i*56)+37] += values[365] * B[(i*56)+12];
C[(i*56)+38] += values[366] * B[(i*56)+12];
C[(i*56)+39] += values[367] * B[(i*56)+12];
C[(i*56)+41] += values[368] * B[(i*56)+12];
C[(i*56)+42] += values[369] * B[(i*56)+12];
C[(i*56)+43] += values[370] * B[(i*56)+12];
C[(i*56)+44] += values[371] * B[(i*56)+12];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*56)+13]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a13_0 = _mm256_loadu_pd(&values[372]);
c13_0 = _mm256_add_pd(c13_0, _mm256_mul_pd(a13_0, b13));
_mm256_storeu_pd(&C[(i*56)+20], c13_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a13_0 = _mm_loadu_pd(&values[372]);
c13_0 = _mm_add_pd(c13_0, _mm_mul_pd(a13_0, b13));
_mm_storeu_pd(&C[(i*56)+20], c13_0);
__m128d c13_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a13_2 = _mm_loadu_pd(&values[374]);
c13_2 = _mm_add_pd(c13_2, _mm_mul_pd(a13_2, b13));
_mm_storeu_pd(&C[(i*56)+22], c13_2);
#endif
__m128d c13_4 = _mm_load_sd(&C[(i*56)+24]);
__m128d a13_4 = _mm_load_sd(&values[376]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, b13));
#endif
_mm_store_sd(&C[(i*56)+24], c13_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_5 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a13_5 = _mm256_loadu_pd(&values[377]);
c13_5 = _mm256_add_pd(c13_5, _mm256_mul_pd(a13_5, b13));
_mm256_storeu_pd(&C[(i*56)+35], c13_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_5 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a13_5 = _mm_loadu_pd(&values[377]);
c13_5 = _mm_add_pd(c13_5, _mm_mul_pd(a13_5, b13));
_mm_storeu_pd(&C[(i*56)+35], c13_5);
__m128d c13_7 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a13_7 = _mm_loadu_pd(&values[379]);
c13_7 = _mm_add_pd(c13_7, _mm_mul_pd(a13_7, b13));
_mm_storeu_pd(&C[(i*56)+37], c13_7);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_9 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a13_9 = _mm256_loadu_pd(&values[381]);
c13_9 = _mm256_add_pd(c13_9, _mm256_mul_pd(a13_9, b13));
_mm256_storeu_pd(&C[(i*56)+39], c13_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_9 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a13_9 = _mm_loadu_pd(&values[381]);
c13_9 = _mm_add_pd(c13_9, _mm_mul_pd(a13_9, b13));
_mm_storeu_pd(&C[(i*56)+39], c13_9);
__m128d c13_11 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a13_11 = _mm_loadu_pd(&values[383]);
c13_11 = _mm_add_pd(c13_11, _mm_mul_pd(a13_11, b13));
_mm_storeu_pd(&C[(i*56)+41], c13_11);
#endif
__m128d c13_13 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a13_13 = _mm_loadu_pd(&values[385]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_13 = _mm_add_pd(c13_13, _mm_mul_pd(a13_13, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_13 = _mm_add_pd(c13_13, _mm_mul_pd(a13_13, b13));
#endif
_mm_storeu_pd(&C[(i*56)+43], c13_13);
__m128d c13_15 = _mm_load_sd(&C[(i*56)+45]);
__m128d a13_15 = _mm_load_sd(&values[387]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_15 = _mm_add_sd(c13_15, _mm_mul_sd(a13_15, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_15 = _mm_add_sd(c13_15, _mm_mul_sd(a13_15, b13));
#endif
_mm_store_sd(&C[(i*56)+45], c13_15);
#else
C[(i*56)+20] += values[372] * B[(i*56)+13];
C[(i*56)+21] += values[373] * B[(i*56)+13];
C[(i*56)+22] += values[374] * B[(i*56)+13];
C[(i*56)+23] += values[375] * B[(i*56)+13];
C[(i*56)+24] += values[376] * B[(i*56)+13];
C[(i*56)+35] += values[377] * B[(i*56)+13];
C[(i*56)+36] += values[378] * B[(i*56)+13];
C[(i*56)+37] += values[379] * B[(i*56)+13];
C[(i*56)+38] += values[380] * B[(i*56)+13];
C[(i*56)+39] += values[381] * B[(i*56)+13];
C[(i*56)+40] += values[382] * B[(i*56)+13];
C[(i*56)+41] += values[383] * B[(i*56)+13];
C[(i*56)+42] += values[384] * B[(i*56)+13];
C[(i*56)+43] += values[385] * B[(i*56)+13];
C[(i*56)+44] += values[386] * B[(i*56)+13];
C[(i*56)+45] += values[387] * B[(i*56)+13];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*56)+14]);
#endif
__m128d c14_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a14_0 = _mm_loadu_pd(&values[388]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
#endif
_mm_storeu_pd(&C[(i*56)+20], c14_0);
__m128d c14_2 = _mm_load_sd(&C[(i*56)+22]);
__m128d a14_2 = _mm_load_sd(&values[390]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, b14));
#endif
_mm_store_sd(&C[(i*56)+22], c14_2);
__m128d c14_3 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a14_3 = _mm_loadu_pd(&values[391]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, b14));
#endif
_mm_storeu_pd(&C[(i*56)+25], c14_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_5 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a14_5 = _mm256_loadu_pd(&values[393]);
c14_5 = _mm256_add_pd(c14_5, _mm256_mul_pd(a14_5, b14));
_mm256_storeu_pd(&C[(i*56)+35], c14_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_5 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a14_5 = _mm_loadu_pd(&values[393]);
c14_5 = _mm_add_pd(c14_5, _mm_mul_pd(a14_5, b14));
_mm_storeu_pd(&C[(i*56)+35], c14_5);
__m128d c14_7 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a14_7 = _mm_loadu_pd(&values[395]);
c14_7 = _mm_add_pd(c14_7, _mm_mul_pd(a14_7, b14));
_mm_storeu_pd(&C[(i*56)+37], c14_7);
#endif
__m128d c14_9 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a14_9 = _mm_loadu_pd(&values[397]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_9 = _mm_add_pd(c14_9, _mm_mul_pd(a14_9, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_9 = _mm_add_pd(c14_9, _mm_mul_pd(a14_9, b14));
#endif
_mm_storeu_pd(&C[(i*56)+41], c14_9);
__m128d c14_11 = _mm_load_sd(&C[(i*56)+43]);
__m128d a14_11 = _mm_load_sd(&values[399]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_11 = _mm_add_sd(c14_11, _mm_mul_sd(a14_11, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_11 = _mm_add_sd(c14_11, _mm_mul_sd(a14_11, b14));
#endif
_mm_store_sd(&C[(i*56)+43], c14_11);
__m128d c14_12 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a14_12 = _mm_loadu_pd(&values[400]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_12 = _mm_add_pd(c14_12, _mm_mul_pd(a14_12, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_12 = _mm_add_pd(c14_12, _mm_mul_pd(a14_12, b14));
#endif
_mm_storeu_pd(&C[(i*56)+46], c14_12);
#else
C[(i*56)+20] += values[388] * B[(i*56)+14];
C[(i*56)+21] += values[389] * B[(i*56)+14];
C[(i*56)+22] += values[390] * B[(i*56)+14];
C[(i*56)+25] += values[391] * B[(i*56)+14];
C[(i*56)+26] += values[392] * B[(i*56)+14];
C[(i*56)+35] += values[393] * B[(i*56)+14];
C[(i*56)+36] += values[394] * B[(i*56)+14];
C[(i*56)+37] += values[395] * B[(i*56)+14];
C[(i*56)+38] += values[396] * B[(i*56)+14];
C[(i*56)+41] += values[397] * B[(i*56)+14];
C[(i*56)+42] += values[398] * B[(i*56)+14];
C[(i*56)+43] += values[399] * B[(i*56)+14];
C[(i*56)+46] += values[400] * B[(i*56)+14];
C[(i*56)+47] += values[401] * B[(i*56)+14];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a15_0 = _mm256_loadu_pd(&values[402]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*56)+20], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a15_0 = _mm_loadu_pd(&values[402]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*56)+20], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a15_2 = _mm_loadu_pd(&values[404]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*56)+22], c15_2);
#endif
__m128d c15_4 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a15_4 = _mm_loadu_pd(&values[406]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, b15));
#endif
_mm_storeu_pd(&C[(i*56)+25], c15_4);
__m128d c15_6 = _mm_load_sd(&C[(i*56)+27]);
__m128d a15_6 = _mm_load_sd(&values[408]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*56)+27], c15_6);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_7 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a15_7 = _mm256_loadu_pd(&values[409]);
c15_7 = _mm256_add_pd(c15_7, _mm256_mul_pd(a15_7, b15));
_mm256_storeu_pd(&C[(i*56)+35], c15_7);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_7 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a15_7 = _mm_loadu_pd(&values[409]);
c15_7 = _mm_add_pd(c15_7, _mm_mul_pd(a15_7, b15));
_mm_storeu_pd(&C[(i*56)+35], c15_7);
__m128d c15_9 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a15_9 = _mm_loadu_pd(&values[411]);
c15_9 = _mm_add_pd(c15_9, _mm_mul_pd(a15_9, b15));
_mm_storeu_pd(&C[(i*56)+37], c15_9);
#endif
__m128d c15_11 = _mm_load_sd(&C[(i*56)+39]);
__m128d a15_11 = _mm_load_sd(&values[413]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_11 = _mm_add_sd(c15_11, _mm_mul_sd(a15_11, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_11 = _mm_add_sd(c15_11, _mm_mul_sd(a15_11, b15));
#endif
_mm_store_sd(&C[(i*56)+39], c15_11);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_12 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a15_12 = _mm256_loadu_pd(&values[414]);
c15_12 = _mm256_add_pd(c15_12, _mm256_mul_pd(a15_12, b15));
_mm256_storeu_pd(&C[(i*56)+41], c15_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_12 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a15_12 = _mm_loadu_pd(&values[414]);
c15_12 = _mm_add_pd(c15_12, _mm_mul_pd(a15_12, b15));
_mm_storeu_pd(&C[(i*56)+41], c15_12);
__m128d c15_14 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a15_14 = _mm_loadu_pd(&values[416]);
c15_14 = _mm_add_pd(c15_14, _mm_mul_pd(a15_14, b15));
_mm_storeu_pd(&C[(i*56)+43], c15_14);
#endif
__m128d c15_16 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a15_16 = _mm_loadu_pd(&values[418]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_16 = _mm_add_pd(c15_16, _mm_mul_pd(a15_16, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_16 = _mm_add_pd(c15_16, _mm_mul_pd(a15_16, b15));
#endif
_mm_storeu_pd(&C[(i*56)+46], c15_16);
__m128d c15_18 = _mm_load_sd(&C[(i*56)+48]);
__m128d a15_18 = _mm_load_sd(&values[420]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_18 = _mm_add_sd(c15_18, _mm_mul_sd(a15_18, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_18 = _mm_add_sd(c15_18, _mm_mul_sd(a15_18, b15));
#endif
_mm_store_sd(&C[(i*56)+48], c15_18);
#else
C[(i*56)+20] += values[402] * B[(i*56)+15];
C[(i*56)+21] += values[403] * B[(i*56)+15];
C[(i*56)+22] += values[404] * B[(i*56)+15];
C[(i*56)+23] += values[405] * B[(i*56)+15];
C[(i*56)+25] += values[406] * B[(i*56)+15];
C[(i*56)+26] += values[407] * B[(i*56)+15];
C[(i*56)+27] += values[408] * B[(i*56)+15];
C[(i*56)+35] += values[409] * B[(i*56)+15];
C[(i*56)+36] += values[410] * B[(i*56)+15];
C[(i*56)+37] += values[411] * B[(i*56)+15];
C[(i*56)+38] += values[412] * B[(i*56)+15];
C[(i*56)+39] += values[413] * B[(i*56)+15];
C[(i*56)+41] += values[414] * B[(i*56)+15];
C[(i*56)+42] += values[415] * B[(i*56)+15];
C[(i*56)+43] += values[416] * B[(i*56)+15];
C[(i*56)+44] += values[417] * B[(i*56)+15];
C[(i*56)+46] += values[418] * B[(i*56)+15];
C[(i*56)+47] += values[419] * B[(i*56)+15];
C[(i*56)+48] += values[420] * B[(i*56)+15];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*56)+16]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a16_0 = _mm256_loadu_pd(&values[421]);
c16_0 = _mm256_add_pd(c16_0, _mm256_mul_pd(a16_0, b16));
_mm256_storeu_pd(&C[(i*56)+20], c16_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a16_0 = _mm_loadu_pd(&values[421]);
c16_0 = _mm_add_pd(c16_0, _mm_mul_pd(a16_0, b16));
_mm_storeu_pd(&C[(i*56)+20], c16_0);
__m128d c16_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a16_2 = _mm_loadu_pd(&values[423]);
c16_2 = _mm_add_pd(c16_2, _mm_mul_pd(a16_2, b16));
_mm_storeu_pd(&C[(i*56)+22], c16_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_4 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a16_4 = _mm256_loadu_pd(&values[425]);
c16_4 = _mm256_add_pd(c16_4, _mm256_mul_pd(a16_4, b16));
_mm256_storeu_pd(&C[(i*56)+24], c16_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_4 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a16_4 = _mm_loadu_pd(&values[425]);
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, b16));
_mm_storeu_pd(&C[(i*56)+24], c16_4);
__m128d c16_6 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a16_6 = _mm_loadu_pd(&values[427]);
c16_6 = _mm_add_pd(c16_6, _mm_mul_pd(a16_6, b16));
_mm_storeu_pd(&C[(i*56)+26], c16_6);
#endif
__m128d c16_8 = _mm_load_sd(&C[(i*56)+28]);
__m128d a16_8 = _mm_load_sd(&values[429]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, b16));
#endif
_mm_store_sd(&C[(i*56)+28], c16_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_9 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a16_9 = _mm256_loadu_pd(&values[430]);
c16_9 = _mm256_add_pd(c16_9, _mm256_mul_pd(a16_9, b16));
_mm256_storeu_pd(&C[(i*56)+35], c16_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_9 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a16_9 = _mm_loadu_pd(&values[430]);
c16_9 = _mm_add_pd(c16_9, _mm_mul_pd(a16_9, b16));
_mm_storeu_pd(&C[(i*56)+35], c16_9);
__m128d c16_11 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a16_11 = _mm_loadu_pd(&values[432]);
c16_11 = _mm_add_pd(c16_11, _mm_mul_pd(a16_11, b16));
_mm_storeu_pd(&C[(i*56)+37], c16_11);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_13 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a16_13 = _mm256_loadu_pd(&values[434]);
c16_13 = _mm256_add_pd(c16_13, _mm256_mul_pd(a16_13, b16));
_mm256_storeu_pd(&C[(i*56)+39], c16_13);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_13 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a16_13 = _mm_loadu_pd(&values[434]);
c16_13 = _mm_add_pd(c16_13, _mm_mul_pd(a16_13, b16));
_mm_storeu_pd(&C[(i*56)+39], c16_13);
__m128d c16_15 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a16_15 = _mm_loadu_pd(&values[436]);
c16_15 = _mm_add_pd(c16_15, _mm_mul_pd(a16_15, b16));
_mm_storeu_pd(&C[(i*56)+41], c16_15);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_17 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a16_17 = _mm256_loadu_pd(&values[438]);
c16_17 = _mm256_add_pd(c16_17, _mm256_mul_pd(a16_17, b16));
_mm256_storeu_pd(&C[(i*56)+43], c16_17);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_17 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a16_17 = _mm_loadu_pd(&values[438]);
c16_17 = _mm_add_pd(c16_17, _mm_mul_pd(a16_17, b16));
_mm_storeu_pd(&C[(i*56)+43], c16_17);
__m128d c16_19 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a16_19 = _mm_loadu_pd(&values[440]);
c16_19 = _mm_add_pd(c16_19, _mm_mul_pd(a16_19, b16));
_mm_storeu_pd(&C[(i*56)+45], c16_19);
#endif
__m128d c16_21 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a16_21 = _mm_loadu_pd(&values[442]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_21 = _mm_add_pd(c16_21, _mm_mul_pd(a16_21, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_21 = _mm_add_pd(c16_21, _mm_mul_pd(a16_21, b16));
#endif
_mm_storeu_pd(&C[(i*56)+47], c16_21);
__m128d c16_23 = _mm_load_sd(&C[(i*56)+49]);
__m128d a16_23 = _mm_load_sd(&values[444]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_23 = _mm_add_sd(c16_23, _mm_mul_sd(a16_23, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_23 = _mm_add_sd(c16_23, _mm_mul_sd(a16_23, b16));
#endif
_mm_store_sd(&C[(i*56)+49], c16_23);
#else
C[(i*56)+20] += values[421] * B[(i*56)+16];
C[(i*56)+21] += values[422] * B[(i*56)+16];
C[(i*56)+22] += values[423] * B[(i*56)+16];
C[(i*56)+23] += values[424] * B[(i*56)+16];
C[(i*56)+24] += values[425] * B[(i*56)+16];
C[(i*56)+25] += values[426] * B[(i*56)+16];
C[(i*56)+26] += values[427] * B[(i*56)+16];
C[(i*56)+27] += values[428] * B[(i*56)+16];
C[(i*56)+28] += values[429] * B[(i*56)+16];
C[(i*56)+35] += values[430] * B[(i*56)+16];
C[(i*56)+36] += values[431] * B[(i*56)+16];
C[(i*56)+37] += values[432] * B[(i*56)+16];
C[(i*56)+38] += values[433] * B[(i*56)+16];
C[(i*56)+39] += values[434] * B[(i*56)+16];
C[(i*56)+40] += values[435] * B[(i*56)+16];
C[(i*56)+41] += values[436] * B[(i*56)+16];
C[(i*56)+42] += values[437] * B[(i*56)+16];
C[(i*56)+43] += values[438] * B[(i*56)+16];
C[(i*56)+44] += values[439] * B[(i*56)+16];
C[(i*56)+45] += values[440] * B[(i*56)+16];
C[(i*56)+46] += values[441] * B[(i*56)+16];
C[(i*56)+47] += values[442] * B[(i*56)+16];
C[(i*56)+48] += values[443] * B[(i*56)+16];
C[(i*56)+49] += values[444] * B[(i*56)+16];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a17_0 = _mm256_loadu_pd(&values[445]);
c17_0 = _mm256_add_pd(c17_0, _mm256_mul_pd(a17_0, b17));
_mm256_storeu_pd(&C[(i*56)+20], c17_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a17_0 = _mm_loadu_pd(&values[445]);
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, b17));
_mm_storeu_pd(&C[(i*56)+20], c17_0);
__m128d c17_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a17_2 = _mm_loadu_pd(&values[447]);
c17_2 = _mm_add_pd(c17_2, _mm_mul_pd(a17_2, b17));
_mm_storeu_pd(&C[(i*56)+22], c17_2);
#endif
__m128d c17_4 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a17_4 = _mm_loadu_pd(&values[449]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, b17));
#endif
_mm_storeu_pd(&C[(i*56)+25], c17_4);
__m128d c17_6 = _mm_load_sd(&C[(i*56)+27]);
__m128d a17_6 = _mm_load_sd(&values[451]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, b17));
#endif
_mm_store_sd(&C[(i*56)+27], c17_6);
__m128d c17_7 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a17_7 = _mm_loadu_pd(&values[452]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, b17));
#endif
_mm_storeu_pd(&C[(i*56)+29], c17_7);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_9 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a17_9 = _mm256_loadu_pd(&values[454]);
c17_9 = _mm256_add_pd(c17_9, _mm256_mul_pd(a17_9, b17));
_mm256_storeu_pd(&C[(i*56)+35], c17_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_9 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a17_9 = _mm_loadu_pd(&values[454]);
c17_9 = _mm_add_pd(c17_9, _mm_mul_pd(a17_9, b17));
_mm_storeu_pd(&C[(i*56)+35], c17_9);
__m128d c17_11 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a17_11 = _mm_loadu_pd(&values[456]);
c17_11 = _mm_add_pd(c17_11, _mm_mul_pd(a17_11, b17));
_mm_storeu_pd(&C[(i*56)+37], c17_11);
#endif
__m128d c17_13 = _mm_load_sd(&C[(i*56)+39]);
__m128d a17_13 = _mm_load_sd(&values[458]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_13 = _mm_add_sd(c17_13, _mm_mul_sd(a17_13, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_13 = _mm_add_sd(c17_13, _mm_mul_sd(a17_13, b17));
#endif
_mm_store_sd(&C[(i*56)+39], c17_13);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_14 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a17_14 = _mm256_loadu_pd(&values[459]);
c17_14 = _mm256_add_pd(c17_14, _mm256_mul_pd(a17_14, b17));
_mm256_storeu_pd(&C[(i*56)+41], c17_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_14 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a17_14 = _mm_loadu_pd(&values[459]);
c17_14 = _mm_add_pd(c17_14, _mm_mul_pd(a17_14, b17));
_mm_storeu_pd(&C[(i*56)+41], c17_14);
__m128d c17_16 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a17_16 = _mm_loadu_pd(&values[461]);
c17_16 = _mm_add_pd(c17_16, _mm_mul_pd(a17_16, b17));
_mm_storeu_pd(&C[(i*56)+43], c17_16);
#endif
__m128d c17_18 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a17_18 = _mm_loadu_pd(&values[463]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_18 = _mm_add_pd(c17_18, _mm_mul_pd(a17_18, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_18 = _mm_add_pd(c17_18, _mm_mul_pd(a17_18, b17));
#endif
_mm_storeu_pd(&C[(i*56)+46], c17_18);
__m128d c17_20 = _mm_load_sd(&C[(i*56)+48]);
__m128d a17_20 = _mm_load_sd(&values[465]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_20 = _mm_add_sd(c17_20, _mm_mul_sd(a17_20, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_20 = _mm_add_sd(c17_20, _mm_mul_sd(a17_20, b17));
#endif
_mm_store_sd(&C[(i*56)+48], c17_20);
__m128d c17_21 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a17_21 = _mm_loadu_pd(&values[466]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_21 = _mm_add_pd(c17_21, _mm_mul_pd(a17_21, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_21 = _mm_add_pd(c17_21, _mm_mul_pd(a17_21, b17));
#endif
_mm_storeu_pd(&C[(i*56)+50], c17_21);
#else
C[(i*56)+20] += values[445] * B[(i*56)+17];
C[(i*56)+21] += values[446] * B[(i*56)+17];
C[(i*56)+22] += values[447] * B[(i*56)+17];
C[(i*56)+23] += values[448] * B[(i*56)+17];
C[(i*56)+25] += values[449] * B[(i*56)+17];
C[(i*56)+26] += values[450] * B[(i*56)+17];
C[(i*56)+27] += values[451] * B[(i*56)+17];
C[(i*56)+29] += values[452] * B[(i*56)+17];
C[(i*56)+30] += values[453] * B[(i*56)+17];
C[(i*56)+35] += values[454] * B[(i*56)+17];
C[(i*56)+36] += values[455] * B[(i*56)+17];
C[(i*56)+37] += values[456] * B[(i*56)+17];
C[(i*56)+38] += values[457] * B[(i*56)+17];
C[(i*56)+39] += values[458] * B[(i*56)+17];
C[(i*56)+41] += values[459] * B[(i*56)+17];
C[(i*56)+42] += values[460] * B[(i*56)+17];
C[(i*56)+43] += values[461] * B[(i*56)+17];
C[(i*56)+44] += values[462] * B[(i*56)+17];
C[(i*56)+46] += values[463] * B[(i*56)+17];
C[(i*56)+47] += values[464] * B[(i*56)+17];
C[(i*56)+48] += values[465] * B[(i*56)+17];
C[(i*56)+50] += values[466] * B[(i*56)+17];
C[(i*56)+51] += values[467] * B[(i*56)+17];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*56)+18]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a18_0 = _mm256_loadu_pd(&values[468]);
c18_0 = _mm256_add_pd(c18_0, _mm256_mul_pd(a18_0, b18));
_mm256_storeu_pd(&C[(i*56)+20], c18_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a18_0 = _mm_loadu_pd(&values[468]);
c18_0 = _mm_add_pd(c18_0, _mm_mul_pd(a18_0, b18));
_mm_storeu_pd(&C[(i*56)+20], c18_0);
__m128d c18_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a18_2 = _mm_loadu_pd(&values[470]);
c18_2 = _mm_add_pd(c18_2, _mm_mul_pd(a18_2, b18));
_mm_storeu_pd(&C[(i*56)+22], c18_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_4 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a18_4 = _mm256_loadu_pd(&values[472]);
c18_4 = _mm256_add_pd(c18_4, _mm256_mul_pd(a18_4, b18));
_mm256_storeu_pd(&C[(i*56)+24], c18_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_4 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a18_4 = _mm_loadu_pd(&values[472]);
c18_4 = _mm_add_pd(c18_4, _mm_mul_pd(a18_4, b18));
_mm_storeu_pd(&C[(i*56)+24], c18_4);
__m128d c18_6 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a18_6 = _mm_loadu_pd(&values[474]);
c18_6 = _mm_add_pd(c18_6, _mm_mul_pd(a18_6, b18));
_mm_storeu_pd(&C[(i*56)+26], c18_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_8 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a18_8 = _mm256_loadu_pd(&values[476]);
c18_8 = _mm256_add_pd(c18_8, _mm256_mul_pd(a18_8, b18));
_mm256_storeu_pd(&C[(i*56)+28], c18_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_8 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a18_8 = _mm_loadu_pd(&values[476]);
c18_8 = _mm_add_pd(c18_8, _mm_mul_pd(a18_8, b18));
_mm_storeu_pd(&C[(i*56)+28], c18_8);
__m128d c18_10 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a18_10 = _mm_loadu_pd(&values[478]);
c18_10 = _mm_add_pd(c18_10, _mm_mul_pd(a18_10, b18));
_mm_storeu_pd(&C[(i*56)+30], c18_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_12 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a18_12 = _mm256_loadu_pd(&values[480]);
c18_12 = _mm256_add_pd(c18_12, _mm256_mul_pd(a18_12, b18));
_mm256_storeu_pd(&C[(i*56)+35], c18_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_12 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a18_12 = _mm_loadu_pd(&values[480]);
c18_12 = _mm_add_pd(c18_12, _mm_mul_pd(a18_12, b18));
_mm_storeu_pd(&C[(i*56)+35], c18_12);
__m128d c18_14 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a18_14 = _mm_loadu_pd(&values[482]);
c18_14 = _mm_add_pd(c18_14, _mm_mul_pd(a18_14, b18));
_mm_storeu_pd(&C[(i*56)+37], c18_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_16 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a18_16 = _mm256_loadu_pd(&values[484]);
c18_16 = _mm256_add_pd(c18_16, _mm256_mul_pd(a18_16, b18));
_mm256_storeu_pd(&C[(i*56)+39], c18_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_16 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a18_16 = _mm_loadu_pd(&values[484]);
c18_16 = _mm_add_pd(c18_16, _mm_mul_pd(a18_16, b18));
_mm_storeu_pd(&C[(i*56)+39], c18_16);
__m128d c18_18 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a18_18 = _mm_loadu_pd(&values[486]);
c18_18 = _mm_add_pd(c18_18, _mm_mul_pd(a18_18, b18));
_mm_storeu_pd(&C[(i*56)+41], c18_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_20 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a18_20 = _mm256_loadu_pd(&values[488]);
c18_20 = _mm256_add_pd(c18_20, _mm256_mul_pd(a18_20, b18));
_mm256_storeu_pd(&C[(i*56)+43], c18_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_20 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a18_20 = _mm_loadu_pd(&values[488]);
c18_20 = _mm_add_pd(c18_20, _mm_mul_pd(a18_20, b18));
_mm_storeu_pd(&C[(i*56)+43], c18_20);
__m128d c18_22 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a18_22 = _mm_loadu_pd(&values[490]);
c18_22 = _mm_add_pd(c18_22, _mm_mul_pd(a18_22, b18));
_mm_storeu_pd(&C[(i*56)+45], c18_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_24 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a18_24 = _mm256_loadu_pd(&values[492]);
c18_24 = _mm256_add_pd(c18_24, _mm256_mul_pd(a18_24, b18));
_mm256_storeu_pd(&C[(i*56)+47], c18_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_24 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a18_24 = _mm_loadu_pd(&values[492]);
c18_24 = _mm_add_pd(c18_24, _mm_mul_pd(a18_24, b18));
_mm_storeu_pd(&C[(i*56)+47], c18_24);
__m128d c18_26 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a18_26 = _mm_loadu_pd(&values[494]);
c18_26 = _mm_add_pd(c18_26, _mm_mul_pd(a18_26, b18));
_mm_storeu_pd(&C[(i*56)+49], c18_26);
#endif
__m128d c18_28 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a18_28 = _mm_loadu_pd(&values[496]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_28 = _mm_add_pd(c18_28, _mm_mul_pd(a18_28, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_28 = _mm_add_pd(c18_28, _mm_mul_pd(a18_28, b18));
#endif
_mm_storeu_pd(&C[(i*56)+51], c18_28);
#else
C[(i*56)+20] += values[468] * B[(i*56)+18];
C[(i*56)+21] += values[469] * B[(i*56)+18];
C[(i*56)+22] += values[470] * B[(i*56)+18];
C[(i*56)+23] += values[471] * B[(i*56)+18];
C[(i*56)+24] += values[472] * B[(i*56)+18];
C[(i*56)+25] += values[473] * B[(i*56)+18];
C[(i*56)+26] += values[474] * B[(i*56)+18];
C[(i*56)+27] += values[475] * B[(i*56)+18];
C[(i*56)+28] += values[476] * B[(i*56)+18];
C[(i*56)+29] += values[477] * B[(i*56)+18];
C[(i*56)+30] += values[478] * B[(i*56)+18];
C[(i*56)+31] += values[479] * B[(i*56)+18];
C[(i*56)+35] += values[480] * B[(i*56)+18];
C[(i*56)+36] += values[481] * B[(i*56)+18];
C[(i*56)+37] += values[482] * B[(i*56)+18];
C[(i*56)+38] += values[483] * B[(i*56)+18];
C[(i*56)+39] += values[484] * B[(i*56)+18];
C[(i*56)+40] += values[485] * B[(i*56)+18];
C[(i*56)+41] += values[486] * B[(i*56)+18];
C[(i*56)+42] += values[487] * B[(i*56)+18];
C[(i*56)+43] += values[488] * B[(i*56)+18];
C[(i*56)+44] += values[489] * B[(i*56)+18];
C[(i*56)+45] += values[490] * B[(i*56)+18];
C[(i*56)+46] += values[491] * B[(i*56)+18];
C[(i*56)+47] += values[492] * B[(i*56)+18];
C[(i*56)+48] += values[493] * B[(i*56)+18];
C[(i*56)+49] += values[494] * B[(i*56)+18];
C[(i*56)+50] += values[495] * B[(i*56)+18];
C[(i*56)+51] += values[496] * B[(i*56)+18];
C[(i*56)+52] += values[497] * B[(i*56)+18];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*56)+19]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a19_0 = _mm256_loadu_pd(&values[498]);
c19_0 = _mm256_add_pd(c19_0, _mm256_mul_pd(a19_0, b19));
_mm256_storeu_pd(&C[(i*56)+20], c19_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a19_0 = _mm_loadu_pd(&values[498]);
c19_0 = _mm_add_pd(c19_0, _mm_mul_pd(a19_0, b19));
_mm_storeu_pd(&C[(i*56)+20], c19_0);
__m128d c19_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a19_2 = _mm_loadu_pd(&values[500]);
c19_2 = _mm_add_pd(c19_2, _mm_mul_pd(a19_2, b19));
_mm_storeu_pd(&C[(i*56)+22], c19_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_4 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a19_4 = _mm256_loadu_pd(&values[502]);
c19_4 = _mm256_add_pd(c19_4, _mm256_mul_pd(a19_4, b19));
_mm256_storeu_pd(&C[(i*56)+24], c19_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_4 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a19_4 = _mm_loadu_pd(&values[502]);
c19_4 = _mm_add_pd(c19_4, _mm_mul_pd(a19_4, b19));
_mm_storeu_pd(&C[(i*56)+24], c19_4);
__m128d c19_6 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a19_6 = _mm_loadu_pd(&values[504]);
c19_6 = _mm_add_pd(c19_6, _mm_mul_pd(a19_6, b19));
_mm_storeu_pd(&C[(i*56)+26], c19_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_8 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a19_8 = _mm256_loadu_pd(&values[506]);
c19_8 = _mm256_add_pd(c19_8, _mm256_mul_pd(a19_8, b19));
_mm256_storeu_pd(&C[(i*56)+28], c19_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_8 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a19_8 = _mm_loadu_pd(&values[506]);
c19_8 = _mm_add_pd(c19_8, _mm_mul_pd(a19_8, b19));
_mm_storeu_pd(&C[(i*56)+28], c19_8);
__m128d c19_10 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a19_10 = _mm_loadu_pd(&values[508]);
c19_10 = _mm_add_pd(c19_10, _mm_mul_pd(a19_10, b19));
_mm_storeu_pd(&C[(i*56)+30], c19_10);
#endif
__m128d c19_12 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a19_12 = _mm_loadu_pd(&values[510]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_12 = _mm_add_pd(c19_12, _mm_mul_pd(a19_12, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_12 = _mm_add_pd(c19_12, _mm_mul_pd(a19_12, b19));
#endif
_mm_storeu_pd(&C[(i*56)+32], c19_12);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_14 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a19_14 = _mm256_loadu_pd(&values[512]);
c19_14 = _mm256_add_pd(c19_14, _mm256_mul_pd(a19_14, b19));
_mm256_storeu_pd(&C[(i*56)+35], c19_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_14 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a19_14 = _mm_loadu_pd(&values[512]);
c19_14 = _mm_add_pd(c19_14, _mm_mul_pd(a19_14, b19));
_mm_storeu_pd(&C[(i*56)+35], c19_14);
__m128d c19_16 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a19_16 = _mm_loadu_pd(&values[514]);
c19_16 = _mm_add_pd(c19_16, _mm_mul_pd(a19_16, b19));
_mm_storeu_pd(&C[(i*56)+37], c19_16);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_18 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a19_18 = _mm256_loadu_pd(&values[516]);
c19_18 = _mm256_add_pd(c19_18, _mm256_mul_pd(a19_18, b19));
_mm256_storeu_pd(&C[(i*56)+39], c19_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_18 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a19_18 = _mm_loadu_pd(&values[516]);
c19_18 = _mm_add_pd(c19_18, _mm_mul_pd(a19_18, b19));
_mm_storeu_pd(&C[(i*56)+39], c19_18);
__m128d c19_20 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a19_20 = _mm_loadu_pd(&values[518]);
c19_20 = _mm_add_pd(c19_20, _mm_mul_pd(a19_20, b19));
_mm_storeu_pd(&C[(i*56)+41], c19_20);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_22 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a19_22 = _mm256_loadu_pd(&values[520]);
c19_22 = _mm256_add_pd(c19_22, _mm256_mul_pd(a19_22, b19));
_mm256_storeu_pd(&C[(i*56)+43], c19_22);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_22 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a19_22 = _mm_loadu_pd(&values[520]);
c19_22 = _mm_add_pd(c19_22, _mm_mul_pd(a19_22, b19));
_mm_storeu_pd(&C[(i*56)+43], c19_22);
__m128d c19_24 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a19_24 = _mm_loadu_pd(&values[522]);
c19_24 = _mm_add_pd(c19_24, _mm_mul_pd(a19_24, b19));
_mm_storeu_pd(&C[(i*56)+45], c19_24);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_26 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a19_26 = _mm256_loadu_pd(&values[524]);
c19_26 = _mm256_add_pd(c19_26, _mm256_mul_pd(a19_26, b19));
_mm256_storeu_pd(&C[(i*56)+47], c19_26);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_26 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a19_26 = _mm_loadu_pd(&values[524]);
c19_26 = _mm_add_pd(c19_26, _mm_mul_pd(a19_26, b19));
_mm_storeu_pd(&C[(i*56)+47], c19_26);
__m128d c19_28 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a19_28 = _mm_loadu_pd(&values[526]);
c19_28 = _mm_add_pd(c19_28, _mm_mul_pd(a19_28, b19));
_mm_storeu_pd(&C[(i*56)+49], c19_28);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_30 = _mm256_loadu_pd(&C[(i*56)+51]);
__m256d a19_30 = _mm256_loadu_pd(&values[528]);
c19_30 = _mm256_add_pd(c19_30, _mm256_mul_pd(a19_30, b19));
_mm256_storeu_pd(&C[(i*56)+51], c19_30);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_30 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a19_30 = _mm_loadu_pd(&values[528]);
c19_30 = _mm_add_pd(c19_30, _mm_mul_pd(a19_30, b19));
_mm_storeu_pd(&C[(i*56)+51], c19_30);
__m128d c19_32 = _mm_loadu_pd(&C[(i*56)+53]);
__m128d a19_32 = _mm_loadu_pd(&values[530]);
c19_32 = _mm_add_pd(c19_32, _mm_mul_pd(a19_32, b19));
_mm_storeu_pd(&C[(i*56)+53], c19_32);
#endif
#else
C[(i*56)+20] += values[498] * B[(i*56)+19];
C[(i*56)+21] += values[499] * B[(i*56)+19];
C[(i*56)+22] += values[500] * B[(i*56)+19];
C[(i*56)+23] += values[501] * B[(i*56)+19];
C[(i*56)+24] += values[502] * B[(i*56)+19];
C[(i*56)+25] += values[503] * B[(i*56)+19];
C[(i*56)+26] += values[504] * B[(i*56)+19];
C[(i*56)+27] += values[505] * B[(i*56)+19];
C[(i*56)+28] += values[506] * B[(i*56)+19];
C[(i*56)+29] += values[507] * B[(i*56)+19];
C[(i*56)+30] += values[508] * B[(i*56)+19];
C[(i*56)+31] += values[509] * B[(i*56)+19];
C[(i*56)+32] += values[510] * B[(i*56)+19];
C[(i*56)+33] += values[511] * B[(i*56)+19];
C[(i*56)+35] += values[512] * B[(i*56)+19];
C[(i*56)+36] += values[513] * B[(i*56)+19];
C[(i*56)+37] += values[514] * B[(i*56)+19];
C[(i*56)+38] += values[515] * B[(i*56)+19];
C[(i*56)+39] += values[516] * B[(i*56)+19];
C[(i*56)+40] += values[517] * B[(i*56)+19];
C[(i*56)+41] += values[518] * B[(i*56)+19];
C[(i*56)+42] += values[519] * B[(i*56)+19];
C[(i*56)+43] += values[520] * B[(i*56)+19];
C[(i*56)+44] += values[521] * B[(i*56)+19];
C[(i*56)+45] += values[522] * B[(i*56)+19];
C[(i*56)+46] += values[523] * B[(i*56)+19];
C[(i*56)+47] += values[524] * B[(i*56)+19];
C[(i*56)+48] += values[525] * B[(i*56)+19];
C[(i*56)+49] += values[526] * B[(i*56)+19];
C[(i*56)+50] += values[527] * B[(i*56)+19];
C[(i*56)+51] += values[528] * B[(i*56)+19];
C[(i*56)+52] += values[529] * B[(i*56)+19];
C[(i*56)+53] += values[530] * B[(i*56)+19];
C[(i*56)+54] += values[531] * B[(i*56)+19];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*56)+20]);
#endif
__m128d c20_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a20_0 = _mm_loadu_pd(&values[532]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, b20));
#endif
_mm_storeu_pd(&C[(i*56)+35], c20_0);
#else
C[(i*56)+35] += values[532] * B[(i*56)+20];
C[(i*56)+36] += values[533] * B[(i*56)+20];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*56)+21]);
#endif
__m128d c21_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a21_0 = _mm_loadu_pd(&values[534]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, b21));
#endif
_mm_storeu_pd(&C[(i*56)+35], c21_0);
__m128d c21_2 = _mm_load_sd(&C[(i*56)+37]);
__m128d a21_2 = _mm_load_sd(&values[536]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_2 = _mm_add_sd(c21_2, _mm_mul_sd(a21_2, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_2 = _mm_add_sd(c21_2, _mm_mul_sd(a21_2, b21));
#endif
_mm_store_sd(&C[(i*56)+37], c21_2);
#else
C[(i*56)+35] += values[534] * B[(i*56)+21];
C[(i*56)+36] += values[535] * B[(i*56)+21];
C[(i*56)+37] += values[536] * B[(i*56)+21];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a22_0 = _mm256_loadu_pd(&values[537]);
c22_0 = _mm256_add_pd(c22_0, _mm256_mul_pd(a22_0, b22));
_mm256_storeu_pd(&C[(i*56)+35], c22_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a22_0 = _mm_loadu_pd(&values[537]);
c22_0 = _mm_add_pd(c22_0, _mm_mul_pd(a22_0, b22));
_mm_storeu_pd(&C[(i*56)+35], c22_0);
__m128d c22_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a22_2 = _mm_loadu_pd(&values[539]);
c22_2 = _mm_add_pd(c22_2, _mm_mul_pd(a22_2, b22));
_mm_storeu_pd(&C[(i*56)+37], c22_2);
#endif
#else
C[(i*56)+35] += values[537] * B[(i*56)+22];
C[(i*56)+36] += values[538] * B[(i*56)+22];
C[(i*56)+37] += values[539] * B[(i*56)+22];
C[(i*56)+38] += values[540] * B[(i*56)+22];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a23_0 = _mm256_loadu_pd(&values[541]);
c23_0 = _mm256_add_pd(c23_0, _mm256_mul_pd(a23_0, b23));
_mm256_storeu_pd(&C[(i*56)+35], c23_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a23_0 = _mm_loadu_pd(&values[541]);
c23_0 = _mm_add_pd(c23_0, _mm_mul_pd(a23_0, b23));
_mm_storeu_pd(&C[(i*56)+35], c23_0);
__m128d c23_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a23_2 = _mm_loadu_pd(&values[543]);
c23_2 = _mm_add_pd(c23_2, _mm_mul_pd(a23_2, b23));
_mm_storeu_pd(&C[(i*56)+37], c23_2);
#endif
__m128d c23_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a23_4 = _mm_load_sd(&values[545]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_4 = _mm_add_sd(c23_4, _mm_mul_sd(a23_4, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_4 = _mm_add_sd(c23_4, _mm_mul_sd(a23_4, b23));
#endif
_mm_store_sd(&C[(i*56)+39], c23_4);
#else
C[(i*56)+35] += values[541] * B[(i*56)+23];
C[(i*56)+36] += values[542] * B[(i*56)+23];
C[(i*56)+37] += values[543] * B[(i*56)+23];
C[(i*56)+38] += values[544] * B[(i*56)+23];
C[(i*56)+39] += values[545] * B[(i*56)+23];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b24 = _mm256_broadcast_sd(&B[(i*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b24 = _mm_loaddup_pd(&B[(i*56)+24]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c24_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a24_0 = _mm256_loadu_pd(&values[546]);
c24_0 = _mm256_add_pd(c24_0, _mm256_mul_pd(a24_0, b24));
_mm256_storeu_pd(&C[(i*56)+35], c24_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c24_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a24_0 = _mm_loadu_pd(&values[546]);
c24_0 = _mm_add_pd(c24_0, _mm_mul_pd(a24_0, b24));
_mm_storeu_pd(&C[(i*56)+35], c24_0);
__m128d c24_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a24_2 = _mm_loadu_pd(&values[548]);
c24_2 = _mm_add_pd(c24_2, _mm_mul_pd(a24_2, b24));
_mm_storeu_pd(&C[(i*56)+37], c24_2);
#endif
__m128d c24_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a24_4 = _mm_loadu_pd(&values[550]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, b24));
#endif
_mm_storeu_pd(&C[(i*56)+39], c24_4);
#else
C[(i*56)+35] += values[546] * B[(i*56)+24];
C[(i*56)+36] += values[547] * B[(i*56)+24];
C[(i*56)+37] += values[548] * B[(i*56)+24];
C[(i*56)+38] += values[549] * B[(i*56)+24];
C[(i*56)+39] += values[550] * B[(i*56)+24];
C[(i*56)+40] += values[551] * B[(i*56)+24];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*56)+25]);
#endif
__m128d c25_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a25_0 = _mm_loadu_pd(&values[552]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, b25));
#endif
_mm_storeu_pd(&C[(i*56)+35], c25_0);
__m128d c25_2 = _mm_load_sd(&C[(i*56)+37]);
__m128d a25_2 = _mm_load_sd(&values[554]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_2 = _mm_add_sd(c25_2, _mm_mul_sd(a25_2, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_2 = _mm_add_sd(c25_2, _mm_mul_sd(a25_2, b25));
#endif
_mm_store_sd(&C[(i*56)+37], c25_2);
__m128d c25_3 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a25_3 = _mm_loadu_pd(&values[555]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_3 = _mm_add_pd(c25_3, _mm_mul_pd(a25_3, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_3 = _mm_add_pd(c25_3, _mm_mul_pd(a25_3, b25));
#endif
_mm_storeu_pd(&C[(i*56)+41], c25_3);
#else
C[(i*56)+35] += values[552] * B[(i*56)+25];
C[(i*56)+36] += values[553] * B[(i*56)+25];
C[(i*56)+37] += values[554] * B[(i*56)+25];
C[(i*56)+41] += values[555] * B[(i*56)+25];
C[(i*56)+42] += values[556] * B[(i*56)+25];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a26_0 = _mm256_loadu_pd(&values[557]);
c26_0 = _mm256_add_pd(c26_0, _mm256_mul_pd(a26_0, b26));
_mm256_storeu_pd(&C[(i*56)+35], c26_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a26_0 = _mm_loadu_pd(&values[557]);
c26_0 = _mm_add_pd(c26_0, _mm_mul_pd(a26_0, b26));
_mm_storeu_pd(&C[(i*56)+35], c26_0);
__m128d c26_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a26_2 = _mm_loadu_pd(&values[559]);
c26_2 = _mm_add_pd(c26_2, _mm_mul_pd(a26_2, b26));
_mm_storeu_pd(&C[(i*56)+37], c26_2);
#endif
__m128d c26_4 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a26_4 = _mm_loadu_pd(&values[561]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, b26));
#endif
_mm_storeu_pd(&C[(i*56)+41], c26_4);
__m128d c26_6 = _mm_load_sd(&C[(i*56)+43]);
__m128d a26_6 = _mm_load_sd(&values[563]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_6 = _mm_add_sd(c26_6, _mm_mul_sd(a26_6, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_6 = _mm_add_sd(c26_6, _mm_mul_sd(a26_6, b26));
#endif
_mm_store_sd(&C[(i*56)+43], c26_6);
#else
C[(i*56)+35] += values[557] * B[(i*56)+26];
C[(i*56)+36] += values[558] * B[(i*56)+26];
C[(i*56)+37] += values[559] * B[(i*56)+26];
C[(i*56)+38] += values[560] * B[(i*56)+26];
C[(i*56)+41] += values[561] * B[(i*56)+26];
C[(i*56)+42] += values[562] * B[(i*56)+26];
C[(i*56)+43] += values[563] * B[(i*56)+26];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a27_0 = _mm256_loadu_pd(&values[564]);
c27_0 = _mm256_add_pd(c27_0, _mm256_mul_pd(a27_0, b27));
_mm256_storeu_pd(&C[(i*56)+35], c27_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a27_0 = _mm_loadu_pd(&values[564]);
c27_0 = _mm_add_pd(c27_0, _mm_mul_pd(a27_0, b27));
_mm_storeu_pd(&C[(i*56)+35], c27_0);
__m128d c27_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a27_2 = _mm_loadu_pd(&values[566]);
c27_2 = _mm_add_pd(c27_2, _mm_mul_pd(a27_2, b27));
_mm_storeu_pd(&C[(i*56)+37], c27_2);
#endif
__m128d c27_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a27_4 = _mm_load_sd(&values[568]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_4 = _mm_add_sd(c27_4, _mm_mul_sd(a27_4, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_4 = _mm_add_sd(c27_4, _mm_mul_sd(a27_4, b27));
#endif
_mm_store_sd(&C[(i*56)+39], c27_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_5 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a27_5 = _mm256_loadu_pd(&values[569]);
c27_5 = _mm256_add_pd(c27_5, _mm256_mul_pd(a27_5, b27));
_mm256_storeu_pd(&C[(i*56)+41], c27_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_5 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a27_5 = _mm_loadu_pd(&values[569]);
c27_5 = _mm_add_pd(c27_5, _mm_mul_pd(a27_5, b27));
_mm_storeu_pd(&C[(i*56)+41], c27_5);
__m128d c27_7 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a27_7 = _mm_loadu_pd(&values[571]);
c27_7 = _mm_add_pd(c27_7, _mm_mul_pd(a27_7, b27));
_mm_storeu_pd(&C[(i*56)+43], c27_7);
#endif
#else
C[(i*56)+35] += values[564] * B[(i*56)+27];
C[(i*56)+36] += values[565] * B[(i*56)+27];
C[(i*56)+37] += values[566] * B[(i*56)+27];
C[(i*56)+38] += values[567] * B[(i*56)+27];
C[(i*56)+39] += values[568] * B[(i*56)+27];
C[(i*56)+41] += values[569] * B[(i*56)+27];
C[(i*56)+42] += values[570] * B[(i*56)+27];
C[(i*56)+43] += values[571] * B[(i*56)+27];
C[(i*56)+44] += values[572] * B[(i*56)+27];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b28 = _mm256_broadcast_sd(&B[(i*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b28 = _mm_loaddup_pd(&B[(i*56)+28]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c28_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a28_0 = _mm256_loadu_pd(&values[573]);
c28_0 = _mm256_add_pd(c28_0, _mm256_mul_pd(a28_0, b28));
_mm256_storeu_pd(&C[(i*56)+35], c28_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c28_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a28_0 = _mm_loadu_pd(&values[573]);
c28_0 = _mm_add_pd(c28_0, _mm_mul_pd(a28_0, b28));
_mm_storeu_pd(&C[(i*56)+35], c28_0);
__m128d c28_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a28_2 = _mm_loadu_pd(&values[575]);
c28_2 = _mm_add_pd(c28_2, _mm_mul_pd(a28_2, b28));
_mm_storeu_pd(&C[(i*56)+37], c28_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c28_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a28_4 = _mm256_loadu_pd(&values[577]);
c28_4 = _mm256_add_pd(c28_4, _mm256_mul_pd(a28_4, b28));
_mm256_storeu_pd(&C[(i*56)+39], c28_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c28_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a28_4 = _mm_loadu_pd(&values[577]);
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, b28));
_mm_storeu_pd(&C[(i*56)+39], c28_4);
__m128d c28_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a28_6 = _mm_loadu_pd(&values[579]);
c28_6 = _mm_add_pd(c28_6, _mm_mul_pd(a28_6, b28));
_mm_storeu_pd(&C[(i*56)+41], c28_6);
#endif
__m128d c28_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a28_8 = _mm_loadu_pd(&values[581]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, b28));
#endif
_mm_storeu_pd(&C[(i*56)+43], c28_8);
__m128d c28_10 = _mm_load_sd(&C[(i*56)+45]);
__m128d a28_10 = _mm_load_sd(&values[583]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_10 = _mm_add_sd(c28_10, _mm_mul_sd(a28_10, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_10 = _mm_add_sd(c28_10, _mm_mul_sd(a28_10, b28));
#endif
_mm_store_sd(&C[(i*56)+45], c28_10);
#else
C[(i*56)+35] += values[573] * B[(i*56)+28];
C[(i*56)+36] += values[574] * B[(i*56)+28];
C[(i*56)+37] += values[575] * B[(i*56)+28];
C[(i*56)+38] += values[576] * B[(i*56)+28];
C[(i*56)+39] += values[577] * B[(i*56)+28];
C[(i*56)+40] += values[578] * B[(i*56)+28];
C[(i*56)+41] += values[579] * B[(i*56)+28];
C[(i*56)+42] += values[580] * B[(i*56)+28];
C[(i*56)+43] += values[581] * B[(i*56)+28];
C[(i*56)+44] += values[582] * B[(i*56)+28];
C[(i*56)+45] += values[583] * B[(i*56)+28];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c29_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a29_0 = _mm256_loadu_pd(&values[584]);
c29_0 = _mm256_add_pd(c29_0, _mm256_mul_pd(a29_0, b29));
_mm256_storeu_pd(&C[(i*56)+35], c29_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c29_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a29_0 = _mm_loadu_pd(&values[584]);
c29_0 = _mm_add_pd(c29_0, _mm_mul_pd(a29_0, b29));
_mm_storeu_pd(&C[(i*56)+35], c29_0);
__m128d c29_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a29_2 = _mm_loadu_pd(&values[586]);
c29_2 = _mm_add_pd(c29_2, _mm_mul_pd(a29_2, b29));
_mm_storeu_pd(&C[(i*56)+37], c29_2);
#endif
__m128d c29_4 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a29_4 = _mm_loadu_pd(&values[588]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, b29));
#endif
_mm_storeu_pd(&C[(i*56)+41], c29_4);
__m128d c29_6 = _mm_load_sd(&C[(i*56)+43]);
__m128d a29_6 = _mm_load_sd(&values[590]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, b29));
#endif
_mm_store_sd(&C[(i*56)+43], c29_6);
__m128d c29_7 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a29_7 = _mm_loadu_pd(&values[591]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, b29));
#endif
_mm_storeu_pd(&C[(i*56)+46], c29_7);
#else
C[(i*56)+35] += values[584] * B[(i*56)+29];
C[(i*56)+36] += values[585] * B[(i*56)+29];
C[(i*56)+37] += values[586] * B[(i*56)+29];
C[(i*56)+38] += values[587] * B[(i*56)+29];
C[(i*56)+41] += values[588] * B[(i*56)+29];
C[(i*56)+42] += values[589] * B[(i*56)+29];
C[(i*56)+43] += values[590] * B[(i*56)+29];
C[(i*56)+46] += values[591] * B[(i*56)+29];
C[(i*56)+47] += values[592] * B[(i*56)+29];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a30_0 = _mm256_loadu_pd(&values[593]);
c30_0 = _mm256_add_pd(c30_0, _mm256_mul_pd(a30_0, b30));
_mm256_storeu_pd(&C[(i*56)+35], c30_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a30_0 = _mm_loadu_pd(&values[593]);
c30_0 = _mm_add_pd(c30_0, _mm_mul_pd(a30_0, b30));
_mm_storeu_pd(&C[(i*56)+35], c30_0);
__m128d c30_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a30_2 = _mm_loadu_pd(&values[595]);
c30_2 = _mm_add_pd(c30_2, _mm_mul_pd(a30_2, b30));
_mm_storeu_pd(&C[(i*56)+37], c30_2);
#endif
__m128d c30_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a30_4 = _mm_load_sd(&values[597]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, b30));
#endif
_mm_store_sd(&C[(i*56)+39], c30_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_5 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a30_5 = _mm256_loadu_pd(&values[598]);
c30_5 = _mm256_add_pd(c30_5, _mm256_mul_pd(a30_5, b30));
_mm256_storeu_pd(&C[(i*56)+41], c30_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_5 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a30_5 = _mm_loadu_pd(&values[598]);
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, b30));
_mm_storeu_pd(&C[(i*56)+41], c30_5);
__m128d c30_7 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a30_7 = _mm_loadu_pd(&values[600]);
c30_7 = _mm_add_pd(c30_7, _mm_mul_pd(a30_7, b30));
_mm_storeu_pd(&C[(i*56)+43], c30_7);
#endif
__m128d c30_9 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a30_9 = _mm_loadu_pd(&values[602]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, b30));
#endif
_mm_storeu_pd(&C[(i*56)+46], c30_9);
__m128d c30_11 = _mm_load_sd(&C[(i*56)+48]);
__m128d a30_11 = _mm_load_sd(&values[604]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, b30));
#endif
_mm_store_sd(&C[(i*56)+48], c30_11);
#else
C[(i*56)+35] += values[593] * B[(i*56)+30];
C[(i*56)+36] += values[594] * B[(i*56)+30];
C[(i*56)+37] += values[595] * B[(i*56)+30];
C[(i*56)+38] += values[596] * B[(i*56)+30];
C[(i*56)+39] += values[597] * B[(i*56)+30];
C[(i*56)+41] += values[598] * B[(i*56)+30];
C[(i*56)+42] += values[599] * B[(i*56)+30];
C[(i*56)+43] += values[600] * B[(i*56)+30];
C[(i*56)+44] += values[601] * B[(i*56)+30];
C[(i*56)+46] += values[602] * B[(i*56)+30];
C[(i*56)+47] += values[603] * B[(i*56)+30];
C[(i*56)+48] += values[604] * B[(i*56)+30];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b31 = _mm256_broadcast_sd(&B[(i*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b31 = _mm_loaddup_pd(&B[(i*56)+31]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c31_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a31_0 = _mm256_loadu_pd(&values[605]);
c31_0 = _mm256_add_pd(c31_0, _mm256_mul_pd(a31_0, b31));
_mm256_storeu_pd(&C[(i*56)+35], c31_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c31_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a31_0 = _mm_loadu_pd(&values[605]);
c31_0 = _mm_add_pd(c31_0, _mm_mul_pd(a31_0, b31));
_mm_storeu_pd(&C[(i*56)+35], c31_0);
__m128d c31_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a31_2 = _mm_loadu_pd(&values[607]);
c31_2 = _mm_add_pd(c31_2, _mm_mul_pd(a31_2, b31));
_mm_storeu_pd(&C[(i*56)+37], c31_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c31_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a31_4 = _mm256_loadu_pd(&values[609]);
c31_4 = _mm256_add_pd(c31_4, _mm256_mul_pd(a31_4, b31));
_mm256_storeu_pd(&C[(i*56)+39], c31_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c31_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a31_4 = _mm_loadu_pd(&values[609]);
c31_4 = _mm_add_pd(c31_4, _mm_mul_pd(a31_4, b31));
_mm_storeu_pd(&C[(i*56)+39], c31_4);
__m128d c31_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a31_6 = _mm_loadu_pd(&values[611]);
c31_6 = _mm_add_pd(c31_6, _mm_mul_pd(a31_6, b31));
_mm_storeu_pd(&C[(i*56)+41], c31_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c31_8 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a31_8 = _mm256_loadu_pd(&values[613]);
c31_8 = _mm256_add_pd(c31_8, _mm256_mul_pd(a31_8, b31));
_mm256_storeu_pd(&C[(i*56)+43], c31_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c31_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a31_8 = _mm_loadu_pd(&values[613]);
c31_8 = _mm_add_pd(c31_8, _mm_mul_pd(a31_8, b31));
_mm_storeu_pd(&C[(i*56)+43], c31_8);
__m128d c31_10 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a31_10 = _mm_loadu_pd(&values[615]);
c31_10 = _mm_add_pd(c31_10, _mm_mul_pd(a31_10, b31));
_mm_storeu_pd(&C[(i*56)+45], c31_10);
#endif
__m128d c31_12 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a31_12 = _mm_loadu_pd(&values[617]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_12 = _mm_add_pd(c31_12, _mm_mul_pd(a31_12, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_12 = _mm_add_pd(c31_12, _mm_mul_pd(a31_12, b31));
#endif
_mm_storeu_pd(&C[(i*56)+47], c31_12);
__m128d c31_14 = _mm_load_sd(&C[(i*56)+49]);
__m128d a31_14 = _mm_load_sd(&values[619]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_14 = _mm_add_sd(c31_14, _mm_mul_sd(a31_14, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_14 = _mm_add_sd(c31_14, _mm_mul_sd(a31_14, b31));
#endif
_mm_store_sd(&C[(i*56)+49], c31_14);
#else
C[(i*56)+35] += values[605] * B[(i*56)+31];
C[(i*56)+36] += values[606] * B[(i*56)+31];
C[(i*56)+37] += values[607] * B[(i*56)+31];
C[(i*56)+38] += values[608] * B[(i*56)+31];
C[(i*56)+39] += values[609] * B[(i*56)+31];
C[(i*56)+40] += values[610] * B[(i*56)+31];
C[(i*56)+41] += values[611] * B[(i*56)+31];
C[(i*56)+42] += values[612] * B[(i*56)+31];
C[(i*56)+43] += values[613] * B[(i*56)+31];
C[(i*56)+44] += values[614] * B[(i*56)+31];
C[(i*56)+45] += values[615] * B[(i*56)+31];
C[(i*56)+46] += values[616] * B[(i*56)+31];
C[(i*56)+47] += values[617] * B[(i*56)+31];
C[(i*56)+48] += values[618] * B[(i*56)+31];
C[(i*56)+49] += values[619] * B[(i*56)+31];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c32_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a32_0 = _mm256_loadu_pd(&values[620]);
c32_0 = _mm256_add_pd(c32_0, _mm256_mul_pd(a32_0, b32));
_mm256_storeu_pd(&C[(i*56)+35], c32_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c32_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a32_0 = _mm_loadu_pd(&values[620]);
c32_0 = _mm_add_pd(c32_0, _mm_mul_pd(a32_0, b32));
_mm_storeu_pd(&C[(i*56)+35], c32_0);
__m128d c32_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a32_2 = _mm_loadu_pd(&values[622]);
c32_2 = _mm_add_pd(c32_2, _mm_mul_pd(a32_2, b32));
_mm_storeu_pd(&C[(i*56)+37], c32_2);
#endif
__m128d c32_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a32_4 = _mm_load_sd(&values[624]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, b32));
#endif
_mm_store_sd(&C[(i*56)+39], c32_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c32_5 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a32_5 = _mm256_loadu_pd(&values[625]);
c32_5 = _mm256_add_pd(c32_5, _mm256_mul_pd(a32_5, b32));
_mm256_storeu_pd(&C[(i*56)+41], c32_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c32_5 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a32_5 = _mm_loadu_pd(&values[625]);
c32_5 = _mm_add_pd(c32_5, _mm_mul_pd(a32_5, b32));
_mm_storeu_pd(&C[(i*56)+41], c32_5);
__m128d c32_7 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a32_7 = _mm_loadu_pd(&values[627]);
c32_7 = _mm_add_pd(c32_7, _mm_mul_pd(a32_7, b32));
_mm_storeu_pd(&C[(i*56)+43], c32_7);
#endif
__m128d c32_9 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a32_9 = _mm_loadu_pd(&values[629]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_9 = _mm_add_pd(c32_9, _mm_mul_pd(a32_9, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_9 = _mm_add_pd(c32_9, _mm_mul_pd(a32_9, b32));
#endif
_mm_storeu_pd(&C[(i*56)+46], c32_9);
__m128d c32_11 = _mm_load_sd(&C[(i*56)+48]);
__m128d a32_11 = _mm_load_sd(&values[631]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_11 = _mm_add_sd(c32_11, _mm_mul_sd(a32_11, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_11 = _mm_add_sd(c32_11, _mm_mul_sd(a32_11, b32));
#endif
_mm_store_sd(&C[(i*56)+48], c32_11);
__m128d c32_12 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a32_12 = _mm_loadu_pd(&values[632]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_12 = _mm_add_pd(c32_12, _mm_mul_pd(a32_12, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_12 = _mm_add_pd(c32_12, _mm_mul_pd(a32_12, b32));
#endif
_mm_storeu_pd(&C[(i*56)+50], c32_12);
#else
C[(i*56)+35] += values[620] * B[(i*56)+32];
C[(i*56)+36] += values[621] * B[(i*56)+32];
C[(i*56)+37] += values[622] * B[(i*56)+32];
C[(i*56)+38] += values[623] * B[(i*56)+32];
C[(i*56)+39] += values[624] * B[(i*56)+32];
C[(i*56)+41] += values[625] * B[(i*56)+32];
C[(i*56)+42] += values[626] * B[(i*56)+32];
C[(i*56)+43] += values[627] * B[(i*56)+32];
C[(i*56)+44] += values[628] * B[(i*56)+32];
C[(i*56)+46] += values[629] * B[(i*56)+32];
C[(i*56)+47] += values[630] * B[(i*56)+32];
C[(i*56)+48] += values[631] * B[(i*56)+32];
C[(i*56)+50] += values[632] * B[(i*56)+32];
C[(i*56)+51] += values[633] * B[(i*56)+32];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b33 = _mm256_broadcast_sd(&B[(i*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b33 = _mm_loaddup_pd(&B[(i*56)+33]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a33_0 = _mm256_loadu_pd(&values[634]);
c33_0 = _mm256_add_pd(c33_0, _mm256_mul_pd(a33_0, b33));
_mm256_storeu_pd(&C[(i*56)+35], c33_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a33_0 = _mm_loadu_pd(&values[634]);
c33_0 = _mm_add_pd(c33_0, _mm_mul_pd(a33_0, b33));
_mm_storeu_pd(&C[(i*56)+35], c33_0);
__m128d c33_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a33_2 = _mm_loadu_pd(&values[636]);
c33_2 = _mm_add_pd(c33_2, _mm_mul_pd(a33_2, b33));
_mm_storeu_pd(&C[(i*56)+37], c33_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a33_4 = _mm256_loadu_pd(&values[638]);
c33_4 = _mm256_add_pd(c33_4, _mm256_mul_pd(a33_4, b33));
_mm256_storeu_pd(&C[(i*56)+39], c33_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a33_4 = _mm_loadu_pd(&values[638]);
c33_4 = _mm_add_pd(c33_4, _mm_mul_pd(a33_4, b33));
_mm_storeu_pd(&C[(i*56)+39], c33_4);
__m128d c33_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a33_6 = _mm_loadu_pd(&values[640]);
c33_6 = _mm_add_pd(c33_6, _mm_mul_pd(a33_6, b33));
_mm_storeu_pd(&C[(i*56)+41], c33_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_8 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a33_8 = _mm256_loadu_pd(&values[642]);
c33_8 = _mm256_add_pd(c33_8, _mm256_mul_pd(a33_8, b33));
_mm256_storeu_pd(&C[(i*56)+43], c33_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a33_8 = _mm_loadu_pd(&values[642]);
c33_8 = _mm_add_pd(c33_8, _mm_mul_pd(a33_8, b33));
_mm_storeu_pd(&C[(i*56)+43], c33_8);
__m128d c33_10 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a33_10 = _mm_loadu_pd(&values[644]);
c33_10 = _mm_add_pd(c33_10, _mm_mul_pd(a33_10, b33));
_mm_storeu_pd(&C[(i*56)+45], c33_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_12 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a33_12 = _mm256_loadu_pd(&values[646]);
c33_12 = _mm256_add_pd(c33_12, _mm256_mul_pd(a33_12, b33));
_mm256_storeu_pd(&C[(i*56)+47], c33_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_12 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a33_12 = _mm_loadu_pd(&values[646]);
c33_12 = _mm_add_pd(c33_12, _mm_mul_pd(a33_12, b33));
_mm_storeu_pd(&C[(i*56)+47], c33_12);
__m128d c33_14 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a33_14 = _mm_loadu_pd(&values[648]);
c33_14 = _mm_add_pd(c33_14, _mm_mul_pd(a33_14, b33));
_mm_storeu_pd(&C[(i*56)+49], c33_14);
#endif
__m128d c33_16 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a33_16 = _mm_loadu_pd(&values[650]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_16 = _mm_add_pd(c33_16, _mm_mul_pd(a33_16, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_16 = _mm_add_pd(c33_16, _mm_mul_pd(a33_16, b33));
#endif
_mm_storeu_pd(&C[(i*56)+51], c33_16);
#else
C[(i*56)+35] += values[634] * B[(i*56)+33];
C[(i*56)+36] += values[635] * B[(i*56)+33];
C[(i*56)+37] += values[636] * B[(i*56)+33];
C[(i*56)+38] += values[637] * B[(i*56)+33];
C[(i*56)+39] += values[638] * B[(i*56)+33];
C[(i*56)+40] += values[639] * B[(i*56)+33];
C[(i*56)+41] += values[640] * B[(i*56)+33];
C[(i*56)+42] += values[641] * B[(i*56)+33];
C[(i*56)+43] += values[642] * B[(i*56)+33];
C[(i*56)+44] += values[643] * B[(i*56)+33];
C[(i*56)+45] += values[644] * B[(i*56)+33];
C[(i*56)+46] += values[645] * B[(i*56)+33];
C[(i*56)+47] += values[646] * B[(i*56)+33];
C[(i*56)+48] += values[647] * B[(i*56)+33];
C[(i*56)+49] += values[648] * B[(i*56)+33];
C[(i*56)+50] += values[649] * B[(i*56)+33];
C[(i*56)+51] += values[650] * B[(i*56)+33];
C[(i*56)+52] += values[651] * B[(i*56)+33];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b34 = _mm256_broadcast_sd(&B[(i*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b34 = _mm_loaddup_pd(&B[(i*56)+34]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a34_0 = _mm256_loadu_pd(&values[652]);
c34_0 = _mm256_add_pd(c34_0, _mm256_mul_pd(a34_0, b34));
_mm256_storeu_pd(&C[(i*56)+35], c34_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a34_0 = _mm_loadu_pd(&values[652]);
c34_0 = _mm_add_pd(c34_0, _mm_mul_pd(a34_0, b34));
_mm_storeu_pd(&C[(i*56)+35], c34_0);
__m128d c34_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a34_2 = _mm_loadu_pd(&values[654]);
c34_2 = _mm_add_pd(c34_2, _mm_mul_pd(a34_2, b34));
_mm_storeu_pd(&C[(i*56)+37], c34_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a34_4 = _mm256_loadu_pd(&values[656]);
c34_4 = _mm256_add_pd(c34_4, _mm256_mul_pd(a34_4, b34));
_mm256_storeu_pd(&C[(i*56)+39], c34_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a34_4 = _mm_loadu_pd(&values[656]);
c34_4 = _mm_add_pd(c34_4, _mm_mul_pd(a34_4, b34));
_mm_storeu_pd(&C[(i*56)+39], c34_4);
__m128d c34_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a34_6 = _mm_loadu_pd(&values[658]);
c34_6 = _mm_add_pd(c34_6, _mm_mul_pd(a34_6, b34));
_mm_storeu_pd(&C[(i*56)+41], c34_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_8 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a34_8 = _mm256_loadu_pd(&values[660]);
c34_8 = _mm256_add_pd(c34_8, _mm256_mul_pd(a34_8, b34));
_mm256_storeu_pd(&C[(i*56)+43], c34_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a34_8 = _mm_loadu_pd(&values[660]);
c34_8 = _mm_add_pd(c34_8, _mm_mul_pd(a34_8, b34));
_mm_storeu_pd(&C[(i*56)+43], c34_8);
__m128d c34_10 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a34_10 = _mm_loadu_pd(&values[662]);
c34_10 = _mm_add_pd(c34_10, _mm_mul_pd(a34_10, b34));
_mm_storeu_pd(&C[(i*56)+45], c34_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_12 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a34_12 = _mm256_loadu_pd(&values[664]);
c34_12 = _mm256_add_pd(c34_12, _mm256_mul_pd(a34_12, b34));
_mm256_storeu_pd(&C[(i*56)+47], c34_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_12 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a34_12 = _mm_loadu_pd(&values[664]);
c34_12 = _mm_add_pd(c34_12, _mm_mul_pd(a34_12, b34));
_mm_storeu_pd(&C[(i*56)+47], c34_12);
__m128d c34_14 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a34_14 = _mm_loadu_pd(&values[666]);
c34_14 = _mm_add_pd(c34_14, _mm_mul_pd(a34_14, b34));
_mm_storeu_pd(&C[(i*56)+49], c34_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_16 = _mm256_loadu_pd(&C[(i*56)+51]);
__m256d a34_16 = _mm256_loadu_pd(&values[668]);
c34_16 = _mm256_add_pd(c34_16, _mm256_mul_pd(a34_16, b34));
_mm256_storeu_pd(&C[(i*56)+51], c34_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_16 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a34_16 = _mm_loadu_pd(&values[668]);
c34_16 = _mm_add_pd(c34_16, _mm_mul_pd(a34_16, b34));
_mm_storeu_pd(&C[(i*56)+51], c34_16);
__m128d c34_18 = _mm_loadu_pd(&C[(i*56)+53]);
__m128d a34_18 = _mm_loadu_pd(&values[670]);
c34_18 = _mm_add_pd(c34_18, _mm_mul_pd(a34_18, b34));
_mm_storeu_pd(&C[(i*56)+53], c34_18);
#endif
#else
C[(i*56)+35] += values[652] * B[(i*56)+34];
C[(i*56)+36] += values[653] * B[(i*56)+34];
C[(i*56)+37] += values[654] * B[(i*56)+34];
C[(i*56)+38] += values[655] * B[(i*56)+34];
C[(i*56)+39] += values[656] * B[(i*56)+34];
C[(i*56)+40] += values[657] * B[(i*56)+34];
C[(i*56)+41] += values[658] * B[(i*56)+34];
C[(i*56)+42] += values[659] * B[(i*56)+34];
C[(i*56)+43] += values[660] * B[(i*56)+34];
C[(i*56)+44] += values[661] * B[(i*56)+34];
C[(i*56)+45] += values[662] * B[(i*56)+34];
C[(i*56)+46] += values[663] * B[(i*56)+34];
C[(i*56)+47] += values[664] * B[(i*56)+34];
C[(i*56)+48] += values[665] * B[(i*56)+34];
C[(i*56)+49] += values[666] * B[(i*56)+34];
C[(i*56)+50] += values[667] * B[(i*56)+34];
C[(i*56)+51] += values[668] * B[(i*56)+34];
C[(i*56)+52] += values[669] * B[(i*56)+34];
C[(i*56)+53] += values[670] * B[(i*56)+34];
C[(i*56)+54] += values[671] * B[(i*56)+34];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 12096;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivM_9_56(double* values, double* B, double* C, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < 56; m++) {
    C[(i*56)+m] = 0.0;
  }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b0 = _mm256_broadcast_sd(&B[(i*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b0 = _mm_loaddup_pd(&B[(i*56)+0]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_0 = _mm256_loadu_pd(&C[(i*56)+1]);
__m256d a0_0 = _mm256_loadu_pd(&values[0]);
c0_0 = _mm256_add_pd(c0_0, _mm256_mul_pd(a0_0, b0));
_mm256_storeu_pd(&C[(i*56)+1], c0_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_0 = _mm_loadu_pd(&C[(i*56)+1]);
__m128d a0_0 = _mm_loadu_pd(&values[0]);
c0_0 = _mm_add_pd(c0_0, _mm_mul_pd(a0_0, b0));
_mm_storeu_pd(&C[(i*56)+1], c0_0);
__m128d c0_2 = _mm_loadu_pd(&C[(i*56)+3]);
__m128d a0_2 = _mm_loadu_pd(&values[2]);
c0_2 = _mm_add_pd(c0_2, _mm_mul_pd(a0_2, b0));
_mm_storeu_pd(&C[(i*56)+3], c0_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a0_4 = _mm256_loadu_pd(&values[4]);
c0_4 = _mm256_add_pd(c0_4, _mm256_mul_pd(a0_4, b0));
_mm256_storeu_pd(&C[(i*56)+5], c0_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a0_4 = _mm_loadu_pd(&values[4]);
c0_4 = _mm_add_pd(c0_4, _mm_mul_pd(a0_4, b0));
_mm_storeu_pd(&C[(i*56)+5], c0_4);
__m128d c0_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a0_6 = _mm_loadu_pd(&values[6]);
c0_6 = _mm_add_pd(c0_6, _mm_mul_pd(a0_6, b0));
_mm_storeu_pd(&C[(i*56)+7], c0_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_8 = _mm256_loadu_pd(&C[(i*56)+9]);
__m256d a0_8 = _mm256_loadu_pd(&values[8]);
c0_8 = _mm256_add_pd(c0_8, _mm256_mul_pd(a0_8, b0));
_mm256_storeu_pd(&C[(i*56)+9], c0_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_8 = _mm_loadu_pd(&C[(i*56)+9]);
__m128d a0_8 = _mm_loadu_pd(&values[8]);
c0_8 = _mm_add_pd(c0_8, _mm_mul_pd(a0_8, b0));
_mm_storeu_pd(&C[(i*56)+9], c0_8);
__m128d c0_10 = _mm_loadu_pd(&C[(i*56)+11]);
__m128d a0_10 = _mm_loadu_pd(&values[10]);
c0_10 = _mm_add_pd(c0_10, _mm_mul_pd(a0_10, b0));
_mm_storeu_pd(&C[(i*56)+11], c0_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_12 = _mm256_loadu_pd(&C[(i*56)+13]);
__m256d a0_12 = _mm256_loadu_pd(&values[12]);
c0_12 = _mm256_add_pd(c0_12, _mm256_mul_pd(a0_12, b0));
_mm256_storeu_pd(&C[(i*56)+13], c0_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_12 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a0_12 = _mm_loadu_pd(&values[12]);
c0_12 = _mm_add_pd(c0_12, _mm_mul_pd(a0_12, b0));
_mm_storeu_pd(&C[(i*56)+13], c0_12);
__m128d c0_14 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a0_14 = _mm_loadu_pd(&values[14]);
c0_14 = _mm_add_pd(c0_14, _mm_mul_pd(a0_14, b0));
_mm_storeu_pd(&C[(i*56)+15], c0_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_16 = _mm256_loadu_pd(&C[(i*56)+17]);
__m256d a0_16 = _mm256_loadu_pd(&values[16]);
c0_16 = _mm256_add_pd(c0_16, _mm256_mul_pd(a0_16, b0));
_mm256_storeu_pd(&C[(i*56)+17], c0_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_16 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a0_16 = _mm_loadu_pd(&values[16]);
c0_16 = _mm_add_pd(c0_16, _mm_mul_pd(a0_16, b0));
_mm_storeu_pd(&C[(i*56)+17], c0_16);
__m128d c0_18 = _mm_loadu_pd(&C[(i*56)+19]);
__m128d a0_18 = _mm_loadu_pd(&values[18]);
c0_18 = _mm_add_pd(c0_18, _mm_mul_pd(a0_18, b0));
_mm_storeu_pd(&C[(i*56)+19], c0_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_20 = _mm256_loadu_pd(&C[(i*56)+21]);
__m256d a0_20 = _mm256_loadu_pd(&values[20]);
c0_20 = _mm256_add_pd(c0_20, _mm256_mul_pd(a0_20, b0));
_mm256_storeu_pd(&C[(i*56)+21], c0_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_20 = _mm_loadu_pd(&C[(i*56)+21]);
__m128d a0_20 = _mm_loadu_pd(&values[20]);
c0_20 = _mm_add_pd(c0_20, _mm_mul_pd(a0_20, b0));
_mm_storeu_pd(&C[(i*56)+21], c0_20);
__m128d c0_22 = _mm_loadu_pd(&C[(i*56)+23]);
__m128d a0_22 = _mm_loadu_pd(&values[22]);
c0_22 = _mm_add_pd(c0_22, _mm_mul_pd(a0_22, b0));
_mm_storeu_pd(&C[(i*56)+23], c0_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_24 = _mm256_loadu_pd(&C[(i*56)+25]);
__m256d a0_24 = _mm256_loadu_pd(&values[24]);
c0_24 = _mm256_add_pd(c0_24, _mm256_mul_pd(a0_24, b0));
_mm256_storeu_pd(&C[(i*56)+25], c0_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_24 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a0_24 = _mm_loadu_pd(&values[24]);
c0_24 = _mm_add_pd(c0_24, _mm_mul_pd(a0_24, b0));
_mm_storeu_pd(&C[(i*56)+25], c0_24);
__m128d c0_26 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a0_26 = _mm_loadu_pd(&values[26]);
c0_26 = _mm_add_pd(c0_26, _mm_mul_pd(a0_26, b0));
_mm_storeu_pd(&C[(i*56)+27], c0_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_28 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a0_28 = _mm256_loadu_pd(&values[28]);
c0_28 = _mm256_add_pd(c0_28, _mm256_mul_pd(a0_28, b0));
_mm256_storeu_pd(&C[(i*56)+29], c0_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_28 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a0_28 = _mm_loadu_pd(&values[28]);
c0_28 = _mm_add_pd(c0_28, _mm_mul_pd(a0_28, b0));
_mm_storeu_pd(&C[(i*56)+29], c0_28);
__m128d c0_30 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a0_30 = _mm_loadu_pd(&values[30]);
c0_30 = _mm_add_pd(c0_30, _mm_mul_pd(a0_30, b0));
_mm_storeu_pd(&C[(i*56)+31], c0_30);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_32 = _mm256_loadu_pd(&C[(i*56)+33]);
__m256d a0_32 = _mm256_loadu_pd(&values[32]);
c0_32 = _mm256_add_pd(c0_32, _mm256_mul_pd(a0_32, b0));
_mm256_storeu_pd(&C[(i*56)+33], c0_32);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_32 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a0_32 = _mm_loadu_pd(&values[32]);
c0_32 = _mm_add_pd(c0_32, _mm_mul_pd(a0_32, b0));
_mm_storeu_pd(&C[(i*56)+33], c0_32);
__m128d c0_34 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a0_34 = _mm_loadu_pd(&values[34]);
c0_34 = _mm_add_pd(c0_34, _mm_mul_pd(a0_34, b0));
_mm_storeu_pd(&C[(i*56)+35], c0_34);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_36 = _mm256_loadu_pd(&C[(i*56)+37]);
__m256d a0_36 = _mm256_loadu_pd(&values[36]);
c0_36 = _mm256_add_pd(c0_36, _mm256_mul_pd(a0_36, b0));
_mm256_storeu_pd(&C[(i*56)+37], c0_36);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_36 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a0_36 = _mm_loadu_pd(&values[36]);
c0_36 = _mm_add_pd(c0_36, _mm_mul_pd(a0_36, b0));
_mm_storeu_pd(&C[(i*56)+37], c0_36);
__m128d c0_38 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a0_38 = _mm_loadu_pd(&values[38]);
c0_38 = _mm_add_pd(c0_38, _mm_mul_pd(a0_38, b0));
_mm_storeu_pd(&C[(i*56)+39], c0_38);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_40 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a0_40 = _mm256_loadu_pd(&values[40]);
c0_40 = _mm256_add_pd(c0_40, _mm256_mul_pd(a0_40, b0));
_mm256_storeu_pd(&C[(i*56)+41], c0_40);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_40 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a0_40 = _mm_loadu_pd(&values[40]);
c0_40 = _mm_add_pd(c0_40, _mm_mul_pd(a0_40, b0));
_mm_storeu_pd(&C[(i*56)+41], c0_40);
__m128d c0_42 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a0_42 = _mm_loadu_pd(&values[42]);
c0_42 = _mm_add_pd(c0_42, _mm_mul_pd(a0_42, b0));
_mm_storeu_pd(&C[(i*56)+43], c0_42);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_44 = _mm256_loadu_pd(&C[(i*56)+45]);
__m256d a0_44 = _mm256_loadu_pd(&values[44]);
c0_44 = _mm256_add_pd(c0_44, _mm256_mul_pd(a0_44, b0));
_mm256_storeu_pd(&C[(i*56)+45], c0_44);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_44 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a0_44 = _mm_loadu_pd(&values[44]);
c0_44 = _mm_add_pd(c0_44, _mm_mul_pd(a0_44, b0));
_mm_storeu_pd(&C[(i*56)+45], c0_44);
__m128d c0_46 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a0_46 = _mm_loadu_pd(&values[46]);
c0_46 = _mm_add_pd(c0_46, _mm_mul_pd(a0_46, b0));
_mm_storeu_pd(&C[(i*56)+47], c0_46);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c0_48 = _mm256_loadu_pd(&C[(i*56)+49]);
__m256d a0_48 = _mm256_loadu_pd(&values[48]);
c0_48 = _mm256_add_pd(c0_48, _mm256_mul_pd(a0_48, b0));
_mm256_storeu_pd(&C[(i*56)+49], c0_48);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c0_48 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a0_48 = _mm_loadu_pd(&values[48]);
c0_48 = _mm_add_pd(c0_48, _mm_mul_pd(a0_48, b0));
_mm_storeu_pd(&C[(i*56)+49], c0_48);
__m128d c0_50 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a0_50 = _mm_loadu_pd(&values[50]);
c0_50 = _mm_add_pd(c0_50, _mm_mul_pd(a0_50, b0));
_mm_storeu_pd(&C[(i*56)+51], c0_50);
#endif
__m128d c0_52 = _mm_loadu_pd(&C[(i*56)+53]);
__m128d a0_52 = _mm_loadu_pd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_52 = _mm_add_pd(c0_52, _mm_mul_pd(a0_52, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_52 = _mm_add_pd(c0_52, _mm_mul_pd(a0_52, b0));
#endif
_mm_storeu_pd(&C[(i*56)+53], c0_52);
__m128d c0_54 = _mm_load_sd(&C[(i*56)+55]);
__m128d a0_54 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c0_54 = _mm_add_sd(c0_54, _mm_mul_sd(a0_54, _mm256_castpd256_pd128(b0)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c0_54 = _mm_add_sd(c0_54, _mm_mul_sd(a0_54, b0));
#endif
_mm_store_sd(&C[(i*56)+55], c0_54);
#else
C[(i*56)+1] += values[0] * B[(i*56)+0];
C[(i*56)+2] += values[1] * B[(i*56)+0];
C[(i*56)+3] += values[2] * B[(i*56)+0];
C[(i*56)+4] += values[3] * B[(i*56)+0];
C[(i*56)+5] += values[4] * B[(i*56)+0];
C[(i*56)+6] += values[5] * B[(i*56)+0];
C[(i*56)+7] += values[6] * B[(i*56)+0];
C[(i*56)+8] += values[7] * B[(i*56)+0];
C[(i*56)+9] += values[8] * B[(i*56)+0];
C[(i*56)+10] += values[9] * B[(i*56)+0];
C[(i*56)+11] += values[10] * B[(i*56)+0];
C[(i*56)+12] += values[11] * B[(i*56)+0];
C[(i*56)+13] += values[12] * B[(i*56)+0];
C[(i*56)+14] += values[13] * B[(i*56)+0];
C[(i*56)+15] += values[14] * B[(i*56)+0];
C[(i*56)+16] += values[15] * B[(i*56)+0];
C[(i*56)+17] += values[16] * B[(i*56)+0];
C[(i*56)+18] += values[17] * B[(i*56)+0];
C[(i*56)+19] += values[18] * B[(i*56)+0];
C[(i*56)+20] += values[19] * B[(i*56)+0];
C[(i*56)+21] += values[20] * B[(i*56)+0];
C[(i*56)+22] += values[21] * B[(i*56)+0];
C[(i*56)+23] += values[22] * B[(i*56)+0];
C[(i*56)+24] += values[23] * B[(i*56)+0];
C[(i*56)+25] += values[24] * B[(i*56)+0];
C[(i*56)+26] += values[25] * B[(i*56)+0];
C[(i*56)+27] += values[26] * B[(i*56)+0];
C[(i*56)+28] += values[27] * B[(i*56)+0];
C[(i*56)+29] += values[28] * B[(i*56)+0];
C[(i*56)+30] += values[29] * B[(i*56)+0];
C[(i*56)+31] += values[30] * B[(i*56)+0];
C[(i*56)+32] += values[31] * B[(i*56)+0];
C[(i*56)+33] += values[32] * B[(i*56)+0];
C[(i*56)+34] += values[33] * B[(i*56)+0];
C[(i*56)+35] += values[34] * B[(i*56)+0];
C[(i*56)+36] += values[35] * B[(i*56)+0];
C[(i*56)+37] += values[36] * B[(i*56)+0];
C[(i*56)+38] += values[37] * B[(i*56)+0];
C[(i*56)+39] += values[38] * B[(i*56)+0];
C[(i*56)+40] += values[39] * B[(i*56)+0];
C[(i*56)+41] += values[40] * B[(i*56)+0];
C[(i*56)+42] += values[41] * B[(i*56)+0];
C[(i*56)+43] += values[42] * B[(i*56)+0];
C[(i*56)+44] += values[43] * B[(i*56)+0];
C[(i*56)+45] += values[44] * B[(i*56)+0];
C[(i*56)+46] += values[45] * B[(i*56)+0];
C[(i*56)+47] += values[46] * B[(i*56)+0];
C[(i*56)+48] += values[47] * B[(i*56)+0];
C[(i*56)+49] += values[48] * B[(i*56)+0];
C[(i*56)+50] += values[49] * B[(i*56)+0];
C[(i*56)+51] += values[50] * B[(i*56)+0];
C[(i*56)+52] += values[51] * B[(i*56)+0];
C[(i*56)+53] += values[52] * B[(i*56)+0];
C[(i*56)+54] += values[53] * B[(i*56)+0];
C[(i*56)+55] += values[54] * B[(i*56)+0];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*56)+1]);
#endif
__m128d c1_0 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a1_0 = _mm_loadu_pd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_pd(c1_0, _mm_mul_pd(a1_0, b1));
#endif
_mm_storeu_pd(&C[(i*56)+4], c1_0);
__m128d c1_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a1_2 = _mm_load_sd(&values[57]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_2 = _mm_add_sd(c1_2, _mm_mul_sd(a1_2, b1));
#endif
_mm_store_sd(&C[(i*56)+7], c1_2);
__m128d c1_3 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a1_3 = _mm_loadu_pd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_3 = _mm_add_pd(c1_3, _mm_mul_pd(a1_3, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_3 = _mm_add_pd(c1_3, _mm_mul_pd(a1_3, b1));
#endif
_mm_storeu_pd(&C[(i*56)+10], c1_3);
__m128d c1_5 = _mm_load_sd(&C[(i*56)+12]);
__m128d a1_5 = _mm_load_sd(&values[60]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_5 = _mm_add_sd(c1_5, _mm_mul_sd(a1_5, b1));
#endif
_mm_store_sd(&C[(i*56)+12], c1_5);
__m128d c1_6 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a1_6 = _mm_loadu_pd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_6 = _mm_add_pd(c1_6, _mm_mul_pd(a1_6, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_6 = _mm_add_pd(c1_6, _mm_mul_pd(a1_6, b1));
#endif
_mm_storeu_pd(&C[(i*56)+14], c1_6);
__m128d c1_8 = _mm_load_sd(&C[(i*56)+17]);
__m128d a1_8 = _mm_load_sd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_8 = _mm_add_sd(c1_8, _mm_mul_sd(a1_8, b1));
#endif
_mm_store_sd(&C[(i*56)+17], c1_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_9 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a1_9 = _mm256_loadu_pd(&values[64]);
c1_9 = _mm256_add_pd(c1_9, _mm256_mul_pd(a1_9, b1));
_mm256_storeu_pd(&C[(i*56)+20], c1_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_9 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a1_9 = _mm_loadu_pd(&values[64]);
c1_9 = _mm_add_pd(c1_9, _mm_mul_pd(a1_9, b1));
_mm_storeu_pd(&C[(i*56)+20], c1_9);
__m128d c1_11 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a1_11 = _mm_loadu_pd(&values[66]);
c1_11 = _mm_add_pd(c1_11, _mm_mul_pd(a1_11, b1));
_mm_storeu_pd(&C[(i*56)+22], c1_11);
#endif
__m128d c1_13 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a1_13 = _mm_loadu_pd(&values[68]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_13 = _mm_add_pd(c1_13, _mm_mul_pd(a1_13, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_13 = _mm_add_pd(c1_13, _mm_mul_pd(a1_13, b1));
#endif
_mm_storeu_pd(&C[(i*56)+25], c1_13);
__m128d c1_15 = _mm_load_sd(&C[(i*56)+27]);
__m128d a1_15 = _mm_load_sd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_15 = _mm_add_sd(c1_15, _mm_mul_sd(a1_15, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_15 = _mm_add_sd(c1_15, _mm_mul_sd(a1_15, b1));
#endif
_mm_store_sd(&C[(i*56)+27], c1_15);
__m128d c1_16 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a1_16 = _mm_loadu_pd(&values[71]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_16 = _mm_add_pd(c1_16, _mm_mul_pd(a1_16, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_16 = _mm_add_pd(c1_16, _mm_mul_pd(a1_16, b1));
#endif
_mm_storeu_pd(&C[(i*56)+29], c1_16);
__m128d c1_18 = _mm_load_sd(&C[(i*56)+32]);
__m128d a1_18 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_18 = _mm_add_sd(c1_18, _mm_mul_sd(a1_18, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_18 = _mm_add_sd(c1_18, _mm_mul_sd(a1_18, b1));
#endif
_mm_store_sd(&C[(i*56)+32], c1_18);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_19 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a1_19 = _mm256_loadu_pd(&values[74]);
c1_19 = _mm256_add_pd(c1_19, _mm256_mul_pd(a1_19, b1));
_mm256_storeu_pd(&C[(i*56)+35], c1_19);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_19 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a1_19 = _mm_loadu_pd(&values[74]);
c1_19 = _mm_add_pd(c1_19, _mm_mul_pd(a1_19, b1));
_mm_storeu_pd(&C[(i*56)+35], c1_19);
__m128d c1_21 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a1_21 = _mm_loadu_pd(&values[76]);
c1_21 = _mm_add_pd(c1_21, _mm_mul_pd(a1_21, b1));
_mm_storeu_pd(&C[(i*56)+37], c1_21);
#endif
__m128d c1_23 = _mm_load_sd(&C[(i*56)+39]);
__m128d a1_23 = _mm_load_sd(&values[78]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_23 = _mm_add_sd(c1_23, _mm_mul_sd(a1_23, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_23 = _mm_add_sd(c1_23, _mm_mul_sd(a1_23, b1));
#endif
_mm_store_sd(&C[(i*56)+39], c1_23);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c1_24 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a1_24 = _mm256_loadu_pd(&values[79]);
c1_24 = _mm256_add_pd(c1_24, _mm256_mul_pd(a1_24, b1));
_mm256_storeu_pd(&C[(i*56)+41], c1_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c1_24 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a1_24 = _mm_loadu_pd(&values[79]);
c1_24 = _mm_add_pd(c1_24, _mm_mul_pd(a1_24, b1));
_mm_storeu_pd(&C[(i*56)+41], c1_24);
__m128d c1_26 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a1_26 = _mm_loadu_pd(&values[81]);
c1_26 = _mm_add_pd(c1_26, _mm_mul_pd(a1_26, b1));
_mm_storeu_pd(&C[(i*56)+43], c1_26);
#endif
__m128d c1_28 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a1_28 = _mm_loadu_pd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_28 = _mm_add_pd(c1_28, _mm_mul_pd(a1_28, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_28 = _mm_add_pd(c1_28, _mm_mul_pd(a1_28, b1));
#endif
_mm_storeu_pd(&C[(i*56)+46], c1_28);
__m128d c1_30 = _mm_load_sd(&C[(i*56)+48]);
__m128d a1_30 = _mm_load_sd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_30 = _mm_add_sd(c1_30, _mm_mul_sd(a1_30, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_30 = _mm_add_sd(c1_30, _mm_mul_sd(a1_30, b1));
#endif
_mm_store_sd(&C[(i*56)+48], c1_30);
__m128d c1_31 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a1_31 = _mm_loadu_pd(&values[86]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_31 = _mm_add_pd(c1_31, _mm_mul_pd(a1_31, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_31 = _mm_add_pd(c1_31, _mm_mul_pd(a1_31, b1));
#endif
_mm_storeu_pd(&C[(i*56)+50], c1_31);
__m128d c1_33 = _mm_load_sd(&C[(i*56)+53]);
__m128d a1_33 = _mm_load_sd(&values[88]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_33 = _mm_add_sd(c1_33, _mm_mul_sd(a1_33, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_33 = _mm_add_sd(c1_33, _mm_mul_sd(a1_33, b1));
#endif
_mm_store_sd(&C[(i*56)+53], c1_33);
#else
C[(i*56)+4] += values[55] * B[(i*56)+1];
C[(i*56)+5] += values[56] * B[(i*56)+1];
C[(i*56)+7] += values[57] * B[(i*56)+1];
C[(i*56)+10] += values[58] * B[(i*56)+1];
C[(i*56)+11] += values[59] * B[(i*56)+1];
C[(i*56)+12] += values[60] * B[(i*56)+1];
C[(i*56)+14] += values[61] * B[(i*56)+1];
C[(i*56)+15] += values[62] * B[(i*56)+1];
C[(i*56)+17] += values[63] * B[(i*56)+1];
C[(i*56)+20] += values[64] * B[(i*56)+1];
C[(i*56)+21] += values[65] * B[(i*56)+1];
C[(i*56)+22] += values[66] * B[(i*56)+1];
C[(i*56)+23] += values[67] * B[(i*56)+1];
C[(i*56)+25] += values[68] * B[(i*56)+1];
C[(i*56)+26] += values[69] * B[(i*56)+1];
C[(i*56)+27] += values[70] * B[(i*56)+1];
C[(i*56)+29] += values[71] * B[(i*56)+1];
C[(i*56)+30] += values[72] * B[(i*56)+1];
C[(i*56)+32] += values[73] * B[(i*56)+1];
C[(i*56)+35] += values[74] * B[(i*56)+1];
C[(i*56)+36] += values[75] * B[(i*56)+1];
C[(i*56)+37] += values[76] * B[(i*56)+1];
C[(i*56)+38] += values[77] * B[(i*56)+1];
C[(i*56)+39] += values[78] * B[(i*56)+1];
C[(i*56)+41] += values[79] * B[(i*56)+1];
C[(i*56)+42] += values[80] * B[(i*56)+1];
C[(i*56)+43] += values[81] * B[(i*56)+1];
C[(i*56)+44] += values[82] * B[(i*56)+1];
C[(i*56)+46] += values[83] * B[(i*56)+1];
C[(i*56)+47] += values[84] * B[(i*56)+1];
C[(i*56)+48] += values[85] * B[(i*56)+1];
C[(i*56)+50] += values[86] * B[(i*56)+1];
C[(i*56)+51] += values[87] * B[(i*56)+1];
C[(i*56)+53] += values[88] * B[(i*56)+1];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*56)+2]);
#endif
__m128d c2_0 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a2_0 = _mm_loadu_pd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_pd(c2_0, _mm_mul_pd(a2_0, b2));
#endif
_mm_storeu_pd(&C[(i*56)+4], c2_0);
__m128d c2_2 = _mm_load_sd(&C[(i*56)+6]);
__m128d a2_2 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_2 = _mm_add_sd(c2_2, _mm_mul_sd(a2_2, b2));
#endif
_mm_store_sd(&C[(i*56)+6], c2_2);
__m128d c2_3 = _mm_load_sd(&C[(i*56)+8]);
__m128d a2_3 = _mm_load_sd(&values[92]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_3 = _mm_add_sd(c2_3, _mm_mul_sd(a2_3, b2));
#endif
_mm_store_sd(&C[(i*56)+8], c2_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_4 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a2_4 = _mm256_loadu_pd(&values[93]);
c2_4 = _mm256_add_pd(c2_4, _mm256_mul_pd(a2_4, b2));
_mm256_storeu_pd(&C[(i*56)+10], c2_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_4 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a2_4 = _mm_loadu_pd(&values[93]);
c2_4 = _mm_add_pd(c2_4, _mm_mul_pd(a2_4, b2));
_mm_storeu_pd(&C[(i*56)+10], c2_4);
__m128d c2_6 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a2_6 = _mm_loadu_pd(&values[95]);
c2_6 = _mm_add_pd(c2_6, _mm_mul_pd(a2_6, b2));
_mm_storeu_pd(&C[(i*56)+12], c2_6);
#endif
__m128d c2_8 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a2_8 = _mm_loadu_pd(&values[97]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_8 = _mm_add_pd(c2_8, _mm_mul_pd(a2_8, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_8 = _mm_add_pd(c2_8, _mm_mul_pd(a2_8, b2));
#endif
_mm_storeu_pd(&C[(i*56)+14], c2_8);
__m128d c2_10 = _mm_load_sd(&C[(i*56)+16]);
__m128d a2_10 = _mm_load_sd(&values[99]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_10 = _mm_add_sd(c2_10, _mm_mul_sd(a2_10, b2));
#endif
_mm_store_sd(&C[(i*56)+16], c2_10);
__m128d c2_11 = _mm_load_sd(&C[(i*56)+18]);
__m128d a2_11 = _mm_load_sd(&values[100]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_11 = _mm_add_sd(c2_11, _mm_mul_sd(a2_11, b2));
#endif
_mm_store_sd(&C[(i*56)+18], c2_11);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_12 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a2_12 = _mm256_loadu_pd(&values[101]);
c2_12 = _mm256_add_pd(c2_12, _mm256_mul_pd(a2_12, b2));
_mm256_storeu_pd(&C[(i*56)+20], c2_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_12 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a2_12 = _mm_loadu_pd(&values[101]);
c2_12 = _mm_add_pd(c2_12, _mm_mul_pd(a2_12, b2));
_mm_storeu_pd(&C[(i*56)+20], c2_12);
__m128d c2_14 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a2_14 = _mm_loadu_pd(&values[103]);
c2_14 = _mm_add_pd(c2_14, _mm_mul_pd(a2_14, b2));
_mm_storeu_pd(&C[(i*56)+22], c2_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_16 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a2_16 = _mm256_loadu_pd(&values[105]);
c2_16 = _mm256_add_pd(c2_16, _mm256_mul_pd(a2_16, b2));
_mm256_storeu_pd(&C[(i*56)+24], c2_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_16 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a2_16 = _mm_loadu_pd(&values[105]);
c2_16 = _mm_add_pd(c2_16, _mm_mul_pd(a2_16, b2));
_mm_storeu_pd(&C[(i*56)+24], c2_16);
__m128d c2_18 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a2_18 = _mm_loadu_pd(&values[107]);
c2_18 = _mm_add_pd(c2_18, _mm_mul_pd(a2_18, b2));
_mm_storeu_pd(&C[(i*56)+26], c2_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_20 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a2_20 = _mm256_loadu_pd(&values[109]);
c2_20 = _mm256_add_pd(c2_20, _mm256_mul_pd(a2_20, b2));
_mm256_storeu_pd(&C[(i*56)+28], c2_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_20 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a2_20 = _mm_loadu_pd(&values[109]);
c2_20 = _mm_add_pd(c2_20, _mm_mul_pd(a2_20, b2));
_mm_storeu_pd(&C[(i*56)+28], c2_20);
__m128d c2_22 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a2_22 = _mm_loadu_pd(&values[111]);
c2_22 = _mm_add_pd(c2_22, _mm_mul_pd(a2_22, b2));
_mm_storeu_pd(&C[(i*56)+30], c2_22);
#endif
__m128d c2_24 = _mm_load_sd(&C[(i*56)+33]);
__m128d a2_24 = _mm_load_sd(&values[113]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_24 = _mm_add_sd(c2_24, _mm_mul_sd(a2_24, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_24 = _mm_add_sd(c2_24, _mm_mul_sd(a2_24, b2));
#endif
_mm_store_sd(&C[(i*56)+33], c2_24);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_25 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a2_25 = _mm256_loadu_pd(&values[114]);
c2_25 = _mm256_add_pd(c2_25, _mm256_mul_pd(a2_25, b2));
_mm256_storeu_pd(&C[(i*56)+35], c2_25);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_25 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a2_25 = _mm_loadu_pd(&values[114]);
c2_25 = _mm_add_pd(c2_25, _mm_mul_pd(a2_25, b2));
_mm_storeu_pd(&C[(i*56)+35], c2_25);
__m128d c2_27 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a2_27 = _mm_loadu_pd(&values[116]);
c2_27 = _mm_add_pd(c2_27, _mm_mul_pd(a2_27, b2));
_mm_storeu_pd(&C[(i*56)+37], c2_27);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_29 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a2_29 = _mm256_loadu_pd(&values[118]);
c2_29 = _mm256_add_pd(c2_29, _mm256_mul_pd(a2_29, b2));
_mm256_storeu_pd(&C[(i*56)+39], c2_29);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_29 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a2_29 = _mm_loadu_pd(&values[118]);
c2_29 = _mm_add_pd(c2_29, _mm_mul_pd(a2_29, b2));
_mm_storeu_pd(&C[(i*56)+39], c2_29);
__m128d c2_31 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a2_31 = _mm_loadu_pd(&values[120]);
c2_31 = _mm_add_pd(c2_31, _mm_mul_pd(a2_31, b2));
_mm_storeu_pd(&C[(i*56)+41], c2_31);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_33 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a2_33 = _mm256_loadu_pd(&values[122]);
c2_33 = _mm256_add_pd(c2_33, _mm256_mul_pd(a2_33, b2));
_mm256_storeu_pd(&C[(i*56)+43], c2_33);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_33 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a2_33 = _mm_loadu_pd(&values[122]);
c2_33 = _mm_add_pd(c2_33, _mm_mul_pd(a2_33, b2));
_mm_storeu_pd(&C[(i*56)+43], c2_33);
__m128d c2_35 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a2_35 = _mm_loadu_pd(&values[124]);
c2_35 = _mm_add_pd(c2_35, _mm_mul_pd(a2_35, b2));
_mm_storeu_pd(&C[(i*56)+45], c2_35);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c2_37 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a2_37 = _mm256_loadu_pd(&values[126]);
c2_37 = _mm256_add_pd(c2_37, _mm256_mul_pd(a2_37, b2));
_mm256_storeu_pd(&C[(i*56)+47], c2_37);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c2_37 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a2_37 = _mm_loadu_pd(&values[126]);
c2_37 = _mm_add_pd(c2_37, _mm_mul_pd(a2_37, b2));
_mm_storeu_pd(&C[(i*56)+47], c2_37);
__m128d c2_39 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a2_39 = _mm_loadu_pd(&values[128]);
c2_39 = _mm_add_pd(c2_39, _mm_mul_pd(a2_39, b2));
_mm_storeu_pd(&C[(i*56)+49], c2_39);
#endif
__m128d c2_41 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a2_41 = _mm_loadu_pd(&values[130]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_41 = _mm_add_pd(c2_41, _mm_mul_pd(a2_41, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_41 = _mm_add_pd(c2_41, _mm_mul_pd(a2_41, b2));
#endif
_mm_storeu_pd(&C[(i*56)+51], c2_41);
__m128d c2_43 = _mm_load_sd(&C[(i*56)+54]);
__m128d a2_43 = _mm_load_sd(&values[132]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_43 = _mm_add_sd(c2_43, _mm_mul_sd(a2_43, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_43 = _mm_add_sd(c2_43, _mm_mul_sd(a2_43, b2));
#endif
_mm_store_sd(&C[(i*56)+54], c2_43);
#else
C[(i*56)+4] += values[89] * B[(i*56)+2];
C[(i*56)+5] += values[90] * B[(i*56)+2];
C[(i*56)+6] += values[91] * B[(i*56)+2];
C[(i*56)+8] += values[92] * B[(i*56)+2];
C[(i*56)+10] += values[93] * B[(i*56)+2];
C[(i*56)+11] += values[94] * B[(i*56)+2];
C[(i*56)+12] += values[95] * B[(i*56)+2];
C[(i*56)+13] += values[96] * B[(i*56)+2];
C[(i*56)+14] += values[97] * B[(i*56)+2];
C[(i*56)+15] += values[98] * B[(i*56)+2];
C[(i*56)+16] += values[99] * B[(i*56)+2];
C[(i*56)+18] += values[100] * B[(i*56)+2];
C[(i*56)+20] += values[101] * B[(i*56)+2];
C[(i*56)+21] += values[102] * B[(i*56)+2];
C[(i*56)+22] += values[103] * B[(i*56)+2];
C[(i*56)+23] += values[104] * B[(i*56)+2];
C[(i*56)+24] += values[105] * B[(i*56)+2];
C[(i*56)+25] += values[106] * B[(i*56)+2];
C[(i*56)+26] += values[107] * B[(i*56)+2];
C[(i*56)+27] += values[108] * B[(i*56)+2];
C[(i*56)+28] += values[109] * B[(i*56)+2];
C[(i*56)+29] += values[110] * B[(i*56)+2];
C[(i*56)+30] += values[111] * B[(i*56)+2];
C[(i*56)+31] += values[112] * B[(i*56)+2];
C[(i*56)+33] += values[113] * B[(i*56)+2];
C[(i*56)+35] += values[114] * B[(i*56)+2];
C[(i*56)+36] += values[115] * B[(i*56)+2];
C[(i*56)+37] += values[116] * B[(i*56)+2];
C[(i*56)+38] += values[117] * B[(i*56)+2];
C[(i*56)+39] += values[118] * B[(i*56)+2];
C[(i*56)+40] += values[119] * B[(i*56)+2];
C[(i*56)+41] += values[120] * B[(i*56)+2];
C[(i*56)+42] += values[121] * B[(i*56)+2];
C[(i*56)+43] += values[122] * B[(i*56)+2];
C[(i*56)+44] += values[123] * B[(i*56)+2];
C[(i*56)+45] += values[124] * B[(i*56)+2];
C[(i*56)+46] += values[125] * B[(i*56)+2];
C[(i*56)+47] += values[126] * B[(i*56)+2];
C[(i*56)+48] += values[127] * B[(i*56)+2];
C[(i*56)+49] += values[128] * B[(i*56)+2];
C[(i*56)+50] += values[129] * B[(i*56)+2];
C[(i*56)+51] += values[130] * B[(i*56)+2];
C[(i*56)+52] += values[131] * B[(i*56)+2];
C[(i*56)+54] += values[132] * B[(i*56)+2];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*56)+3]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_0 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a3_0 = _mm256_loadu_pd(&values[133]);
c3_0 = _mm256_add_pd(c3_0, _mm256_mul_pd(a3_0, b3));
_mm256_storeu_pd(&C[(i*56)+4], c3_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_0 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a3_0 = _mm_loadu_pd(&values[133]);
c3_0 = _mm_add_pd(c3_0, _mm_mul_pd(a3_0, b3));
_mm_storeu_pd(&C[(i*56)+4], c3_0);
__m128d c3_2 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a3_2 = _mm_loadu_pd(&values[135]);
c3_2 = _mm_add_pd(c3_2, _mm_mul_pd(a3_2, b3));
_mm_storeu_pd(&C[(i*56)+6], c3_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_4 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a3_4 = _mm256_loadu_pd(&values[137]);
c3_4 = _mm256_add_pd(c3_4, _mm256_mul_pd(a3_4, b3));
_mm256_storeu_pd(&C[(i*56)+8], c3_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a3_4 = _mm_loadu_pd(&values[137]);
c3_4 = _mm_add_pd(c3_4, _mm_mul_pd(a3_4, b3));
_mm_storeu_pd(&C[(i*56)+8], c3_4);
__m128d c3_6 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a3_6 = _mm_loadu_pd(&values[139]);
c3_6 = _mm_add_pd(c3_6, _mm_mul_pd(a3_6, b3));
_mm_storeu_pd(&C[(i*56)+10], c3_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_8 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a3_8 = _mm256_loadu_pd(&values[141]);
c3_8 = _mm256_add_pd(c3_8, _mm256_mul_pd(a3_8, b3));
_mm256_storeu_pd(&C[(i*56)+12], c3_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_8 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a3_8 = _mm_loadu_pd(&values[141]);
c3_8 = _mm_add_pd(c3_8, _mm_mul_pd(a3_8, b3));
_mm_storeu_pd(&C[(i*56)+12], c3_8);
__m128d c3_10 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a3_10 = _mm_loadu_pd(&values[143]);
c3_10 = _mm_add_pd(c3_10, _mm_mul_pd(a3_10, b3));
_mm_storeu_pd(&C[(i*56)+14], c3_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_12 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a3_12 = _mm256_loadu_pd(&values[145]);
c3_12 = _mm256_add_pd(c3_12, _mm256_mul_pd(a3_12, b3));
_mm256_storeu_pd(&C[(i*56)+16], c3_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_12 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a3_12 = _mm_loadu_pd(&values[145]);
c3_12 = _mm_add_pd(c3_12, _mm_mul_pd(a3_12, b3));
_mm_storeu_pd(&C[(i*56)+16], c3_12);
__m128d c3_14 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a3_14 = _mm_loadu_pd(&values[147]);
c3_14 = _mm_add_pd(c3_14, _mm_mul_pd(a3_14, b3));
_mm_storeu_pd(&C[(i*56)+18], c3_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_16 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a3_16 = _mm256_loadu_pd(&values[149]);
c3_16 = _mm256_add_pd(c3_16, _mm256_mul_pd(a3_16, b3));
_mm256_storeu_pd(&C[(i*56)+20], c3_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_16 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a3_16 = _mm_loadu_pd(&values[149]);
c3_16 = _mm_add_pd(c3_16, _mm_mul_pd(a3_16, b3));
_mm_storeu_pd(&C[(i*56)+20], c3_16);
__m128d c3_18 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a3_18 = _mm_loadu_pd(&values[151]);
c3_18 = _mm_add_pd(c3_18, _mm_mul_pd(a3_18, b3));
_mm_storeu_pd(&C[(i*56)+22], c3_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_20 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a3_20 = _mm256_loadu_pd(&values[153]);
c3_20 = _mm256_add_pd(c3_20, _mm256_mul_pd(a3_20, b3));
_mm256_storeu_pd(&C[(i*56)+24], c3_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_20 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a3_20 = _mm_loadu_pd(&values[153]);
c3_20 = _mm_add_pd(c3_20, _mm_mul_pd(a3_20, b3));
_mm_storeu_pd(&C[(i*56)+24], c3_20);
__m128d c3_22 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a3_22 = _mm_loadu_pd(&values[155]);
c3_22 = _mm_add_pd(c3_22, _mm_mul_pd(a3_22, b3));
_mm_storeu_pd(&C[(i*56)+26], c3_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_24 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a3_24 = _mm256_loadu_pd(&values[157]);
c3_24 = _mm256_add_pd(c3_24, _mm256_mul_pd(a3_24, b3));
_mm256_storeu_pd(&C[(i*56)+28], c3_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_24 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a3_24 = _mm_loadu_pd(&values[157]);
c3_24 = _mm_add_pd(c3_24, _mm_mul_pd(a3_24, b3));
_mm_storeu_pd(&C[(i*56)+28], c3_24);
__m128d c3_26 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a3_26 = _mm_loadu_pd(&values[159]);
c3_26 = _mm_add_pd(c3_26, _mm_mul_pd(a3_26, b3));
_mm_storeu_pd(&C[(i*56)+30], c3_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_28 = _mm256_loadu_pd(&C[(i*56)+32]);
__m256d a3_28 = _mm256_loadu_pd(&values[161]);
c3_28 = _mm256_add_pd(c3_28, _mm256_mul_pd(a3_28, b3));
_mm256_storeu_pd(&C[(i*56)+32], c3_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_28 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a3_28 = _mm_loadu_pd(&values[161]);
c3_28 = _mm_add_pd(c3_28, _mm_mul_pd(a3_28, b3));
_mm_storeu_pd(&C[(i*56)+32], c3_28);
__m128d c3_30 = _mm_loadu_pd(&C[(i*56)+34]);
__m128d a3_30 = _mm_loadu_pd(&values[163]);
c3_30 = _mm_add_pd(c3_30, _mm_mul_pd(a3_30, b3));
_mm_storeu_pd(&C[(i*56)+34], c3_30);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_32 = _mm256_loadu_pd(&C[(i*56)+36]);
__m256d a3_32 = _mm256_loadu_pd(&values[165]);
c3_32 = _mm256_add_pd(c3_32, _mm256_mul_pd(a3_32, b3));
_mm256_storeu_pd(&C[(i*56)+36], c3_32);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_32 = _mm_loadu_pd(&C[(i*56)+36]);
__m128d a3_32 = _mm_loadu_pd(&values[165]);
c3_32 = _mm_add_pd(c3_32, _mm_mul_pd(a3_32, b3));
_mm_storeu_pd(&C[(i*56)+36], c3_32);
__m128d c3_34 = _mm_loadu_pd(&C[(i*56)+38]);
__m128d a3_34 = _mm_loadu_pd(&values[167]);
c3_34 = _mm_add_pd(c3_34, _mm_mul_pd(a3_34, b3));
_mm_storeu_pd(&C[(i*56)+38], c3_34);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_36 = _mm256_loadu_pd(&C[(i*56)+40]);
__m256d a3_36 = _mm256_loadu_pd(&values[169]);
c3_36 = _mm256_add_pd(c3_36, _mm256_mul_pd(a3_36, b3));
_mm256_storeu_pd(&C[(i*56)+40], c3_36);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_36 = _mm_loadu_pd(&C[(i*56)+40]);
__m128d a3_36 = _mm_loadu_pd(&values[169]);
c3_36 = _mm_add_pd(c3_36, _mm_mul_pd(a3_36, b3));
_mm_storeu_pd(&C[(i*56)+40], c3_36);
__m128d c3_38 = _mm_loadu_pd(&C[(i*56)+42]);
__m128d a3_38 = _mm_loadu_pd(&values[171]);
c3_38 = _mm_add_pd(c3_38, _mm_mul_pd(a3_38, b3));
_mm_storeu_pd(&C[(i*56)+42], c3_38);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_40 = _mm256_loadu_pd(&C[(i*56)+44]);
__m256d a3_40 = _mm256_loadu_pd(&values[173]);
c3_40 = _mm256_add_pd(c3_40, _mm256_mul_pd(a3_40, b3));
_mm256_storeu_pd(&C[(i*56)+44], c3_40);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_40 = _mm_loadu_pd(&C[(i*56)+44]);
__m128d a3_40 = _mm_loadu_pd(&values[173]);
c3_40 = _mm_add_pd(c3_40, _mm_mul_pd(a3_40, b3));
_mm_storeu_pd(&C[(i*56)+44], c3_40);
__m128d c3_42 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a3_42 = _mm_loadu_pd(&values[175]);
c3_42 = _mm_add_pd(c3_42, _mm_mul_pd(a3_42, b3));
_mm_storeu_pd(&C[(i*56)+46], c3_42);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_44 = _mm256_loadu_pd(&C[(i*56)+48]);
__m256d a3_44 = _mm256_loadu_pd(&values[177]);
c3_44 = _mm256_add_pd(c3_44, _mm256_mul_pd(a3_44, b3));
_mm256_storeu_pd(&C[(i*56)+48], c3_44);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_44 = _mm_loadu_pd(&C[(i*56)+48]);
__m128d a3_44 = _mm_loadu_pd(&values[177]);
c3_44 = _mm_add_pd(c3_44, _mm_mul_pd(a3_44, b3));
_mm_storeu_pd(&C[(i*56)+48], c3_44);
__m128d c3_46 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a3_46 = _mm_loadu_pd(&values[179]);
c3_46 = _mm_add_pd(c3_46, _mm_mul_pd(a3_46, b3));
_mm_storeu_pd(&C[(i*56)+50], c3_46);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c3_48 = _mm256_loadu_pd(&C[(i*56)+52]);
__m256d a3_48 = _mm256_loadu_pd(&values[181]);
c3_48 = _mm256_add_pd(c3_48, _mm256_mul_pd(a3_48, b3));
_mm256_storeu_pd(&C[(i*56)+52], c3_48);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c3_48 = _mm_loadu_pd(&C[(i*56)+52]);
__m128d a3_48 = _mm_loadu_pd(&values[181]);
c3_48 = _mm_add_pd(c3_48, _mm_mul_pd(a3_48, b3));
_mm_storeu_pd(&C[(i*56)+52], c3_48);
__m128d c3_50 = _mm_loadu_pd(&C[(i*56)+54]);
__m128d a3_50 = _mm_loadu_pd(&values[183]);
c3_50 = _mm_add_pd(c3_50, _mm_mul_pd(a3_50, b3));
_mm_storeu_pd(&C[(i*56)+54], c3_50);
#endif
#else
C[(i*56)+4] += values[133] * B[(i*56)+3];
C[(i*56)+5] += values[134] * B[(i*56)+3];
C[(i*56)+6] += values[135] * B[(i*56)+3];
C[(i*56)+7] += values[136] * B[(i*56)+3];
C[(i*56)+8] += values[137] * B[(i*56)+3];
C[(i*56)+9] += values[138] * B[(i*56)+3];
C[(i*56)+10] += values[139] * B[(i*56)+3];
C[(i*56)+11] += values[140] * B[(i*56)+3];
C[(i*56)+12] += values[141] * B[(i*56)+3];
C[(i*56)+13] += values[142] * B[(i*56)+3];
C[(i*56)+14] += values[143] * B[(i*56)+3];
C[(i*56)+15] += values[144] * B[(i*56)+3];
C[(i*56)+16] += values[145] * B[(i*56)+3];
C[(i*56)+17] += values[146] * B[(i*56)+3];
C[(i*56)+18] += values[147] * B[(i*56)+3];
C[(i*56)+19] += values[148] * B[(i*56)+3];
C[(i*56)+20] += values[149] * B[(i*56)+3];
C[(i*56)+21] += values[150] * B[(i*56)+3];
C[(i*56)+22] += values[151] * B[(i*56)+3];
C[(i*56)+23] += values[152] * B[(i*56)+3];
C[(i*56)+24] += values[153] * B[(i*56)+3];
C[(i*56)+25] += values[154] * B[(i*56)+3];
C[(i*56)+26] += values[155] * B[(i*56)+3];
C[(i*56)+27] += values[156] * B[(i*56)+3];
C[(i*56)+28] += values[157] * B[(i*56)+3];
C[(i*56)+29] += values[158] * B[(i*56)+3];
C[(i*56)+30] += values[159] * B[(i*56)+3];
C[(i*56)+31] += values[160] * B[(i*56)+3];
C[(i*56)+32] += values[161] * B[(i*56)+3];
C[(i*56)+33] += values[162] * B[(i*56)+3];
C[(i*56)+34] += values[163] * B[(i*56)+3];
C[(i*56)+35] += values[164] * B[(i*56)+3];
C[(i*56)+36] += values[165] * B[(i*56)+3];
C[(i*56)+37] += values[166] * B[(i*56)+3];
C[(i*56)+38] += values[167] * B[(i*56)+3];
C[(i*56)+39] += values[168] * B[(i*56)+3];
C[(i*56)+40] += values[169] * B[(i*56)+3];
C[(i*56)+41] += values[170] * B[(i*56)+3];
C[(i*56)+42] += values[171] * B[(i*56)+3];
C[(i*56)+43] += values[172] * B[(i*56)+3];
C[(i*56)+44] += values[173] * B[(i*56)+3];
C[(i*56)+45] += values[174] * B[(i*56)+3];
C[(i*56)+46] += values[175] * B[(i*56)+3];
C[(i*56)+47] += values[176] * B[(i*56)+3];
C[(i*56)+48] += values[177] * B[(i*56)+3];
C[(i*56)+49] += values[178] * B[(i*56)+3];
C[(i*56)+50] += values[179] * B[(i*56)+3];
C[(i*56)+51] += values[180] * B[(i*56)+3];
C[(i*56)+52] += values[181] * B[(i*56)+3];
C[(i*56)+53] += values[182] * B[(i*56)+3];
C[(i*56)+54] += values[183] * B[(i*56)+3];
C[(i*56)+55] += values[184] * B[(i*56)+3];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*56)+4]);
#endif
__m128d c4_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a4_0 = _mm_loadu_pd(&values[185]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
#endif
_mm_storeu_pd(&C[(i*56)+10], c4_0);
__m128d c4_2 = _mm_load_sd(&C[(i*56)+14]);
__m128d a4_2 = _mm_load_sd(&values[187]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_2 = _mm_add_sd(c4_2, _mm_mul_sd(a4_2, b4));
#endif
_mm_store_sd(&C[(i*56)+14], c4_2);
__m128d c4_3 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a4_3 = _mm_loadu_pd(&values[188]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_3 = _mm_add_pd(c4_3, _mm_mul_pd(a4_3, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_3 = _mm_add_pd(c4_3, _mm_mul_pd(a4_3, b4));
#endif
_mm_storeu_pd(&C[(i*56)+20], c4_3);
__m128d c4_5 = _mm_load_sd(&C[(i*56)+22]);
__m128d a4_5 = _mm_load_sd(&values[190]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_5 = _mm_add_sd(c4_5, _mm_mul_sd(a4_5, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_5 = _mm_add_sd(c4_5, _mm_mul_sd(a4_5, b4));
#endif
_mm_store_sd(&C[(i*56)+22], c4_5);
__m128d c4_6 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a4_6 = _mm_loadu_pd(&values[191]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_6 = _mm_add_pd(c4_6, _mm_mul_pd(a4_6, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_6 = _mm_add_pd(c4_6, _mm_mul_pd(a4_6, b4));
#endif
_mm_storeu_pd(&C[(i*56)+25], c4_6);
__m128d c4_8 = _mm_load_sd(&C[(i*56)+29]);
__m128d a4_8 = _mm_load_sd(&values[193]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_8 = _mm_add_sd(c4_8, _mm_mul_sd(a4_8, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_8 = _mm_add_sd(c4_8, _mm_mul_sd(a4_8, b4));
#endif
_mm_store_sd(&C[(i*56)+29], c4_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_9 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a4_9 = _mm256_loadu_pd(&values[194]);
c4_9 = _mm256_add_pd(c4_9, _mm256_mul_pd(a4_9, b4));
_mm256_storeu_pd(&C[(i*56)+35], c4_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_9 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a4_9 = _mm_loadu_pd(&values[194]);
c4_9 = _mm_add_pd(c4_9, _mm_mul_pd(a4_9, b4));
_mm_storeu_pd(&C[(i*56)+35], c4_9);
__m128d c4_11 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a4_11 = _mm_loadu_pd(&values[196]);
c4_11 = _mm_add_pd(c4_11, _mm_mul_pd(a4_11, b4));
_mm_storeu_pd(&C[(i*56)+37], c4_11);
#endif
__m128d c4_13 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a4_13 = _mm_loadu_pd(&values[198]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_13 = _mm_add_pd(c4_13, _mm_mul_pd(a4_13, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_13 = _mm_add_pd(c4_13, _mm_mul_pd(a4_13, b4));
#endif
_mm_storeu_pd(&C[(i*56)+41], c4_13);
__m128d c4_15 = _mm_load_sd(&C[(i*56)+43]);
__m128d a4_15 = _mm_load_sd(&values[200]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_15 = _mm_add_sd(c4_15, _mm_mul_sd(a4_15, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_15 = _mm_add_sd(c4_15, _mm_mul_sd(a4_15, b4));
#endif
_mm_store_sd(&C[(i*56)+43], c4_15);
__m128d c4_16 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a4_16 = _mm_loadu_pd(&values[201]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_16 = _mm_add_pd(c4_16, _mm_mul_pd(a4_16, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_16 = _mm_add_pd(c4_16, _mm_mul_pd(a4_16, b4));
#endif
_mm_storeu_pd(&C[(i*56)+46], c4_16);
__m128d c4_18 = _mm_load_sd(&C[(i*56)+50]);
__m128d a4_18 = _mm_load_sd(&values[203]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_18 = _mm_add_sd(c4_18, _mm_mul_sd(a4_18, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_18 = _mm_add_sd(c4_18, _mm_mul_sd(a4_18, b4));
#endif
_mm_store_sd(&C[(i*56)+50], c4_18);
#else
C[(i*56)+10] += values[185] * B[(i*56)+4];
C[(i*56)+11] += values[186] * B[(i*56)+4];
C[(i*56)+14] += values[187] * B[(i*56)+4];
C[(i*56)+20] += values[188] * B[(i*56)+4];
C[(i*56)+21] += values[189] * B[(i*56)+4];
C[(i*56)+22] += values[190] * B[(i*56)+4];
C[(i*56)+25] += values[191] * B[(i*56)+4];
C[(i*56)+26] += values[192] * B[(i*56)+4];
C[(i*56)+29] += values[193] * B[(i*56)+4];
C[(i*56)+35] += values[194] * B[(i*56)+4];
C[(i*56)+36] += values[195] * B[(i*56)+4];
C[(i*56)+37] += values[196] * B[(i*56)+4];
C[(i*56)+38] += values[197] * B[(i*56)+4];
C[(i*56)+41] += values[198] * B[(i*56)+4];
C[(i*56)+42] += values[199] * B[(i*56)+4];
C[(i*56)+43] += values[200] * B[(i*56)+4];
C[(i*56)+46] += values[201] * B[(i*56)+4];
C[(i*56)+47] += values[202] * B[(i*56)+4];
C[(i*56)+50] += values[203] * B[(i*56)+4];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*56)+5]);
#endif
__m128d c5_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a5_0 = _mm_loadu_pd(&values[204]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
#endif
_mm_storeu_pd(&C[(i*56)+10], c5_0);
__m128d c5_2 = _mm_load_sd(&C[(i*56)+12]);
__m128d a5_2 = _mm_load_sd(&values[206]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_2 = _mm_add_sd(c5_2, _mm_mul_sd(a5_2, b5));
#endif
_mm_store_sd(&C[(i*56)+12], c5_2);
__m128d c5_3 = _mm_load_sd(&C[(i*56)+15]);
__m128d a5_3 = _mm_load_sd(&values[207]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_3 = _mm_add_sd(c5_3, _mm_mul_sd(a5_3, b5));
#endif
_mm_store_sd(&C[(i*56)+15], c5_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_4 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a5_4 = _mm256_loadu_pd(&values[208]);
c5_4 = _mm256_add_pd(c5_4, _mm256_mul_pd(a5_4, b5));
_mm256_storeu_pd(&C[(i*56)+20], c5_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_4 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a5_4 = _mm_loadu_pd(&values[208]);
c5_4 = _mm_add_pd(c5_4, _mm_mul_pd(a5_4, b5));
_mm_storeu_pd(&C[(i*56)+20], c5_4);
__m128d c5_6 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a5_6 = _mm_loadu_pd(&values[210]);
c5_6 = _mm_add_pd(c5_6, _mm_mul_pd(a5_6, b5));
_mm_storeu_pd(&C[(i*56)+22], c5_6);
#endif
__m128d c5_8 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a5_8 = _mm_loadu_pd(&values[212]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_8 = _mm_add_pd(c5_8, _mm_mul_pd(a5_8, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_8 = _mm_add_pd(c5_8, _mm_mul_pd(a5_8, b5));
#endif
_mm_storeu_pd(&C[(i*56)+25], c5_8);
__m128d c5_10 = _mm_load_sd(&C[(i*56)+27]);
__m128d a5_10 = _mm_load_sd(&values[214]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_10 = _mm_add_sd(c5_10, _mm_mul_sd(a5_10, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_10 = _mm_add_sd(c5_10, _mm_mul_sd(a5_10, b5));
#endif
_mm_store_sd(&C[(i*56)+27], c5_10);
__m128d c5_11 = _mm_load_sd(&C[(i*56)+30]);
__m128d a5_11 = _mm_load_sd(&values[215]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_11 = _mm_add_sd(c5_11, _mm_mul_sd(a5_11, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_11 = _mm_add_sd(c5_11, _mm_mul_sd(a5_11, b5));
#endif
_mm_store_sd(&C[(i*56)+30], c5_11);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_12 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a5_12 = _mm256_loadu_pd(&values[216]);
c5_12 = _mm256_add_pd(c5_12, _mm256_mul_pd(a5_12, b5));
_mm256_storeu_pd(&C[(i*56)+35], c5_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_12 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a5_12 = _mm_loadu_pd(&values[216]);
c5_12 = _mm_add_pd(c5_12, _mm_mul_pd(a5_12, b5));
_mm_storeu_pd(&C[(i*56)+35], c5_12);
__m128d c5_14 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a5_14 = _mm_loadu_pd(&values[218]);
c5_14 = _mm_add_pd(c5_14, _mm_mul_pd(a5_14, b5));
_mm_storeu_pd(&C[(i*56)+37], c5_14);
#endif
__m128d c5_16 = _mm_load_sd(&C[(i*56)+39]);
__m128d a5_16 = _mm_load_sd(&values[220]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_16 = _mm_add_sd(c5_16, _mm_mul_sd(a5_16, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_16 = _mm_add_sd(c5_16, _mm_mul_sd(a5_16, b5));
#endif
_mm_store_sd(&C[(i*56)+39], c5_16);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_17 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a5_17 = _mm256_loadu_pd(&values[221]);
c5_17 = _mm256_add_pd(c5_17, _mm256_mul_pd(a5_17, b5));
_mm256_storeu_pd(&C[(i*56)+41], c5_17);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_17 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a5_17 = _mm_loadu_pd(&values[221]);
c5_17 = _mm_add_pd(c5_17, _mm_mul_pd(a5_17, b5));
_mm_storeu_pd(&C[(i*56)+41], c5_17);
__m128d c5_19 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a5_19 = _mm_loadu_pd(&values[223]);
c5_19 = _mm_add_pd(c5_19, _mm_mul_pd(a5_19, b5));
_mm_storeu_pd(&C[(i*56)+43], c5_19);
#endif
__m128d c5_21 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a5_21 = _mm_loadu_pd(&values[225]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_21 = _mm_add_pd(c5_21, _mm_mul_pd(a5_21, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_21 = _mm_add_pd(c5_21, _mm_mul_pd(a5_21, b5));
#endif
_mm_storeu_pd(&C[(i*56)+46], c5_21);
__m128d c5_23 = _mm_load_sd(&C[(i*56)+48]);
__m128d a5_23 = _mm_load_sd(&values[227]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_23 = _mm_add_sd(c5_23, _mm_mul_sd(a5_23, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_23 = _mm_add_sd(c5_23, _mm_mul_sd(a5_23, b5));
#endif
_mm_store_sd(&C[(i*56)+48], c5_23);
__m128d c5_24 = _mm_load_sd(&C[(i*56)+51]);
__m128d a5_24 = _mm_load_sd(&values[228]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_24 = _mm_add_sd(c5_24, _mm_mul_sd(a5_24, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_24 = _mm_add_sd(c5_24, _mm_mul_sd(a5_24, b5));
#endif
_mm_store_sd(&C[(i*56)+51], c5_24);
#else
C[(i*56)+10] += values[204] * B[(i*56)+5];
C[(i*56)+11] += values[205] * B[(i*56)+5];
C[(i*56)+12] += values[206] * B[(i*56)+5];
C[(i*56)+15] += values[207] * B[(i*56)+5];
C[(i*56)+20] += values[208] * B[(i*56)+5];
C[(i*56)+21] += values[209] * B[(i*56)+5];
C[(i*56)+22] += values[210] * B[(i*56)+5];
C[(i*56)+23] += values[211] * B[(i*56)+5];
C[(i*56)+25] += values[212] * B[(i*56)+5];
C[(i*56)+26] += values[213] * B[(i*56)+5];
C[(i*56)+27] += values[214] * B[(i*56)+5];
C[(i*56)+30] += values[215] * B[(i*56)+5];
C[(i*56)+35] += values[216] * B[(i*56)+5];
C[(i*56)+36] += values[217] * B[(i*56)+5];
C[(i*56)+37] += values[218] * B[(i*56)+5];
C[(i*56)+38] += values[219] * B[(i*56)+5];
C[(i*56)+39] += values[220] * B[(i*56)+5];
C[(i*56)+41] += values[221] * B[(i*56)+5];
C[(i*56)+42] += values[222] * B[(i*56)+5];
C[(i*56)+43] += values[223] * B[(i*56)+5];
C[(i*56)+44] += values[224] * B[(i*56)+5];
C[(i*56)+46] += values[225] * B[(i*56)+5];
C[(i*56)+47] += values[226] * B[(i*56)+5];
C[(i*56)+48] += values[227] * B[(i*56)+5];
C[(i*56)+51] += values[228] * B[(i*56)+5];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*56)+6]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_0 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a6_0 = _mm256_loadu_pd(&values[229]);
c6_0 = _mm256_add_pd(c6_0, _mm256_mul_pd(a6_0, b6));
_mm256_storeu_pd(&C[(i*56)+10], c6_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a6_0 = _mm_loadu_pd(&values[229]);
c6_0 = _mm_add_pd(c6_0, _mm_mul_pd(a6_0, b6));
_mm_storeu_pd(&C[(i*56)+10], c6_0);
__m128d c6_2 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a6_2 = _mm_loadu_pd(&values[231]);
c6_2 = _mm_add_pd(c6_2, _mm_mul_pd(a6_2, b6));
_mm_storeu_pd(&C[(i*56)+12], c6_2);
#endif
__m128d c6_4 = _mm_load_sd(&C[(i*56)+16]);
__m128d a6_4 = _mm_load_sd(&values[233]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_4 = _mm_add_sd(c6_4, _mm_mul_sd(a6_4, b6));
#endif
_mm_store_sd(&C[(i*56)+16], c6_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_5 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a6_5 = _mm256_loadu_pd(&values[234]);
c6_5 = _mm256_add_pd(c6_5, _mm256_mul_pd(a6_5, b6));
_mm256_storeu_pd(&C[(i*56)+20], c6_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_5 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a6_5 = _mm_loadu_pd(&values[234]);
c6_5 = _mm_add_pd(c6_5, _mm_mul_pd(a6_5, b6));
_mm_storeu_pd(&C[(i*56)+20], c6_5);
__m128d c6_7 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a6_7 = _mm_loadu_pd(&values[236]);
c6_7 = _mm_add_pd(c6_7, _mm_mul_pd(a6_7, b6));
_mm_storeu_pd(&C[(i*56)+22], c6_7);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_9 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a6_9 = _mm256_loadu_pd(&values[238]);
c6_9 = _mm256_add_pd(c6_9, _mm256_mul_pd(a6_9, b6));
_mm256_storeu_pd(&C[(i*56)+24], c6_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_9 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a6_9 = _mm_loadu_pd(&values[238]);
c6_9 = _mm_add_pd(c6_9, _mm_mul_pd(a6_9, b6));
_mm_storeu_pd(&C[(i*56)+24], c6_9);
__m128d c6_11 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a6_11 = _mm_loadu_pd(&values[240]);
c6_11 = _mm_add_pd(c6_11, _mm_mul_pd(a6_11, b6));
_mm_storeu_pd(&C[(i*56)+26], c6_11);
#endif
__m128d c6_13 = _mm_load_sd(&C[(i*56)+28]);
__m128d a6_13 = _mm_load_sd(&values[242]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_13 = _mm_add_sd(c6_13, _mm_mul_sd(a6_13, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_13 = _mm_add_sd(c6_13, _mm_mul_sd(a6_13, b6));
#endif
_mm_store_sd(&C[(i*56)+28], c6_13);
__m128d c6_14 = _mm_load_sd(&C[(i*56)+31]);
__m128d a6_14 = _mm_load_sd(&values[243]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_14 = _mm_add_sd(c6_14, _mm_mul_sd(a6_14, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_14 = _mm_add_sd(c6_14, _mm_mul_sd(a6_14, b6));
#endif
_mm_store_sd(&C[(i*56)+31], c6_14);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_15 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a6_15 = _mm256_loadu_pd(&values[244]);
c6_15 = _mm256_add_pd(c6_15, _mm256_mul_pd(a6_15, b6));
_mm256_storeu_pd(&C[(i*56)+35], c6_15);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_15 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a6_15 = _mm_loadu_pd(&values[244]);
c6_15 = _mm_add_pd(c6_15, _mm_mul_pd(a6_15, b6));
_mm_storeu_pd(&C[(i*56)+35], c6_15);
__m128d c6_17 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a6_17 = _mm_loadu_pd(&values[246]);
c6_17 = _mm_add_pd(c6_17, _mm_mul_pd(a6_17, b6));
_mm_storeu_pd(&C[(i*56)+37], c6_17);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_19 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a6_19 = _mm256_loadu_pd(&values[248]);
c6_19 = _mm256_add_pd(c6_19, _mm256_mul_pd(a6_19, b6));
_mm256_storeu_pd(&C[(i*56)+39], c6_19);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_19 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a6_19 = _mm_loadu_pd(&values[248]);
c6_19 = _mm_add_pd(c6_19, _mm_mul_pd(a6_19, b6));
_mm_storeu_pd(&C[(i*56)+39], c6_19);
__m128d c6_21 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a6_21 = _mm_loadu_pd(&values[250]);
c6_21 = _mm_add_pd(c6_21, _mm_mul_pd(a6_21, b6));
_mm_storeu_pd(&C[(i*56)+41], c6_21);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c6_23 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a6_23 = _mm256_loadu_pd(&values[252]);
c6_23 = _mm256_add_pd(c6_23, _mm256_mul_pd(a6_23, b6));
_mm256_storeu_pd(&C[(i*56)+43], c6_23);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c6_23 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a6_23 = _mm_loadu_pd(&values[252]);
c6_23 = _mm_add_pd(c6_23, _mm_mul_pd(a6_23, b6));
_mm_storeu_pd(&C[(i*56)+43], c6_23);
__m128d c6_25 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a6_25 = _mm_loadu_pd(&values[254]);
c6_25 = _mm_add_pd(c6_25, _mm_mul_pd(a6_25, b6));
_mm_storeu_pd(&C[(i*56)+45], c6_25);
#endif
__m128d c6_27 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a6_27 = _mm_loadu_pd(&values[256]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_27 = _mm_add_pd(c6_27, _mm_mul_pd(a6_27, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_27 = _mm_add_pd(c6_27, _mm_mul_pd(a6_27, b6));
#endif
_mm_storeu_pd(&C[(i*56)+47], c6_27);
__m128d c6_29 = _mm_load_sd(&C[(i*56)+49]);
__m128d a6_29 = _mm_load_sd(&values[258]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_29 = _mm_add_sd(c6_29, _mm_mul_sd(a6_29, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_29 = _mm_add_sd(c6_29, _mm_mul_sd(a6_29, b6));
#endif
_mm_store_sd(&C[(i*56)+49], c6_29);
__m128d c6_30 = _mm_load_sd(&C[(i*56)+52]);
__m128d a6_30 = _mm_load_sd(&values[259]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_30 = _mm_add_sd(c6_30, _mm_mul_sd(a6_30, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_30 = _mm_add_sd(c6_30, _mm_mul_sd(a6_30, b6));
#endif
_mm_store_sd(&C[(i*56)+52], c6_30);
#else
C[(i*56)+10] += values[229] * B[(i*56)+6];
C[(i*56)+11] += values[230] * B[(i*56)+6];
C[(i*56)+12] += values[231] * B[(i*56)+6];
C[(i*56)+13] += values[232] * B[(i*56)+6];
C[(i*56)+16] += values[233] * B[(i*56)+6];
C[(i*56)+20] += values[234] * B[(i*56)+6];
C[(i*56)+21] += values[235] * B[(i*56)+6];
C[(i*56)+22] += values[236] * B[(i*56)+6];
C[(i*56)+23] += values[237] * B[(i*56)+6];
C[(i*56)+24] += values[238] * B[(i*56)+6];
C[(i*56)+25] += values[239] * B[(i*56)+6];
C[(i*56)+26] += values[240] * B[(i*56)+6];
C[(i*56)+27] += values[241] * B[(i*56)+6];
C[(i*56)+28] += values[242] * B[(i*56)+6];
C[(i*56)+31] += values[243] * B[(i*56)+6];
C[(i*56)+35] += values[244] * B[(i*56)+6];
C[(i*56)+36] += values[245] * B[(i*56)+6];
C[(i*56)+37] += values[246] * B[(i*56)+6];
C[(i*56)+38] += values[247] * B[(i*56)+6];
C[(i*56)+39] += values[248] * B[(i*56)+6];
C[(i*56)+40] += values[249] * B[(i*56)+6];
C[(i*56)+41] += values[250] * B[(i*56)+6];
C[(i*56)+42] += values[251] * B[(i*56)+6];
C[(i*56)+43] += values[252] * B[(i*56)+6];
C[(i*56)+44] += values[253] * B[(i*56)+6];
C[(i*56)+45] += values[254] * B[(i*56)+6];
C[(i*56)+46] += values[255] * B[(i*56)+6];
C[(i*56)+47] += values[256] * B[(i*56)+6];
C[(i*56)+48] += values[257] * B[(i*56)+6];
C[(i*56)+49] += values[258] * B[(i*56)+6];
C[(i*56)+52] += values[259] * B[(i*56)+6];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*56)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a7_0 = _mm_loadu_pd(&values[260]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*56)+10], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*56)+12]);
__m128d a7_2 = _mm_load_sd(&values[262]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*56)+12], c7_2);
__m128d c7_3 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a7_3 = _mm_loadu_pd(&values[263]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_3 = _mm_add_pd(c7_3, _mm_mul_pd(a7_3, b7));
#endif
_mm_storeu_pd(&C[(i*56)+14], c7_3);
__m128d c7_5 = _mm_load_sd(&C[(i*56)+17]);
__m128d a7_5 = _mm_load_sd(&values[265]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_5 = _mm_add_sd(c7_5, _mm_mul_sd(a7_5, b7));
#endif
_mm_store_sd(&C[(i*56)+17], c7_5);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_6 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a7_6 = _mm256_loadu_pd(&values[266]);
c7_6 = _mm256_add_pd(c7_6, _mm256_mul_pd(a7_6, b7));
_mm256_storeu_pd(&C[(i*56)+20], c7_6);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_6 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a7_6 = _mm_loadu_pd(&values[266]);
c7_6 = _mm_add_pd(c7_6, _mm_mul_pd(a7_6, b7));
_mm_storeu_pd(&C[(i*56)+20], c7_6);
__m128d c7_8 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a7_8 = _mm_loadu_pd(&values[268]);
c7_8 = _mm_add_pd(c7_8, _mm_mul_pd(a7_8, b7));
_mm_storeu_pd(&C[(i*56)+22], c7_8);
#endif
__m128d c7_10 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a7_10 = _mm_loadu_pd(&values[270]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_10 = _mm_add_pd(c7_10, _mm_mul_pd(a7_10, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_10 = _mm_add_pd(c7_10, _mm_mul_pd(a7_10, b7));
#endif
_mm_storeu_pd(&C[(i*56)+25], c7_10);
__m128d c7_12 = _mm_load_sd(&C[(i*56)+27]);
__m128d a7_12 = _mm_load_sd(&values[272]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_12 = _mm_add_sd(c7_12, _mm_mul_sd(a7_12, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_12 = _mm_add_sd(c7_12, _mm_mul_sd(a7_12, b7));
#endif
_mm_store_sd(&C[(i*56)+27], c7_12);
__m128d c7_13 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a7_13 = _mm_loadu_pd(&values[273]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_13 = _mm_add_pd(c7_13, _mm_mul_pd(a7_13, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_13 = _mm_add_pd(c7_13, _mm_mul_pd(a7_13, b7));
#endif
_mm_storeu_pd(&C[(i*56)+29], c7_13);
__m128d c7_15 = _mm_load_sd(&C[(i*56)+32]);
__m128d a7_15 = _mm_load_sd(&values[275]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_15 = _mm_add_sd(c7_15, _mm_mul_sd(a7_15, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_15 = _mm_add_sd(c7_15, _mm_mul_sd(a7_15, b7));
#endif
_mm_store_sd(&C[(i*56)+32], c7_15);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_16 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a7_16 = _mm256_loadu_pd(&values[276]);
c7_16 = _mm256_add_pd(c7_16, _mm256_mul_pd(a7_16, b7));
_mm256_storeu_pd(&C[(i*56)+35], c7_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_16 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a7_16 = _mm_loadu_pd(&values[276]);
c7_16 = _mm_add_pd(c7_16, _mm_mul_pd(a7_16, b7));
_mm_storeu_pd(&C[(i*56)+35], c7_16);
__m128d c7_18 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a7_18 = _mm_loadu_pd(&values[278]);
c7_18 = _mm_add_pd(c7_18, _mm_mul_pd(a7_18, b7));
_mm_storeu_pd(&C[(i*56)+37], c7_18);
#endif
__m128d c7_20 = _mm_load_sd(&C[(i*56)+39]);
__m128d a7_20 = _mm_load_sd(&values[280]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_20 = _mm_add_sd(c7_20, _mm_mul_sd(a7_20, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_20 = _mm_add_sd(c7_20, _mm_mul_sd(a7_20, b7));
#endif
_mm_store_sd(&C[(i*56)+39], c7_20);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c7_21 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a7_21 = _mm256_loadu_pd(&values[281]);
c7_21 = _mm256_add_pd(c7_21, _mm256_mul_pd(a7_21, b7));
_mm256_storeu_pd(&C[(i*56)+41], c7_21);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c7_21 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a7_21 = _mm_loadu_pd(&values[281]);
c7_21 = _mm_add_pd(c7_21, _mm_mul_pd(a7_21, b7));
_mm_storeu_pd(&C[(i*56)+41], c7_21);
__m128d c7_23 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a7_23 = _mm_loadu_pd(&values[283]);
c7_23 = _mm_add_pd(c7_23, _mm_mul_pd(a7_23, b7));
_mm_storeu_pd(&C[(i*56)+43], c7_23);
#endif
__m128d c7_25 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a7_25 = _mm_loadu_pd(&values[285]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_25 = _mm_add_pd(c7_25, _mm_mul_pd(a7_25, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_25 = _mm_add_pd(c7_25, _mm_mul_pd(a7_25, b7));
#endif
_mm_storeu_pd(&C[(i*56)+46], c7_25);
__m128d c7_27 = _mm_load_sd(&C[(i*56)+48]);
__m128d a7_27 = _mm_load_sd(&values[287]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_27 = _mm_add_sd(c7_27, _mm_mul_sd(a7_27, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_27 = _mm_add_sd(c7_27, _mm_mul_sd(a7_27, b7));
#endif
_mm_store_sd(&C[(i*56)+48], c7_27);
__m128d c7_28 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a7_28 = _mm_loadu_pd(&values[288]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_28 = _mm_add_pd(c7_28, _mm_mul_pd(a7_28, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_28 = _mm_add_pd(c7_28, _mm_mul_pd(a7_28, b7));
#endif
_mm_storeu_pd(&C[(i*56)+50], c7_28);
__m128d c7_30 = _mm_load_sd(&C[(i*56)+53]);
__m128d a7_30 = _mm_load_sd(&values[290]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_30 = _mm_add_sd(c7_30, _mm_mul_sd(a7_30, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_30 = _mm_add_sd(c7_30, _mm_mul_sd(a7_30, b7));
#endif
_mm_store_sd(&C[(i*56)+53], c7_30);
#else
C[(i*56)+10] += values[260] * B[(i*56)+7];
C[(i*56)+11] += values[261] * B[(i*56)+7];
C[(i*56)+12] += values[262] * B[(i*56)+7];
C[(i*56)+14] += values[263] * B[(i*56)+7];
C[(i*56)+15] += values[264] * B[(i*56)+7];
C[(i*56)+17] += values[265] * B[(i*56)+7];
C[(i*56)+20] += values[266] * B[(i*56)+7];
C[(i*56)+21] += values[267] * B[(i*56)+7];
C[(i*56)+22] += values[268] * B[(i*56)+7];
C[(i*56)+23] += values[269] * B[(i*56)+7];
C[(i*56)+25] += values[270] * B[(i*56)+7];
C[(i*56)+26] += values[271] * B[(i*56)+7];
C[(i*56)+27] += values[272] * B[(i*56)+7];
C[(i*56)+29] += values[273] * B[(i*56)+7];
C[(i*56)+30] += values[274] * B[(i*56)+7];
C[(i*56)+32] += values[275] * B[(i*56)+7];
C[(i*56)+35] += values[276] * B[(i*56)+7];
C[(i*56)+36] += values[277] * B[(i*56)+7];
C[(i*56)+37] += values[278] * B[(i*56)+7];
C[(i*56)+38] += values[279] * B[(i*56)+7];
C[(i*56)+39] += values[280] * B[(i*56)+7];
C[(i*56)+41] += values[281] * B[(i*56)+7];
C[(i*56)+42] += values[282] * B[(i*56)+7];
C[(i*56)+43] += values[283] * B[(i*56)+7];
C[(i*56)+44] += values[284] * B[(i*56)+7];
C[(i*56)+46] += values[285] * B[(i*56)+7];
C[(i*56)+47] += values[286] * B[(i*56)+7];
C[(i*56)+48] += values[287] * B[(i*56)+7];
C[(i*56)+50] += values[288] * B[(i*56)+7];
C[(i*56)+51] += values[289] * B[(i*56)+7];
C[(i*56)+53] += values[290] * B[(i*56)+7];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*56)+8]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_0 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a8_0 = _mm256_loadu_pd(&values[291]);
c8_0 = _mm256_add_pd(c8_0, _mm256_mul_pd(a8_0, b8));
_mm256_storeu_pd(&C[(i*56)+10], c8_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a8_0 = _mm_loadu_pd(&values[291]);
c8_0 = _mm_add_pd(c8_0, _mm_mul_pd(a8_0, b8));
_mm_storeu_pd(&C[(i*56)+10], c8_0);
__m128d c8_2 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a8_2 = _mm_loadu_pd(&values[293]);
c8_2 = _mm_add_pd(c8_2, _mm_mul_pd(a8_2, b8));
_mm_storeu_pd(&C[(i*56)+12], c8_2);
#endif
__m128d c8_4 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a8_4 = _mm_loadu_pd(&values[295]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_4 = _mm_add_pd(c8_4, _mm_mul_pd(a8_4, b8));
#endif
_mm_storeu_pd(&C[(i*56)+14], c8_4);
__m128d c8_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a8_6 = _mm_load_sd(&values[297]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_6 = _mm_add_sd(c8_6, _mm_mul_sd(a8_6, b8));
#endif
_mm_store_sd(&C[(i*56)+16], c8_6);
__m128d c8_7 = _mm_load_sd(&C[(i*56)+18]);
__m128d a8_7 = _mm_load_sd(&values[298]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_7 = _mm_add_sd(c8_7, _mm_mul_sd(a8_7, b8));
#endif
_mm_store_sd(&C[(i*56)+18], c8_7);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_8 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a8_8 = _mm256_loadu_pd(&values[299]);
c8_8 = _mm256_add_pd(c8_8, _mm256_mul_pd(a8_8, b8));
_mm256_storeu_pd(&C[(i*56)+20], c8_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_8 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a8_8 = _mm_loadu_pd(&values[299]);
c8_8 = _mm_add_pd(c8_8, _mm_mul_pd(a8_8, b8));
_mm_storeu_pd(&C[(i*56)+20], c8_8);
__m128d c8_10 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a8_10 = _mm_loadu_pd(&values[301]);
c8_10 = _mm_add_pd(c8_10, _mm_mul_pd(a8_10, b8));
_mm_storeu_pd(&C[(i*56)+22], c8_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_12 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a8_12 = _mm256_loadu_pd(&values[303]);
c8_12 = _mm256_add_pd(c8_12, _mm256_mul_pd(a8_12, b8));
_mm256_storeu_pd(&C[(i*56)+24], c8_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_12 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a8_12 = _mm_loadu_pd(&values[303]);
c8_12 = _mm_add_pd(c8_12, _mm_mul_pd(a8_12, b8));
_mm_storeu_pd(&C[(i*56)+24], c8_12);
__m128d c8_14 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a8_14 = _mm_loadu_pd(&values[305]);
c8_14 = _mm_add_pd(c8_14, _mm_mul_pd(a8_14, b8));
_mm_storeu_pd(&C[(i*56)+26], c8_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_16 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a8_16 = _mm256_loadu_pd(&values[307]);
c8_16 = _mm256_add_pd(c8_16, _mm256_mul_pd(a8_16, b8));
_mm256_storeu_pd(&C[(i*56)+28], c8_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_16 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a8_16 = _mm_loadu_pd(&values[307]);
c8_16 = _mm_add_pd(c8_16, _mm_mul_pd(a8_16, b8));
_mm_storeu_pd(&C[(i*56)+28], c8_16);
__m128d c8_18 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a8_18 = _mm_loadu_pd(&values[309]);
c8_18 = _mm_add_pd(c8_18, _mm_mul_pd(a8_18, b8));
_mm_storeu_pd(&C[(i*56)+30], c8_18);
#endif
__m128d c8_20 = _mm_load_sd(&C[(i*56)+33]);
__m128d a8_20 = _mm_load_sd(&values[311]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_20 = _mm_add_sd(c8_20, _mm_mul_sd(a8_20, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_20 = _mm_add_sd(c8_20, _mm_mul_sd(a8_20, b8));
#endif
_mm_store_sd(&C[(i*56)+33], c8_20);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_21 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a8_21 = _mm256_loadu_pd(&values[312]);
c8_21 = _mm256_add_pd(c8_21, _mm256_mul_pd(a8_21, b8));
_mm256_storeu_pd(&C[(i*56)+35], c8_21);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_21 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a8_21 = _mm_loadu_pd(&values[312]);
c8_21 = _mm_add_pd(c8_21, _mm_mul_pd(a8_21, b8));
_mm_storeu_pd(&C[(i*56)+35], c8_21);
__m128d c8_23 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a8_23 = _mm_loadu_pd(&values[314]);
c8_23 = _mm_add_pd(c8_23, _mm_mul_pd(a8_23, b8));
_mm_storeu_pd(&C[(i*56)+37], c8_23);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_25 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a8_25 = _mm256_loadu_pd(&values[316]);
c8_25 = _mm256_add_pd(c8_25, _mm256_mul_pd(a8_25, b8));
_mm256_storeu_pd(&C[(i*56)+39], c8_25);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_25 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a8_25 = _mm_loadu_pd(&values[316]);
c8_25 = _mm_add_pd(c8_25, _mm_mul_pd(a8_25, b8));
_mm_storeu_pd(&C[(i*56)+39], c8_25);
__m128d c8_27 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a8_27 = _mm_loadu_pd(&values[318]);
c8_27 = _mm_add_pd(c8_27, _mm_mul_pd(a8_27, b8));
_mm_storeu_pd(&C[(i*56)+41], c8_27);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_29 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a8_29 = _mm256_loadu_pd(&values[320]);
c8_29 = _mm256_add_pd(c8_29, _mm256_mul_pd(a8_29, b8));
_mm256_storeu_pd(&C[(i*56)+43], c8_29);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_29 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a8_29 = _mm_loadu_pd(&values[320]);
c8_29 = _mm_add_pd(c8_29, _mm_mul_pd(a8_29, b8));
_mm_storeu_pd(&C[(i*56)+43], c8_29);
__m128d c8_31 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a8_31 = _mm_loadu_pd(&values[322]);
c8_31 = _mm_add_pd(c8_31, _mm_mul_pd(a8_31, b8));
_mm_storeu_pd(&C[(i*56)+45], c8_31);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c8_33 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a8_33 = _mm256_loadu_pd(&values[324]);
c8_33 = _mm256_add_pd(c8_33, _mm256_mul_pd(a8_33, b8));
_mm256_storeu_pd(&C[(i*56)+47], c8_33);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c8_33 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a8_33 = _mm_loadu_pd(&values[324]);
c8_33 = _mm_add_pd(c8_33, _mm_mul_pd(a8_33, b8));
_mm_storeu_pd(&C[(i*56)+47], c8_33);
__m128d c8_35 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a8_35 = _mm_loadu_pd(&values[326]);
c8_35 = _mm_add_pd(c8_35, _mm_mul_pd(a8_35, b8));
_mm_storeu_pd(&C[(i*56)+49], c8_35);
#endif
__m128d c8_37 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a8_37 = _mm_loadu_pd(&values[328]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_37 = _mm_add_pd(c8_37, _mm_mul_pd(a8_37, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_37 = _mm_add_pd(c8_37, _mm_mul_pd(a8_37, b8));
#endif
_mm_storeu_pd(&C[(i*56)+51], c8_37);
__m128d c8_39 = _mm_load_sd(&C[(i*56)+54]);
__m128d a8_39 = _mm_load_sd(&values[330]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_39 = _mm_add_sd(c8_39, _mm_mul_sd(a8_39, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_39 = _mm_add_sd(c8_39, _mm_mul_sd(a8_39, b8));
#endif
_mm_store_sd(&C[(i*56)+54], c8_39);
#else
C[(i*56)+10] += values[291] * B[(i*56)+8];
C[(i*56)+11] += values[292] * B[(i*56)+8];
C[(i*56)+12] += values[293] * B[(i*56)+8];
C[(i*56)+13] += values[294] * B[(i*56)+8];
C[(i*56)+14] += values[295] * B[(i*56)+8];
C[(i*56)+15] += values[296] * B[(i*56)+8];
C[(i*56)+16] += values[297] * B[(i*56)+8];
C[(i*56)+18] += values[298] * B[(i*56)+8];
C[(i*56)+20] += values[299] * B[(i*56)+8];
C[(i*56)+21] += values[300] * B[(i*56)+8];
C[(i*56)+22] += values[301] * B[(i*56)+8];
C[(i*56)+23] += values[302] * B[(i*56)+8];
C[(i*56)+24] += values[303] * B[(i*56)+8];
C[(i*56)+25] += values[304] * B[(i*56)+8];
C[(i*56)+26] += values[305] * B[(i*56)+8];
C[(i*56)+27] += values[306] * B[(i*56)+8];
C[(i*56)+28] += values[307] * B[(i*56)+8];
C[(i*56)+29] += values[308] * B[(i*56)+8];
C[(i*56)+30] += values[309] * B[(i*56)+8];
C[(i*56)+31] += values[310] * B[(i*56)+8];
C[(i*56)+33] += values[311] * B[(i*56)+8];
C[(i*56)+35] += values[312] * B[(i*56)+8];
C[(i*56)+36] += values[313] * B[(i*56)+8];
C[(i*56)+37] += values[314] * B[(i*56)+8];
C[(i*56)+38] += values[315] * B[(i*56)+8];
C[(i*56)+39] += values[316] * B[(i*56)+8];
C[(i*56)+40] += values[317] * B[(i*56)+8];
C[(i*56)+41] += values[318] * B[(i*56)+8];
C[(i*56)+42] += values[319] * B[(i*56)+8];
C[(i*56)+43] += values[320] * B[(i*56)+8];
C[(i*56)+44] += values[321] * B[(i*56)+8];
C[(i*56)+45] += values[322] * B[(i*56)+8];
C[(i*56)+46] += values[323] * B[(i*56)+8];
C[(i*56)+47] += values[324] * B[(i*56)+8];
C[(i*56)+48] += values[325] * B[(i*56)+8];
C[(i*56)+49] += values[326] * B[(i*56)+8];
C[(i*56)+50] += values[327] * B[(i*56)+8];
C[(i*56)+51] += values[328] * B[(i*56)+8];
C[(i*56)+52] += values[329] * B[(i*56)+8];
C[(i*56)+54] += values[330] * B[(i*56)+8];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*56)+9]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_0 = _mm256_loadu_pd(&C[(i*56)+10]);
__m256d a9_0 = _mm256_loadu_pd(&values[331]);
c9_0 = _mm256_add_pd(c9_0, _mm256_mul_pd(a9_0, b9));
_mm256_storeu_pd(&C[(i*56)+10], c9_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_0 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a9_0 = _mm_loadu_pd(&values[331]);
c9_0 = _mm_add_pd(c9_0, _mm_mul_pd(a9_0, b9));
_mm_storeu_pd(&C[(i*56)+10], c9_0);
__m128d c9_2 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a9_2 = _mm_loadu_pd(&values[333]);
c9_2 = _mm_add_pd(c9_2, _mm_mul_pd(a9_2, b9));
_mm_storeu_pd(&C[(i*56)+12], c9_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_4 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a9_4 = _mm256_loadu_pd(&values[335]);
c9_4 = _mm256_add_pd(c9_4, _mm256_mul_pd(a9_4, b9));
_mm256_storeu_pd(&C[(i*56)+14], c9_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_4 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a9_4 = _mm_loadu_pd(&values[335]);
c9_4 = _mm_add_pd(c9_4, _mm_mul_pd(a9_4, b9));
_mm_storeu_pd(&C[(i*56)+14], c9_4);
__m128d c9_6 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a9_6 = _mm_loadu_pd(&values[337]);
c9_6 = _mm_add_pd(c9_6, _mm_mul_pd(a9_6, b9));
_mm_storeu_pd(&C[(i*56)+16], c9_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_8 = _mm256_loadu_pd(&C[(i*56)+18]);
__m256d a9_8 = _mm256_loadu_pd(&values[339]);
c9_8 = _mm256_add_pd(c9_8, _mm256_mul_pd(a9_8, b9));
_mm256_storeu_pd(&C[(i*56)+18], c9_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a9_8 = _mm_loadu_pd(&values[339]);
c9_8 = _mm_add_pd(c9_8, _mm_mul_pd(a9_8, b9));
_mm_storeu_pd(&C[(i*56)+18], c9_8);
__m128d c9_10 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a9_10 = _mm_loadu_pd(&values[341]);
c9_10 = _mm_add_pd(c9_10, _mm_mul_pd(a9_10, b9));
_mm_storeu_pd(&C[(i*56)+20], c9_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_12 = _mm256_loadu_pd(&C[(i*56)+22]);
__m256d a9_12 = _mm256_loadu_pd(&values[343]);
c9_12 = _mm256_add_pd(c9_12, _mm256_mul_pd(a9_12, b9));
_mm256_storeu_pd(&C[(i*56)+22], c9_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_12 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a9_12 = _mm_loadu_pd(&values[343]);
c9_12 = _mm_add_pd(c9_12, _mm_mul_pd(a9_12, b9));
_mm_storeu_pd(&C[(i*56)+22], c9_12);
__m128d c9_14 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a9_14 = _mm_loadu_pd(&values[345]);
c9_14 = _mm_add_pd(c9_14, _mm_mul_pd(a9_14, b9));
_mm_storeu_pd(&C[(i*56)+24], c9_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_16 = _mm256_loadu_pd(&C[(i*56)+26]);
__m256d a9_16 = _mm256_loadu_pd(&values[347]);
c9_16 = _mm256_add_pd(c9_16, _mm256_mul_pd(a9_16, b9));
_mm256_storeu_pd(&C[(i*56)+26], c9_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_16 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a9_16 = _mm_loadu_pd(&values[347]);
c9_16 = _mm_add_pd(c9_16, _mm_mul_pd(a9_16, b9));
_mm_storeu_pd(&C[(i*56)+26], c9_16);
__m128d c9_18 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a9_18 = _mm_loadu_pd(&values[349]);
c9_18 = _mm_add_pd(c9_18, _mm_mul_pd(a9_18, b9));
_mm_storeu_pd(&C[(i*56)+28], c9_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_20 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a9_20 = _mm256_loadu_pd(&values[351]);
c9_20 = _mm256_add_pd(c9_20, _mm256_mul_pd(a9_20, b9));
_mm256_storeu_pd(&C[(i*56)+30], c9_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_20 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a9_20 = _mm_loadu_pd(&values[351]);
c9_20 = _mm_add_pd(c9_20, _mm_mul_pd(a9_20, b9));
_mm_storeu_pd(&C[(i*56)+30], c9_20);
__m128d c9_22 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a9_22 = _mm_loadu_pd(&values[353]);
c9_22 = _mm_add_pd(c9_22, _mm_mul_pd(a9_22, b9));
_mm_storeu_pd(&C[(i*56)+32], c9_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_24 = _mm256_loadu_pd(&C[(i*56)+34]);
__m256d a9_24 = _mm256_loadu_pd(&values[355]);
c9_24 = _mm256_add_pd(c9_24, _mm256_mul_pd(a9_24, b9));
_mm256_storeu_pd(&C[(i*56)+34], c9_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_24 = _mm_loadu_pd(&C[(i*56)+34]);
__m128d a9_24 = _mm_loadu_pd(&values[355]);
c9_24 = _mm_add_pd(c9_24, _mm_mul_pd(a9_24, b9));
_mm_storeu_pd(&C[(i*56)+34], c9_24);
__m128d c9_26 = _mm_loadu_pd(&C[(i*56)+36]);
__m128d a9_26 = _mm_loadu_pd(&values[357]);
c9_26 = _mm_add_pd(c9_26, _mm_mul_pd(a9_26, b9));
_mm_storeu_pd(&C[(i*56)+36], c9_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_28 = _mm256_loadu_pd(&C[(i*56)+38]);
__m256d a9_28 = _mm256_loadu_pd(&values[359]);
c9_28 = _mm256_add_pd(c9_28, _mm256_mul_pd(a9_28, b9));
_mm256_storeu_pd(&C[(i*56)+38], c9_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_28 = _mm_loadu_pd(&C[(i*56)+38]);
__m128d a9_28 = _mm_loadu_pd(&values[359]);
c9_28 = _mm_add_pd(c9_28, _mm_mul_pd(a9_28, b9));
_mm_storeu_pd(&C[(i*56)+38], c9_28);
__m128d c9_30 = _mm_loadu_pd(&C[(i*56)+40]);
__m128d a9_30 = _mm_loadu_pd(&values[361]);
c9_30 = _mm_add_pd(c9_30, _mm_mul_pd(a9_30, b9));
_mm_storeu_pd(&C[(i*56)+40], c9_30);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_32 = _mm256_loadu_pd(&C[(i*56)+42]);
__m256d a9_32 = _mm256_loadu_pd(&values[363]);
c9_32 = _mm256_add_pd(c9_32, _mm256_mul_pd(a9_32, b9));
_mm256_storeu_pd(&C[(i*56)+42], c9_32);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_32 = _mm_loadu_pd(&C[(i*56)+42]);
__m128d a9_32 = _mm_loadu_pd(&values[363]);
c9_32 = _mm_add_pd(c9_32, _mm_mul_pd(a9_32, b9));
_mm_storeu_pd(&C[(i*56)+42], c9_32);
__m128d c9_34 = _mm_loadu_pd(&C[(i*56)+44]);
__m128d a9_34 = _mm_loadu_pd(&values[365]);
c9_34 = _mm_add_pd(c9_34, _mm_mul_pd(a9_34, b9));
_mm_storeu_pd(&C[(i*56)+44], c9_34);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_36 = _mm256_loadu_pd(&C[(i*56)+46]);
__m256d a9_36 = _mm256_loadu_pd(&values[367]);
c9_36 = _mm256_add_pd(c9_36, _mm256_mul_pd(a9_36, b9));
_mm256_storeu_pd(&C[(i*56)+46], c9_36);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_36 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a9_36 = _mm_loadu_pd(&values[367]);
c9_36 = _mm_add_pd(c9_36, _mm_mul_pd(a9_36, b9));
_mm_storeu_pd(&C[(i*56)+46], c9_36);
__m128d c9_38 = _mm_loadu_pd(&C[(i*56)+48]);
__m128d a9_38 = _mm_loadu_pd(&values[369]);
c9_38 = _mm_add_pd(c9_38, _mm_mul_pd(a9_38, b9));
_mm_storeu_pd(&C[(i*56)+48], c9_38);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c9_40 = _mm256_loadu_pd(&C[(i*56)+50]);
__m256d a9_40 = _mm256_loadu_pd(&values[371]);
c9_40 = _mm256_add_pd(c9_40, _mm256_mul_pd(a9_40, b9));
_mm256_storeu_pd(&C[(i*56)+50], c9_40);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c9_40 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a9_40 = _mm_loadu_pd(&values[371]);
c9_40 = _mm_add_pd(c9_40, _mm_mul_pd(a9_40, b9));
_mm_storeu_pd(&C[(i*56)+50], c9_40);
__m128d c9_42 = _mm_loadu_pd(&C[(i*56)+52]);
__m128d a9_42 = _mm_loadu_pd(&values[373]);
c9_42 = _mm_add_pd(c9_42, _mm_mul_pd(a9_42, b9));
_mm_storeu_pd(&C[(i*56)+52], c9_42);
#endif
__m128d c9_44 = _mm_loadu_pd(&C[(i*56)+54]);
__m128d a9_44 = _mm_loadu_pd(&values[375]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_44 = _mm_add_pd(c9_44, _mm_mul_pd(a9_44, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_44 = _mm_add_pd(c9_44, _mm_mul_pd(a9_44, b9));
#endif
_mm_storeu_pd(&C[(i*56)+54], c9_44);
#else
C[(i*56)+10] += values[331] * B[(i*56)+9];
C[(i*56)+11] += values[332] * B[(i*56)+9];
C[(i*56)+12] += values[333] * B[(i*56)+9];
C[(i*56)+13] += values[334] * B[(i*56)+9];
C[(i*56)+14] += values[335] * B[(i*56)+9];
C[(i*56)+15] += values[336] * B[(i*56)+9];
C[(i*56)+16] += values[337] * B[(i*56)+9];
C[(i*56)+17] += values[338] * B[(i*56)+9];
C[(i*56)+18] += values[339] * B[(i*56)+9];
C[(i*56)+19] += values[340] * B[(i*56)+9];
C[(i*56)+20] += values[341] * B[(i*56)+9];
C[(i*56)+21] += values[342] * B[(i*56)+9];
C[(i*56)+22] += values[343] * B[(i*56)+9];
C[(i*56)+23] += values[344] * B[(i*56)+9];
C[(i*56)+24] += values[345] * B[(i*56)+9];
C[(i*56)+25] += values[346] * B[(i*56)+9];
C[(i*56)+26] += values[347] * B[(i*56)+9];
C[(i*56)+27] += values[348] * B[(i*56)+9];
C[(i*56)+28] += values[349] * B[(i*56)+9];
C[(i*56)+29] += values[350] * B[(i*56)+9];
C[(i*56)+30] += values[351] * B[(i*56)+9];
C[(i*56)+31] += values[352] * B[(i*56)+9];
C[(i*56)+32] += values[353] * B[(i*56)+9];
C[(i*56)+33] += values[354] * B[(i*56)+9];
C[(i*56)+34] += values[355] * B[(i*56)+9];
C[(i*56)+35] += values[356] * B[(i*56)+9];
C[(i*56)+36] += values[357] * B[(i*56)+9];
C[(i*56)+37] += values[358] * B[(i*56)+9];
C[(i*56)+38] += values[359] * B[(i*56)+9];
C[(i*56)+39] += values[360] * B[(i*56)+9];
C[(i*56)+40] += values[361] * B[(i*56)+9];
C[(i*56)+41] += values[362] * B[(i*56)+9];
C[(i*56)+42] += values[363] * B[(i*56)+9];
C[(i*56)+43] += values[364] * B[(i*56)+9];
C[(i*56)+44] += values[365] * B[(i*56)+9];
C[(i*56)+45] += values[366] * B[(i*56)+9];
C[(i*56)+46] += values[367] * B[(i*56)+9];
C[(i*56)+47] += values[368] * B[(i*56)+9];
C[(i*56)+48] += values[369] * B[(i*56)+9];
C[(i*56)+49] += values[370] * B[(i*56)+9];
C[(i*56)+50] += values[371] * B[(i*56)+9];
C[(i*56)+51] += values[372] * B[(i*56)+9];
C[(i*56)+52] += values[373] * B[(i*56)+9];
C[(i*56)+53] += values[374] * B[(i*56)+9];
C[(i*56)+54] += values[375] * B[(i*56)+9];
C[(i*56)+55] += values[376] * B[(i*56)+9];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*56)+10]);
#endif
__m128d c10_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a10_0 = _mm_loadu_pd(&values[377]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
#endif
_mm_storeu_pd(&C[(i*56)+20], c10_0);
__m128d c10_2 = _mm_load_sd(&C[(i*56)+25]);
__m128d a10_2 = _mm_load_sd(&values[379]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_2 = _mm_add_sd(c10_2, _mm_mul_sd(a10_2, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_2 = _mm_add_sd(c10_2, _mm_mul_sd(a10_2, b10));
#endif
_mm_store_sd(&C[(i*56)+25], c10_2);
__m128d c10_3 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a10_3 = _mm_loadu_pd(&values[380]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_3 = _mm_add_pd(c10_3, _mm_mul_pd(a10_3, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_3 = _mm_add_pd(c10_3, _mm_mul_pd(a10_3, b10));
#endif
_mm_storeu_pd(&C[(i*56)+35], c10_3);
__m128d c10_5 = _mm_load_sd(&C[(i*56)+37]);
__m128d a10_5 = _mm_load_sd(&values[382]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_5 = _mm_add_sd(c10_5, _mm_mul_sd(a10_5, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_5 = _mm_add_sd(c10_5, _mm_mul_sd(a10_5, b10));
#endif
_mm_store_sd(&C[(i*56)+37], c10_5);
__m128d c10_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a10_6 = _mm_loadu_pd(&values[383]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, b10));
#endif
_mm_storeu_pd(&C[(i*56)+41], c10_6);
__m128d c10_8 = _mm_load_sd(&C[(i*56)+46]);
__m128d a10_8 = _mm_load_sd(&values[385]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_8 = _mm_add_sd(c10_8, _mm_mul_sd(a10_8, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_8 = _mm_add_sd(c10_8, _mm_mul_sd(a10_8, b10));
#endif
_mm_store_sd(&C[(i*56)+46], c10_8);
#else
C[(i*56)+20] += values[377] * B[(i*56)+10];
C[(i*56)+21] += values[378] * B[(i*56)+10];
C[(i*56)+25] += values[379] * B[(i*56)+10];
C[(i*56)+35] += values[380] * B[(i*56)+10];
C[(i*56)+36] += values[381] * B[(i*56)+10];
C[(i*56)+37] += values[382] * B[(i*56)+10];
C[(i*56)+41] += values[383] * B[(i*56)+10];
C[(i*56)+42] += values[384] * B[(i*56)+10];
C[(i*56)+46] += values[385] * B[(i*56)+10];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*56)+11]);
#endif
__m128d c11_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a11_0 = _mm_loadu_pd(&values[386]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
#endif
_mm_storeu_pd(&C[(i*56)+20], c11_0);
__m128d c11_2 = _mm_load_sd(&C[(i*56)+22]);
__m128d a11_2 = _mm_load_sd(&values[388]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*56)+22], c11_2);
__m128d c11_3 = _mm_load_sd(&C[(i*56)+26]);
__m128d a11_3 = _mm_load_sd(&values[389]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_3 = _mm_add_sd(c11_3, _mm_mul_sd(a11_3, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_3 = _mm_add_sd(c11_3, _mm_mul_sd(a11_3, b11));
#endif
_mm_store_sd(&C[(i*56)+26], c11_3);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_4 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a11_4 = _mm256_loadu_pd(&values[390]);
c11_4 = _mm256_add_pd(c11_4, _mm256_mul_pd(a11_4, b11));
_mm256_storeu_pd(&C[(i*56)+35], c11_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_4 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a11_4 = _mm_loadu_pd(&values[390]);
c11_4 = _mm_add_pd(c11_4, _mm_mul_pd(a11_4, b11));
_mm_storeu_pd(&C[(i*56)+35], c11_4);
__m128d c11_6 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a11_6 = _mm_loadu_pd(&values[392]);
c11_6 = _mm_add_pd(c11_6, _mm_mul_pd(a11_6, b11));
_mm_storeu_pd(&C[(i*56)+37], c11_6);
#endif
__m128d c11_8 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a11_8 = _mm_loadu_pd(&values[394]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, b11));
#endif
_mm_storeu_pd(&C[(i*56)+41], c11_8);
__m128d c11_10 = _mm_load_sd(&C[(i*56)+43]);
__m128d a11_10 = _mm_load_sd(&values[396]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_10 = _mm_add_sd(c11_10, _mm_mul_sd(a11_10, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_10 = _mm_add_sd(c11_10, _mm_mul_sd(a11_10, b11));
#endif
_mm_store_sd(&C[(i*56)+43], c11_10);
__m128d c11_11 = _mm_load_sd(&C[(i*56)+47]);
__m128d a11_11 = _mm_load_sd(&values[397]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_11 = _mm_add_sd(c11_11, _mm_mul_sd(a11_11, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_11 = _mm_add_sd(c11_11, _mm_mul_sd(a11_11, b11));
#endif
_mm_store_sd(&C[(i*56)+47], c11_11);
#else
C[(i*56)+20] += values[386] * B[(i*56)+11];
C[(i*56)+21] += values[387] * B[(i*56)+11];
C[(i*56)+22] += values[388] * B[(i*56)+11];
C[(i*56)+26] += values[389] * B[(i*56)+11];
C[(i*56)+35] += values[390] * B[(i*56)+11];
C[(i*56)+36] += values[391] * B[(i*56)+11];
C[(i*56)+37] += values[392] * B[(i*56)+11];
C[(i*56)+38] += values[393] * B[(i*56)+11];
C[(i*56)+41] += values[394] * B[(i*56)+11];
C[(i*56)+42] += values[395] * B[(i*56)+11];
C[(i*56)+43] += values[396] * B[(i*56)+11];
C[(i*56)+47] += values[397] * B[(i*56)+11];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a12_0 = _mm256_loadu_pd(&values[398]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*56)+20], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a12_0 = _mm_loadu_pd(&values[398]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*56)+20], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a12_2 = _mm_loadu_pd(&values[400]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*56)+22], c12_2);
#endif
__m128d c12_4 = _mm_load_sd(&C[(i*56)+27]);
__m128d a12_4 = _mm_load_sd(&values[402]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_4 = _mm_add_sd(c12_4, _mm_mul_sd(a12_4, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_4 = _mm_add_sd(c12_4, _mm_mul_sd(a12_4, b12));
#endif
_mm_store_sd(&C[(i*56)+27], c12_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_5 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a12_5 = _mm256_loadu_pd(&values[403]);
c12_5 = _mm256_add_pd(c12_5, _mm256_mul_pd(a12_5, b12));
_mm256_storeu_pd(&C[(i*56)+35], c12_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_5 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a12_5 = _mm_loadu_pd(&values[403]);
c12_5 = _mm_add_pd(c12_5, _mm_mul_pd(a12_5, b12));
_mm_storeu_pd(&C[(i*56)+35], c12_5);
__m128d c12_7 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a12_7 = _mm_loadu_pd(&values[405]);
c12_7 = _mm_add_pd(c12_7, _mm_mul_pd(a12_7, b12));
_mm_storeu_pd(&C[(i*56)+37], c12_7);
#endif
__m128d c12_9 = _mm_load_sd(&C[(i*56)+39]);
__m128d a12_9 = _mm_load_sd(&values[407]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_9 = _mm_add_sd(c12_9, _mm_mul_sd(a12_9, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_9 = _mm_add_sd(c12_9, _mm_mul_sd(a12_9, b12));
#endif
_mm_store_sd(&C[(i*56)+39], c12_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_10 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a12_10 = _mm256_loadu_pd(&values[408]);
c12_10 = _mm256_add_pd(c12_10, _mm256_mul_pd(a12_10, b12));
_mm256_storeu_pd(&C[(i*56)+41], c12_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_10 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a12_10 = _mm_loadu_pd(&values[408]);
c12_10 = _mm_add_pd(c12_10, _mm_mul_pd(a12_10, b12));
_mm_storeu_pd(&C[(i*56)+41], c12_10);
__m128d c12_12 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a12_12 = _mm_loadu_pd(&values[410]);
c12_12 = _mm_add_pd(c12_12, _mm_mul_pd(a12_12, b12));
_mm_storeu_pd(&C[(i*56)+43], c12_12);
#endif
__m128d c12_14 = _mm_load_sd(&C[(i*56)+48]);
__m128d a12_14 = _mm_load_sd(&values[412]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_14 = _mm_add_sd(c12_14, _mm_mul_sd(a12_14, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_14 = _mm_add_sd(c12_14, _mm_mul_sd(a12_14, b12));
#endif
_mm_store_sd(&C[(i*56)+48], c12_14);
#else
C[(i*56)+20] += values[398] * B[(i*56)+12];
C[(i*56)+21] += values[399] * B[(i*56)+12];
C[(i*56)+22] += values[400] * B[(i*56)+12];
C[(i*56)+23] += values[401] * B[(i*56)+12];
C[(i*56)+27] += values[402] * B[(i*56)+12];
C[(i*56)+35] += values[403] * B[(i*56)+12];
C[(i*56)+36] += values[404] * B[(i*56)+12];
C[(i*56)+37] += values[405] * B[(i*56)+12];
C[(i*56)+38] += values[406] * B[(i*56)+12];
C[(i*56)+39] += values[407] * B[(i*56)+12];
C[(i*56)+41] += values[408] * B[(i*56)+12];
C[(i*56)+42] += values[409] * B[(i*56)+12];
C[(i*56)+43] += values[410] * B[(i*56)+12];
C[(i*56)+44] += values[411] * B[(i*56)+12];
C[(i*56)+48] += values[412] * B[(i*56)+12];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*56)+13]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a13_0 = _mm256_loadu_pd(&values[413]);
c13_0 = _mm256_add_pd(c13_0, _mm256_mul_pd(a13_0, b13));
_mm256_storeu_pd(&C[(i*56)+20], c13_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a13_0 = _mm_loadu_pd(&values[413]);
c13_0 = _mm_add_pd(c13_0, _mm_mul_pd(a13_0, b13));
_mm_storeu_pd(&C[(i*56)+20], c13_0);
__m128d c13_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a13_2 = _mm_loadu_pd(&values[415]);
c13_2 = _mm_add_pd(c13_2, _mm_mul_pd(a13_2, b13));
_mm_storeu_pd(&C[(i*56)+22], c13_2);
#endif
__m128d c13_4 = _mm_load_sd(&C[(i*56)+24]);
__m128d a13_4 = _mm_load_sd(&values[417]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_sd(c13_4, _mm_mul_sd(a13_4, b13));
#endif
_mm_store_sd(&C[(i*56)+24], c13_4);
__m128d c13_5 = _mm_load_sd(&C[(i*56)+28]);
__m128d a13_5 = _mm_load_sd(&values[418]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_5 = _mm_add_sd(c13_5, _mm_mul_sd(a13_5, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_5 = _mm_add_sd(c13_5, _mm_mul_sd(a13_5, b13));
#endif
_mm_store_sd(&C[(i*56)+28], c13_5);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_6 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a13_6 = _mm256_loadu_pd(&values[419]);
c13_6 = _mm256_add_pd(c13_6, _mm256_mul_pd(a13_6, b13));
_mm256_storeu_pd(&C[(i*56)+35], c13_6);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_6 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a13_6 = _mm_loadu_pd(&values[419]);
c13_6 = _mm_add_pd(c13_6, _mm_mul_pd(a13_6, b13));
_mm_storeu_pd(&C[(i*56)+35], c13_6);
__m128d c13_8 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a13_8 = _mm_loadu_pd(&values[421]);
c13_8 = _mm_add_pd(c13_8, _mm_mul_pd(a13_8, b13));
_mm_storeu_pd(&C[(i*56)+37], c13_8);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c13_10 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a13_10 = _mm256_loadu_pd(&values[423]);
c13_10 = _mm256_add_pd(c13_10, _mm256_mul_pd(a13_10, b13));
_mm256_storeu_pd(&C[(i*56)+39], c13_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c13_10 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a13_10 = _mm_loadu_pd(&values[423]);
c13_10 = _mm_add_pd(c13_10, _mm_mul_pd(a13_10, b13));
_mm_storeu_pd(&C[(i*56)+39], c13_10);
__m128d c13_12 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a13_12 = _mm_loadu_pd(&values[425]);
c13_12 = _mm_add_pd(c13_12, _mm_mul_pd(a13_12, b13));
_mm_storeu_pd(&C[(i*56)+41], c13_12);
#endif
__m128d c13_14 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a13_14 = _mm_loadu_pd(&values[427]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_14 = _mm_add_pd(c13_14, _mm_mul_pd(a13_14, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_14 = _mm_add_pd(c13_14, _mm_mul_pd(a13_14, b13));
#endif
_mm_storeu_pd(&C[(i*56)+43], c13_14);
__m128d c13_16 = _mm_load_sd(&C[(i*56)+45]);
__m128d a13_16 = _mm_load_sd(&values[429]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_16 = _mm_add_sd(c13_16, _mm_mul_sd(a13_16, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_16 = _mm_add_sd(c13_16, _mm_mul_sd(a13_16, b13));
#endif
_mm_store_sd(&C[(i*56)+45], c13_16);
__m128d c13_17 = _mm_load_sd(&C[(i*56)+49]);
__m128d a13_17 = _mm_load_sd(&values[430]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_17 = _mm_add_sd(c13_17, _mm_mul_sd(a13_17, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_17 = _mm_add_sd(c13_17, _mm_mul_sd(a13_17, b13));
#endif
_mm_store_sd(&C[(i*56)+49], c13_17);
#else
C[(i*56)+20] += values[413] * B[(i*56)+13];
C[(i*56)+21] += values[414] * B[(i*56)+13];
C[(i*56)+22] += values[415] * B[(i*56)+13];
C[(i*56)+23] += values[416] * B[(i*56)+13];
C[(i*56)+24] += values[417] * B[(i*56)+13];
C[(i*56)+28] += values[418] * B[(i*56)+13];
C[(i*56)+35] += values[419] * B[(i*56)+13];
C[(i*56)+36] += values[420] * B[(i*56)+13];
C[(i*56)+37] += values[421] * B[(i*56)+13];
C[(i*56)+38] += values[422] * B[(i*56)+13];
C[(i*56)+39] += values[423] * B[(i*56)+13];
C[(i*56)+40] += values[424] * B[(i*56)+13];
C[(i*56)+41] += values[425] * B[(i*56)+13];
C[(i*56)+42] += values[426] * B[(i*56)+13];
C[(i*56)+43] += values[427] * B[(i*56)+13];
C[(i*56)+44] += values[428] * B[(i*56)+13];
C[(i*56)+45] += values[429] * B[(i*56)+13];
C[(i*56)+49] += values[430] * B[(i*56)+13];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*56)+14]);
#endif
__m128d c14_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a14_0 = _mm_loadu_pd(&values[431]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
#endif
_mm_storeu_pd(&C[(i*56)+20], c14_0);
__m128d c14_2 = _mm_load_sd(&C[(i*56)+22]);
__m128d a14_2 = _mm_load_sd(&values[433]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_2 = _mm_add_sd(c14_2, _mm_mul_sd(a14_2, b14));
#endif
_mm_store_sd(&C[(i*56)+22], c14_2);
__m128d c14_3 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a14_3 = _mm_loadu_pd(&values[434]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_3 = _mm_add_pd(c14_3, _mm_mul_pd(a14_3, b14));
#endif
_mm_storeu_pd(&C[(i*56)+25], c14_3);
__m128d c14_5 = _mm_load_sd(&C[(i*56)+29]);
__m128d a14_5 = _mm_load_sd(&values[436]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_5 = _mm_add_sd(c14_5, _mm_mul_sd(a14_5, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_5 = _mm_add_sd(c14_5, _mm_mul_sd(a14_5, b14));
#endif
_mm_store_sd(&C[(i*56)+29], c14_5);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_6 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a14_6 = _mm256_loadu_pd(&values[437]);
c14_6 = _mm256_add_pd(c14_6, _mm256_mul_pd(a14_6, b14));
_mm256_storeu_pd(&C[(i*56)+35], c14_6);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_6 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a14_6 = _mm_loadu_pd(&values[437]);
c14_6 = _mm_add_pd(c14_6, _mm_mul_pd(a14_6, b14));
_mm_storeu_pd(&C[(i*56)+35], c14_6);
__m128d c14_8 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a14_8 = _mm_loadu_pd(&values[439]);
c14_8 = _mm_add_pd(c14_8, _mm_mul_pd(a14_8, b14));
_mm_storeu_pd(&C[(i*56)+37], c14_8);
#endif
__m128d c14_10 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a14_10 = _mm_loadu_pd(&values[441]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_10 = _mm_add_pd(c14_10, _mm_mul_pd(a14_10, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_10 = _mm_add_pd(c14_10, _mm_mul_pd(a14_10, b14));
#endif
_mm_storeu_pd(&C[(i*56)+41], c14_10);
__m128d c14_12 = _mm_load_sd(&C[(i*56)+43]);
__m128d a14_12 = _mm_load_sd(&values[443]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_12 = _mm_add_sd(c14_12, _mm_mul_sd(a14_12, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_12 = _mm_add_sd(c14_12, _mm_mul_sd(a14_12, b14));
#endif
_mm_store_sd(&C[(i*56)+43], c14_12);
__m128d c14_13 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a14_13 = _mm_loadu_pd(&values[444]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_13 = _mm_add_pd(c14_13, _mm_mul_pd(a14_13, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_13 = _mm_add_pd(c14_13, _mm_mul_pd(a14_13, b14));
#endif
_mm_storeu_pd(&C[(i*56)+46], c14_13);
__m128d c14_15 = _mm_load_sd(&C[(i*56)+50]);
__m128d a14_15 = _mm_load_sd(&values[446]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_15 = _mm_add_sd(c14_15, _mm_mul_sd(a14_15, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_15 = _mm_add_sd(c14_15, _mm_mul_sd(a14_15, b14));
#endif
_mm_store_sd(&C[(i*56)+50], c14_15);
#else
C[(i*56)+20] += values[431] * B[(i*56)+14];
C[(i*56)+21] += values[432] * B[(i*56)+14];
C[(i*56)+22] += values[433] * B[(i*56)+14];
C[(i*56)+25] += values[434] * B[(i*56)+14];
C[(i*56)+26] += values[435] * B[(i*56)+14];
C[(i*56)+29] += values[436] * B[(i*56)+14];
C[(i*56)+35] += values[437] * B[(i*56)+14];
C[(i*56)+36] += values[438] * B[(i*56)+14];
C[(i*56)+37] += values[439] * B[(i*56)+14];
C[(i*56)+38] += values[440] * B[(i*56)+14];
C[(i*56)+41] += values[441] * B[(i*56)+14];
C[(i*56)+42] += values[442] * B[(i*56)+14];
C[(i*56)+43] += values[443] * B[(i*56)+14];
C[(i*56)+46] += values[444] * B[(i*56)+14];
C[(i*56)+47] += values[445] * B[(i*56)+14];
C[(i*56)+50] += values[446] * B[(i*56)+14];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a15_0 = _mm256_loadu_pd(&values[447]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*56)+20], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a15_0 = _mm_loadu_pd(&values[447]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*56)+20], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a15_2 = _mm_loadu_pd(&values[449]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*56)+22], c15_2);
#endif
__m128d c15_4 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a15_4 = _mm_loadu_pd(&values[451]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, b15));
#endif
_mm_storeu_pd(&C[(i*56)+25], c15_4);
__m128d c15_6 = _mm_load_sd(&C[(i*56)+27]);
__m128d a15_6 = _mm_load_sd(&values[453]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*56)+27], c15_6);
__m128d c15_7 = _mm_load_sd(&C[(i*56)+30]);
__m128d a15_7 = _mm_load_sd(&values[454]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, b15));
#endif
_mm_store_sd(&C[(i*56)+30], c15_7);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_8 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a15_8 = _mm256_loadu_pd(&values[455]);
c15_8 = _mm256_add_pd(c15_8, _mm256_mul_pd(a15_8, b15));
_mm256_storeu_pd(&C[(i*56)+35], c15_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_8 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a15_8 = _mm_loadu_pd(&values[455]);
c15_8 = _mm_add_pd(c15_8, _mm_mul_pd(a15_8, b15));
_mm_storeu_pd(&C[(i*56)+35], c15_8);
__m128d c15_10 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a15_10 = _mm_loadu_pd(&values[457]);
c15_10 = _mm_add_pd(c15_10, _mm_mul_pd(a15_10, b15));
_mm_storeu_pd(&C[(i*56)+37], c15_10);
#endif
__m128d c15_12 = _mm_load_sd(&C[(i*56)+39]);
__m128d a15_12 = _mm_load_sd(&values[459]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_12 = _mm_add_sd(c15_12, _mm_mul_sd(a15_12, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_12 = _mm_add_sd(c15_12, _mm_mul_sd(a15_12, b15));
#endif
_mm_store_sd(&C[(i*56)+39], c15_12);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_13 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a15_13 = _mm256_loadu_pd(&values[460]);
c15_13 = _mm256_add_pd(c15_13, _mm256_mul_pd(a15_13, b15));
_mm256_storeu_pd(&C[(i*56)+41], c15_13);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_13 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a15_13 = _mm_loadu_pd(&values[460]);
c15_13 = _mm_add_pd(c15_13, _mm_mul_pd(a15_13, b15));
_mm_storeu_pd(&C[(i*56)+41], c15_13);
__m128d c15_15 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a15_15 = _mm_loadu_pd(&values[462]);
c15_15 = _mm_add_pd(c15_15, _mm_mul_pd(a15_15, b15));
_mm_storeu_pd(&C[(i*56)+43], c15_15);
#endif
__m128d c15_17 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a15_17 = _mm_loadu_pd(&values[464]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_17 = _mm_add_pd(c15_17, _mm_mul_pd(a15_17, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_17 = _mm_add_pd(c15_17, _mm_mul_pd(a15_17, b15));
#endif
_mm_storeu_pd(&C[(i*56)+46], c15_17);
__m128d c15_19 = _mm_load_sd(&C[(i*56)+48]);
__m128d a15_19 = _mm_load_sd(&values[466]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_19 = _mm_add_sd(c15_19, _mm_mul_sd(a15_19, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_19 = _mm_add_sd(c15_19, _mm_mul_sd(a15_19, b15));
#endif
_mm_store_sd(&C[(i*56)+48], c15_19);
__m128d c15_20 = _mm_load_sd(&C[(i*56)+51]);
__m128d a15_20 = _mm_load_sd(&values[467]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_20 = _mm_add_sd(c15_20, _mm_mul_sd(a15_20, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_20 = _mm_add_sd(c15_20, _mm_mul_sd(a15_20, b15));
#endif
_mm_store_sd(&C[(i*56)+51], c15_20);
#else
C[(i*56)+20] += values[447] * B[(i*56)+15];
C[(i*56)+21] += values[448] * B[(i*56)+15];
C[(i*56)+22] += values[449] * B[(i*56)+15];
C[(i*56)+23] += values[450] * B[(i*56)+15];
C[(i*56)+25] += values[451] * B[(i*56)+15];
C[(i*56)+26] += values[452] * B[(i*56)+15];
C[(i*56)+27] += values[453] * B[(i*56)+15];
C[(i*56)+30] += values[454] * B[(i*56)+15];
C[(i*56)+35] += values[455] * B[(i*56)+15];
C[(i*56)+36] += values[456] * B[(i*56)+15];
C[(i*56)+37] += values[457] * B[(i*56)+15];
C[(i*56)+38] += values[458] * B[(i*56)+15];
C[(i*56)+39] += values[459] * B[(i*56)+15];
C[(i*56)+41] += values[460] * B[(i*56)+15];
C[(i*56)+42] += values[461] * B[(i*56)+15];
C[(i*56)+43] += values[462] * B[(i*56)+15];
C[(i*56)+44] += values[463] * B[(i*56)+15];
C[(i*56)+46] += values[464] * B[(i*56)+15];
C[(i*56)+47] += values[465] * B[(i*56)+15];
C[(i*56)+48] += values[466] * B[(i*56)+15];
C[(i*56)+51] += values[467] * B[(i*56)+15];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*56)+16]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a16_0 = _mm256_loadu_pd(&values[468]);
c16_0 = _mm256_add_pd(c16_0, _mm256_mul_pd(a16_0, b16));
_mm256_storeu_pd(&C[(i*56)+20], c16_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a16_0 = _mm_loadu_pd(&values[468]);
c16_0 = _mm_add_pd(c16_0, _mm_mul_pd(a16_0, b16));
_mm_storeu_pd(&C[(i*56)+20], c16_0);
__m128d c16_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a16_2 = _mm_loadu_pd(&values[470]);
c16_2 = _mm_add_pd(c16_2, _mm_mul_pd(a16_2, b16));
_mm_storeu_pd(&C[(i*56)+22], c16_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_4 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a16_4 = _mm256_loadu_pd(&values[472]);
c16_4 = _mm256_add_pd(c16_4, _mm256_mul_pd(a16_4, b16));
_mm256_storeu_pd(&C[(i*56)+24], c16_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_4 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a16_4 = _mm_loadu_pd(&values[472]);
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, b16));
_mm_storeu_pd(&C[(i*56)+24], c16_4);
__m128d c16_6 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a16_6 = _mm_loadu_pd(&values[474]);
c16_6 = _mm_add_pd(c16_6, _mm_mul_pd(a16_6, b16));
_mm_storeu_pd(&C[(i*56)+26], c16_6);
#endif
__m128d c16_8 = _mm_load_sd(&C[(i*56)+28]);
__m128d a16_8 = _mm_load_sd(&values[476]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_8 = _mm_add_sd(c16_8, _mm_mul_sd(a16_8, b16));
#endif
_mm_store_sd(&C[(i*56)+28], c16_8);
__m128d c16_9 = _mm_load_sd(&C[(i*56)+31]);
__m128d a16_9 = _mm_load_sd(&values[477]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_9 = _mm_add_sd(c16_9, _mm_mul_sd(a16_9, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_9 = _mm_add_sd(c16_9, _mm_mul_sd(a16_9, b16));
#endif
_mm_store_sd(&C[(i*56)+31], c16_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_10 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a16_10 = _mm256_loadu_pd(&values[478]);
c16_10 = _mm256_add_pd(c16_10, _mm256_mul_pd(a16_10, b16));
_mm256_storeu_pd(&C[(i*56)+35], c16_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_10 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a16_10 = _mm_loadu_pd(&values[478]);
c16_10 = _mm_add_pd(c16_10, _mm_mul_pd(a16_10, b16));
_mm_storeu_pd(&C[(i*56)+35], c16_10);
__m128d c16_12 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a16_12 = _mm_loadu_pd(&values[480]);
c16_12 = _mm_add_pd(c16_12, _mm_mul_pd(a16_12, b16));
_mm_storeu_pd(&C[(i*56)+37], c16_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_14 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a16_14 = _mm256_loadu_pd(&values[482]);
c16_14 = _mm256_add_pd(c16_14, _mm256_mul_pd(a16_14, b16));
_mm256_storeu_pd(&C[(i*56)+39], c16_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_14 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a16_14 = _mm_loadu_pd(&values[482]);
c16_14 = _mm_add_pd(c16_14, _mm_mul_pd(a16_14, b16));
_mm_storeu_pd(&C[(i*56)+39], c16_14);
__m128d c16_16 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a16_16 = _mm_loadu_pd(&values[484]);
c16_16 = _mm_add_pd(c16_16, _mm_mul_pd(a16_16, b16));
_mm_storeu_pd(&C[(i*56)+41], c16_16);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c16_18 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a16_18 = _mm256_loadu_pd(&values[486]);
c16_18 = _mm256_add_pd(c16_18, _mm256_mul_pd(a16_18, b16));
_mm256_storeu_pd(&C[(i*56)+43], c16_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c16_18 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a16_18 = _mm_loadu_pd(&values[486]);
c16_18 = _mm_add_pd(c16_18, _mm_mul_pd(a16_18, b16));
_mm_storeu_pd(&C[(i*56)+43], c16_18);
__m128d c16_20 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a16_20 = _mm_loadu_pd(&values[488]);
c16_20 = _mm_add_pd(c16_20, _mm_mul_pd(a16_20, b16));
_mm_storeu_pd(&C[(i*56)+45], c16_20);
#endif
__m128d c16_22 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a16_22 = _mm_loadu_pd(&values[490]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_22 = _mm_add_pd(c16_22, _mm_mul_pd(a16_22, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_22 = _mm_add_pd(c16_22, _mm_mul_pd(a16_22, b16));
#endif
_mm_storeu_pd(&C[(i*56)+47], c16_22);
__m128d c16_24 = _mm_load_sd(&C[(i*56)+49]);
__m128d a16_24 = _mm_load_sd(&values[492]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_24 = _mm_add_sd(c16_24, _mm_mul_sd(a16_24, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_24 = _mm_add_sd(c16_24, _mm_mul_sd(a16_24, b16));
#endif
_mm_store_sd(&C[(i*56)+49], c16_24);
__m128d c16_25 = _mm_load_sd(&C[(i*56)+52]);
__m128d a16_25 = _mm_load_sd(&values[493]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_25 = _mm_add_sd(c16_25, _mm_mul_sd(a16_25, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_25 = _mm_add_sd(c16_25, _mm_mul_sd(a16_25, b16));
#endif
_mm_store_sd(&C[(i*56)+52], c16_25);
#else
C[(i*56)+20] += values[468] * B[(i*56)+16];
C[(i*56)+21] += values[469] * B[(i*56)+16];
C[(i*56)+22] += values[470] * B[(i*56)+16];
C[(i*56)+23] += values[471] * B[(i*56)+16];
C[(i*56)+24] += values[472] * B[(i*56)+16];
C[(i*56)+25] += values[473] * B[(i*56)+16];
C[(i*56)+26] += values[474] * B[(i*56)+16];
C[(i*56)+27] += values[475] * B[(i*56)+16];
C[(i*56)+28] += values[476] * B[(i*56)+16];
C[(i*56)+31] += values[477] * B[(i*56)+16];
C[(i*56)+35] += values[478] * B[(i*56)+16];
C[(i*56)+36] += values[479] * B[(i*56)+16];
C[(i*56)+37] += values[480] * B[(i*56)+16];
C[(i*56)+38] += values[481] * B[(i*56)+16];
C[(i*56)+39] += values[482] * B[(i*56)+16];
C[(i*56)+40] += values[483] * B[(i*56)+16];
C[(i*56)+41] += values[484] * B[(i*56)+16];
C[(i*56)+42] += values[485] * B[(i*56)+16];
C[(i*56)+43] += values[486] * B[(i*56)+16];
C[(i*56)+44] += values[487] * B[(i*56)+16];
C[(i*56)+45] += values[488] * B[(i*56)+16];
C[(i*56)+46] += values[489] * B[(i*56)+16];
C[(i*56)+47] += values[490] * B[(i*56)+16];
C[(i*56)+48] += values[491] * B[(i*56)+16];
C[(i*56)+49] += values[492] * B[(i*56)+16];
C[(i*56)+52] += values[493] * B[(i*56)+16];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a17_0 = _mm256_loadu_pd(&values[494]);
c17_0 = _mm256_add_pd(c17_0, _mm256_mul_pd(a17_0, b17));
_mm256_storeu_pd(&C[(i*56)+20], c17_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a17_0 = _mm_loadu_pd(&values[494]);
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, b17));
_mm_storeu_pd(&C[(i*56)+20], c17_0);
__m128d c17_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a17_2 = _mm_loadu_pd(&values[496]);
c17_2 = _mm_add_pd(c17_2, _mm_mul_pd(a17_2, b17));
_mm_storeu_pd(&C[(i*56)+22], c17_2);
#endif
__m128d c17_4 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a17_4 = _mm_loadu_pd(&values[498]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_pd(c17_4, _mm_mul_pd(a17_4, b17));
#endif
_mm_storeu_pd(&C[(i*56)+25], c17_4);
__m128d c17_6 = _mm_load_sd(&C[(i*56)+27]);
__m128d a17_6 = _mm_load_sd(&values[500]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_6 = _mm_add_sd(c17_6, _mm_mul_sd(a17_6, b17));
#endif
_mm_store_sd(&C[(i*56)+27], c17_6);
__m128d c17_7 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a17_7 = _mm_loadu_pd(&values[501]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_7 = _mm_add_pd(c17_7, _mm_mul_pd(a17_7, b17));
#endif
_mm_storeu_pd(&C[(i*56)+29], c17_7);
__m128d c17_9 = _mm_load_sd(&C[(i*56)+32]);
__m128d a17_9 = _mm_load_sd(&values[503]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_9 = _mm_add_sd(c17_9, _mm_mul_sd(a17_9, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_9 = _mm_add_sd(c17_9, _mm_mul_sd(a17_9, b17));
#endif
_mm_store_sd(&C[(i*56)+32], c17_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_10 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a17_10 = _mm256_loadu_pd(&values[504]);
c17_10 = _mm256_add_pd(c17_10, _mm256_mul_pd(a17_10, b17));
_mm256_storeu_pd(&C[(i*56)+35], c17_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_10 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a17_10 = _mm_loadu_pd(&values[504]);
c17_10 = _mm_add_pd(c17_10, _mm_mul_pd(a17_10, b17));
_mm_storeu_pd(&C[(i*56)+35], c17_10);
__m128d c17_12 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a17_12 = _mm_loadu_pd(&values[506]);
c17_12 = _mm_add_pd(c17_12, _mm_mul_pd(a17_12, b17));
_mm_storeu_pd(&C[(i*56)+37], c17_12);
#endif
__m128d c17_14 = _mm_load_sd(&C[(i*56)+39]);
__m128d a17_14 = _mm_load_sd(&values[508]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_14 = _mm_add_sd(c17_14, _mm_mul_sd(a17_14, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_14 = _mm_add_sd(c17_14, _mm_mul_sd(a17_14, b17));
#endif
_mm_store_sd(&C[(i*56)+39], c17_14);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c17_15 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a17_15 = _mm256_loadu_pd(&values[509]);
c17_15 = _mm256_add_pd(c17_15, _mm256_mul_pd(a17_15, b17));
_mm256_storeu_pd(&C[(i*56)+41], c17_15);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c17_15 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a17_15 = _mm_loadu_pd(&values[509]);
c17_15 = _mm_add_pd(c17_15, _mm_mul_pd(a17_15, b17));
_mm_storeu_pd(&C[(i*56)+41], c17_15);
__m128d c17_17 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a17_17 = _mm_loadu_pd(&values[511]);
c17_17 = _mm_add_pd(c17_17, _mm_mul_pd(a17_17, b17));
_mm_storeu_pd(&C[(i*56)+43], c17_17);
#endif
__m128d c17_19 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a17_19 = _mm_loadu_pd(&values[513]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_19 = _mm_add_pd(c17_19, _mm_mul_pd(a17_19, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_19 = _mm_add_pd(c17_19, _mm_mul_pd(a17_19, b17));
#endif
_mm_storeu_pd(&C[(i*56)+46], c17_19);
__m128d c17_21 = _mm_load_sd(&C[(i*56)+48]);
__m128d a17_21 = _mm_load_sd(&values[515]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_21 = _mm_add_sd(c17_21, _mm_mul_sd(a17_21, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_21 = _mm_add_sd(c17_21, _mm_mul_sd(a17_21, b17));
#endif
_mm_store_sd(&C[(i*56)+48], c17_21);
__m128d c17_22 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a17_22 = _mm_loadu_pd(&values[516]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_22 = _mm_add_pd(c17_22, _mm_mul_pd(a17_22, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_22 = _mm_add_pd(c17_22, _mm_mul_pd(a17_22, b17));
#endif
_mm_storeu_pd(&C[(i*56)+50], c17_22);
__m128d c17_24 = _mm_load_sd(&C[(i*56)+53]);
__m128d a17_24 = _mm_load_sd(&values[518]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_24 = _mm_add_sd(c17_24, _mm_mul_sd(a17_24, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_24 = _mm_add_sd(c17_24, _mm_mul_sd(a17_24, b17));
#endif
_mm_store_sd(&C[(i*56)+53], c17_24);
#else
C[(i*56)+20] += values[494] * B[(i*56)+17];
C[(i*56)+21] += values[495] * B[(i*56)+17];
C[(i*56)+22] += values[496] * B[(i*56)+17];
C[(i*56)+23] += values[497] * B[(i*56)+17];
C[(i*56)+25] += values[498] * B[(i*56)+17];
C[(i*56)+26] += values[499] * B[(i*56)+17];
C[(i*56)+27] += values[500] * B[(i*56)+17];
C[(i*56)+29] += values[501] * B[(i*56)+17];
C[(i*56)+30] += values[502] * B[(i*56)+17];
C[(i*56)+32] += values[503] * B[(i*56)+17];
C[(i*56)+35] += values[504] * B[(i*56)+17];
C[(i*56)+36] += values[505] * B[(i*56)+17];
C[(i*56)+37] += values[506] * B[(i*56)+17];
C[(i*56)+38] += values[507] * B[(i*56)+17];
C[(i*56)+39] += values[508] * B[(i*56)+17];
C[(i*56)+41] += values[509] * B[(i*56)+17];
C[(i*56)+42] += values[510] * B[(i*56)+17];
C[(i*56)+43] += values[511] * B[(i*56)+17];
C[(i*56)+44] += values[512] * B[(i*56)+17];
C[(i*56)+46] += values[513] * B[(i*56)+17];
C[(i*56)+47] += values[514] * B[(i*56)+17];
C[(i*56)+48] += values[515] * B[(i*56)+17];
C[(i*56)+50] += values[516] * B[(i*56)+17];
C[(i*56)+51] += values[517] * B[(i*56)+17];
C[(i*56)+53] += values[518] * B[(i*56)+17];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*56)+18]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a18_0 = _mm256_loadu_pd(&values[519]);
c18_0 = _mm256_add_pd(c18_0, _mm256_mul_pd(a18_0, b18));
_mm256_storeu_pd(&C[(i*56)+20], c18_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a18_0 = _mm_loadu_pd(&values[519]);
c18_0 = _mm_add_pd(c18_0, _mm_mul_pd(a18_0, b18));
_mm_storeu_pd(&C[(i*56)+20], c18_0);
__m128d c18_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a18_2 = _mm_loadu_pd(&values[521]);
c18_2 = _mm_add_pd(c18_2, _mm_mul_pd(a18_2, b18));
_mm_storeu_pd(&C[(i*56)+22], c18_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_4 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a18_4 = _mm256_loadu_pd(&values[523]);
c18_4 = _mm256_add_pd(c18_4, _mm256_mul_pd(a18_4, b18));
_mm256_storeu_pd(&C[(i*56)+24], c18_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_4 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a18_4 = _mm_loadu_pd(&values[523]);
c18_4 = _mm_add_pd(c18_4, _mm_mul_pd(a18_4, b18));
_mm_storeu_pd(&C[(i*56)+24], c18_4);
__m128d c18_6 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a18_6 = _mm_loadu_pd(&values[525]);
c18_6 = _mm_add_pd(c18_6, _mm_mul_pd(a18_6, b18));
_mm_storeu_pd(&C[(i*56)+26], c18_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_8 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a18_8 = _mm256_loadu_pd(&values[527]);
c18_8 = _mm256_add_pd(c18_8, _mm256_mul_pd(a18_8, b18));
_mm256_storeu_pd(&C[(i*56)+28], c18_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_8 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a18_8 = _mm_loadu_pd(&values[527]);
c18_8 = _mm_add_pd(c18_8, _mm_mul_pd(a18_8, b18));
_mm_storeu_pd(&C[(i*56)+28], c18_8);
__m128d c18_10 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a18_10 = _mm_loadu_pd(&values[529]);
c18_10 = _mm_add_pd(c18_10, _mm_mul_pd(a18_10, b18));
_mm_storeu_pd(&C[(i*56)+30], c18_10);
#endif
__m128d c18_12 = _mm_load_sd(&C[(i*56)+33]);
__m128d a18_12 = _mm_load_sd(&values[531]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_12 = _mm_add_sd(c18_12, _mm_mul_sd(a18_12, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_12 = _mm_add_sd(c18_12, _mm_mul_sd(a18_12, b18));
#endif
_mm_store_sd(&C[(i*56)+33], c18_12);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_13 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a18_13 = _mm256_loadu_pd(&values[532]);
c18_13 = _mm256_add_pd(c18_13, _mm256_mul_pd(a18_13, b18));
_mm256_storeu_pd(&C[(i*56)+35], c18_13);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_13 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a18_13 = _mm_loadu_pd(&values[532]);
c18_13 = _mm_add_pd(c18_13, _mm_mul_pd(a18_13, b18));
_mm_storeu_pd(&C[(i*56)+35], c18_13);
__m128d c18_15 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a18_15 = _mm_loadu_pd(&values[534]);
c18_15 = _mm_add_pd(c18_15, _mm_mul_pd(a18_15, b18));
_mm_storeu_pd(&C[(i*56)+37], c18_15);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_17 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a18_17 = _mm256_loadu_pd(&values[536]);
c18_17 = _mm256_add_pd(c18_17, _mm256_mul_pd(a18_17, b18));
_mm256_storeu_pd(&C[(i*56)+39], c18_17);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_17 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a18_17 = _mm_loadu_pd(&values[536]);
c18_17 = _mm_add_pd(c18_17, _mm_mul_pd(a18_17, b18));
_mm_storeu_pd(&C[(i*56)+39], c18_17);
__m128d c18_19 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a18_19 = _mm_loadu_pd(&values[538]);
c18_19 = _mm_add_pd(c18_19, _mm_mul_pd(a18_19, b18));
_mm_storeu_pd(&C[(i*56)+41], c18_19);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_21 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a18_21 = _mm256_loadu_pd(&values[540]);
c18_21 = _mm256_add_pd(c18_21, _mm256_mul_pd(a18_21, b18));
_mm256_storeu_pd(&C[(i*56)+43], c18_21);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_21 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a18_21 = _mm_loadu_pd(&values[540]);
c18_21 = _mm_add_pd(c18_21, _mm_mul_pd(a18_21, b18));
_mm_storeu_pd(&C[(i*56)+43], c18_21);
__m128d c18_23 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a18_23 = _mm_loadu_pd(&values[542]);
c18_23 = _mm_add_pd(c18_23, _mm_mul_pd(a18_23, b18));
_mm_storeu_pd(&C[(i*56)+45], c18_23);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c18_25 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a18_25 = _mm256_loadu_pd(&values[544]);
c18_25 = _mm256_add_pd(c18_25, _mm256_mul_pd(a18_25, b18));
_mm256_storeu_pd(&C[(i*56)+47], c18_25);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c18_25 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a18_25 = _mm_loadu_pd(&values[544]);
c18_25 = _mm_add_pd(c18_25, _mm_mul_pd(a18_25, b18));
_mm_storeu_pd(&C[(i*56)+47], c18_25);
__m128d c18_27 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a18_27 = _mm_loadu_pd(&values[546]);
c18_27 = _mm_add_pd(c18_27, _mm_mul_pd(a18_27, b18));
_mm_storeu_pd(&C[(i*56)+49], c18_27);
#endif
__m128d c18_29 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a18_29 = _mm_loadu_pd(&values[548]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_29 = _mm_add_pd(c18_29, _mm_mul_pd(a18_29, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_29 = _mm_add_pd(c18_29, _mm_mul_pd(a18_29, b18));
#endif
_mm_storeu_pd(&C[(i*56)+51], c18_29);
__m128d c18_31 = _mm_load_sd(&C[(i*56)+54]);
__m128d a18_31 = _mm_load_sd(&values[550]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_31 = _mm_add_sd(c18_31, _mm_mul_sd(a18_31, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_31 = _mm_add_sd(c18_31, _mm_mul_sd(a18_31, b18));
#endif
_mm_store_sd(&C[(i*56)+54], c18_31);
#else
C[(i*56)+20] += values[519] * B[(i*56)+18];
C[(i*56)+21] += values[520] * B[(i*56)+18];
C[(i*56)+22] += values[521] * B[(i*56)+18];
C[(i*56)+23] += values[522] * B[(i*56)+18];
C[(i*56)+24] += values[523] * B[(i*56)+18];
C[(i*56)+25] += values[524] * B[(i*56)+18];
C[(i*56)+26] += values[525] * B[(i*56)+18];
C[(i*56)+27] += values[526] * B[(i*56)+18];
C[(i*56)+28] += values[527] * B[(i*56)+18];
C[(i*56)+29] += values[528] * B[(i*56)+18];
C[(i*56)+30] += values[529] * B[(i*56)+18];
C[(i*56)+31] += values[530] * B[(i*56)+18];
C[(i*56)+33] += values[531] * B[(i*56)+18];
C[(i*56)+35] += values[532] * B[(i*56)+18];
C[(i*56)+36] += values[533] * B[(i*56)+18];
C[(i*56)+37] += values[534] * B[(i*56)+18];
C[(i*56)+38] += values[535] * B[(i*56)+18];
C[(i*56)+39] += values[536] * B[(i*56)+18];
C[(i*56)+40] += values[537] * B[(i*56)+18];
C[(i*56)+41] += values[538] * B[(i*56)+18];
C[(i*56)+42] += values[539] * B[(i*56)+18];
C[(i*56)+43] += values[540] * B[(i*56)+18];
C[(i*56)+44] += values[541] * B[(i*56)+18];
C[(i*56)+45] += values[542] * B[(i*56)+18];
C[(i*56)+46] += values[543] * B[(i*56)+18];
C[(i*56)+47] += values[544] * B[(i*56)+18];
C[(i*56)+48] += values[545] * B[(i*56)+18];
C[(i*56)+49] += values[546] * B[(i*56)+18];
C[(i*56)+50] += values[547] * B[(i*56)+18];
C[(i*56)+51] += values[548] * B[(i*56)+18];
C[(i*56)+52] += values[549] * B[(i*56)+18];
C[(i*56)+54] += values[550] * B[(i*56)+18];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*56)+19]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_0 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a19_0 = _mm256_loadu_pd(&values[551]);
c19_0 = _mm256_add_pd(c19_0, _mm256_mul_pd(a19_0, b19));
_mm256_storeu_pd(&C[(i*56)+20], c19_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_0 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a19_0 = _mm_loadu_pd(&values[551]);
c19_0 = _mm_add_pd(c19_0, _mm_mul_pd(a19_0, b19));
_mm_storeu_pd(&C[(i*56)+20], c19_0);
__m128d c19_2 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a19_2 = _mm_loadu_pd(&values[553]);
c19_2 = _mm_add_pd(c19_2, _mm_mul_pd(a19_2, b19));
_mm_storeu_pd(&C[(i*56)+22], c19_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_4 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a19_4 = _mm256_loadu_pd(&values[555]);
c19_4 = _mm256_add_pd(c19_4, _mm256_mul_pd(a19_4, b19));
_mm256_storeu_pd(&C[(i*56)+24], c19_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_4 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a19_4 = _mm_loadu_pd(&values[555]);
c19_4 = _mm_add_pd(c19_4, _mm_mul_pd(a19_4, b19));
_mm_storeu_pd(&C[(i*56)+24], c19_4);
__m128d c19_6 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a19_6 = _mm_loadu_pd(&values[557]);
c19_6 = _mm_add_pd(c19_6, _mm_mul_pd(a19_6, b19));
_mm_storeu_pd(&C[(i*56)+26], c19_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_8 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a19_8 = _mm256_loadu_pd(&values[559]);
c19_8 = _mm256_add_pd(c19_8, _mm256_mul_pd(a19_8, b19));
_mm256_storeu_pd(&C[(i*56)+28], c19_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_8 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a19_8 = _mm_loadu_pd(&values[559]);
c19_8 = _mm_add_pd(c19_8, _mm_mul_pd(a19_8, b19));
_mm_storeu_pd(&C[(i*56)+28], c19_8);
__m128d c19_10 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a19_10 = _mm_loadu_pd(&values[561]);
c19_10 = _mm_add_pd(c19_10, _mm_mul_pd(a19_10, b19));
_mm_storeu_pd(&C[(i*56)+30], c19_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_12 = _mm256_loadu_pd(&C[(i*56)+32]);
__m256d a19_12 = _mm256_loadu_pd(&values[563]);
c19_12 = _mm256_add_pd(c19_12, _mm256_mul_pd(a19_12, b19));
_mm256_storeu_pd(&C[(i*56)+32], c19_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_12 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a19_12 = _mm_loadu_pd(&values[563]);
c19_12 = _mm_add_pd(c19_12, _mm_mul_pd(a19_12, b19));
_mm_storeu_pd(&C[(i*56)+32], c19_12);
__m128d c19_14 = _mm_loadu_pd(&C[(i*56)+34]);
__m128d a19_14 = _mm_loadu_pd(&values[565]);
c19_14 = _mm_add_pd(c19_14, _mm_mul_pd(a19_14, b19));
_mm_storeu_pd(&C[(i*56)+34], c19_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_16 = _mm256_loadu_pd(&C[(i*56)+36]);
__m256d a19_16 = _mm256_loadu_pd(&values[567]);
c19_16 = _mm256_add_pd(c19_16, _mm256_mul_pd(a19_16, b19));
_mm256_storeu_pd(&C[(i*56)+36], c19_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_16 = _mm_loadu_pd(&C[(i*56)+36]);
__m128d a19_16 = _mm_loadu_pd(&values[567]);
c19_16 = _mm_add_pd(c19_16, _mm_mul_pd(a19_16, b19));
_mm_storeu_pd(&C[(i*56)+36], c19_16);
__m128d c19_18 = _mm_loadu_pd(&C[(i*56)+38]);
__m128d a19_18 = _mm_loadu_pd(&values[569]);
c19_18 = _mm_add_pd(c19_18, _mm_mul_pd(a19_18, b19));
_mm_storeu_pd(&C[(i*56)+38], c19_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_20 = _mm256_loadu_pd(&C[(i*56)+40]);
__m256d a19_20 = _mm256_loadu_pd(&values[571]);
c19_20 = _mm256_add_pd(c19_20, _mm256_mul_pd(a19_20, b19));
_mm256_storeu_pd(&C[(i*56)+40], c19_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_20 = _mm_loadu_pd(&C[(i*56)+40]);
__m128d a19_20 = _mm_loadu_pd(&values[571]);
c19_20 = _mm_add_pd(c19_20, _mm_mul_pd(a19_20, b19));
_mm_storeu_pd(&C[(i*56)+40], c19_20);
__m128d c19_22 = _mm_loadu_pd(&C[(i*56)+42]);
__m128d a19_22 = _mm_loadu_pd(&values[573]);
c19_22 = _mm_add_pd(c19_22, _mm_mul_pd(a19_22, b19));
_mm_storeu_pd(&C[(i*56)+42], c19_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_24 = _mm256_loadu_pd(&C[(i*56)+44]);
__m256d a19_24 = _mm256_loadu_pd(&values[575]);
c19_24 = _mm256_add_pd(c19_24, _mm256_mul_pd(a19_24, b19));
_mm256_storeu_pd(&C[(i*56)+44], c19_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_24 = _mm_loadu_pd(&C[(i*56)+44]);
__m128d a19_24 = _mm_loadu_pd(&values[575]);
c19_24 = _mm_add_pd(c19_24, _mm_mul_pd(a19_24, b19));
_mm_storeu_pd(&C[(i*56)+44], c19_24);
__m128d c19_26 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a19_26 = _mm_loadu_pd(&values[577]);
c19_26 = _mm_add_pd(c19_26, _mm_mul_pd(a19_26, b19));
_mm_storeu_pd(&C[(i*56)+46], c19_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_28 = _mm256_loadu_pd(&C[(i*56)+48]);
__m256d a19_28 = _mm256_loadu_pd(&values[579]);
c19_28 = _mm256_add_pd(c19_28, _mm256_mul_pd(a19_28, b19));
_mm256_storeu_pd(&C[(i*56)+48], c19_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_28 = _mm_loadu_pd(&C[(i*56)+48]);
__m128d a19_28 = _mm_loadu_pd(&values[579]);
c19_28 = _mm_add_pd(c19_28, _mm_mul_pd(a19_28, b19));
_mm_storeu_pd(&C[(i*56)+48], c19_28);
__m128d c19_30 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a19_30 = _mm_loadu_pd(&values[581]);
c19_30 = _mm_add_pd(c19_30, _mm_mul_pd(a19_30, b19));
_mm_storeu_pd(&C[(i*56)+50], c19_30);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c19_32 = _mm256_loadu_pd(&C[(i*56)+52]);
__m256d a19_32 = _mm256_loadu_pd(&values[583]);
c19_32 = _mm256_add_pd(c19_32, _mm256_mul_pd(a19_32, b19));
_mm256_storeu_pd(&C[(i*56)+52], c19_32);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c19_32 = _mm_loadu_pd(&C[(i*56)+52]);
__m128d a19_32 = _mm_loadu_pd(&values[583]);
c19_32 = _mm_add_pd(c19_32, _mm_mul_pd(a19_32, b19));
_mm_storeu_pd(&C[(i*56)+52], c19_32);
__m128d c19_34 = _mm_loadu_pd(&C[(i*56)+54]);
__m128d a19_34 = _mm_loadu_pd(&values[585]);
c19_34 = _mm_add_pd(c19_34, _mm_mul_pd(a19_34, b19));
_mm_storeu_pd(&C[(i*56)+54], c19_34);
#endif
#else
C[(i*56)+20] += values[551] * B[(i*56)+19];
C[(i*56)+21] += values[552] * B[(i*56)+19];
C[(i*56)+22] += values[553] * B[(i*56)+19];
C[(i*56)+23] += values[554] * B[(i*56)+19];
C[(i*56)+24] += values[555] * B[(i*56)+19];
C[(i*56)+25] += values[556] * B[(i*56)+19];
C[(i*56)+26] += values[557] * B[(i*56)+19];
C[(i*56)+27] += values[558] * B[(i*56)+19];
C[(i*56)+28] += values[559] * B[(i*56)+19];
C[(i*56)+29] += values[560] * B[(i*56)+19];
C[(i*56)+30] += values[561] * B[(i*56)+19];
C[(i*56)+31] += values[562] * B[(i*56)+19];
C[(i*56)+32] += values[563] * B[(i*56)+19];
C[(i*56)+33] += values[564] * B[(i*56)+19];
C[(i*56)+34] += values[565] * B[(i*56)+19];
C[(i*56)+35] += values[566] * B[(i*56)+19];
C[(i*56)+36] += values[567] * B[(i*56)+19];
C[(i*56)+37] += values[568] * B[(i*56)+19];
C[(i*56)+38] += values[569] * B[(i*56)+19];
C[(i*56)+39] += values[570] * B[(i*56)+19];
C[(i*56)+40] += values[571] * B[(i*56)+19];
C[(i*56)+41] += values[572] * B[(i*56)+19];
C[(i*56)+42] += values[573] * B[(i*56)+19];
C[(i*56)+43] += values[574] * B[(i*56)+19];
C[(i*56)+44] += values[575] * B[(i*56)+19];
C[(i*56)+45] += values[576] * B[(i*56)+19];
C[(i*56)+46] += values[577] * B[(i*56)+19];
C[(i*56)+47] += values[578] * B[(i*56)+19];
C[(i*56)+48] += values[579] * B[(i*56)+19];
C[(i*56)+49] += values[580] * B[(i*56)+19];
C[(i*56)+50] += values[581] * B[(i*56)+19];
C[(i*56)+51] += values[582] * B[(i*56)+19];
C[(i*56)+52] += values[583] * B[(i*56)+19];
C[(i*56)+53] += values[584] * B[(i*56)+19];
C[(i*56)+54] += values[585] * B[(i*56)+19];
C[(i*56)+55] += values[586] * B[(i*56)+19];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*56)+20]);
#endif
__m128d c20_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a20_0 = _mm_loadu_pd(&values[587]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, b20));
#endif
_mm_storeu_pd(&C[(i*56)+35], c20_0);
__m128d c20_2 = _mm_load_sd(&C[(i*56)+41]);
__m128d a20_2 = _mm_load_sd(&values[589]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_2 = _mm_add_sd(c20_2, _mm_mul_sd(a20_2, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_2 = _mm_add_sd(c20_2, _mm_mul_sd(a20_2, b20));
#endif
_mm_store_sd(&C[(i*56)+41], c20_2);
#else
C[(i*56)+35] += values[587] * B[(i*56)+20];
C[(i*56)+36] += values[588] * B[(i*56)+20];
C[(i*56)+41] += values[589] * B[(i*56)+20];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*56)+21]);
#endif
__m128d c21_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a21_0 = _mm_loadu_pd(&values[590]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, b21));
#endif
_mm_storeu_pd(&C[(i*56)+35], c21_0);
__m128d c21_2 = _mm_load_sd(&C[(i*56)+37]);
__m128d a21_2 = _mm_load_sd(&values[592]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_2 = _mm_add_sd(c21_2, _mm_mul_sd(a21_2, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_2 = _mm_add_sd(c21_2, _mm_mul_sd(a21_2, b21));
#endif
_mm_store_sd(&C[(i*56)+37], c21_2);
__m128d c21_3 = _mm_load_sd(&C[(i*56)+42]);
__m128d a21_3 = _mm_load_sd(&values[593]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_3 = _mm_add_sd(c21_3, _mm_mul_sd(a21_3, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_3 = _mm_add_sd(c21_3, _mm_mul_sd(a21_3, b21));
#endif
_mm_store_sd(&C[(i*56)+42], c21_3);
#else
C[(i*56)+35] += values[590] * B[(i*56)+21];
C[(i*56)+36] += values[591] * B[(i*56)+21];
C[(i*56)+37] += values[592] * B[(i*56)+21];
C[(i*56)+42] += values[593] * B[(i*56)+21];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a22_0 = _mm256_loadu_pd(&values[594]);
c22_0 = _mm256_add_pd(c22_0, _mm256_mul_pd(a22_0, b22));
_mm256_storeu_pd(&C[(i*56)+35], c22_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a22_0 = _mm_loadu_pd(&values[594]);
c22_0 = _mm_add_pd(c22_0, _mm_mul_pd(a22_0, b22));
_mm_storeu_pd(&C[(i*56)+35], c22_0);
__m128d c22_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a22_2 = _mm_loadu_pd(&values[596]);
c22_2 = _mm_add_pd(c22_2, _mm_mul_pd(a22_2, b22));
_mm_storeu_pd(&C[(i*56)+37], c22_2);
#endif
__m128d c22_4 = _mm_load_sd(&C[(i*56)+43]);
__m128d a22_4 = _mm_load_sd(&values[598]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_4 = _mm_add_sd(c22_4, _mm_mul_sd(a22_4, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_4 = _mm_add_sd(c22_4, _mm_mul_sd(a22_4, b22));
#endif
_mm_store_sd(&C[(i*56)+43], c22_4);
#else
C[(i*56)+35] += values[594] * B[(i*56)+22];
C[(i*56)+36] += values[595] * B[(i*56)+22];
C[(i*56)+37] += values[596] * B[(i*56)+22];
C[(i*56)+38] += values[597] * B[(i*56)+22];
C[(i*56)+43] += values[598] * B[(i*56)+22];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a23_0 = _mm256_loadu_pd(&values[599]);
c23_0 = _mm256_add_pd(c23_0, _mm256_mul_pd(a23_0, b23));
_mm256_storeu_pd(&C[(i*56)+35], c23_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a23_0 = _mm_loadu_pd(&values[599]);
c23_0 = _mm_add_pd(c23_0, _mm_mul_pd(a23_0, b23));
_mm_storeu_pd(&C[(i*56)+35], c23_0);
__m128d c23_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a23_2 = _mm_loadu_pd(&values[601]);
c23_2 = _mm_add_pd(c23_2, _mm_mul_pd(a23_2, b23));
_mm_storeu_pd(&C[(i*56)+37], c23_2);
#endif
__m128d c23_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a23_4 = _mm_load_sd(&values[603]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_4 = _mm_add_sd(c23_4, _mm_mul_sd(a23_4, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_4 = _mm_add_sd(c23_4, _mm_mul_sd(a23_4, b23));
#endif
_mm_store_sd(&C[(i*56)+39], c23_4);
__m128d c23_5 = _mm_load_sd(&C[(i*56)+44]);
__m128d a23_5 = _mm_load_sd(&values[604]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_5 = _mm_add_sd(c23_5, _mm_mul_sd(a23_5, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_5 = _mm_add_sd(c23_5, _mm_mul_sd(a23_5, b23));
#endif
_mm_store_sd(&C[(i*56)+44], c23_5);
#else
C[(i*56)+35] += values[599] * B[(i*56)+23];
C[(i*56)+36] += values[600] * B[(i*56)+23];
C[(i*56)+37] += values[601] * B[(i*56)+23];
C[(i*56)+38] += values[602] * B[(i*56)+23];
C[(i*56)+39] += values[603] * B[(i*56)+23];
C[(i*56)+44] += values[604] * B[(i*56)+23];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b24 = _mm256_broadcast_sd(&B[(i*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b24 = _mm_loaddup_pd(&B[(i*56)+24]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c24_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a24_0 = _mm256_loadu_pd(&values[605]);
c24_0 = _mm256_add_pd(c24_0, _mm256_mul_pd(a24_0, b24));
_mm256_storeu_pd(&C[(i*56)+35], c24_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c24_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a24_0 = _mm_loadu_pd(&values[605]);
c24_0 = _mm_add_pd(c24_0, _mm_mul_pd(a24_0, b24));
_mm_storeu_pd(&C[(i*56)+35], c24_0);
__m128d c24_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a24_2 = _mm_loadu_pd(&values[607]);
c24_2 = _mm_add_pd(c24_2, _mm_mul_pd(a24_2, b24));
_mm_storeu_pd(&C[(i*56)+37], c24_2);
#endif
__m128d c24_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a24_4 = _mm_loadu_pd(&values[609]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, b24));
#endif
_mm_storeu_pd(&C[(i*56)+39], c24_4);
__m128d c24_6 = _mm_load_sd(&C[(i*56)+45]);
__m128d a24_6 = _mm_load_sd(&values[611]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, b24));
#endif
_mm_store_sd(&C[(i*56)+45], c24_6);
#else
C[(i*56)+35] += values[605] * B[(i*56)+24];
C[(i*56)+36] += values[606] * B[(i*56)+24];
C[(i*56)+37] += values[607] * B[(i*56)+24];
C[(i*56)+38] += values[608] * B[(i*56)+24];
C[(i*56)+39] += values[609] * B[(i*56)+24];
C[(i*56)+40] += values[610] * B[(i*56)+24];
C[(i*56)+45] += values[611] * B[(i*56)+24];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*56)+25]);
#endif
__m128d c25_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a25_0 = _mm_loadu_pd(&values[612]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, b25));
#endif
_mm_storeu_pd(&C[(i*56)+35], c25_0);
__m128d c25_2 = _mm_load_sd(&C[(i*56)+37]);
__m128d a25_2 = _mm_load_sd(&values[614]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_2 = _mm_add_sd(c25_2, _mm_mul_sd(a25_2, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_2 = _mm_add_sd(c25_2, _mm_mul_sd(a25_2, b25));
#endif
_mm_store_sd(&C[(i*56)+37], c25_2);
__m128d c25_3 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a25_3 = _mm_loadu_pd(&values[615]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_3 = _mm_add_pd(c25_3, _mm_mul_pd(a25_3, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_3 = _mm_add_pd(c25_3, _mm_mul_pd(a25_3, b25));
#endif
_mm_storeu_pd(&C[(i*56)+41], c25_3);
__m128d c25_5 = _mm_load_sd(&C[(i*56)+46]);
__m128d a25_5 = _mm_load_sd(&values[617]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_5 = _mm_add_sd(c25_5, _mm_mul_sd(a25_5, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_5 = _mm_add_sd(c25_5, _mm_mul_sd(a25_5, b25));
#endif
_mm_store_sd(&C[(i*56)+46], c25_5);
#else
C[(i*56)+35] += values[612] * B[(i*56)+25];
C[(i*56)+36] += values[613] * B[(i*56)+25];
C[(i*56)+37] += values[614] * B[(i*56)+25];
C[(i*56)+41] += values[615] * B[(i*56)+25];
C[(i*56)+42] += values[616] * B[(i*56)+25];
C[(i*56)+46] += values[617] * B[(i*56)+25];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a26_0 = _mm256_loadu_pd(&values[618]);
c26_0 = _mm256_add_pd(c26_0, _mm256_mul_pd(a26_0, b26));
_mm256_storeu_pd(&C[(i*56)+35], c26_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a26_0 = _mm_loadu_pd(&values[618]);
c26_0 = _mm_add_pd(c26_0, _mm_mul_pd(a26_0, b26));
_mm_storeu_pd(&C[(i*56)+35], c26_0);
__m128d c26_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a26_2 = _mm_loadu_pd(&values[620]);
c26_2 = _mm_add_pd(c26_2, _mm_mul_pd(a26_2, b26));
_mm_storeu_pd(&C[(i*56)+37], c26_2);
#endif
__m128d c26_4 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a26_4 = _mm_loadu_pd(&values[622]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, b26));
#endif
_mm_storeu_pd(&C[(i*56)+41], c26_4);
__m128d c26_6 = _mm_load_sd(&C[(i*56)+43]);
__m128d a26_6 = _mm_load_sd(&values[624]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_6 = _mm_add_sd(c26_6, _mm_mul_sd(a26_6, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_6 = _mm_add_sd(c26_6, _mm_mul_sd(a26_6, b26));
#endif
_mm_store_sd(&C[(i*56)+43], c26_6);
__m128d c26_7 = _mm_load_sd(&C[(i*56)+47]);
__m128d a26_7 = _mm_load_sd(&values[625]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_7 = _mm_add_sd(c26_7, _mm_mul_sd(a26_7, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_7 = _mm_add_sd(c26_7, _mm_mul_sd(a26_7, b26));
#endif
_mm_store_sd(&C[(i*56)+47], c26_7);
#else
C[(i*56)+35] += values[618] * B[(i*56)+26];
C[(i*56)+36] += values[619] * B[(i*56)+26];
C[(i*56)+37] += values[620] * B[(i*56)+26];
C[(i*56)+38] += values[621] * B[(i*56)+26];
C[(i*56)+41] += values[622] * B[(i*56)+26];
C[(i*56)+42] += values[623] * B[(i*56)+26];
C[(i*56)+43] += values[624] * B[(i*56)+26];
C[(i*56)+47] += values[625] * B[(i*56)+26];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a27_0 = _mm256_loadu_pd(&values[626]);
c27_0 = _mm256_add_pd(c27_0, _mm256_mul_pd(a27_0, b27));
_mm256_storeu_pd(&C[(i*56)+35], c27_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a27_0 = _mm_loadu_pd(&values[626]);
c27_0 = _mm_add_pd(c27_0, _mm_mul_pd(a27_0, b27));
_mm_storeu_pd(&C[(i*56)+35], c27_0);
__m128d c27_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a27_2 = _mm_loadu_pd(&values[628]);
c27_2 = _mm_add_pd(c27_2, _mm_mul_pd(a27_2, b27));
_mm_storeu_pd(&C[(i*56)+37], c27_2);
#endif
__m128d c27_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a27_4 = _mm_load_sd(&values[630]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_4 = _mm_add_sd(c27_4, _mm_mul_sd(a27_4, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_4 = _mm_add_sd(c27_4, _mm_mul_sd(a27_4, b27));
#endif
_mm_store_sd(&C[(i*56)+39], c27_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_5 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a27_5 = _mm256_loadu_pd(&values[631]);
c27_5 = _mm256_add_pd(c27_5, _mm256_mul_pd(a27_5, b27));
_mm256_storeu_pd(&C[(i*56)+41], c27_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_5 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a27_5 = _mm_loadu_pd(&values[631]);
c27_5 = _mm_add_pd(c27_5, _mm_mul_pd(a27_5, b27));
_mm_storeu_pd(&C[(i*56)+41], c27_5);
__m128d c27_7 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a27_7 = _mm_loadu_pd(&values[633]);
c27_7 = _mm_add_pd(c27_7, _mm_mul_pd(a27_7, b27));
_mm_storeu_pd(&C[(i*56)+43], c27_7);
#endif
__m128d c27_9 = _mm_load_sd(&C[(i*56)+48]);
__m128d a27_9 = _mm_load_sd(&values[635]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_9 = _mm_add_sd(c27_9, _mm_mul_sd(a27_9, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_9 = _mm_add_sd(c27_9, _mm_mul_sd(a27_9, b27));
#endif
_mm_store_sd(&C[(i*56)+48], c27_9);
#else
C[(i*56)+35] += values[626] * B[(i*56)+27];
C[(i*56)+36] += values[627] * B[(i*56)+27];
C[(i*56)+37] += values[628] * B[(i*56)+27];
C[(i*56)+38] += values[629] * B[(i*56)+27];
C[(i*56)+39] += values[630] * B[(i*56)+27];
C[(i*56)+41] += values[631] * B[(i*56)+27];
C[(i*56)+42] += values[632] * B[(i*56)+27];
C[(i*56)+43] += values[633] * B[(i*56)+27];
C[(i*56)+44] += values[634] * B[(i*56)+27];
C[(i*56)+48] += values[635] * B[(i*56)+27];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b28 = _mm256_broadcast_sd(&B[(i*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b28 = _mm_loaddup_pd(&B[(i*56)+28]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c28_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a28_0 = _mm256_loadu_pd(&values[636]);
c28_0 = _mm256_add_pd(c28_0, _mm256_mul_pd(a28_0, b28));
_mm256_storeu_pd(&C[(i*56)+35], c28_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c28_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a28_0 = _mm_loadu_pd(&values[636]);
c28_0 = _mm_add_pd(c28_0, _mm_mul_pd(a28_0, b28));
_mm_storeu_pd(&C[(i*56)+35], c28_0);
__m128d c28_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a28_2 = _mm_loadu_pd(&values[638]);
c28_2 = _mm_add_pd(c28_2, _mm_mul_pd(a28_2, b28));
_mm_storeu_pd(&C[(i*56)+37], c28_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c28_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a28_4 = _mm256_loadu_pd(&values[640]);
c28_4 = _mm256_add_pd(c28_4, _mm256_mul_pd(a28_4, b28));
_mm256_storeu_pd(&C[(i*56)+39], c28_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c28_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a28_4 = _mm_loadu_pd(&values[640]);
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, b28));
_mm_storeu_pd(&C[(i*56)+39], c28_4);
__m128d c28_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a28_6 = _mm_loadu_pd(&values[642]);
c28_6 = _mm_add_pd(c28_6, _mm_mul_pd(a28_6, b28));
_mm_storeu_pd(&C[(i*56)+41], c28_6);
#endif
__m128d c28_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a28_8 = _mm_loadu_pd(&values[644]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, b28));
#endif
_mm_storeu_pd(&C[(i*56)+43], c28_8);
__m128d c28_10 = _mm_load_sd(&C[(i*56)+45]);
__m128d a28_10 = _mm_load_sd(&values[646]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_10 = _mm_add_sd(c28_10, _mm_mul_sd(a28_10, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_10 = _mm_add_sd(c28_10, _mm_mul_sd(a28_10, b28));
#endif
_mm_store_sd(&C[(i*56)+45], c28_10);
__m128d c28_11 = _mm_load_sd(&C[(i*56)+49]);
__m128d a28_11 = _mm_load_sd(&values[647]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_11 = _mm_add_sd(c28_11, _mm_mul_sd(a28_11, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_11 = _mm_add_sd(c28_11, _mm_mul_sd(a28_11, b28));
#endif
_mm_store_sd(&C[(i*56)+49], c28_11);
#else
C[(i*56)+35] += values[636] * B[(i*56)+28];
C[(i*56)+36] += values[637] * B[(i*56)+28];
C[(i*56)+37] += values[638] * B[(i*56)+28];
C[(i*56)+38] += values[639] * B[(i*56)+28];
C[(i*56)+39] += values[640] * B[(i*56)+28];
C[(i*56)+40] += values[641] * B[(i*56)+28];
C[(i*56)+41] += values[642] * B[(i*56)+28];
C[(i*56)+42] += values[643] * B[(i*56)+28];
C[(i*56)+43] += values[644] * B[(i*56)+28];
C[(i*56)+44] += values[645] * B[(i*56)+28];
C[(i*56)+45] += values[646] * B[(i*56)+28];
C[(i*56)+49] += values[647] * B[(i*56)+28];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c29_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a29_0 = _mm256_loadu_pd(&values[648]);
c29_0 = _mm256_add_pd(c29_0, _mm256_mul_pd(a29_0, b29));
_mm256_storeu_pd(&C[(i*56)+35], c29_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c29_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a29_0 = _mm_loadu_pd(&values[648]);
c29_0 = _mm_add_pd(c29_0, _mm_mul_pd(a29_0, b29));
_mm_storeu_pd(&C[(i*56)+35], c29_0);
__m128d c29_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a29_2 = _mm_loadu_pd(&values[650]);
c29_2 = _mm_add_pd(c29_2, _mm_mul_pd(a29_2, b29));
_mm_storeu_pd(&C[(i*56)+37], c29_2);
#endif
__m128d c29_4 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a29_4 = _mm_loadu_pd(&values[652]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, b29));
#endif
_mm_storeu_pd(&C[(i*56)+41], c29_4);
__m128d c29_6 = _mm_load_sd(&C[(i*56)+43]);
__m128d a29_6 = _mm_load_sd(&values[654]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, b29));
#endif
_mm_store_sd(&C[(i*56)+43], c29_6);
__m128d c29_7 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a29_7 = _mm_loadu_pd(&values[655]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, b29));
#endif
_mm_storeu_pd(&C[(i*56)+46], c29_7);
__m128d c29_9 = _mm_load_sd(&C[(i*56)+50]);
__m128d a29_9 = _mm_load_sd(&values[657]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_9 = _mm_add_sd(c29_9, _mm_mul_sd(a29_9, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_9 = _mm_add_sd(c29_9, _mm_mul_sd(a29_9, b29));
#endif
_mm_store_sd(&C[(i*56)+50], c29_9);
#else
C[(i*56)+35] += values[648] * B[(i*56)+29];
C[(i*56)+36] += values[649] * B[(i*56)+29];
C[(i*56)+37] += values[650] * B[(i*56)+29];
C[(i*56)+38] += values[651] * B[(i*56)+29];
C[(i*56)+41] += values[652] * B[(i*56)+29];
C[(i*56)+42] += values[653] * B[(i*56)+29];
C[(i*56)+43] += values[654] * B[(i*56)+29];
C[(i*56)+46] += values[655] * B[(i*56)+29];
C[(i*56)+47] += values[656] * B[(i*56)+29];
C[(i*56)+50] += values[657] * B[(i*56)+29];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a30_0 = _mm256_loadu_pd(&values[658]);
c30_0 = _mm256_add_pd(c30_0, _mm256_mul_pd(a30_0, b30));
_mm256_storeu_pd(&C[(i*56)+35], c30_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a30_0 = _mm_loadu_pd(&values[658]);
c30_0 = _mm_add_pd(c30_0, _mm_mul_pd(a30_0, b30));
_mm_storeu_pd(&C[(i*56)+35], c30_0);
__m128d c30_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a30_2 = _mm_loadu_pd(&values[660]);
c30_2 = _mm_add_pd(c30_2, _mm_mul_pd(a30_2, b30));
_mm_storeu_pd(&C[(i*56)+37], c30_2);
#endif
__m128d c30_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a30_4 = _mm_load_sd(&values[662]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, b30));
#endif
_mm_store_sd(&C[(i*56)+39], c30_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_5 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a30_5 = _mm256_loadu_pd(&values[663]);
c30_5 = _mm256_add_pd(c30_5, _mm256_mul_pd(a30_5, b30));
_mm256_storeu_pd(&C[(i*56)+41], c30_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_5 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a30_5 = _mm_loadu_pd(&values[663]);
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, b30));
_mm_storeu_pd(&C[(i*56)+41], c30_5);
__m128d c30_7 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a30_7 = _mm_loadu_pd(&values[665]);
c30_7 = _mm_add_pd(c30_7, _mm_mul_pd(a30_7, b30));
_mm_storeu_pd(&C[(i*56)+43], c30_7);
#endif
__m128d c30_9 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a30_9 = _mm_loadu_pd(&values[667]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, b30));
#endif
_mm_storeu_pd(&C[(i*56)+46], c30_9);
__m128d c30_11 = _mm_load_sd(&C[(i*56)+48]);
__m128d a30_11 = _mm_load_sd(&values[669]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, b30));
#endif
_mm_store_sd(&C[(i*56)+48], c30_11);
__m128d c30_12 = _mm_load_sd(&C[(i*56)+51]);
__m128d a30_12 = _mm_load_sd(&values[670]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_12 = _mm_add_sd(c30_12, _mm_mul_sd(a30_12, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_12 = _mm_add_sd(c30_12, _mm_mul_sd(a30_12, b30));
#endif
_mm_store_sd(&C[(i*56)+51], c30_12);
#else
C[(i*56)+35] += values[658] * B[(i*56)+30];
C[(i*56)+36] += values[659] * B[(i*56)+30];
C[(i*56)+37] += values[660] * B[(i*56)+30];
C[(i*56)+38] += values[661] * B[(i*56)+30];
C[(i*56)+39] += values[662] * B[(i*56)+30];
C[(i*56)+41] += values[663] * B[(i*56)+30];
C[(i*56)+42] += values[664] * B[(i*56)+30];
C[(i*56)+43] += values[665] * B[(i*56)+30];
C[(i*56)+44] += values[666] * B[(i*56)+30];
C[(i*56)+46] += values[667] * B[(i*56)+30];
C[(i*56)+47] += values[668] * B[(i*56)+30];
C[(i*56)+48] += values[669] * B[(i*56)+30];
C[(i*56)+51] += values[670] * B[(i*56)+30];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b31 = _mm256_broadcast_sd(&B[(i*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b31 = _mm_loaddup_pd(&B[(i*56)+31]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c31_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a31_0 = _mm256_loadu_pd(&values[671]);
c31_0 = _mm256_add_pd(c31_0, _mm256_mul_pd(a31_0, b31));
_mm256_storeu_pd(&C[(i*56)+35], c31_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c31_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a31_0 = _mm_loadu_pd(&values[671]);
c31_0 = _mm_add_pd(c31_0, _mm_mul_pd(a31_0, b31));
_mm_storeu_pd(&C[(i*56)+35], c31_0);
__m128d c31_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a31_2 = _mm_loadu_pd(&values[673]);
c31_2 = _mm_add_pd(c31_2, _mm_mul_pd(a31_2, b31));
_mm_storeu_pd(&C[(i*56)+37], c31_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c31_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a31_4 = _mm256_loadu_pd(&values[675]);
c31_4 = _mm256_add_pd(c31_4, _mm256_mul_pd(a31_4, b31));
_mm256_storeu_pd(&C[(i*56)+39], c31_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c31_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a31_4 = _mm_loadu_pd(&values[675]);
c31_4 = _mm_add_pd(c31_4, _mm_mul_pd(a31_4, b31));
_mm_storeu_pd(&C[(i*56)+39], c31_4);
__m128d c31_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a31_6 = _mm_loadu_pd(&values[677]);
c31_6 = _mm_add_pd(c31_6, _mm_mul_pd(a31_6, b31));
_mm_storeu_pd(&C[(i*56)+41], c31_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c31_8 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a31_8 = _mm256_loadu_pd(&values[679]);
c31_8 = _mm256_add_pd(c31_8, _mm256_mul_pd(a31_8, b31));
_mm256_storeu_pd(&C[(i*56)+43], c31_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c31_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a31_8 = _mm_loadu_pd(&values[679]);
c31_8 = _mm_add_pd(c31_8, _mm_mul_pd(a31_8, b31));
_mm_storeu_pd(&C[(i*56)+43], c31_8);
__m128d c31_10 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a31_10 = _mm_loadu_pd(&values[681]);
c31_10 = _mm_add_pd(c31_10, _mm_mul_pd(a31_10, b31));
_mm_storeu_pd(&C[(i*56)+45], c31_10);
#endif
__m128d c31_12 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a31_12 = _mm_loadu_pd(&values[683]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_12 = _mm_add_pd(c31_12, _mm_mul_pd(a31_12, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_12 = _mm_add_pd(c31_12, _mm_mul_pd(a31_12, b31));
#endif
_mm_storeu_pd(&C[(i*56)+47], c31_12);
__m128d c31_14 = _mm_load_sd(&C[(i*56)+49]);
__m128d a31_14 = _mm_load_sd(&values[685]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_14 = _mm_add_sd(c31_14, _mm_mul_sd(a31_14, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_14 = _mm_add_sd(c31_14, _mm_mul_sd(a31_14, b31));
#endif
_mm_store_sd(&C[(i*56)+49], c31_14);
__m128d c31_15 = _mm_load_sd(&C[(i*56)+52]);
__m128d a31_15 = _mm_load_sd(&values[686]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_15 = _mm_add_sd(c31_15, _mm_mul_sd(a31_15, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_15 = _mm_add_sd(c31_15, _mm_mul_sd(a31_15, b31));
#endif
_mm_store_sd(&C[(i*56)+52], c31_15);
#else
C[(i*56)+35] += values[671] * B[(i*56)+31];
C[(i*56)+36] += values[672] * B[(i*56)+31];
C[(i*56)+37] += values[673] * B[(i*56)+31];
C[(i*56)+38] += values[674] * B[(i*56)+31];
C[(i*56)+39] += values[675] * B[(i*56)+31];
C[(i*56)+40] += values[676] * B[(i*56)+31];
C[(i*56)+41] += values[677] * B[(i*56)+31];
C[(i*56)+42] += values[678] * B[(i*56)+31];
C[(i*56)+43] += values[679] * B[(i*56)+31];
C[(i*56)+44] += values[680] * B[(i*56)+31];
C[(i*56)+45] += values[681] * B[(i*56)+31];
C[(i*56)+46] += values[682] * B[(i*56)+31];
C[(i*56)+47] += values[683] * B[(i*56)+31];
C[(i*56)+48] += values[684] * B[(i*56)+31];
C[(i*56)+49] += values[685] * B[(i*56)+31];
C[(i*56)+52] += values[686] * B[(i*56)+31];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c32_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a32_0 = _mm256_loadu_pd(&values[687]);
c32_0 = _mm256_add_pd(c32_0, _mm256_mul_pd(a32_0, b32));
_mm256_storeu_pd(&C[(i*56)+35], c32_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c32_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a32_0 = _mm_loadu_pd(&values[687]);
c32_0 = _mm_add_pd(c32_0, _mm_mul_pd(a32_0, b32));
_mm_storeu_pd(&C[(i*56)+35], c32_0);
__m128d c32_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a32_2 = _mm_loadu_pd(&values[689]);
c32_2 = _mm_add_pd(c32_2, _mm_mul_pd(a32_2, b32));
_mm_storeu_pd(&C[(i*56)+37], c32_2);
#endif
__m128d c32_4 = _mm_load_sd(&C[(i*56)+39]);
__m128d a32_4 = _mm_load_sd(&values[691]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, b32));
#endif
_mm_store_sd(&C[(i*56)+39], c32_4);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c32_5 = _mm256_loadu_pd(&C[(i*56)+41]);
__m256d a32_5 = _mm256_loadu_pd(&values[692]);
c32_5 = _mm256_add_pd(c32_5, _mm256_mul_pd(a32_5, b32));
_mm256_storeu_pd(&C[(i*56)+41], c32_5);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c32_5 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a32_5 = _mm_loadu_pd(&values[692]);
c32_5 = _mm_add_pd(c32_5, _mm_mul_pd(a32_5, b32));
_mm_storeu_pd(&C[(i*56)+41], c32_5);
__m128d c32_7 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a32_7 = _mm_loadu_pd(&values[694]);
c32_7 = _mm_add_pd(c32_7, _mm_mul_pd(a32_7, b32));
_mm_storeu_pd(&C[(i*56)+43], c32_7);
#endif
__m128d c32_9 = _mm_loadu_pd(&C[(i*56)+46]);
__m128d a32_9 = _mm_loadu_pd(&values[696]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_9 = _mm_add_pd(c32_9, _mm_mul_pd(a32_9, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_9 = _mm_add_pd(c32_9, _mm_mul_pd(a32_9, b32));
#endif
_mm_storeu_pd(&C[(i*56)+46], c32_9);
__m128d c32_11 = _mm_load_sd(&C[(i*56)+48]);
__m128d a32_11 = _mm_load_sd(&values[698]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_11 = _mm_add_sd(c32_11, _mm_mul_sd(a32_11, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_11 = _mm_add_sd(c32_11, _mm_mul_sd(a32_11, b32));
#endif
_mm_store_sd(&C[(i*56)+48], c32_11);
__m128d c32_12 = _mm_loadu_pd(&C[(i*56)+50]);
__m128d a32_12 = _mm_loadu_pd(&values[699]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_12 = _mm_add_pd(c32_12, _mm_mul_pd(a32_12, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_12 = _mm_add_pd(c32_12, _mm_mul_pd(a32_12, b32));
#endif
_mm_storeu_pd(&C[(i*56)+50], c32_12);
__m128d c32_14 = _mm_load_sd(&C[(i*56)+53]);
__m128d a32_14 = _mm_load_sd(&values[701]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_14 = _mm_add_sd(c32_14, _mm_mul_sd(a32_14, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_14 = _mm_add_sd(c32_14, _mm_mul_sd(a32_14, b32));
#endif
_mm_store_sd(&C[(i*56)+53], c32_14);
#else
C[(i*56)+35] += values[687] * B[(i*56)+32];
C[(i*56)+36] += values[688] * B[(i*56)+32];
C[(i*56)+37] += values[689] * B[(i*56)+32];
C[(i*56)+38] += values[690] * B[(i*56)+32];
C[(i*56)+39] += values[691] * B[(i*56)+32];
C[(i*56)+41] += values[692] * B[(i*56)+32];
C[(i*56)+42] += values[693] * B[(i*56)+32];
C[(i*56)+43] += values[694] * B[(i*56)+32];
C[(i*56)+44] += values[695] * B[(i*56)+32];
C[(i*56)+46] += values[696] * B[(i*56)+32];
C[(i*56)+47] += values[697] * B[(i*56)+32];
C[(i*56)+48] += values[698] * B[(i*56)+32];
C[(i*56)+50] += values[699] * B[(i*56)+32];
C[(i*56)+51] += values[700] * B[(i*56)+32];
C[(i*56)+53] += values[701] * B[(i*56)+32];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b33 = _mm256_broadcast_sd(&B[(i*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b33 = _mm_loaddup_pd(&B[(i*56)+33]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a33_0 = _mm256_loadu_pd(&values[702]);
c33_0 = _mm256_add_pd(c33_0, _mm256_mul_pd(a33_0, b33));
_mm256_storeu_pd(&C[(i*56)+35], c33_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a33_0 = _mm_loadu_pd(&values[702]);
c33_0 = _mm_add_pd(c33_0, _mm_mul_pd(a33_0, b33));
_mm_storeu_pd(&C[(i*56)+35], c33_0);
__m128d c33_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a33_2 = _mm_loadu_pd(&values[704]);
c33_2 = _mm_add_pd(c33_2, _mm_mul_pd(a33_2, b33));
_mm_storeu_pd(&C[(i*56)+37], c33_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a33_4 = _mm256_loadu_pd(&values[706]);
c33_4 = _mm256_add_pd(c33_4, _mm256_mul_pd(a33_4, b33));
_mm256_storeu_pd(&C[(i*56)+39], c33_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a33_4 = _mm_loadu_pd(&values[706]);
c33_4 = _mm_add_pd(c33_4, _mm_mul_pd(a33_4, b33));
_mm_storeu_pd(&C[(i*56)+39], c33_4);
__m128d c33_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a33_6 = _mm_loadu_pd(&values[708]);
c33_6 = _mm_add_pd(c33_6, _mm_mul_pd(a33_6, b33));
_mm_storeu_pd(&C[(i*56)+41], c33_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_8 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a33_8 = _mm256_loadu_pd(&values[710]);
c33_8 = _mm256_add_pd(c33_8, _mm256_mul_pd(a33_8, b33));
_mm256_storeu_pd(&C[(i*56)+43], c33_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a33_8 = _mm_loadu_pd(&values[710]);
c33_8 = _mm_add_pd(c33_8, _mm_mul_pd(a33_8, b33));
_mm_storeu_pd(&C[(i*56)+43], c33_8);
__m128d c33_10 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a33_10 = _mm_loadu_pd(&values[712]);
c33_10 = _mm_add_pd(c33_10, _mm_mul_pd(a33_10, b33));
_mm_storeu_pd(&C[(i*56)+45], c33_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c33_12 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a33_12 = _mm256_loadu_pd(&values[714]);
c33_12 = _mm256_add_pd(c33_12, _mm256_mul_pd(a33_12, b33));
_mm256_storeu_pd(&C[(i*56)+47], c33_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c33_12 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a33_12 = _mm_loadu_pd(&values[714]);
c33_12 = _mm_add_pd(c33_12, _mm_mul_pd(a33_12, b33));
_mm_storeu_pd(&C[(i*56)+47], c33_12);
__m128d c33_14 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a33_14 = _mm_loadu_pd(&values[716]);
c33_14 = _mm_add_pd(c33_14, _mm_mul_pd(a33_14, b33));
_mm_storeu_pd(&C[(i*56)+49], c33_14);
#endif
__m128d c33_16 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a33_16 = _mm_loadu_pd(&values[718]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_16 = _mm_add_pd(c33_16, _mm_mul_pd(a33_16, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_16 = _mm_add_pd(c33_16, _mm_mul_pd(a33_16, b33));
#endif
_mm_storeu_pd(&C[(i*56)+51], c33_16);
__m128d c33_18 = _mm_load_sd(&C[(i*56)+54]);
__m128d a33_18 = _mm_load_sd(&values[720]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_18 = _mm_add_sd(c33_18, _mm_mul_sd(a33_18, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_18 = _mm_add_sd(c33_18, _mm_mul_sd(a33_18, b33));
#endif
_mm_store_sd(&C[(i*56)+54], c33_18);
#else
C[(i*56)+35] += values[702] * B[(i*56)+33];
C[(i*56)+36] += values[703] * B[(i*56)+33];
C[(i*56)+37] += values[704] * B[(i*56)+33];
C[(i*56)+38] += values[705] * B[(i*56)+33];
C[(i*56)+39] += values[706] * B[(i*56)+33];
C[(i*56)+40] += values[707] * B[(i*56)+33];
C[(i*56)+41] += values[708] * B[(i*56)+33];
C[(i*56)+42] += values[709] * B[(i*56)+33];
C[(i*56)+43] += values[710] * B[(i*56)+33];
C[(i*56)+44] += values[711] * B[(i*56)+33];
C[(i*56)+45] += values[712] * B[(i*56)+33];
C[(i*56)+46] += values[713] * B[(i*56)+33];
C[(i*56)+47] += values[714] * B[(i*56)+33];
C[(i*56)+48] += values[715] * B[(i*56)+33];
C[(i*56)+49] += values[716] * B[(i*56)+33];
C[(i*56)+50] += values[717] * B[(i*56)+33];
C[(i*56)+51] += values[718] * B[(i*56)+33];
C[(i*56)+52] += values[719] * B[(i*56)+33];
C[(i*56)+54] += values[720] * B[(i*56)+33];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b34 = _mm256_broadcast_sd(&B[(i*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b34 = _mm_loaddup_pd(&B[(i*56)+34]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_0 = _mm256_loadu_pd(&C[(i*56)+35]);
__m256d a34_0 = _mm256_loadu_pd(&values[721]);
c34_0 = _mm256_add_pd(c34_0, _mm256_mul_pd(a34_0, b34));
_mm256_storeu_pd(&C[(i*56)+35], c34_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_0 = _mm_loadu_pd(&C[(i*56)+35]);
__m128d a34_0 = _mm_loadu_pd(&values[721]);
c34_0 = _mm_add_pd(c34_0, _mm_mul_pd(a34_0, b34));
_mm_storeu_pd(&C[(i*56)+35], c34_0);
__m128d c34_2 = _mm_loadu_pd(&C[(i*56)+37]);
__m128d a34_2 = _mm_loadu_pd(&values[723]);
c34_2 = _mm_add_pd(c34_2, _mm_mul_pd(a34_2, b34));
_mm_storeu_pd(&C[(i*56)+37], c34_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_4 = _mm256_loadu_pd(&C[(i*56)+39]);
__m256d a34_4 = _mm256_loadu_pd(&values[725]);
c34_4 = _mm256_add_pd(c34_4, _mm256_mul_pd(a34_4, b34));
_mm256_storeu_pd(&C[(i*56)+39], c34_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_4 = _mm_loadu_pd(&C[(i*56)+39]);
__m128d a34_4 = _mm_loadu_pd(&values[725]);
c34_4 = _mm_add_pd(c34_4, _mm_mul_pd(a34_4, b34));
_mm_storeu_pd(&C[(i*56)+39], c34_4);
__m128d c34_6 = _mm_loadu_pd(&C[(i*56)+41]);
__m128d a34_6 = _mm_loadu_pd(&values[727]);
c34_6 = _mm_add_pd(c34_6, _mm_mul_pd(a34_6, b34));
_mm_storeu_pd(&C[(i*56)+41], c34_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_8 = _mm256_loadu_pd(&C[(i*56)+43]);
__m256d a34_8 = _mm256_loadu_pd(&values[729]);
c34_8 = _mm256_add_pd(c34_8, _mm256_mul_pd(a34_8, b34));
_mm256_storeu_pd(&C[(i*56)+43], c34_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_8 = _mm_loadu_pd(&C[(i*56)+43]);
__m128d a34_8 = _mm_loadu_pd(&values[729]);
c34_8 = _mm_add_pd(c34_8, _mm_mul_pd(a34_8, b34));
_mm_storeu_pd(&C[(i*56)+43], c34_8);
__m128d c34_10 = _mm_loadu_pd(&C[(i*56)+45]);
__m128d a34_10 = _mm_loadu_pd(&values[731]);
c34_10 = _mm_add_pd(c34_10, _mm_mul_pd(a34_10, b34));
_mm_storeu_pd(&C[(i*56)+45], c34_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_12 = _mm256_loadu_pd(&C[(i*56)+47]);
__m256d a34_12 = _mm256_loadu_pd(&values[733]);
c34_12 = _mm256_add_pd(c34_12, _mm256_mul_pd(a34_12, b34));
_mm256_storeu_pd(&C[(i*56)+47], c34_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_12 = _mm_loadu_pd(&C[(i*56)+47]);
__m128d a34_12 = _mm_loadu_pd(&values[733]);
c34_12 = _mm_add_pd(c34_12, _mm_mul_pd(a34_12, b34));
_mm_storeu_pd(&C[(i*56)+47], c34_12);
__m128d c34_14 = _mm_loadu_pd(&C[(i*56)+49]);
__m128d a34_14 = _mm_loadu_pd(&values[735]);
c34_14 = _mm_add_pd(c34_14, _mm_mul_pd(a34_14, b34));
_mm_storeu_pd(&C[(i*56)+49], c34_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c34_16 = _mm256_loadu_pd(&C[(i*56)+51]);
__m256d a34_16 = _mm256_loadu_pd(&values[737]);
c34_16 = _mm256_add_pd(c34_16, _mm256_mul_pd(a34_16, b34));
_mm256_storeu_pd(&C[(i*56)+51], c34_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c34_16 = _mm_loadu_pd(&C[(i*56)+51]);
__m128d a34_16 = _mm_loadu_pd(&values[737]);
c34_16 = _mm_add_pd(c34_16, _mm_mul_pd(a34_16, b34));
_mm_storeu_pd(&C[(i*56)+51], c34_16);
__m128d c34_18 = _mm_loadu_pd(&C[(i*56)+53]);
__m128d a34_18 = _mm_loadu_pd(&values[739]);
c34_18 = _mm_add_pd(c34_18, _mm_mul_pd(a34_18, b34));
_mm_storeu_pd(&C[(i*56)+53], c34_18);
#endif
__m128d c34_20 = _mm_load_sd(&C[(i*56)+55]);
__m128d a34_20 = _mm_load_sd(&values[741]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_20 = _mm_add_sd(c34_20, _mm_mul_sd(a34_20, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_20 = _mm_add_sd(c34_20, _mm_mul_sd(a34_20, b34));
#endif
_mm_store_sd(&C[(i*56)+55], c34_20);
#else
C[(i*56)+35] += values[721] * B[(i*56)+34];
C[(i*56)+36] += values[722] * B[(i*56)+34];
C[(i*56)+37] += values[723] * B[(i*56)+34];
C[(i*56)+38] += values[724] * B[(i*56)+34];
C[(i*56)+39] += values[725] * B[(i*56)+34];
C[(i*56)+40] += values[726] * B[(i*56)+34];
C[(i*56)+41] += values[727] * B[(i*56)+34];
C[(i*56)+42] += values[728] * B[(i*56)+34];
C[(i*56)+43] += values[729] * B[(i*56)+34];
C[(i*56)+44] += values[730] * B[(i*56)+34];
C[(i*56)+45] += values[731] * B[(i*56)+34];
C[(i*56)+46] += values[732] * B[(i*56)+34];
C[(i*56)+47] += values[733] * B[(i*56)+34];
C[(i*56)+48] += values[734] * B[(i*56)+34];
C[(i*56)+49] += values[735] * B[(i*56)+34];
C[(i*56)+50] += values[736] * B[(i*56)+34];
C[(i*56)+51] += values[737] * B[(i*56)+34];
C[(i*56)+52] += values[738] * B[(i*56)+34];
C[(i*56)+53] += values[739] * B[(i*56)+34];
C[(i*56)+54] += values[740] * B[(i*56)+34];
C[(i*56)+55] += values[741] * B[(i*56)+34];
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif

}

#ifndef NDEBUG
num_flops += 13356;
#endif

}

inline void generatedMatrixMultiplication_kXiDivMT_9_56(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*56)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*56)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*56)+0], c1_0);
#else
C[(i*56)+0] += values[0] * B[(i*56)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*56)+4]);
#endif
__m128d c4_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a4_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, _mm256_castpd256_pd128(b4)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c4_0 = _mm_add_sd(c4_0, _mm_mul_sd(a4_0, b4));
#endif
_mm_store_sd(&C[(i*56)+1], c4_0);
#else
C[(i*56)+1] += values[1] * B[(i*56)+4];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*56)+5]);
#endif
__m128d c5_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a5_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_0 = _mm_add_sd(c5_0, _mm_mul_sd(a5_0, b5));
#endif
_mm_store_sd(&C[(i*56)+0], c5_0);
__m128d c5_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a5_1 = _mm_loadu_pd(&values[3]);
#if defined(__SSE3__) && defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, _mm256_castpd256_pd128(b5)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c5_1 = _mm_add_pd(c5_1, _mm_mul_pd(a5_1, b5));
#endif
_mm_storeu_pd(&C[(i*56)+2], c5_1);
#else
C[(i*56)+0] += values[2] * B[(i*56)+5];
C[(i*56)+2] += values[3] * B[(i*56)+5];
C[(i*56)+3] += values[4] * B[(i*56)+5];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*56)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a7_0 = _mm_load_sd(&values[5]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*56)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a7_1 = _mm_load_sd(&values[6]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*56)+3], c7_1);
#else
C[(i*56)+0] += values[5] * B[(i*56)+7];
C[(i*56)+3] += values[6] * B[(i*56)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*56)+10]);
#endif
__m128d c10_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a10_0 = _mm_load_sd(&values[7]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_0 = _mm_add_sd(c10_0, _mm_mul_sd(a10_0, b10));
#endif
_mm_store_sd(&C[(i*56)+0], c10_0);
__m128d c10_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a10_1 = _mm_loadu_pd(&values[8]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_1 = _mm_add_pd(c10_1, _mm_mul_pd(a10_1, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_1 = _mm_add_pd(c10_1, _mm_mul_pd(a10_1, b10));
#endif
_mm_storeu_pd(&C[(i*56)+2], c10_1);
__m128d c10_3 = _mm_load_sd(&C[(i*56)+4]);
__m128d a10_3 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_3 = _mm_add_sd(c10_3, _mm_mul_sd(a10_3, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_3 = _mm_add_sd(c10_3, _mm_mul_sd(a10_3, b10));
#endif
_mm_store_sd(&C[(i*56)+4], c10_3);
__m128d c10_4 = _mm_load_sd(&C[(i*56)+6]);
__m128d a10_4 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_4 = _mm_add_sd(c10_4, _mm_mul_sd(a10_4, b10));
#endif
_mm_store_sd(&C[(i*56)+6], c10_4);
__m128d c10_5 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a10_5 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_5 = _mm_add_pd(c10_5, _mm_mul_pd(a10_5, b10));
#endif
_mm_storeu_pd(&C[(i*56)+8], c10_5);
#else
C[(i*56)+0] += values[7] * B[(i*56)+10];
C[(i*56)+2] += values[8] * B[(i*56)+10];
C[(i*56)+3] += values[9] * B[(i*56)+10];
C[(i*56)+4] += values[10] * B[(i*56)+10];
C[(i*56)+6] += values[11] * B[(i*56)+10];
C[(i*56)+8] += values[12] * B[(i*56)+10];
C[(i*56)+9] += values[13] * B[(i*56)+10];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*56)+11]);
#endif
__m128d c11_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a11_0 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_0 = _mm_add_sd(c11_0, _mm_mul_sd(a11_0, b11));
#endif
_mm_store_sd(&C[(i*56)+1], c11_0);
__m128d c11_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a11_1 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_1 = _mm_add_sd(c11_1, _mm_mul_sd(a11_1, b11));
#endif
_mm_store_sd(&C[(i*56)+5], c11_1);
__m128d c11_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a11_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_2 = _mm_add_sd(c11_2, _mm_mul_sd(a11_2, b11));
#endif
_mm_store_sd(&C[(i*56)+7], c11_2);
#else
C[(i*56)+1] += values[14] * B[(i*56)+11];
C[(i*56)+5] += values[15] * B[(i*56)+11];
C[(i*56)+7] += values[16] * B[(i*56)+11];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*56)+12]);
#endif
__m128d c12_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a12_0 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_0 = _mm_add_sd(c12_0, _mm_mul_sd(a12_0, b12));
#endif
_mm_store_sd(&C[(i*56)+0], c12_0);
__m128d c12_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a12_1 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_1 = _mm_add_pd(c12_1, _mm_mul_pd(a12_1, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_1 = _mm_add_pd(c12_1, _mm_mul_pd(a12_1, b12));
#endif
_mm_storeu_pd(&C[(i*56)+2], c12_1);
__m128d c12_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a12_3 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_3 = _mm_add_sd(c12_3, _mm_mul_sd(a12_3, b12));
#endif
_mm_store_sd(&C[(i*56)+6], c12_3);
__m128d c12_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a12_4 = _mm_loadu_pd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
#endif
_mm_storeu_pd(&C[(i*56)+8], c12_4);
#else
C[(i*56)+0] += values[17] * B[(i*56)+12];
C[(i*56)+2] += values[18] * B[(i*56)+12];
C[(i*56)+3] += values[19] * B[(i*56)+12];
C[(i*56)+6] += values[20] * B[(i*56)+12];
C[(i*56)+8] += values[21] * B[(i*56)+12];
C[(i*56)+9] += values[22] * B[(i*56)+12];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*56)+14]);
#endif
__m128d c14_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a14_0 = _mm_load_sd(&values[23]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_0 = _mm_add_sd(c14_0, _mm_mul_sd(a14_0, b14));
#endif
_mm_store_sd(&C[(i*56)+1], c14_0);
__m128d c14_1 = _mm_load_sd(&C[(i*56)+7]);
__m128d a14_1 = _mm_load_sd(&values[24]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_1 = _mm_add_sd(c14_1, _mm_mul_sd(a14_1, b14));
#endif
_mm_store_sd(&C[(i*56)+7], c14_1);
#else
C[(i*56)+1] += values[23] * B[(i*56)+14];
C[(i*56)+7] += values[24] * B[(i*56)+14];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*56)+15]);
#endif
__m128d c15_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a15_0 = _mm_load_sd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_0 = _mm_add_sd(c15_0, _mm_mul_sd(a15_0, b15));
#endif
_mm_store_sd(&C[(i*56)+0], c15_0);
__m128d c15_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a15_1 = _mm_loadu_pd(&values[26]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_1 = _mm_add_pd(c15_1, _mm_mul_pd(a15_1, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_1 = _mm_add_pd(c15_1, _mm_mul_pd(a15_1, b15));
#endif
_mm_storeu_pd(&C[(i*56)+2], c15_1);
__m128d c15_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a15_3 = _mm_loadu_pd(&values[28]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_3 = _mm_add_pd(c15_3, _mm_mul_pd(a15_3, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_3 = _mm_add_pd(c15_3, _mm_mul_pd(a15_3, b15));
#endif
_mm_storeu_pd(&C[(i*56)+8], c15_3);
#else
C[(i*56)+0] += values[25] * B[(i*56)+15];
C[(i*56)+2] += values[26] * B[(i*56)+15];
C[(i*56)+3] += values[27] * B[(i*56)+15];
C[(i*56)+8] += values[28] * B[(i*56)+15];
C[(i*56)+9] += values[29] * B[(i*56)+15];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*56)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a17_0 = _mm_load_sd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*56)+0], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a17_1 = _mm_load_sd(&values[31]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*56)+3], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a17_2 = _mm_load_sd(&values[32]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*56)+9], c17_2);
#else
C[(i*56)+0] += values[30] * B[(i*56)+17];
C[(i*56)+3] += values[31] * B[(i*56)+17];
C[(i*56)+9] += values[32] * B[(i*56)+17];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 20, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*56)+20]);
#endif
__m128d c20_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a20_0 = _mm_load_sd(&values[33]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_0 = _mm_add_sd(c20_0, _mm_mul_sd(a20_0, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_0 = _mm_add_sd(c20_0, _mm_mul_sd(a20_0, b20));
#endif
_mm_store_sd(&C[(i*56)+1], c20_0);
__m128d c20_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a20_1 = _mm_load_sd(&values[34]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_1 = _mm_add_sd(c20_1, _mm_mul_sd(a20_1, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_1 = _mm_add_sd(c20_1, _mm_mul_sd(a20_1, b20));
#endif
_mm_store_sd(&C[(i*56)+5], c20_1);
__m128d c20_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a20_2 = _mm_load_sd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_2 = _mm_add_sd(c20_2, _mm_mul_sd(a20_2, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_2 = _mm_add_sd(c20_2, _mm_mul_sd(a20_2, b20));
#endif
_mm_store_sd(&C[(i*56)+7], c20_2);
__m128d c20_3 = _mm_load_sd(&C[(i*56)+10]);
__m128d a20_3 = _mm_load_sd(&values[36]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_3 = _mm_add_sd(c20_3, _mm_mul_sd(a20_3, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_3 = _mm_add_sd(c20_3, _mm_mul_sd(a20_3, b20));
#endif
_mm_store_sd(&C[(i*56)+10], c20_3);
__m128d c20_4 = _mm_load_sd(&C[(i*56)+12]);
__m128d a20_4 = _mm_load_sd(&values[37]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_4 = _mm_add_sd(c20_4, _mm_mul_sd(a20_4, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_4 = _mm_add_sd(c20_4, _mm_mul_sd(a20_4, b20));
#endif
_mm_store_sd(&C[(i*56)+12], c20_4);
__m128d c20_5 = _mm_load_sd(&C[(i*56)+15]);
__m128d a20_5 = _mm_load_sd(&values[38]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_5 = _mm_add_sd(c20_5, _mm_mul_sd(a20_5, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_5 = _mm_add_sd(c20_5, _mm_mul_sd(a20_5, b20));
#endif
_mm_store_sd(&C[(i*56)+15], c20_5);
__m128d c20_6 = _mm_load_sd(&C[(i*56)+17]);
__m128d a20_6 = _mm_load_sd(&values[39]);
#if defined(__SSE3__) && defined(__AVX256__)
c20_6 = _mm_add_sd(c20_6, _mm_mul_sd(a20_6, _mm256_castpd256_pd128(b20)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c20_6 = _mm_add_sd(c20_6, _mm_mul_sd(a20_6, b20));
#endif
_mm_store_sd(&C[(i*56)+17], c20_6);
#else
C[(i*56)+1] += values[33] * B[(i*56)+20];
C[(i*56)+5] += values[34] * B[(i*56)+20];
C[(i*56)+7] += values[35] * B[(i*56)+20];
C[(i*56)+10] += values[36] * B[(i*56)+20];
C[(i*56)+12] += values[37] * B[(i*56)+20];
C[(i*56)+15] += values[38] * B[(i*56)+20];
C[(i*56)+17] += values[39] * B[(i*56)+20];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*56)+21]);
#endif
__m128d c21_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a21_0 = _mm_load_sd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_0 = _mm_add_sd(c21_0, _mm_mul_sd(a21_0, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_0 = _mm_add_sd(c21_0, _mm_mul_sd(a21_0, b21));
#endif
_mm_store_sd(&C[(i*56)+0], c21_0);
__m128d c21_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a21_1 = _mm_loadu_pd(&values[41]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_1 = _mm_add_pd(c21_1, _mm_mul_pd(a21_1, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_1 = _mm_add_pd(c21_1, _mm_mul_pd(a21_1, b21));
#endif
_mm_storeu_pd(&C[(i*56)+2], c21_1);
__m128d c21_3 = _mm_load_sd(&C[(i*56)+4]);
__m128d a21_3 = _mm_load_sd(&values[43]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_3 = _mm_add_sd(c21_3, _mm_mul_sd(a21_3, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_3 = _mm_add_sd(c21_3, _mm_mul_sd(a21_3, b21));
#endif
_mm_store_sd(&C[(i*56)+4], c21_3);
__m128d c21_4 = _mm_load_sd(&C[(i*56)+6]);
__m128d a21_4 = _mm_load_sd(&values[44]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_4 = _mm_add_sd(c21_4, _mm_mul_sd(a21_4, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_4 = _mm_add_sd(c21_4, _mm_mul_sd(a21_4, b21));
#endif
_mm_store_sd(&C[(i*56)+6], c21_4);
__m128d c21_5 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a21_5 = _mm_loadu_pd(&values[45]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_5 = _mm_add_pd(c21_5, _mm_mul_pd(a21_5, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_5 = _mm_add_pd(c21_5, _mm_mul_pd(a21_5, b21));
#endif
_mm_storeu_pd(&C[(i*56)+8], c21_5);
__m128d c21_7 = _mm_load_sd(&C[(i*56)+11]);
__m128d a21_7 = _mm_load_sd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_7 = _mm_add_sd(c21_7, _mm_mul_sd(a21_7, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_7 = _mm_add_sd(c21_7, _mm_mul_sd(a21_7, b21));
#endif
_mm_store_sd(&C[(i*56)+11], c21_7);
__m128d c21_8 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a21_8 = _mm_loadu_pd(&values[48]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, b21));
#endif
_mm_storeu_pd(&C[(i*56)+13], c21_8);
__m128d c21_10 = _mm_load_sd(&C[(i*56)+16]);
__m128d a21_10 = _mm_load_sd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_10 = _mm_add_sd(c21_10, _mm_mul_sd(a21_10, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_10 = _mm_add_sd(c21_10, _mm_mul_sd(a21_10, b21));
#endif
_mm_store_sd(&C[(i*56)+16], c21_10);
__m128d c21_11 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a21_11 = _mm_loadu_pd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c21_11 = _mm_add_pd(c21_11, _mm_mul_pd(a21_11, _mm256_castpd256_pd128(b21)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c21_11 = _mm_add_pd(c21_11, _mm_mul_pd(a21_11, b21));
#endif
_mm_storeu_pd(&C[(i*56)+18], c21_11);
#else
C[(i*56)+0] += values[40] * B[(i*56)+21];
C[(i*56)+2] += values[41] * B[(i*56)+21];
C[(i*56)+3] += values[42] * B[(i*56)+21];
C[(i*56)+4] += values[43] * B[(i*56)+21];
C[(i*56)+6] += values[44] * B[(i*56)+21];
C[(i*56)+8] += values[45] * B[(i*56)+21];
C[(i*56)+9] += values[46] * B[(i*56)+21];
C[(i*56)+11] += values[47] * B[(i*56)+21];
C[(i*56)+13] += values[48] * B[(i*56)+21];
C[(i*56)+14] += values[49] * B[(i*56)+21];
C[(i*56)+16] += values[50] * B[(i*56)+21];
C[(i*56)+18] += values[51] * B[(i*56)+21];
C[(i*56)+19] += values[52] * B[(i*56)+21];
#endif
#ifndef NDEBUG
num_flops += 26;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*56)+22]);
#endif
__m128d c22_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a22_0 = _mm_load_sd(&values[53]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_0 = _mm_add_sd(c22_0, _mm_mul_sd(a22_0, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_0 = _mm_add_sd(c22_0, _mm_mul_sd(a22_0, b22));
#endif
_mm_store_sd(&C[(i*56)+1], c22_0);
__m128d c22_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a22_1 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_1 = _mm_add_sd(c22_1, _mm_mul_sd(a22_1, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_1 = _mm_add_sd(c22_1, _mm_mul_sd(a22_1, b22));
#endif
_mm_store_sd(&C[(i*56)+5], c22_1);
__m128d c22_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a22_2 = _mm_load_sd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_2 = _mm_add_sd(c22_2, _mm_mul_sd(a22_2, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_2 = _mm_add_sd(c22_2, _mm_mul_sd(a22_2, b22));
#endif
_mm_store_sd(&C[(i*56)+7], c22_2);
__m128d c22_3 = _mm_load_sd(&C[(i*56)+12]);
__m128d a22_3 = _mm_load_sd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_3 = _mm_add_sd(c22_3, _mm_mul_sd(a22_3, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_3 = _mm_add_sd(c22_3, _mm_mul_sd(a22_3, b22));
#endif
_mm_store_sd(&C[(i*56)+12], c22_3);
__m128d c22_4 = _mm_load_sd(&C[(i*56)+15]);
__m128d a22_4 = _mm_load_sd(&values[57]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_4 = _mm_add_sd(c22_4, _mm_mul_sd(a22_4, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_4 = _mm_add_sd(c22_4, _mm_mul_sd(a22_4, b22));
#endif
_mm_store_sd(&C[(i*56)+15], c22_4);
__m128d c22_5 = _mm_load_sd(&C[(i*56)+17]);
__m128d a22_5 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_5 = _mm_add_sd(c22_5, _mm_mul_sd(a22_5, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_5 = _mm_add_sd(c22_5, _mm_mul_sd(a22_5, b22));
#endif
_mm_store_sd(&C[(i*56)+17], c22_5);
#else
C[(i*56)+1] += values[53] * B[(i*56)+22];
C[(i*56)+5] += values[54] * B[(i*56)+22];
C[(i*56)+7] += values[55] * B[(i*56)+22];
C[(i*56)+12] += values[56] * B[(i*56)+22];
C[(i*56)+15] += values[57] * B[(i*56)+22];
C[(i*56)+17] += values[58] * B[(i*56)+22];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*56)+23]);
#endif
__m128d c23_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a23_0 = _mm_load_sd(&values[59]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_0 = _mm_add_sd(c23_0, _mm_mul_sd(a23_0, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_0 = _mm_add_sd(c23_0, _mm_mul_sd(a23_0, b23));
#endif
_mm_store_sd(&C[(i*56)+0], c23_0);
__m128d c23_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a23_1 = _mm_loadu_pd(&values[60]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_1 = _mm_add_pd(c23_1, _mm_mul_pd(a23_1, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_1 = _mm_add_pd(c23_1, _mm_mul_pd(a23_1, b23));
#endif
_mm_storeu_pd(&C[(i*56)+2], c23_1);
__m128d c23_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a23_3 = _mm_load_sd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_3 = _mm_add_sd(c23_3, _mm_mul_sd(a23_3, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_3 = _mm_add_sd(c23_3, _mm_mul_sd(a23_3, b23));
#endif
_mm_store_sd(&C[(i*56)+6], c23_3);
__m128d c23_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a23_4 = _mm_loadu_pd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, b23));
#endif
_mm_storeu_pd(&C[(i*56)+8], c23_4);
__m128d c23_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a23_6 = _mm_load_sd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_6 = _mm_add_sd(c23_6, _mm_mul_sd(a23_6, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_6 = _mm_add_sd(c23_6, _mm_mul_sd(a23_6, b23));
#endif
_mm_store_sd(&C[(i*56)+13], c23_6);
__m128d c23_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a23_7 = _mm_load_sd(&values[66]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_7 = _mm_add_sd(c23_7, _mm_mul_sd(a23_7, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_7 = _mm_add_sd(c23_7, _mm_mul_sd(a23_7, b23));
#endif
_mm_store_sd(&C[(i*56)+16], c23_7);
__m128d c23_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a23_8 = _mm_loadu_pd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_8 = _mm_add_pd(c23_8, _mm_mul_pd(a23_8, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_8 = _mm_add_pd(c23_8, _mm_mul_pd(a23_8, b23));
#endif
_mm_storeu_pd(&C[(i*56)+18], c23_8);
#else
C[(i*56)+0] += values[59] * B[(i*56)+23];
C[(i*56)+2] += values[60] * B[(i*56)+23];
C[(i*56)+3] += values[61] * B[(i*56)+23];
C[(i*56)+6] += values[62] * B[(i*56)+23];
C[(i*56)+8] += values[63] * B[(i*56)+23];
C[(i*56)+9] += values[64] * B[(i*56)+23];
C[(i*56)+13] += values[65] * B[(i*56)+23];
C[(i*56)+16] += values[66] * B[(i*56)+23];
C[(i*56)+18] += values[67] * B[(i*56)+23];
C[(i*56)+19] += values[68] * B[(i*56)+23];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*56)+25]);
#endif
__m128d c25_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a25_0 = _mm_load_sd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_0 = _mm_add_sd(c25_0, _mm_mul_sd(a25_0, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_0 = _mm_add_sd(c25_0, _mm_mul_sd(a25_0, b25));
#endif
_mm_store_sd(&C[(i*56)+0], c25_0);
__m128d c25_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a25_1 = _mm_loadu_pd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_1 = _mm_add_pd(c25_1, _mm_mul_pd(a25_1, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_1 = _mm_add_pd(c25_1, _mm_mul_pd(a25_1, b25));
#endif
_mm_storeu_pd(&C[(i*56)+2], c25_1);
__m128d c25_3 = _mm_load_sd(&C[(i*56)+4]);
__m128d a25_3 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_3 = _mm_add_sd(c25_3, _mm_mul_sd(a25_3, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_3 = _mm_add_sd(c25_3, _mm_mul_sd(a25_3, b25));
#endif
_mm_store_sd(&C[(i*56)+4], c25_3);
__m128d c25_4 = _mm_load_sd(&C[(i*56)+6]);
__m128d a25_4 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_4 = _mm_add_sd(c25_4, _mm_mul_sd(a25_4, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_4 = _mm_add_sd(c25_4, _mm_mul_sd(a25_4, b25));
#endif
_mm_store_sd(&C[(i*56)+6], c25_4);
__m128d c25_5 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a25_5 = _mm_loadu_pd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_5 = _mm_add_pd(c25_5, _mm_mul_pd(a25_5, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_5 = _mm_add_pd(c25_5, _mm_mul_pd(a25_5, b25));
#endif
_mm_storeu_pd(&C[(i*56)+8], c25_5);
__m128d c25_7 = _mm_load_sd(&C[(i*56)+14]);
__m128d a25_7 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_7 = _mm_add_sd(c25_7, _mm_mul_sd(a25_7, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_7 = _mm_add_sd(c25_7, _mm_mul_sd(a25_7, b25));
#endif
_mm_store_sd(&C[(i*56)+14], c25_7);
__m128d c25_8 = _mm_load_sd(&C[(i*56)+16]);
__m128d a25_8 = _mm_load_sd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_8 = _mm_add_sd(c25_8, _mm_mul_sd(a25_8, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_8 = _mm_add_sd(c25_8, _mm_mul_sd(a25_8, b25));
#endif
_mm_store_sd(&C[(i*56)+16], c25_8);
__m128d c25_9 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a25_9 = _mm_loadu_pd(&values[78]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_9 = _mm_add_pd(c25_9, _mm_mul_pd(a25_9, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_9 = _mm_add_pd(c25_9, _mm_mul_pd(a25_9, b25));
#endif
_mm_storeu_pd(&C[(i*56)+18], c25_9);
#else
C[(i*56)+0] += values[69] * B[(i*56)+25];
C[(i*56)+2] += values[70] * B[(i*56)+25];
C[(i*56)+3] += values[71] * B[(i*56)+25];
C[(i*56)+4] += values[72] * B[(i*56)+25];
C[(i*56)+6] += values[73] * B[(i*56)+25];
C[(i*56)+8] += values[74] * B[(i*56)+25];
C[(i*56)+9] += values[75] * B[(i*56)+25];
C[(i*56)+14] += values[76] * B[(i*56)+25];
C[(i*56)+16] += values[77] * B[(i*56)+25];
C[(i*56)+18] += values[78] * B[(i*56)+25];
C[(i*56)+19] += values[79] * B[(i*56)+25];
#endif
#ifndef NDEBUG
num_flops += 22;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*56)+26]);
#endif
__m128d c26_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a26_0 = _mm_load_sd(&values[80]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_0 = _mm_add_sd(c26_0, _mm_mul_sd(a26_0, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_0 = _mm_add_sd(c26_0, _mm_mul_sd(a26_0, b26));
#endif
_mm_store_sd(&C[(i*56)+1], c26_0);
__m128d c26_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a26_1 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_1 = _mm_add_sd(c26_1, _mm_mul_sd(a26_1, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_1 = _mm_add_sd(c26_1, _mm_mul_sd(a26_1, b26));
#endif
_mm_store_sd(&C[(i*56)+5], c26_1);
__m128d c26_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a26_2 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_2 = _mm_add_sd(c26_2, _mm_mul_sd(a26_2, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_2 = _mm_add_sd(c26_2, _mm_mul_sd(a26_2, b26));
#endif
_mm_store_sd(&C[(i*56)+7], c26_2);
__m128d c26_3 = _mm_load_sd(&C[(i*56)+15]);
__m128d a26_3 = _mm_load_sd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_3 = _mm_add_sd(c26_3, _mm_mul_sd(a26_3, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_3 = _mm_add_sd(c26_3, _mm_mul_sd(a26_3, b26));
#endif
_mm_store_sd(&C[(i*56)+15], c26_3);
__m128d c26_4 = _mm_load_sd(&C[(i*56)+17]);
__m128d a26_4 = _mm_load_sd(&values[84]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_4 = _mm_add_sd(c26_4, _mm_mul_sd(a26_4, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_4 = _mm_add_sd(c26_4, _mm_mul_sd(a26_4, b26));
#endif
_mm_store_sd(&C[(i*56)+17], c26_4);
#else
C[(i*56)+1] += values[80] * B[(i*56)+26];
C[(i*56)+5] += values[81] * B[(i*56)+26];
C[(i*56)+7] += values[82] * B[(i*56)+26];
C[(i*56)+15] += values[83] * B[(i*56)+26];
C[(i*56)+17] += values[84] * B[(i*56)+26];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*56)+27]);
#endif
__m128d c27_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a27_0 = _mm_load_sd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_0 = _mm_add_sd(c27_0, _mm_mul_sd(a27_0, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_0 = _mm_add_sd(c27_0, _mm_mul_sd(a27_0, b27));
#endif
_mm_store_sd(&C[(i*56)+0], c27_0);
__m128d c27_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a27_1 = _mm_loadu_pd(&values[86]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_1 = _mm_add_pd(c27_1, _mm_mul_pd(a27_1, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_1 = _mm_add_pd(c27_1, _mm_mul_pd(a27_1, b27));
#endif
_mm_storeu_pd(&C[(i*56)+2], c27_1);
__m128d c27_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a27_3 = _mm_load_sd(&values[88]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_3 = _mm_add_sd(c27_3, _mm_mul_sd(a27_3, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_3 = _mm_add_sd(c27_3, _mm_mul_sd(a27_3, b27));
#endif
_mm_store_sd(&C[(i*56)+6], c27_3);
__m128d c27_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a27_4 = _mm_loadu_pd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, b27));
#endif
_mm_storeu_pd(&C[(i*56)+8], c27_4);
__m128d c27_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a27_6 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_6 = _mm_add_sd(c27_6, _mm_mul_sd(a27_6, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_6 = _mm_add_sd(c27_6, _mm_mul_sd(a27_6, b27));
#endif
_mm_store_sd(&C[(i*56)+16], c27_6);
__m128d c27_7 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a27_7 = _mm_loadu_pd(&values[92]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_7 = _mm_add_pd(c27_7, _mm_mul_pd(a27_7, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_7 = _mm_add_pd(c27_7, _mm_mul_pd(a27_7, b27));
#endif
_mm_storeu_pd(&C[(i*56)+18], c27_7);
#else
C[(i*56)+0] += values[85] * B[(i*56)+27];
C[(i*56)+2] += values[86] * B[(i*56)+27];
C[(i*56)+3] += values[87] * B[(i*56)+27];
C[(i*56)+6] += values[88] * B[(i*56)+27];
C[(i*56)+8] += values[89] * B[(i*56)+27];
C[(i*56)+9] += values[90] * B[(i*56)+27];
C[(i*56)+16] += values[91] * B[(i*56)+27];
C[(i*56)+18] += values[92] * B[(i*56)+27];
C[(i*56)+19] += values[93] * B[(i*56)+27];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*56)+29]);
#endif
__m128d c29_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a29_0 = _mm_load_sd(&values[94]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_0 = _mm_add_sd(c29_0, _mm_mul_sd(a29_0, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_0 = _mm_add_sd(c29_0, _mm_mul_sd(a29_0, b29));
#endif
_mm_store_sd(&C[(i*56)+1], c29_0);
__m128d c29_1 = _mm_load_sd(&C[(i*56)+7]);
__m128d a29_1 = _mm_load_sd(&values[95]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_1 = _mm_add_sd(c29_1, _mm_mul_sd(a29_1, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_1 = _mm_add_sd(c29_1, _mm_mul_sd(a29_1, b29));
#endif
_mm_store_sd(&C[(i*56)+7], c29_1);
__m128d c29_2 = _mm_load_sd(&C[(i*56)+17]);
__m128d a29_2 = _mm_load_sd(&values[96]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_2 = _mm_add_sd(c29_2, _mm_mul_sd(a29_2, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_2 = _mm_add_sd(c29_2, _mm_mul_sd(a29_2, b29));
#endif
_mm_store_sd(&C[(i*56)+17], c29_2);
#else
C[(i*56)+1] += values[94] * B[(i*56)+29];
C[(i*56)+7] += values[95] * B[(i*56)+29];
C[(i*56)+17] += values[96] * B[(i*56)+29];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*56)+30]);
#endif
__m128d c30_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a30_0 = _mm_load_sd(&values[97]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_0 = _mm_add_sd(c30_0, _mm_mul_sd(a30_0, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_0 = _mm_add_sd(c30_0, _mm_mul_sd(a30_0, b30));
#endif
_mm_store_sd(&C[(i*56)+0], c30_0);
__m128d c30_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a30_1 = _mm_loadu_pd(&values[98]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_1 = _mm_add_pd(c30_1, _mm_mul_pd(a30_1, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_1 = _mm_add_pd(c30_1, _mm_mul_pd(a30_1, b30));
#endif
_mm_storeu_pd(&C[(i*56)+2], c30_1);
__m128d c30_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a30_3 = _mm_loadu_pd(&values[100]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_3 = _mm_add_pd(c30_3, _mm_mul_pd(a30_3, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_3 = _mm_add_pd(c30_3, _mm_mul_pd(a30_3, b30));
#endif
_mm_storeu_pd(&C[(i*56)+8], c30_3);
__m128d c30_5 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a30_5 = _mm_loadu_pd(&values[102]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, b30));
#endif
_mm_storeu_pd(&C[(i*56)+18], c30_5);
#else
C[(i*56)+0] += values[97] * B[(i*56)+30];
C[(i*56)+2] += values[98] * B[(i*56)+30];
C[(i*56)+3] += values[99] * B[(i*56)+30];
C[(i*56)+8] += values[100] * B[(i*56)+30];
C[(i*56)+9] += values[101] * B[(i*56)+30];
C[(i*56)+18] += values[102] * B[(i*56)+30];
C[(i*56)+19] += values[103] * B[(i*56)+30];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*56)+32]);
#endif
__m128d c32_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a32_0 = _mm_load_sd(&values[104]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, b32));
#endif
_mm_store_sd(&C[(i*56)+0], c32_0);
__m128d c32_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a32_1 = _mm_load_sd(&values[105]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, b32));
#endif
_mm_store_sd(&C[(i*56)+3], c32_1);
__m128d c32_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a32_2 = _mm_load_sd(&values[106]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, b32));
#endif
_mm_store_sd(&C[(i*56)+9], c32_2);
__m128d c32_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a32_3 = _mm_load_sd(&values[107]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, b32));
#endif
_mm_store_sd(&C[(i*56)+19], c32_3);
#else
C[(i*56)+0] += values[104] * B[(i*56)+32];
C[(i*56)+3] += values[105] * B[(i*56)+32];
C[(i*56)+9] += values[106] * B[(i*56)+32];
C[(i*56)+19] += values[107] * B[(i*56)+32];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 35, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b35 = _mm256_broadcast_sd(&B[(i*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b35 = _mm_loaddup_pd(&B[(i*56)+35]);
#endif
__m128d c35_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a35_0 = _mm_load_sd(&values[108]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_0 = _mm_add_sd(c35_0, _mm_mul_sd(a35_0, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_0 = _mm_add_sd(c35_0, _mm_mul_sd(a35_0, b35));
#endif
_mm_store_sd(&C[(i*56)+0], c35_0);
__m128d c35_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a35_1 = _mm_loadu_pd(&values[109]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_1 = _mm_add_pd(c35_1, _mm_mul_pd(a35_1, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_1 = _mm_add_pd(c35_1, _mm_mul_pd(a35_1, b35));
#endif
_mm_storeu_pd(&C[(i*56)+2], c35_1);
__m128d c35_3 = _mm_load_sd(&C[(i*56)+4]);
__m128d a35_3 = _mm_load_sd(&values[111]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_3 = _mm_add_sd(c35_3, _mm_mul_sd(a35_3, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_3 = _mm_add_sd(c35_3, _mm_mul_sd(a35_3, b35));
#endif
_mm_store_sd(&C[(i*56)+4], c35_3);
__m128d c35_4 = _mm_load_sd(&C[(i*56)+6]);
__m128d a35_4 = _mm_load_sd(&values[112]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_4 = _mm_add_sd(c35_4, _mm_mul_sd(a35_4, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_4 = _mm_add_sd(c35_4, _mm_mul_sd(a35_4, b35));
#endif
_mm_store_sd(&C[(i*56)+6], c35_4);
__m128d c35_5 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a35_5 = _mm_loadu_pd(&values[113]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_5 = _mm_add_pd(c35_5, _mm_mul_pd(a35_5, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_5 = _mm_add_pd(c35_5, _mm_mul_pd(a35_5, b35));
#endif
_mm_storeu_pd(&C[(i*56)+8], c35_5);
__m128d c35_7 = _mm_load_sd(&C[(i*56)+11]);
__m128d a35_7 = _mm_load_sd(&values[115]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_7 = _mm_add_sd(c35_7, _mm_mul_sd(a35_7, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_7 = _mm_add_sd(c35_7, _mm_mul_sd(a35_7, b35));
#endif
_mm_store_sd(&C[(i*56)+11], c35_7);
__m128d c35_8 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a35_8 = _mm_loadu_pd(&values[116]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_8 = _mm_add_pd(c35_8, _mm_mul_pd(a35_8, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_8 = _mm_add_pd(c35_8, _mm_mul_pd(a35_8, b35));
#endif
_mm_storeu_pd(&C[(i*56)+13], c35_8);
__m128d c35_10 = _mm_load_sd(&C[(i*56)+16]);
__m128d a35_10 = _mm_load_sd(&values[118]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_10 = _mm_add_sd(c35_10, _mm_mul_sd(a35_10, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_10 = _mm_add_sd(c35_10, _mm_mul_sd(a35_10, b35));
#endif
_mm_store_sd(&C[(i*56)+16], c35_10);
__m128d c35_11 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a35_11 = _mm_loadu_pd(&values[119]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_11 = _mm_add_pd(c35_11, _mm_mul_pd(a35_11, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_11 = _mm_add_pd(c35_11, _mm_mul_pd(a35_11, b35));
#endif
_mm_storeu_pd(&C[(i*56)+18], c35_11);
__m128d c35_13 = _mm_load_sd(&C[(i*56)+20]);
__m128d a35_13 = _mm_load_sd(&values[121]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_13 = _mm_add_sd(c35_13, _mm_mul_sd(a35_13, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_13 = _mm_add_sd(c35_13, _mm_mul_sd(a35_13, b35));
#endif
_mm_store_sd(&C[(i*56)+20], c35_13);
__m128d c35_14 = _mm_load_sd(&C[(i*56)+22]);
__m128d a35_14 = _mm_load_sd(&values[122]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_14 = _mm_add_sd(c35_14, _mm_mul_sd(a35_14, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_14 = _mm_add_sd(c35_14, _mm_mul_sd(a35_14, b35));
#endif
_mm_store_sd(&C[(i*56)+22], c35_14);
__m128d c35_15 = _mm_load_sd(&C[(i*56)+24]);
__m128d a35_15 = _mm_load_sd(&values[123]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_15 = _mm_add_sd(c35_15, _mm_mul_sd(a35_15, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_15 = _mm_add_sd(c35_15, _mm_mul_sd(a35_15, b35));
#endif
_mm_store_sd(&C[(i*56)+24], c35_15);
__m128d c35_16 = _mm_load_sd(&C[(i*56)+26]);
__m128d a35_16 = _mm_load_sd(&values[124]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_16 = _mm_add_sd(c35_16, _mm_mul_sd(a35_16, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_16 = _mm_add_sd(c35_16, _mm_mul_sd(a35_16, b35));
#endif
_mm_store_sd(&C[(i*56)+26], c35_16);
__m128d c35_17 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a35_17 = _mm_loadu_pd(&values[125]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_17 = _mm_add_pd(c35_17, _mm_mul_pd(a35_17, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_17 = _mm_add_pd(c35_17, _mm_mul_pd(a35_17, b35));
#endif
_mm_storeu_pd(&C[(i*56)+28], c35_17);
__m128d c35_19 = _mm_load_sd(&C[(i*56)+31]);
__m128d a35_19 = _mm_load_sd(&values[127]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_19 = _mm_add_sd(c35_19, _mm_mul_sd(a35_19, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_19 = _mm_add_sd(c35_19, _mm_mul_sd(a35_19, b35));
#endif
_mm_store_sd(&C[(i*56)+31], c35_19);
__m128d c35_20 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a35_20 = _mm_loadu_pd(&values[128]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_20 = _mm_add_pd(c35_20, _mm_mul_pd(a35_20, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_20 = _mm_add_pd(c35_20, _mm_mul_pd(a35_20, b35));
#endif
_mm_storeu_pd(&C[(i*56)+33], c35_20);
#else
C[(i*56)+0] += values[108] * B[(i*56)+35];
C[(i*56)+2] += values[109] * B[(i*56)+35];
C[(i*56)+3] += values[110] * B[(i*56)+35];
C[(i*56)+4] += values[111] * B[(i*56)+35];
C[(i*56)+6] += values[112] * B[(i*56)+35];
C[(i*56)+8] += values[113] * B[(i*56)+35];
C[(i*56)+9] += values[114] * B[(i*56)+35];
C[(i*56)+11] += values[115] * B[(i*56)+35];
C[(i*56)+13] += values[116] * B[(i*56)+35];
C[(i*56)+14] += values[117] * B[(i*56)+35];
C[(i*56)+16] += values[118] * B[(i*56)+35];
C[(i*56)+18] += values[119] * B[(i*56)+35];
C[(i*56)+19] += values[120] * B[(i*56)+35];
C[(i*56)+20] += values[121] * B[(i*56)+35];
C[(i*56)+22] += values[122] * B[(i*56)+35];
C[(i*56)+24] += values[123] * B[(i*56)+35];
C[(i*56)+26] += values[124] * B[(i*56)+35];
C[(i*56)+28] += values[125] * B[(i*56)+35];
C[(i*56)+29] += values[126] * B[(i*56)+35];
C[(i*56)+31] += values[127] * B[(i*56)+35];
C[(i*56)+33] += values[128] * B[(i*56)+35];
C[(i*56)+34] += values[129] * B[(i*56)+35];
#endif
#ifndef NDEBUG
num_flops += 44;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b36 = _mm256_broadcast_sd(&B[(i*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b36 = _mm_loaddup_pd(&B[(i*56)+36]);
#endif
__m128d c36_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a36_0 = _mm_load_sd(&values[130]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_0 = _mm_add_sd(c36_0, _mm_mul_sd(a36_0, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_0 = _mm_add_sd(c36_0, _mm_mul_sd(a36_0, b36));
#endif
_mm_store_sd(&C[(i*56)+1], c36_0);
__m128d c36_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a36_1 = _mm_load_sd(&values[131]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_1 = _mm_add_sd(c36_1, _mm_mul_sd(a36_1, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_1 = _mm_add_sd(c36_1, _mm_mul_sd(a36_1, b36));
#endif
_mm_store_sd(&C[(i*56)+5], c36_1);
__m128d c36_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a36_2 = _mm_load_sd(&values[132]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_2 = _mm_add_sd(c36_2, _mm_mul_sd(a36_2, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_2 = _mm_add_sd(c36_2, _mm_mul_sd(a36_2, b36));
#endif
_mm_store_sd(&C[(i*56)+7], c36_2);
__m128d c36_3 = _mm_load_sd(&C[(i*56)+10]);
__m128d a36_3 = _mm_load_sd(&values[133]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_3 = _mm_add_sd(c36_3, _mm_mul_sd(a36_3, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_3 = _mm_add_sd(c36_3, _mm_mul_sd(a36_3, b36));
#endif
_mm_store_sd(&C[(i*56)+10], c36_3);
__m128d c36_4 = _mm_load_sd(&C[(i*56)+12]);
__m128d a36_4 = _mm_load_sd(&values[134]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_4 = _mm_add_sd(c36_4, _mm_mul_sd(a36_4, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_4 = _mm_add_sd(c36_4, _mm_mul_sd(a36_4, b36));
#endif
_mm_store_sd(&C[(i*56)+12], c36_4);
__m128d c36_5 = _mm_load_sd(&C[(i*56)+15]);
__m128d a36_5 = _mm_load_sd(&values[135]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_5 = _mm_add_sd(c36_5, _mm_mul_sd(a36_5, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_5 = _mm_add_sd(c36_5, _mm_mul_sd(a36_5, b36));
#endif
_mm_store_sd(&C[(i*56)+15], c36_5);
__m128d c36_6 = _mm_load_sd(&C[(i*56)+17]);
__m128d a36_6 = _mm_load_sd(&values[136]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_6 = _mm_add_sd(c36_6, _mm_mul_sd(a36_6, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_6 = _mm_add_sd(c36_6, _mm_mul_sd(a36_6, b36));
#endif
_mm_store_sd(&C[(i*56)+17], c36_6);
__m128d c36_7 = _mm_load_sd(&C[(i*56)+21]);
__m128d a36_7 = _mm_load_sd(&values[137]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_7 = _mm_add_sd(c36_7, _mm_mul_sd(a36_7, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_7 = _mm_add_sd(c36_7, _mm_mul_sd(a36_7, b36));
#endif
_mm_store_sd(&C[(i*56)+21], c36_7);
__m128d c36_8 = _mm_load_sd(&C[(i*56)+23]);
__m128d a36_8 = _mm_load_sd(&values[138]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_8 = _mm_add_sd(c36_8, _mm_mul_sd(a36_8, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_8 = _mm_add_sd(c36_8, _mm_mul_sd(a36_8, b36));
#endif
_mm_store_sd(&C[(i*56)+23], c36_8);
__m128d c36_9 = _mm_load_sd(&C[(i*56)+25]);
__m128d a36_9 = _mm_load_sd(&values[139]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_9 = _mm_add_sd(c36_9, _mm_mul_sd(a36_9, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_9 = _mm_add_sd(c36_9, _mm_mul_sd(a36_9, b36));
#endif
_mm_store_sd(&C[(i*56)+25], c36_9);
__m128d c36_10 = _mm_load_sd(&C[(i*56)+27]);
__m128d a36_10 = _mm_load_sd(&values[140]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_10 = _mm_add_sd(c36_10, _mm_mul_sd(a36_10, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_10 = _mm_add_sd(c36_10, _mm_mul_sd(a36_10, b36));
#endif
_mm_store_sd(&C[(i*56)+27], c36_10);
__m128d c36_11 = _mm_load_sd(&C[(i*56)+30]);
__m128d a36_11 = _mm_load_sd(&values[141]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_11 = _mm_add_sd(c36_11, _mm_mul_sd(a36_11, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_11 = _mm_add_sd(c36_11, _mm_mul_sd(a36_11, b36));
#endif
_mm_store_sd(&C[(i*56)+30], c36_11);
__m128d c36_12 = _mm_load_sd(&C[(i*56)+32]);
__m128d a36_12 = _mm_load_sd(&values[142]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_12 = _mm_add_sd(c36_12, _mm_mul_sd(a36_12, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_12 = _mm_add_sd(c36_12, _mm_mul_sd(a36_12, b36));
#endif
_mm_store_sd(&C[(i*56)+32], c36_12);
#else
C[(i*56)+1] += values[130] * B[(i*56)+36];
C[(i*56)+5] += values[131] * B[(i*56)+36];
C[(i*56)+7] += values[132] * B[(i*56)+36];
C[(i*56)+10] += values[133] * B[(i*56)+36];
C[(i*56)+12] += values[134] * B[(i*56)+36];
C[(i*56)+15] += values[135] * B[(i*56)+36];
C[(i*56)+17] += values[136] * B[(i*56)+36];
C[(i*56)+21] += values[137] * B[(i*56)+36];
C[(i*56)+23] += values[138] * B[(i*56)+36];
C[(i*56)+25] += values[139] * B[(i*56)+36];
C[(i*56)+27] += values[140] * B[(i*56)+36];
C[(i*56)+30] += values[141] * B[(i*56)+36];
C[(i*56)+32] += values[142] * B[(i*56)+36];
#endif
#ifndef NDEBUG
num_flops += 26;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b37 = _mm256_broadcast_sd(&B[(i*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b37 = _mm_loaddup_pd(&B[(i*56)+37]);
#endif
__m128d c37_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a37_0 = _mm_load_sd(&values[143]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_0 = _mm_add_sd(c37_0, _mm_mul_sd(a37_0, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_0 = _mm_add_sd(c37_0, _mm_mul_sd(a37_0, b37));
#endif
_mm_store_sd(&C[(i*56)+0], c37_0);
__m128d c37_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a37_1 = _mm_loadu_pd(&values[144]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_1 = _mm_add_pd(c37_1, _mm_mul_pd(a37_1, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_1 = _mm_add_pd(c37_1, _mm_mul_pd(a37_1, b37));
#endif
_mm_storeu_pd(&C[(i*56)+2], c37_1);
__m128d c37_3 = _mm_load_sd(&C[(i*56)+4]);
__m128d a37_3 = _mm_load_sd(&values[146]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_3 = _mm_add_sd(c37_3, _mm_mul_sd(a37_3, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_3 = _mm_add_sd(c37_3, _mm_mul_sd(a37_3, b37));
#endif
_mm_store_sd(&C[(i*56)+4], c37_3);
__m128d c37_4 = _mm_load_sd(&C[(i*56)+6]);
__m128d a37_4 = _mm_load_sd(&values[147]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_4 = _mm_add_sd(c37_4, _mm_mul_sd(a37_4, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_4 = _mm_add_sd(c37_4, _mm_mul_sd(a37_4, b37));
#endif
_mm_store_sd(&C[(i*56)+6], c37_4);
__m128d c37_5 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a37_5 = _mm_loadu_pd(&values[148]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_5 = _mm_add_pd(c37_5, _mm_mul_pd(a37_5, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_5 = _mm_add_pd(c37_5, _mm_mul_pd(a37_5, b37));
#endif
_mm_storeu_pd(&C[(i*56)+8], c37_5);
__m128d c37_7 = _mm_load_sd(&C[(i*56)+11]);
__m128d a37_7 = _mm_load_sd(&values[150]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_7 = _mm_add_sd(c37_7, _mm_mul_sd(a37_7, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_7 = _mm_add_sd(c37_7, _mm_mul_sd(a37_7, b37));
#endif
_mm_store_sd(&C[(i*56)+11], c37_7);
__m128d c37_8 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a37_8 = _mm_loadu_pd(&values[151]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_8 = _mm_add_pd(c37_8, _mm_mul_pd(a37_8, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_8 = _mm_add_pd(c37_8, _mm_mul_pd(a37_8, b37));
#endif
_mm_storeu_pd(&C[(i*56)+13], c37_8);
__m128d c37_10 = _mm_load_sd(&C[(i*56)+16]);
__m128d a37_10 = _mm_load_sd(&values[153]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_10 = _mm_add_sd(c37_10, _mm_mul_sd(a37_10, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_10 = _mm_add_sd(c37_10, _mm_mul_sd(a37_10, b37));
#endif
_mm_store_sd(&C[(i*56)+16], c37_10);
__m128d c37_11 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a37_11 = _mm_loadu_pd(&values[154]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_11 = _mm_add_pd(c37_11, _mm_mul_pd(a37_11, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_11 = _mm_add_pd(c37_11, _mm_mul_pd(a37_11, b37));
#endif
_mm_storeu_pd(&C[(i*56)+18], c37_11);
__m128d c37_13 = _mm_load_sd(&C[(i*56)+22]);
__m128d a37_13 = _mm_load_sd(&values[156]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_13 = _mm_add_sd(c37_13, _mm_mul_sd(a37_13, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_13 = _mm_add_sd(c37_13, _mm_mul_sd(a37_13, b37));
#endif
_mm_store_sd(&C[(i*56)+22], c37_13);
__m128d c37_14 = _mm_load_sd(&C[(i*56)+24]);
__m128d a37_14 = _mm_load_sd(&values[157]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_14 = _mm_add_sd(c37_14, _mm_mul_sd(a37_14, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_14 = _mm_add_sd(c37_14, _mm_mul_sd(a37_14, b37));
#endif
_mm_store_sd(&C[(i*56)+24], c37_14);
__m128d c37_15 = _mm_load_sd(&C[(i*56)+26]);
__m128d a37_15 = _mm_load_sd(&values[158]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_15 = _mm_add_sd(c37_15, _mm_mul_sd(a37_15, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_15 = _mm_add_sd(c37_15, _mm_mul_sd(a37_15, b37));
#endif
_mm_store_sd(&C[(i*56)+26], c37_15);
__m128d c37_16 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a37_16 = _mm_loadu_pd(&values[159]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_16 = _mm_add_pd(c37_16, _mm_mul_pd(a37_16, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_16 = _mm_add_pd(c37_16, _mm_mul_pd(a37_16, b37));
#endif
_mm_storeu_pd(&C[(i*56)+28], c37_16);
__m128d c37_18 = _mm_load_sd(&C[(i*56)+31]);
__m128d a37_18 = _mm_load_sd(&values[161]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_18 = _mm_add_sd(c37_18, _mm_mul_sd(a37_18, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_18 = _mm_add_sd(c37_18, _mm_mul_sd(a37_18, b37));
#endif
_mm_store_sd(&C[(i*56)+31], c37_18);
__m128d c37_19 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a37_19 = _mm_loadu_pd(&values[162]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_19 = _mm_add_pd(c37_19, _mm_mul_pd(a37_19, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_19 = _mm_add_pd(c37_19, _mm_mul_pd(a37_19, b37));
#endif
_mm_storeu_pd(&C[(i*56)+33], c37_19);
#else
C[(i*56)+0] += values[143] * B[(i*56)+37];
C[(i*56)+2] += values[144] * B[(i*56)+37];
C[(i*56)+3] += values[145] * B[(i*56)+37];
C[(i*56)+4] += values[146] * B[(i*56)+37];
C[(i*56)+6] += values[147] * B[(i*56)+37];
C[(i*56)+8] += values[148] * B[(i*56)+37];
C[(i*56)+9] += values[149] * B[(i*56)+37];
C[(i*56)+11] += values[150] * B[(i*56)+37];
C[(i*56)+13] += values[151] * B[(i*56)+37];
C[(i*56)+14] += values[152] * B[(i*56)+37];
C[(i*56)+16] += values[153] * B[(i*56)+37];
C[(i*56)+18] += values[154] * B[(i*56)+37];
C[(i*56)+19] += values[155] * B[(i*56)+37];
C[(i*56)+22] += values[156] * B[(i*56)+37];
C[(i*56)+24] += values[157] * B[(i*56)+37];
C[(i*56)+26] += values[158] * B[(i*56)+37];
C[(i*56)+28] += values[159] * B[(i*56)+37];
C[(i*56)+29] += values[160] * B[(i*56)+37];
C[(i*56)+31] += values[161] * B[(i*56)+37];
C[(i*56)+33] += values[162] * B[(i*56)+37];
C[(i*56)+34] += values[163] * B[(i*56)+37];
#endif
#ifndef NDEBUG
num_flops += 42;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b38 = _mm256_broadcast_sd(&B[(i*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b38 = _mm_loaddup_pd(&B[(i*56)+38]);
#endif
__m128d c38_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a38_0 = _mm_load_sd(&values[164]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_0 = _mm_add_sd(c38_0, _mm_mul_sd(a38_0, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_0 = _mm_add_sd(c38_0, _mm_mul_sd(a38_0, b38));
#endif
_mm_store_sd(&C[(i*56)+1], c38_0);
__m128d c38_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a38_1 = _mm_load_sd(&values[165]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_1 = _mm_add_sd(c38_1, _mm_mul_sd(a38_1, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_1 = _mm_add_sd(c38_1, _mm_mul_sd(a38_1, b38));
#endif
_mm_store_sd(&C[(i*56)+5], c38_1);
__m128d c38_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a38_2 = _mm_load_sd(&values[166]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_2 = _mm_add_sd(c38_2, _mm_mul_sd(a38_2, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_2 = _mm_add_sd(c38_2, _mm_mul_sd(a38_2, b38));
#endif
_mm_store_sd(&C[(i*56)+7], c38_2);
__m128d c38_3 = _mm_load_sd(&C[(i*56)+12]);
__m128d a38_3 = _mm_load_sd(&values[167]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_3 = _mm_add_sd(c38_3, _mm_mul_sd(a38_3, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_3 = _mm_add_sd(c38_3, _mm_mul_sd(a38_3, b38));
#endif
_mm_store_sd(&C[(i*56)+12], c38_3);
__m128d c38_4 = _mm_load_sd(&C[(i*56)+15]);
__m128d a38_4 = _mm_load_sd(&values[168]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_4 = _mm_add_sd(c38_4, _mm_mul_sd(a38_4, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_4 = _mm_add_sd(c38_4, _mm_mul_sd(a38_4, b38));
#endif
_mm_store_sd(&C[(i*56)+15], c38_4);
__m128d c38_5 = _mm_load_sd(&C[(i*56)+17]);
__m128d a38_5 = _mm_load_sd(&values[169]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_5 = _mm_add_sd(c38_5, _mm_mul_sd(a38_5, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_5 = _mm_add_sd(c38_5, _mm_mul_sd(a38_5, b38));
#endif
_mm_store_sd(&C[(i*56)+17], c38_5);
__m128d c38_6 = _mm_load_sd(&C[(i*56)+23]);
__m128d a38_6 = _mm_load_sd(&values[170]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_6 = _mm_add_sd(c38_6, _mm_mul_sd(a38_6, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_6 = _mm_add_sd(c38_6, _mm_mul_sd(a38_6, b38));
#endif
_mm_store_sd(&C[(i*56)+23], c38_6);
__m128d c38_7 = _mm_load_sd(&C[(i*56)+27]);
__m128d a38_7 = _mm_load_sd(&values[171]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_7 = _mm_add_sd(c38_7, _mm_mul_sd(a38_7, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_7 = _mm_add_sd(c38_7, _mm_mul_sd(a38_7, b38));
#endif
_mm_store_sd(&C[(i*56)+27], c38_7);
__m128d c38_8 = _mm_load_sd(&C[(i*56)+30]);
__m128d a38_8 = _mm_load_sd(&values[172]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_8 = _mm_add_sd(c38_8, _mm_mul_sd(a38_8, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_8 = _mm_add_sd(c38_8, _mm_mul_sd(a38_8, b38));
#endif
_mm_store_sd(&C[(i*56)+30], c38_8);
__m128d c38_9 = _mm_load_sd(&C[(i*56)+32]);
__m128d a38_9 = _mm_load_sd(&values[173]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_9 = _mm_add_sd(c38_9, _mm_mul_sd(a38_9, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_9 = _mm_add_sd(c38_9, _mm_mul_sd(a38_9, b38));
#endif
_mm_store_sd(&C[(i*56)+32], c38_9);
#else
C[(i*56)+1] += values[164] * B[(i*56)+38];
C[(i*56)+5] += values[165] * B[(i*56)+38];
C[(i*56)+7] += values[166] * B[(i*56)+38];
C[(i*56)+12] += values[167] * B[(i*56)+38];
C[(i*56)+15] += values[168] * B[(i*56)+38];
C[(i*56)+17] += values[169] * B[(i*56)+38];
C[(i*56)+23] += values[170] * B[(i*56)+38];
C[(i*56)+27] += values[171] * B[(i*56)+38];
C[(i*56)+30] += values[172] * B[(i*56)+38];
C[(i*56)+32] += values[173] * B[(i*56)+38];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b39 = _mm256_broadcast_sd(&B[(i*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b39 = _mm_loaddup_pd(&B[(i*56)+39]);
#endif
__m128d c39_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a39_0 = _mm_load_sd(&values[174]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_0 = _mm_add_sd(c39_0, _mm_mul_sd(a39_0, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_0 = _mm_add_sd(c39_0, _mm_mul_sd(a39_0, b39));
#endif
_mm_store_sd(&C[(i*56)+0], c39_0);
__m128d c39_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a39_1 = _mm_loadu_pd(&values[175]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_1 = _mm_add_pd(c39_1, _mm_mul_pd(a39_1, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_1 = _mm_add_pd(c39_1, _mm_mul_pd(a39_1, b39));
#endif
_mm_storeu_pd(&C[(i*56)+2], c39_1);
__m128d c39_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a39_3 = _mm_load_sd(&values[177]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_3 = _mm_add_sd(c39_3, _mm_mul_sd(a39_3, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_3 = _mm_add_sd(c39_3, _mm_mul_sd(a39_3, b39));
#endif
_mm_store_sd(&C[(i*56)+6], c39_3);
__m128d c39_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a39_4 = _mm_loadu_pd(&values[178]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_4 = _mm_add_pd(c39_4, _mm_mul_pd(a39_4, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_4 = _mm_add_pd(c39_4, _mm_mul_pd(a39_4, b39));
#endif
_mm_storeu_pd(&C[(i*56)+8], c39_4);
__m128d c39_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a39_6 = _mm_load_sd(&values[180]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_6 = _mm_add_sd(c39_6, _mm_mul_sd(a39_6, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_6 = _mm_add_sd(c39_6, _mm_mul_sd(a39_6, b39));
#endif
_mm_store_sd(&C[(i*56)+13], c39_6);
__m128d c39_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a39_7 = _mm_load_sd(&values[181]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_7 = _mm_add_sd(c39_7, _mm_mul_sd(a39_7, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_7 = _mm_add_sd(c39_7, _mm_mul_sd(a39_7, b39));
#endif
_mm_store_sd(&C[(i*56)+16], c39_7);
__m128d c39_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a39_8 = _mm_loadu_pd(&values[182]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_8 = _mm_add_pd(c39_8, _mm_mul_pd(a39_8, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_8 = _mm_add_pd(c39_8, _mm_mul_pd(a39_8, b39));
#endif
_mm_storeu_pd(&C[(i*56)+18], c39_8);
__m128d c39_10 = _mm_load_sd(&C[(i*56)+24]);
__m128d a39_10 = _mm_load_sd(&values[184]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_10 = _mm_add_sd(c39_10, _mm_mul_sd(a39_10, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_10 = _mm_add_sd(c39_10, _mm_mul_sd(a39_10, b39));
#endif
_mm_store_sd(&C[(i*56)+24], c39_10);
__m128d c39_11 = _mm_load_sd(&C[(i*56)+28]);
__m128d a39_11 = _mm_load_sd(&values[185]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_11 = _mm_add_sd(c39_11, _mm_mul_sd(a39_11, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_11 = _mm_add_sd(c39_11, _mm_mul_sd(a39_11, b39));
#endif
_mm_store_sd(&C[(i*56)+28], c39_11);
__m128d c39_12 = _mm_load_sd(&C[(i*56)+31]);
__m128d a39_12 = _mm_load_sd(&values[186]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_12 = _mm_add_sd(c39_12, _mm_mul_sd(a39_12, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_12 = _mm_add_sd(c39_12, _mm_mul_sd(a39_12, b39));
#endif
_mm_store_sd(&C[(i*56)+31], c39_12);
__m128d c39_13 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a39_13 = _mm_loadu_pd(&values[187]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_13 = _mm_add_pd(c39_13, _mm_mul_pd(a39_13, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_13 = _mm_add_pd(c39_13, _mm_mul_pd(a39_13, b39));
#endif
_mm_storeu_pd(&C[(i*56)+33], c39_13);
#else
C[(i*56)+0] += values[174] * B[(i*56)+39];
C[(i*56)+2] += values[175] * B[(i*56)+39];
C[(i*56)+3] += values[176] * B[(i*56)+39];
C[(i*56)+6] += values[177] * B[(i*56)+39];
C[(i*56)+8] += values[178] * B[(i*56)+39];
C[(i*56)+9] += values[179] * B[(i*56)+39];
C[(i*56)+13] += values[180] * B[(i*56)+39];
C[(i*56)+16] += values[181] * B[(i*56)+39];
C[(i*56)+18] += values[182] * B[(i*56)+39];
C[(i*56)+19] += values[183] * B[(i*56)+39];
C[(i*56)+24] += values[184] * B[(i*56)+39];
C[(i*56)+28] += values[185] * B[(i*56)+39];
C[(i*56)+31] += values[186] * B[(i*56)+39];
C[(i*56)+33] += values[187] * B[(i*56)+39];
C[(i*56)+34] += values[188] * B[(i*56)+39];
#endif
#ifndef NDEBUG
num_flops += 30;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b41 = _mm256_broadcast_sd(&B[(i*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b41 = _mm_loaddup_pd(&B[(i*56)+41]);
#endif
__m128d c41_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a41_0 = _mm_load_sd(&values[189]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_0 = _mm_add_sd(c41_0, _mm_mul_sd(a41_0, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_0 = _mm_add_sd(c41_0, _mm_mul_sd(a41_0, b41));
#endif
_mm_store_sd(&C[(i*56)+1], c41_0);
__m128d c41_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a41_1 = _mm_load_sd(&values[190]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_1 = _mm_add_sd(c41_1, _mm_mul_sd(a41_1, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_1 = _mm_add_sd(c41_1, _mm_mul_sd(a41_1, b41));
#endif
_mm_store_sd(&C[(i*56)+5], c41_1);
__m128d c41_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a41_2 = _mm_load_sd(&values[191]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_2 = _mm_add_sd(c41_2, _mm_mul_sd(a41_2, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_2 = _mm_add_sd(c41_2, _mm_mul_sd(a41_2, b41));
#endif
_mm_store_sd(&C[(i*56)+7], c41_2);
__m128d c41_3 = _mm_load_sd(&C[(i*56)+10]);
__m128d a41_3 = _mm_load_sd(&values[192]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_3 = _mm_add_sd(c41_3, _mm_mul_sd(a41_3, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_3 = _mm_add_sd(c41_3, _mm_mul_sd(a41_3, b41));
#endif
_mm_store_sd(&C[(i*56)+10], c41_3);
__m128d c41_4 = _mm_load_sd(&C[(i*56)+12]);
__m128d a41_4 = _mm_load_sd(&values[193]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_4 = _mm_add_sd(c41_4, _mm_mul_sd(a41_4, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_4 = _mm_add_sd(c41_4, _mm_mul_sd(a41_4, b41));
#endif
_mm_store_sd(&C[(i*56)+12], c41_4);
__m128d c41_5 = _mm_load_sd(&C[(i*56)+15]);
__m128d a41_5 = _mm_load_sd(&values[194]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_5 = _mm_add_sd(c41_5, _mm_mul_sd(a41_5, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_5 = _mm_add_sd(c41_5, _mm_mul_sd(a41_5, b41));
#endif
_mm_store_sd(&C[(i*56)+15], c41_5);
__m128d c41_6 = _mm_load_sd(&C[(i*56)+17]);
__m128d a41_6 = _mm_load_sd(&values[195]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_6 = _mm_add_sd(c41_6, _mm_mul_sd(a41_6, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_6 = _mm_add_sd(c41_6, _mm_mul_sd(a41_6, b41));
#endif
_mm_store_sd(&C[(i*56)+17], c41_6);
__m128d c41_7 = _mm_load_sd(&C[(i*56)+25]);
__m128d a41_7 = _mm_load_sd(&values[196]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_7 = _mm_add_sd(c41_7, _mm_mul_sd(a41_7, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_7 = _mm_add_sd(c41_7, _mm_mul_sd(a41_7, b41));
#endif
_mm_store_sd(&C[(i*56)+25], c41_7);
__m128d c41_8 = _mm_load_sd(&C[(i*56)+27]);
__m128d a41_8 = _mm_load_sd(&values[197]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_8 = _mm_add_sd(c41_8, _mm_mul_sd(a41_8, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_8 = _mm_add_sd(c41_8, _mm_mul_sd(a41_8, b41));
#endif
_mm_store_sd(&C[(i*56)+27], c41_8);
__m128d c41_9 = _mm_load_sd(&C[(i*56)+30]);
__m128d a41_9 = _mm_load_sd(&values[198]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_9 = _mm_add_sd(c41_9, _mm_mul_sd(a41_9, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_9 = _mm_add_sd(c41_9, _mm_mul_sd(a41_9, b41));
#endif
_mm_store_sd(&C[(i*56)+30], c41_9);
__m128d c41_10 = _mm_load_sd(&C[(i*56)+32]);
__m128d a41_10 = _mm_load_sd(&values[199]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_10 = _mm_add_sd(c41_10, _mm_mul_sd(a41_10, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_10 = _mm_add_sd(c41_10, _mm_mul_sd(a41_10, b41));
#endif
_mm_store_sd(&C[(i*56)+32], c41_10);
#else
C[(i*56)+1] += values[189] * B[(i*56)+41];
C[(i*56)+5] += values[190] * B[(i*56)+41];
C[(i*56)+7] += values[191] * B[(i*56)+41];
C[(i*56)+10] += values[192] * B[(i*56)+41];
C[(i*56)+12] += values[193] * B[(i*56)+41];
C[(i*56)+15] += values[194] * B[(i*56)+41];
C[(i*56)+17] += values[195] * B[(i*56)+41];
C[(i*56)+25] += values[196] * B[(i*56)+41];
C[(i*56)+27] += values[197] * B[(i*56)+41];
C[(i*56)+30] += values[198] * B[(i*56)+41];
C[(i*56)+32] += values[199] * B[(i*56)+41];
#endif
#ifndef NDEBUG
num_flops += 22;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b42 = _mm256_broadcast_sd(&B[(i*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b42 = _mm_loaddup_pd(&B[(i*56)+42]);
#endif
__m128d c42_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a42_0 = _mm_load_sd(&values[200]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_0 = _mm_add_sd(c42_0, _mm_mul_sd(a42_0, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_0 = _mm_add_sd(c42_0, _mm_mul_sd(a42_0, b42));
#endif
_mm_store_sd(&C[(i*56)+0], c42_0);
__m128d c42_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a42_1 = _mm_loadu_pd(&values[201]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_1 = _mm_add_pd(c42_1, _mm_mul_pd(a42_1, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_1 = _mm_add_pd(c42_1, _mm_mul_pd(a42_1, b42));
#endif
_mm_storeu_pd(&C[(i*56)+2], c42_1);
__m128d c42_3 = _mm_load_sd(&C[(i*56)+4]);
__m128d a42_3 = _mm_load_sd(&values[203]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_3 = _mm_add_sd(c42_3, _mm_mul_sd(a42_3, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_3 = _mm_add_sd(c42_3, _mm_mul_sd(a42_3, b42));
#endif
_mm_store_sd(&C[(i*56)+4], c42_3);
__m128d c42_4 = _mm_load_sd(&C[(i*56)+6]);
__m128d a42_4 = _mm_load_sd(&values[204]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_4 = _mm_add_sd(c42_4, _mm_mul_sd(a42_4, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_4 = _mm_add_sd(c42_4, _mm_mul_sd(a42_4, b42));
#endif
_mm_store_sd(&C[(i*56)+6], c42_4);
__m128d c42_5 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a42_5 = _mm_loadu_pd(&values[205]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_5 = _mm_add_pd(c42_5, _mm_mul_pd(a42_5, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_5 = _mm_add_pd(c42_5, _mm_mul_pd(a42_5, b42));
#endif
_mm_storeu_pd(&C[(i*56)+8], c42_5);
__m128d c42_7 = _mm_load_sd(&C[(i*56)+11]);
__m128d a42_7 = _mm_load_sd(&values[207]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_7 = _mm_add_sd(c42_7, _mm_mul_sd(a42_7, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_7 = _mm_add_sd(c42_7, _mm_mul_sd(a42_7, b42));
#endif
_mm_store_sd(&C[(i*56)+11], c42_7);
__m128d c42_8 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a42_8 = _mm_loadu_pd(&values[208]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_8 = _mm_add_pd(c42_8, _mm_mul_pd(a42_8, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_8 = _mm_add_pd(c42_8, _mm_mul_pd(a42_8, b42));
#endif
_mm_storeu_pd(&C[(i*56)+13], c42_8);
__m128d c42_10 = _mm_load_sd(&C[(i*56)+16]);
__m128d a42_10 = _mm_load_sd(&values[210]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_10 = _mm_add_sd(c42_10, _mm_mul_sd(a42_10, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_10 = _mm_add_sd(c42_10, _mm_mul_sd(a42_10, b42));
#endif
_mm_store_sd(&C[(i*56)+16], c42_10);
__m128d c42_11 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a42_11 = _mm_loadu_pd(&values[211]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_11 = _mm_add_pd(c42_11, _mm_mul_pd(a42_11, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_11 = _mm_add_pd(c42_11, _mm_mul_pd(a42_11, b42));
#endif
_mm_storeu_pd(&C[(i*56)+18], c42_11);
__m128d c42_13 = _mm_load_sd(&C[(i*56)+26]);
__m128d a42_13 = _mm_load_sd(&values[213]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_13 = _mm_add_sd(c42_13, _mm_mul_sd(a42_13, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_13 = _mm_add_sd(c42_13, _mm_mul_sd(a42_13, b42));
#endif
_mm_store_sd(&C[(i*56)+26], c42_13);
__m128d c42_14 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a42_14 = _mm_loadu_pd(&values[214]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_14 = _mm_add_pd(c42_14, _mm_mul_pd(a42_14, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_14 = _mm_add_pd(c42_14, _mm_mul_pd(a42_14, b42));
#endif
_mm_storeu_pd(&C[(i*56)+28], c42_14);
__m128d c42_16 = _mm_load_sd(&C[(i*56)+31]);
__m128d a42_16 = _mm_load_sd(&values[216]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_16 = _mm_add_sd(c42_16, _mm_mul_sd(a42_16, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_16 = _mm_add_sd(c42_16, _mm_mul_sd(a42_16, b42));
#endif
_mm_store_sd(&C[(i*56)+31], c42_16);
__m128d c42_17 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a42_17 = _mm_loadu_pd(&values[217]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_17 = _mm_add_pd(c42_17, _mm_mul_pd(a42_17, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_17 = _mm_add_pd(c42_17, _mm_mul_pd(a42_17, b42));
#endif
_mm_storeu_pd(&C[(i*56)+33], c42_17);
#else
C[(i*56)+0] += values[200] * B[(i*56)+42];
C[(i*56)+2] += values[201] * B[(i*56)+42];
C[(i*56)+3] += values[202] * B[(i*56)+42];
C[(i*56)+4] += values[203] * B[(i*56)+42];
C[(i*56)+6] += values[204] * B[(i*56)+42];
C[(i*56)+8] += values[205] * B[(i*56)+42];
C[(i*56)+9] += values[206] * B[(i*56)+42];
C[(i*56)+11] += values[207] * B[(i*56)+42];
C[(i*56)+13] += values[208] * B[(i*56)+42];
C[(i*56)+14] += values[209] * B[(i*56)+42];
C[(i*56)+16] += values[210] * B[(i*56)+42];
C[(i*56)+18] += values[211] * B[(i*56)+42];
C[(i*56)+19] += values[212] * B[(i*56)+42];
C[(i*56)+26] += values[213] * B[(i*56)+42];
C[(i*56)+28] += values[214] * B[(i*56)+42];
C[(i*56)+29] += values[215] * B[(i*56)+42];
C[(i*56)+31] += values[216] * B[(i*56)+42];
C[(i*56)+33] += values[217] * B[(i*56)+42];
C[(i*56)+34] += values[218] * B[(i*56)+42];
#endif
#ifndef NDEBUG
num_flops += 38;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b43 = _mm256_broadcast_sd(&B[(i*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b43 = _mm_loaddup_pd(&B[(i*56)+43]);
#endif
__m128d c43_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a43_0 = _mm_load_sd(&values[219]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_0 = _mm_add_sd(c43_0, _mm_mul_sd(a43_0, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_0 = _mm_add_sd(c43_0, _mm_mul_sd(a43_0, b43));
#endif
_mm_store_sd(&C[(i*56)+1], c43_0);
__m128d c43_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a43_1 = _mm_load_sd(&values[220]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_1 = _mm_add_sd(c43_1, _mm_mul_sd(a43_1, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_1 = _mm_add_sd(c43_1, _mm_mul_sd(a43_1, b43));
#endif
_mm_store_sd(&C[(i*56)+5], c43_1);
__m128d c43_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a43_2 = _mm_load_sd(&values[221]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_2 = _mm_add_sd(c43_2, _mm_mul_sd(a43_2, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_2 = _mm_add_sd(c43_2, _mm_mul_sd(a43_2, b43));
#endif
_mm_store_sd(&C[(i*56)+7], c43_2);
__m128d c43_3 = _mm_load_sd(&C[(i*56)+12]);
__m128d a43_3 = _mm_load_sd(&values[222]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_3 = _mm_add_sd(c43_3, _mm_mul_sd(a43_3, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_3 = _mm_add_sd(c43_3, _mm_mul_sd(a43_3, b43));
#endif
_mm_store_sd(&C[(i*56)+12], c43_3);
__m128d c43_4 = _mm_load_sd(&C[(i*56)+15]);
__m128d a43_4 = _mm_load_sd(&values[223]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_4 = _mm_add_sd(c43_4, _mm_mul_sd(a43_4, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_4 = _mm_add_sd(c43_4, _mm_mul_sd(a43_4, b43));
#endif
_mm_store_sd(&C[(i*56)+15], c43_4);
__m128d c43_5 = _mm_load_sd(&C[(i*56)+17]);
__m128d a43_5 = _mm_load_sd(&values[224]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_5 = _mm_add_sd(c43_5, _mm_mul_sd(a43_5, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_5 = _mm_add_sd(c43_5, _mm_mul_sd(a43_5, b43));
#endif
_mm_store_sd(&C[(i*56)+17], c43_5);
__m128d c43_6 = _mm_load_sd(&C[(i*56)+27]);
__m128d a43_6 = _mm_load_sd(&values[225]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_6 = _mm_add_sd(c43_6, _mm_mul_sd(a43_6, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_6 = _mm_add_sd(c43_6, _mm_mul_sd(a43_6, b43));
#endif
_mm_store_sd(&C[(i*56)+27], c43_6);
__m128d c43_7 = _mm_load_sd(&C[(i*56)+30]);
__m128d a43_7 = _mm_load_sd(&values[226]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_7 = _mm_add_sd(c43_7, _mm_mul_sd(a43_7, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_7 = _mm_add_sd(c43_7, _mm_mul_sd(a43_7, b43));
#endif
_mm_store_sd(&C[(i*56)+30], c43_7);
__m128d c43_8 = _mm_load_sd(&C[(i*56)+32]);
__m128d a43_8 = _mm_load_sd(&values[227]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_8 = _mm_add_sd(c43_8, _mm_mul_sd(a43_8, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_8 = _mm_add_sd(c43_8, _mm_mul_sd(a43_8, b43));
#endif
_mm_store_sd(&C[(i*56)+32], c43_8);
#else
C[(i*56)+1] += values[219] * B[(i*56)+43];
C[(i*56)+5] += values[220] * B[(i*56)+43];
C[(i*56)+7] += values[221] * B[(i*56)+43];
C[(i*56)+12] += values[222] * B[(i*56)+43];
C[(i*56)+15] += values[223] * B[(i*56)+43];
C[(i*56)+17] += values[224] * B[(i*56)+43];
C[(i*56)+27] += values[225] * B[(i*56)+43];
C[(i*56)+30] += values[226] * B[(i*56)+43];
C[(i*56)+32] += values[227] * B[(i*56)+43];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b44 = _mm256_broadcast_sd(&B[(i*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b44 = _mm_loaddup_pd(&B[(i*56)+44]);
#endif
__m128d c44_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a44_0 = _mm_load_sd(&values[228]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_0 = _mm_add_sd(c44_0, _mm_mul_sd(a44_0, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_0 = _mm_add_sd(c44_0, _mm_mul_sd(a44_0, b44));
#endif
_mm_store_sd(&C[(i*56)+0], c44_0);
__m128d c44_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a44_1 = _mm_loadu_pd(&values[229]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_1 = _mm_add_pd(c44_1, _mm_mul_pd(a44_1, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_1 = _mm_add_pd(c44_1, _mm_mul_pd(a44_1, b44));
#endif
_mm_storeu_pd(&C[(i*56)+2], c44_1);
__m128d c44_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a44_3 = _mm_load_sd(&values[231]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_3 = _mm_add_sd(c44_3, _mm_mul_sd(a44_3, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_3 = _mm_add_sd(c44_3, _mm_mul_sd(a44_3, b44));
#endif
_mm_store_sd(&C[(i*56)+6], c44_3);
__m128d c44_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a44_4 = _mm_loadu_pd(&values[232]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_4 = _mm_add_pd(c44_4, _mm_mul_pd(a44_4, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_4 = _mm_add_pd(c44_4, _mm_mul_pd(a44_4, b44));
#endif
_mm_storeu_pd(&C[(i*56)+8], c44_4);
__m128d c44_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a44_6 = _mm_load_sd(&values[234]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_6 = _mm_add_sd(c44_6, _mm_mul_sd(a44_6, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_6 = _mm_add_sd(c44_6, _mm_mul_sd(a44_6, b44));
#endif
_mm_store_sd(&C[(i*56)+13], c44_6);
__m128d c44_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a44_7 = _mm_load_sd(&values[235]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_7 = _mm_add_sd(c44_7, _mm_mul_sd(a44_7, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_7 = _mm_add_sd(c44_7, _mm_mul_sd(a44_7, b44));
#endif
_mm_store_sd(&C[(i*56)+16], c44_7);
__m128d c44_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a44_8 = _mm_loadu_pd(&values[236]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_8 = _mm_add_pd(c44_8, _mm_mul_pd(a44_8, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_8 = _mm_add_pd(c44_8, _mm_mul_pd(a44_8, b44));
#endif
_mm_storeu_pd(&C[(i*56)+18], c44_8);
__m128d c44_10 = _mm_load_sd(&C[(i*56)+28]);
__m128d a44_10 = _mm_load_sd(&values[238]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_10 = _mm_add_sd(c44_10, _mm_mul_sd(a44_10, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_10 = _mm_add_sd(c44_10, _mm_mul_sd(a44_10, b44));
#endif
_mm_store_sd(&C[(i*56)+28], c44_10);
__m128d c44_11 = _mm_load_sd(&C[(i*56)+31]);
__m128d a44_11 = _mm_load_sd(&values[239]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_11 = _mm_add_sd(c44_11, _mm_mul_sd(a44_11, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_11 = _mm_add_sd(c44_11, _mm_mul_sd(a44_11, b44));
#endif
_mm_store_sd(&C[(i*56)+31], c44_11);
__m128d c44_12 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a44_12 = _mm_loadu_pd(&values[240]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_12 = _mm_add_pd(c44_12, _mm_mul_pd(a44_12, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_12 = _mm_add_pd(c44_12, _mm_mul_pd(a44_12, b44));
#endif
_mm_storeu_pd(&C[(i*56)+33], c44_12);
#else
C[(i*56)+0] += values[228] * B[(i*56)+44];
C[(i*56)+2] += values[229] * B[(i*56)+44];
C[(i*56)+3] += values[230] * B[(i*56)+44];
C[(i*56)+6] += values[231] * B[(i*56)+44];
C[(i*56)+8] += values[232] * B[(i*56)+44];
C[(i*56)+9] += values[233] * B[(i*56)+44];
C[(i*56)+13] += values[234] * B[(i*56)+44];
C[(i*56)+16] += values[235] * B[(i*56)+44];
C[(i*56)+18] += values[236] * B[(i*56)+44];
C[(i*56)+19] += values[237] * B[(i*56)+44];
C[(i*56)+28] += values[238] * B[(i*56)+44];
C[(i*56)+31] += values[239] * B[(i*56)+44];
C[(i*56)+33] += values[240] * B[(i*56)+44];
C[(i*56)+34] += values[241] * B[(i*56)+44];
#endif
#ifndef NDEBUG
num_flops += 28;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b46 = _mm256_broadcast_sd(&B[(i*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b46 = _mm_loaddup_pd(&B[(i*56)+46]);
#endif
__m128d c46_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a46_0 = _mm_load_sd(&values[242]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_0 = _mm_add_sd(c46_0, _mm_mul_sd(a46_0, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_0 = _mm_add_sd(c46_0, _mm_mul_sd(a46_0, b46));
#endif
_mm_store_sd(&C[(i*56)+0], c46_0);
__m128d c46_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a46_1 = _mm_loadu_pd(&values[243]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_1 = _mm_add_pd(c46_1, _mm_mul_pd(a46_1, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_1 = _mm_add_pd(c46_1, _mm_mul_pd(a46_1, b46));
#endif
_mm_storeu_pd(&C[(i*56)+2], c46_1);
__m128d c46_3 = _mm_load_sd(&C[(i*56)+4]);
__m128d a46_3 = _mm_load_sd(&values[245]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_3 = _mm_add_sd(c46_3, _mm_mul_sd(a46_3, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_3 = _mm_add_sd(c46_3, _mm_mul_sd(a46_3, b46));
#endif
_mm_store_sd(&C[(i*56)+4], c46_3);
__m128d c46_4 = _mm_load_sd(&C[(i*56)+6]);
__m128d a46_4 = _mm_load_sd(&values[246]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_4 = _mm_add_sd(c46_4, _mm_mul_sd(a46_4, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_4 = _mm_add_sd(c46_4, _mm_mul_sd(a46_4, b46));
#endif
_mm_store_sd(&C[(i*56)+6], c46_4);
__m128d c46_5 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a46_5 = _mm_loadu_pd(&values[247]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_5 = _mm_add_pd(c46_5, _mm_mul_pd(a46_5, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_5 = _mm_add_pd(c46_5, _mm_mul_pd(a46_5, b46));
#endif
_mm_storeu_pd(&C[(i*56)+8], c46_5);
__m128d c46_7 = _mm_load_sd(&C[(i*56)+14]);
__m128d a46_7 = _mm_load_sd(&values[249]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_7 = _mm_add_sd(c46_7, _mm_mul_sd(a46_7, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_7 = _mm_add_sd(c46_7, _mm_mul_sd(a46_7, b46));
#endif
_mm_store_sd(&C[(i*56)+14], c46_7);
__m128d c46_8 = _mm_load_sd(&C[(i*56)+16]);
__m128d a46_8 = _mm_load_sd(&values[250]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_8 = _mm_add_sd(c46_8, _mm_mul_sd(a46_8, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_8 = _mm_add_sd(c46_8, _mm_mul_sd(a46_8, b46));
#endif
_mm_store_sd(&C[(i*56)+16], c46_8);
__m128d c46_9 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a46_9 = _mm_loadu_pd(&values[251]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_9 = _mm_add_pd(c46_9, _mm_mul_pd(a46_9, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_9 = _mm_add_pd(c46_9, _mm_mul_pd(a46_9, b46));
#endif
_mm_storeu_pd(&C[(i*56)+18], c46_9);
__m128d c46_11 = _mm_load_sd(&C[(i*56)+29]);
__m128d a46_11 = _mm_load_sd(&values[253]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_11 = _mm_add_sd(c46_11, _mm_mul_sd(a46_11, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_11 = _mm_add_sd(c46_11, _mm_mul_sd(a46_11, b46));
#endif
_mm_store_sd(&C[(i*56)+29], c46_11);
__m128d c46_12 = _mm_load_sd(&C[(i*56)+31]);
__m128d a46_12 = _mm_load_sd(&values[254]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_12 = _mm_add_sd(c46_12, _mm_mul_sd(a46_12, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_12 = _mm_add_sd(c46_12, _mm_mul_sd(a46_12, b46));
#endif
_mm_store_sd(&C[(i*56)+31], c46_12);
__m128d c46_13 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a46_13 = _mm_loadu_pd(&values[255]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_13 = _mm_add_pd(c46_13, _mm_mul_pd(a46_13, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_13 = _mm_add_pd(c46_13, _mm_mul_pd(a46_13, b46));
#endif
_mm_storeu_pd(&C[(i*56)+33], c46_13);
#else
C[(i*56)+0] += values[242] * B[(i*56)+46];
C[(i*56)+2] += values[243] * B[(i*56)+46];
C[(i*56)+3] += values[244] * B[(i*56)+46];
C[(i*56)+4] += values[245] * B[(i*56)+46];
C[(i*56)+6] += values[246] * B[(i*56)+46];
C[(i*56)+8] += values[247] * B[(i*56)+46];
C[(i*56)+9] += values[248] * B[(i*56)+46];
C[(i*56)+14] += values[249] * B[(i*56)+46];
C[(i*56)+16] += values[250] * B[(i*56)+46];
C[(i*56)+18] += values[251] * B[(i*56)+46];
C[(i*56)+19] += values[252] * B[(i*56)+46];
C[(i*56)+29] += values[253] * B[(i*56)+46];
C[(i*56)+31] += values[254] * B[(i*56)+46];
C[(i*56)+33] += values[255] * B[(i*56)+46];
C[(i*56)+34] += values[256] * B[(i*56)+46];
#endif
#ifndef NDEBUG
num_flops += 30;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b47 = _mm256_broadcast_sd(&B[(i*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b47 = _mm_loaddup_pd(&B[(i*56)+47]);
#endif
__m128d c47_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a47_0 = _mm_load_sd(&values[257]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_0 = _mm_add_sd(c47_0, _mm_mul_sd(a47_0, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_0 = _mm_add_sd(c47_0, _mm_mul_sd(a47_0, b47));
#endif
_mm_store_sd(&C[(i*56)+1], c47_0);
__m128d c47_1 = _mm_load_sd(&C[(i*56)+5]);
__m128d a47_1 = _mm_load_sd(&values[258]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_1 = _mm_add_sd(c47_1, _mm_mul_sd(a47_1, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_1 = _mm_add_sd(c47_1, _mm_mul_sd(a47_1, b47));
#endif
_mm_store_sd(&C[(i*56)+5], c47_1);
__m128d c47_2 = _mm_load_sd(&C[(i*56)+7]);
__m128d a47_2 = _mm_load_sd(&values[259]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_2 = _mm_add_sd(c47_2, _mm_mul_sd(a47_2, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_2 = _mm_add_sd(c47_2, _mm_mul_sd(a47_2, b47));
#endif
_mm_store_sd(&C[(i*56)+7], c47_2);
__m128d c47_3 = _mm_load_sd(&C[(i*56)+15]);
__m128d a47_3 = _mm_load_sd(&values[260]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_3 = _mm_add_sd(c47_3, _mm_mul_sd(a47_3, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_3 = _mm_add_sd(c47_3, _mm_mul_sd(a47_3, b47));
#endif
_mm_store_sd(&C[(i*56)+15], c47_3);
__m128d c47_4 = _mm_load_sd(&C[(i*56)+17]);
__m128d a47_4 = _mm_load_sd(&values[261]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_4 = _mm_add_sd(c47_4, _mm_mul_sd(a47_4, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_4 = _mm_add_sd(c47_4, _mm_mul_sd(a47_4, b47));
#endif
_mm_store_sd(&C[(i*56)+17], c47_4);
__m128d c47_5 = _mm_load_sd(&C[(i*56)+30]);
__m128d a47_5 = _mm_load_sd(&values[262]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_5 = _mm_add_sd(c47_5, _mm_mul_sd(a47_5, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_5 = _mm_add_sd(c47_5, _mm_mul_sd(a47_5, b47));
#endif
_mm_store_sd(&C[(i*56)+30], c47_5);
__m128d c47_6 = _mm_load_sd(&C[(i*56)+32]);
__m128d a47_6 = _mm_load_sd(&values[263]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_6 = _mm_add_sd(c47_6, _mm_mul_sd(a47_6, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_6 = _mm_add_sd(c47_6, _mm_mul_sd(a47_6, b47));
#endif
_mm_store_sd(&C[(i*56)+32], c47_6);
#else
C[(i*56)+1] += values[257] * B[(i*56)+47];
C[(i*56)+5] += values[258] * B[(i*56)+47];
C[(i*56)+7] += values[259] * B[(i*56)+47];
C[(i*56)+15] += values[260] * B[(i*56)+47];
C[(i*56)+17] += values[261] * B[(i*56)+47];
C[(i*56)+30] += values[262] * B[(i*56)+47];
C[(i*56)+32] += values[263] * B[(i*56)+47];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b48 = _mm256_broadcast_sd(&B[(i*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b48 = _mm_loaddup_pd(&B[(i*56)+48]);
#endif
__m128d c48_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a48_0 = _mm_load_sd(&values[264]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_0 = _mm_add_sd(c48_0, _mm_mul_sd(a48_0, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_0 = _mm_add_sd(c48_0, _mm_mul_sd(a48_0, b48));
#endif
_mm_store_sd(&C[(i*56)+0], c48_0);
__m128d c48_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a48_1 = _mm_loadu_pd(&values[265]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_1 = _mm_add_pd(c48_1, _mm_mul_pd(a48_1, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_1 = _mm_add_pd(c48_1, _mm_mul_pd(a48_1, b48));
#endif
_mm_storeu_pd(&C[(i*56)+2], c48_1);
__m128d c48_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a48_3 = _mm_load_sd(&values[267]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_3 = _mm_add_sd(c48_3, _mm_mul_sd(a48_3, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_3 = _mm_add_sd(c48_3, _mm_mul_sd(a48_3, b48));
#endif
_mm_store_sd(&C[(i*56)+6], c48_3);
__m128d c48_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a48_4 = _mm_loadu_pd(&values[268]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_4 = _mm_add_pd(c48_4, _mm_mul_pd(a48_4, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_4 = _mm_add_pd(c48_4, _mm_mul_pd(a48_4, b48));
#endif
_mm_storeu_pd(&C[(i*56)+8], c48_4);
__m128d c48_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a48_6 = _mm_load_sd(&values[270]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_6 = _mm_add_sd(c48_6, _mm_mul_sd(a48_6, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_6 = _mm_add_sd(c48_6, _mm_mul_sd(a48_6, b48));
#endif
_mm_store_sd(&C[(i*56)+16], c48_6);
__m128d c48_7 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a48_7 = _mm_loadu_pd(&values[271]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_7 = _mm_add_pd(c48_7, _mm_mul_pd(a48_7, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_7 = _mm_add_pd(c48_7, _mm_mul_pd(a48_7, b48));
#endif
_mm_storeu_pd(&C[(i*56)+18], c48_7);
__m128d c48_9 = _mm_load_sd(&C[(i*56)+31]);
__m128d a48_9 = _mm_load_sd(&values[273]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_9 = _mm_add_sd(c48_9, _mm_mul_sd(a48_9, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_9 = _mm_add_sd(c48_9, _mm_mul_sd(a48_9, b48));
#endif
_mm_store_sd(&C[(i*56)+31], c48_9);
__m128d c48_10 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a48_10 = _mm_loadu_pd(&values[274]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_10 = _mm_add_pd(c48_10, _mm_mul_pd(a48_10, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_10 = _mm_add_pd(c48_10, _mm_mul_pd(a48_10, b48));
#endif
_mm_storeu_pd(&C[(i*56)+33], c48_10);
#else
C[(i*56)+0] += values[264] * B[(i*56)+48];
C[(i*56)+2] += values[265] * B[(i*56)+48];
C[(i*56)+3] += values[266] * B[(i*56)+48];
C[(i*56)+6] += values[267] * B[(i*56)+48];
C[(i*56)+8] += values[268] * B[(i*56)+48];
C[(i*56)+9] += values[269] * B[(i*56)+48];
C[(i*56)+16] += values[270] * B[(i*56)+48];
C[(i*56)+18] += values[271] * B[(i*56)+48];
C[(i*56)+19] += values[272] * B[(i*56)+48];
C[(i*56)+31] += values[273] * B[(i*56)+48];
C[(i*56)+33] += values[274] * B[(i*56)+48];
C[(i*56)+34] += values[275] * B[(i*56)+48];
#endif
#ifndef NDEBUG
num_flops += 24;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b50 = _mm256_broadcast_sd(&B[(i*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b50 = _mm_loaddup_pd(&B[(i*56)+50]);
#endif
__m128d c50_0 = _mm_load_sd(&C[(i*56)+1]);
__m128d a50_0 = _mm_load_sd(&values[276]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_0 = _mm_add_sd(c50_0, _mm_mul_sd(a50_0, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_0 = _mm_add_sd(c50_0, _mm_mul_sd(a50_0, b50));
#endif
_mm_store_sd(&C[(i*56)+1], c50_0);
__m128d c50_1 = _mm_load_sd(&C[(i*56)+7]);
__m128d a50_1 = _mm_load_sd(&values[277]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_1 = _mm_add_sd(c50_1, _mm_mul_sd(a50_1, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_1 = _mm_add_sd(c50_1, _mm_mul_sd(a50_1, b50));
#endif
_mm_store_sd(&C[(i*56)+7], c50_1);
__m128d c50_2 = _mm_load_sd(&C[(i*56)+17]);
__m128d a50_2 = _mm_load_sd(&values[278]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_2 = _mm_add_sd(c50_2, _mm_mul_sd(a50_2, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_2 = _mm_add_sd(c50_2, _mm_mul_sd(a50_2, b50));
#endif
_mm_store_sd(&C[(i*56)+17], c50_2);
__m128d c50_3 = _mm_load_sd(&C[(i*56)+32]);
__m128d a50_3 = _mm_load_sd(&values[279]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_3 = _mm_add_sd(c50_3, _mm_mul_sd(a50_3, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_3 = _mm_add_sd(c50_3, _mm_mul_sd(a50_3, b50));
#endif
_mm_store_sd(&C[(i*56)+32], c50_3);
#else
C[(i*56)+1] += values[276] * B[(i*56)+50];
C[(i*56)+7] += values[277] * B[(i*56)+50];
C[(i*56)+17] += values[278] * B[(i*56)+50];
C[(i*56)+32] += values[279] * B[(i*56)+50];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b51 = _mm256_broadcast_sd(&B[(i*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b51 = _mm_loaddup_pd(&B[(i*56)+51]);
#endif
__m128d c51_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a51_0 = _mm_load_sd(&values[280]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_0 = _mm_add_sd(c51_0, _mm_mul_sd(a51_0, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_0 = _mm_add_sd(c51_0, _mm_mul_sd(a51_0, b51));
#endif
_mm_store_sd(&C[(i*56)+0], c51_0);
__m128d c51_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a51_1 = _mm_loadu_pd(&values[281]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_1 = _mm_add_pd(c51_1, _mm_mul_pd(a51_1, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_1 = _mm_add_pd(c51_1, _mm_mul_pd(a51_1, b51));
#endif
_mm_storeu_pd(&C[(i*56)+2], c51_1);
__m128d c51_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a51_3 = _mm_loadu_pd(&values[283]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_3 = _mm_add_pd(c51_3, _mm_mul_pd(a51_3, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_3 = _mm_add_pd(c51_3, _mm_mul_pd(a51_3, b51));
#endif
_mm_storeu_pd(&C[(i*56)+8], c51_3);
__m128d c51_5 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a51_5 = _mm_loadu_pd(&values[285]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_5 = _mm_add_pd(c51_5, _mm_mul_pd(a51_5, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_5 = _mm_add_pd(c51_5, _mm_mul_pd(a51_5, b51));
#endif
_mm_storeu_pd(&C[(i*56)+18], c51_5);
__m128d c51_7 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a51_7 = _mm_loadu_pd(&values[287]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_7 = _mm_add_pd(c51_7, _mm_mul_pd(a51_7, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_7 = _mm_add_pd(c51_7, _mm_mul_pd(a51_7, b51));
#endif
_mm_storeu_pd(&C[(i*56)+33], c51_7);
#else
C[(i*56)+0] += values[280] * B[(i*56)+51];
C[(i*56)+2] += values[281] * B[(i*56)+51];
C[(i*56)+3] += values[282] * B[(i*56)+51];
C[(i*56)+8] += values[283] * B[(i*56)+51];
C[(i*56)+9] += values[284] * B[(i*56)+51];
C[(i*56)+18] += values[285] * B[(i*56)+51];
C[(i*56)+19] += values[286] * B[(i*56)+51];
C[(i*56)+33] += values[287] * B[(i*56)+51];
C[(i*56)+34] += values[288] * B[(i*56)+51];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b53 = _mm256_broadcast_sd(&B[(i*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b53 = _mm_loaddup_pd(&B[(i*56)+53]);
#endif
__m128d c53_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a53_0 = _mm_load_sd(&values[289]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_0 = _mm_add_sd(c53_0, _mm_mul_sd(a53_0, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_0 = _mm_add_sd(c53_0, _mm_mul_sd(a53_0, b53));
#endif
_mm_store_sd(&C[(i*56)+0], c53_0);
__m128d c53_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a53_1 = _mm_load_sd(&values[290]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_1 = _mm_add_sd(c53_1, _mm_mul_sd(a53_1, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_1 = _mm_add_sd(c53_1, _mm_mul_sd(a53_1, b53));
#endif
_mm_store_sd(&C[(i*56)+3], c53_1);
__m128d c53_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a53_2 = _mm_load_sd(&values[291]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_2 = _mm_add_sd(c53_2, _mm_mul_sd(a53_2, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_2 = _mm_add_sd(c53_2, _mm_mul_sd(a53_2, b53));
#endif
_mm_store_sd(&C[(i*56)+9], c53_2);
__m128d c53_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a53_3 = _mm_load_sd(&values[292]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_3 = _mm_add_sd(c53_3, _mm_mul_sd(a53_3, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_3 = _mm_add_sd(c53_3, _mm_mul_sd(a53_3, b53));
#endif
_mm_store_sd(&C[(i*56)+19], c53_3);
__m128d c53_4 = _mm_load_sd(&C[(i*56)+34]);
__m128d a53_4 = _mm_load_sd(&values[293]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_4 = _mm_add_sd(c53_4, _mm_mul_sd(a53_4, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_4 = _mm_add_sd(c53_4, _mm_mul_sd(a53_4, b53));
#endif
_mm_store_sd(&C[(i*56)+34], c53_4);
#else
C[(i*56)+0] += values[289] * B[(i*56)+53];
C[(i*56)+3] += values[290] * B[(i*56)+53];
C[(i*56)+9] += values[291] * B[(i*56)+53];
C[(i*56)+19] += values[292] * B[(i*56)+53];
C[(i*56)+34] += values[293] * B[(i*56)+53];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kEtaDivMT_9_56(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*56)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*56)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*56)+0], c1_0);
#else
C[(i*56)+0] += values[0] * B[(i*56)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*56)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*56)+0], c2_0);
#else
C[(i*56)+0] += values[1] * B[(i*56)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[2]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*56)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[2]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*56)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[4]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*56)+2], c4_2);
#endif
#else
C[(i*56)+0] += values[2] * B[(i*56)+4];
C[(i*56)+1] += values[3] * B[(i*56)+4];
C[(i*56)+2] += values[4] * B[(i*56)+4];
C[(i*56)+3] += values[5] * B[(i*56)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[6]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*56)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[6]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*56)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[8]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*56)+2], c5_2);
#endif
#else
C[(i*56)+0] += values[6] * B[(i*56)+5];
C[(i*56)+1] += values[7] * B[(i*56)+5];
C[(i*56)+2] += values[8] * B[(i*56)+5];
C[(i*56)+3] += values[9] * B[(i*56)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*56)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a6_0 = _mm_load_sd(&values[10]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*56)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*56)+2], c6_1);
#else
C[(i*56)+0] += values[10] * B[(i*56)+6];
C[(i*56)+2] += values[11] * B[(i*56)+6];
C[(i*56)+3] += values[12] * B[(i*56)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*56)+7]);
#endif
__m128d c7_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a7_0 = _mm_load_sd(&values[13]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_sd(c7_0, _mm_mul_sd(a7_0, b7));
#endif
_mm_store_sd(&C[(i*56)+0], c7_0);
__m128d c7_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a7_1 = _mm_load_sd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_1 = _mm_add_sd(c7_1, _mm_mul_sd(a7_1, b7));
#endif
_mm_store_sd(&C[(i*56)+3], c7_1);
#else
C[(i*56)+0] += values[13] * B[(i*56)+7];
C[(i*56)+3] += values[14] * B[(i*56)+7];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*56)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a8_0 = _mm_load_sd(&values[15]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*56)+0], c8_0);
__m128d c8_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a8_1 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_sd(c8_1, _mm_mul_sd(a8_1, b8));
#endif
_mm_store_sd(&C[(i*56)+3], c8_1);
#else
C[(i*56)+0] += values[15] * B[(i*56)+8];
C[(i*56)+3] += values[16] * B[(i*56)+8];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a10_0 = _mm256_loadu_pd(&values[17]);
c10_0 = _mm256_add_pd(c10_0, _mm256_mul_pd(a10_0, b10));
_mm256_storeu_pd(&C[(i*56)+0], c10_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a10_0 = _mm_loadu_pd(&values[17]);
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
_mm_storeu_pd(&C[(i*56)+0], c10_0);
__m128d c10_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a10_2 = _mm_loadu_pd(&values[19]);
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, b10));
_mm_storeu_pd(&C[(i*56)+2], c10_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a10_4 = _mm256_loadu_pd(&values[21]);
c10_4 = _mm256_add_pd(c10_4, _mm256_mul_pd(a10_4, b10));
_mm256_storeu_pd(&C[(i*56)+4], c10_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a10_4 = _mm_loadu_pd(&values[21]);
c10_4 = _mm_add_pd(c10_4, _mm_mul_pd(a10_4, b10));
_mm_storeu_pd(&C[(i*56)+4], c10_4);
__m128d c10_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a10_6 = _mm_loadu_pd(&values[23]);
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, b10));
_mm_storeu_pd(&C[(i*56)+6], c10_6);
#endif
__m128d c10_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a10_8 = _mm_loadu_pd(&values[25]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, b10));
#endif
_mm_storeu_pd(&C[(i*56)+8], c10_8);
#else
C[(i*56)+0] += values[17] * B[(i*56)+10];
C[(i*56)+1] += values[18] * B[(i*56)+10];
C[(i*56)+2] += values[19] * B[(i*56)+10];
C[(i*56)+3] += values[20] * B[(i*56)+10];
C[(i*56)+4] += values[21] * B[(i*56)+10];
C[(i*56)+5] += values[22] * B[(i*56)+10];
C[(i*56)+6] += values[23] * B[(i*56)+10];
C[(i*56)+7] += values[24] * B[(i*56)+10];
C[(i*56)+8] += values[25] * B[(i*56)+10];
C[(i*56)+9] += values[26] * B[(i*56)+10];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a11_0 = _mm256_loadu_pd(&values[27]);
c11_0 = _mm256_add_pd(c11_0, _mm256_mul_pd(a11_0, b11));
_mm256_storeu_pd(&C[(i*56)+0], c11_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a11_0 = _mm_loadu_pd(&values[27]);
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
_mm_storeu_pd(&C[(i*56)+0], c11_0);
__m128d c11_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a11_2 = _mm_loadu_pd(&values[29]);
c11_2 = _mm_add_pd(c11_2, _mm_mul_pd(a11_2, b11));
_mm_storeu_pd(&C[(i*56)+2], c11_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a11_4 = _mm256_loadu_pd(&values[31]);
c11_4 = _mm256_add_pd(c11_4, _mm256_mul_pd(a11_4, b11));
_mm256_storeu_pd(&C[(i*56)+4], c11_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a11_4 = _mm_loadu_pd(&values[31]);
c11_4 = _mm_add_pd(c11_4, _mm_mul_pd(a11_4, b11));
_mm_storeu_pd(&C[(i*56)+4], c11_4);
__m128d c11_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a11_6 = _mm_loadu_pd(&values[33]);
c11_6 = _mm_add_pd(c11_6, _mm_mul_pd(a11_6, b11));
_mm_storeu_pd(&C[(i*56)+6], c11_6);
#endif
__m128d c11_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a11_8 = _mm_loadu_pd(&values[35]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, b11));
#endif
_mm_storeu_pd(&C[(i*56)+8], c11_8);
#else
C[(i*56)+0] += values[27] * B[(i*56)+11];
C[(i*56)+1] += values[28] * B[(i*56)+11];
C[(i*56)+2] += values[29] * B[(i*56)+11];
C[(i*56)+3] += values[30] * B[(i*56)+11];
C[(i*56)+4] += values[31] * B[(i*56)+11];
C[(i*56)+5] += values[32] * B[(i*56)+11];
C[(i*56)+6] += values[33] * B[(i*56)+11];
C[(i*56)+7] += values[34] * B[(i*56)+11];
C[(i*56)+8] += values[35] * B[(i*56)+11];
C[(i*56)+9] += values[36] * B[(i*56)+11];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a12_0 = _mm256_loadu_pd(&values[37]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*56)+0], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a12_0 = _mm_loadu_pd(&values[37]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*56)+0], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a12_2 = _mm_loadu_pd(&values[39]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*56)+2], c12_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a12_4 = _mm256_loadu_pd(&values[41]);
c12_4 = _mm256_add_pd(c12_4, _mm256_mul_pd(a12_4, b12));
_mm256_storeu_pd(&C[(i*56)+5], c12_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a12_4 = _mm_loadu_pd(&values[41]);
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
_mm_storeu_pd(&C[(i*56)+5], c12_4);
__m128d c12_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a12_6 = _mm_loadu_pd(&values[43]);
c12_6 = _mm_add_pd(c12_6, _mm_mul_pd(a12_6, b12));
_mm_storeu_pd(&C[(i*56)+7], c12_6);
#endif
__m128d c12_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a12_8 = _mm_load_sd(&values[45]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, b12));
#endif
_mm_store_sd(&C[(i*56)+9], c12_8);
#else
C[(i*56)+0] += values[37] * B[(i*56)+12];
C[(i*56)+1] += values[38] * B[(i*56)+12];
C[(i*56)+2] += values[39] * B[(i*56)+12];
C[(i*56)+3] += values[40] * B[(i*56)+12];
C[(i*56)+5] += values[41] * B[(i*56)+12];
C[(i*56)+6] += values[42] * B[(i*56)+12];
C[(i*56)+7] += values[43] * B[(i*56)+12];
C[(i*56)+8] += values[44] * B[(i*56)+12];
C[(i*56)+9] += values[45] * B[(i*56)+12];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*56)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a13_0 = _mm_load_sd(&values[46]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*56)+0], c13_0);
__m128d c13_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a13_1 = _mm_loadu_pd(&values[47]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, b13));
#endif
_mm_storeu_pd(&C[(i*56)+2], c13_1);
__m128d c13_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a13_3 = _mm_load_sd(&values[49]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, b13));
#endif
_mm_store_sd(&C[(i*56)+6], c13_3);
__m128d c13_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a13_4 = _mm_loadu_pd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, b13));
#endif
_mm_storeu_pd(&C[(i*56)+8], c13_4);
#else
C[(i*56)+0] += values[46] * B[(i*56)+13];
C[(i*56)+2] += values[47] * B[(i*56)+13];
C[(i*56)+3] += values[48] * B[(i*56)+13];
C[(i*56)+6] += values[49] * B[(i*56)+13];
C[(i*56)+8] += values[50] * B[(i*56)+13];
C[(i*56)+9] += values[51] * B[(i*56)+13];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a14_0 = _mm256_loadu_pd(&values[52]);
c14_0 = _mm256_add_pd(c14_0, _mm256_mul_pd(a14_0, b14));
_mm256_storeu_pd(&C[(i*56)+0], c14_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a14_0 = _mm_loadu_pd(&values[52]);
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
_mm_storeu_pd(&C[(i*56)+0], c14_0);
__m128d c14_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a14_2 = _mm_loadu_pd(&values[54]);
c14_2 = _mm_add_pd(c14_2, _mm_mul_pd(a14_2, b14));
_mm_storeu_pd(&C[(i*56)+2], c14_2);
#endif
__m128d c14_4 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a14_4 = _mm_loadu_pd(&values[56]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_4 = _mm_add_pd(c14_4, _mm_mul_pd(a14_4, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_4 = _mm_add_pd(c14_4, _mm_mul_pd(a14_4, b14));
#endif
_mm_storeu_pd(&C[(i*56)+7], c14_4);
__m128d c14_6 = _mm_load_sd(&C[(i*56)+9]);
__m128d a14_6 = _mm_load_sd(&values[58]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_6 = _mm_add_sd(c14_6, _mm_mul_sd(a14_6, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_6 = _mm_add_sd(c14_6, _mm_mul_sd(a14_6, b14));
#endif
_mm_store_sd(&C[(i*56)+9], c14_6);
#else
C[(i*56)+0] += values[52] * B[(i*56)+14];
C[(i*56)+1] += values[53] * B[(i*56)+14];
C[(i*56)+2] += values[54] * B[(i*56)+14];
C[(i*56)+3] += values[55] * B[(i*56)+14];
C[(i*56)+7] += values[56] * B[(i*56)+14];
C[(i*56)+8] += values[57] * B[(i*56)+14];
C[(i*56)+9] += values[58] * B[(i*56)+14];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a15_0 = _mm256_loadu_pd(&values[59]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*56)+0], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a15_0 = _mm_loadu_pd(&values[59]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*56)+0], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a15_2 = _mm_loadu_pd(&values[61]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*56)+2], c15_2);
#endif
__m128d c15_4 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a15_4 = _mm_loadu_pd(&values[63]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_pd(c15_4, _mm_mul_pd(a15_4, b15));
#endif
_mm_storeu_pd(&C[(i*56)+7], c15_4);
__m128d c15_6 = _mm_load_sd(&C[(i*56)+9]);
__m128d a15_6 = _mm_load_sd(&values[65]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_6 = _mm_add_sd(c15_6, _mm_mul_sd(a15_6, b15));
#endif
_mm_store_sd(&C[(i*56)+9], c15_6);
#else
C[(i*56)+0] += values[59] * B[(i*56)+15];
C[(i*56)+1] += values[60] * B[(i*56)+15];
C[(i*56)+2] += values[61] * B[(i*56)+15];
C[(i*56)+3] += values[62] * B[(i*56)+15];
C[(i*56)+7] += values[63] * B[(i*56)+15];
C[(i*56)+8] += values[64] * B[(i*56)+15];
C[(i*56)+9] += values[65] * B[(i*56)+15];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*56)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a16_0 = _mm_load_sd(&values[66]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*56)+0], c16_0);
__m128d c16_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a16_1 = _mm_loadu_pd(&values[67]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, b16));
#endif
_mm_storeu_pd(&C[(i*56)+2], c16_1);
__m128d c16_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a16_3 = _mm_loadu_pd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_pd(c16_3, _mm_mul_pd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_pd(c16_3, _mm_mul_pd(a16_3, b16));
#endif
_mm_storeu_pd(&C[(i*56)+8], c16_3);
#else
C[(i*56)+0] += values[66] * B[(i*56)+16];
C[(i*56)+2] += values[67] * B[(i*56)+16];
C[(i*56)+3] += values[68] * B[(i*56)+16];
C[(i*56)+8] += values[69] * B[(i*56)+16];
C[(i*56)+9] += values[70] * B[(i*56)+16];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*56)+17]);
#endif
__m128d c17_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a17_0 = _mm_load_sd(&values[71]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_sd(c17_0, _mm_mul_sd(a17_0, b17));
#endif
_mm_store_sd(&C[(i*56)+0], c17_0);
__m128d c17_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a17_1 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_1 = _mm_add_sd(c17_1, _mm_mul_sd(a17_1, b17));
#endif
_mm_store_sd(&C[(i*56)+3], c17_1);
__m128d c17_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a17_2 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*56)+9], c17_2);
#else
C[(i*56)+0] += values[71] * B[(i*56)+17];
C[(i*56)+3] += values[72] * B[(i*56)+17];
C[(i*56)+9] += values[73] * B[(i*56)+17];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*56)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a18_0 = _mm_load_sd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*56)+0], c18_0);
__m128d c18_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a18_1 = _mm_load_sd(&values[75]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_sd(c18_1, _mm_mul_sd(a18_1, b18));
#endif
_mm_store_sd(&C[(i*56)+3], c18_1);
__m128d c18_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a18_2 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_2 = _mm_add_sd(c18_2, _mm_mul_sd(a18_2, b18));
#endif
_mm_store_sd(&C[(i*56)+9], c18_2);
#else
C[(i*56)+0] += values[74] * B[(i*56)+18];
C[(i*56)+3] += values[75] * B[(i*56)+18];
C[(i*56)+9] += values[76] * B[(i*56)+18];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 20, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a20_0 = _mm256_loadu_pd(&values[77]);
c20_0 = _mm256_add_pd(c20_0, _mm256_mul_pd(a20_0, b20));
_mm256_storeu_pd(&C[(i*56)+0], c20_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a20_0 = _mm_loadu_pd(&values[77]);
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, b20));
_mm_storeu_pd(&C[(i*56)+0], c20_0);
__m128d c20_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a20_2 = _mm_loadu_pd(&values[79]);
c20_2 = _mm_add_pd(c20_2, _mm_mul_pd(a20_2, b20));
_mm_storeu_pd(&C[(i*56)+2], c20_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a20_4 = _mm256_loadu_pd(&values[81]);
c20_4 = _mm256_add_pd(c20_4, _mm256_mul_pd(a20_4, b20));
_mm256_storeu_pd(&C[(i*56)+4], c20_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a20_4 = _mm_loadu_pd(&values[81]);
c20_4 = _mm_add_pd(c20_4, _mm_mul_pd(a20_4, b20));
_mm_storeu_pd(&C[(i*56)+4], c20_4);
__m128d c20_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a20_6 = _mm_loadu_pd(&values[83]);
c20_6 = _mm_add_pd(c20_6, _mm_mul_pd(a20_6, b20));
_mm_storeu_pd(&C[(i*56)+6], c20_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a20_8 = _mm256_loadu_pd(&values[85]);
c20_8 = _mm256_add_pd(c20_8, _mm256_mul_pd(a20_8, b20));
_mm256_storeu_pd(&C[(i*56)+8], c20_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a20_8 = _mm_loadu_pd(&values[85]);
c20_8 = _mm_add_pd(c20_8, _mm_mul_pd(a20_8, b20));
_mm_storeu_pd(&C[(i*56)+8], c20_8);
__m128d c20_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a20_10 = _mm_loadu_pd(&values[87]);
c20_10 = _mm_add_pd(c20_10, _mm_mul_pd(a20_10, b20));
_mm_storeu_pd(&C[(i*56)+10], c20_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a20_12 = _mm256_loadu_pd(&values[89]);
c20_12 = _mm256_add_pd(c20_12, _mm256_mul_pd(a20_12, b20));
_mm256_storeu_pd(&C[(i*56)+12], c20_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a20_12 = _mm_loadu_pd(&values[89]);
c20_12 = _mm_add_pd(c20_12, _mm_mul_pd(a20_12, b20));
_mm_storeu_pd(&C[(i*56)+12], c20_12);
__m128d c20_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a20_14 = _mm_loadu_pd(&values[91]);
c20_14 = _mm_add_pd(c20_14, _mm_mul_pd(a20_14, b20));
_mm_storeu_pd(&C[(i*56)+14], c20_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a20_16 = _mm256_loadu_pd(&values[93]);
c20_16 = _mm256_add_pd(c20_16, _mm256_mul_pd(a20_16, b20));
_mm256_storeu_pd(&C[(i*56)+16], c20_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a20_16 = _mm_loadu_pd(&values[93]);
c20_16 = _mm_add_pd(c20_16, _mm_mul_pd(a20_16, b20));
_mm_storeu_pd(&C[(i*56)+16], c20_16);
__m128d c20_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a20_18 = _mm_loadu_pd(&values[95]);
c20_18 = _mm_add_pd(c20_18, _mm_mul_pd(a20_18, b20));
_mm_storeu_pd(&C[(i*56)+18], c20_18);
#endif
#else
C[(i*56)+0] += values[77] * B[(i*56)+20];
C[(i*56)+1] += values[78] * B[(i*56)+20];
C[(i*56)+2] += values[79] * B[(i*56)+20];
C[(i*56)+3] += values[80] * B[(i*56)+20];
C[(i*56)+4] += values[81] * B[(i*56)+20];
C[(i*56)+5] += values[82] * B[(i*56)+20];
C[(i*56)+6] += values[83] * B[(i*56)+20];
C[(i*56)+7] += values[84] * B[(i*56)+20];
C[(i*56)+8] += values[85] * B[(i*56)+20];
C[(i*56)+9] += values[86] * B[(i*56)+20];
C[(i*56)+10] += values[87] * B[(i*56)+20];
C[(i*56)+11] += values[88] * B[(i*56)+20];
C[(i*56)+12] += values[89] * B[(i*56)+20];
C[(i*56)+13] += values[90] * B[(i*56)+20];
C[(i*56)+14] += values[91] * B[(i*56)+20];
C[(i*56)+15] += values[92] * B[(i*56)+20];
C[(i*56)+16] += values[93] * B[(i*56)+20];
C[(i*56)+17] += values[94] * B[(i*56)+20];
C[(i*56)+18] += values[95] * B[(i*56)+20];
C[(i*56)+19] += values[96] * B[(i*56)+20];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a21_0 = _mm256_loadu_pd(&values[97]);
c21_0 = _mm256_add_pd(c21_0, _mm256_mul_pd(a21_0, b21));
_mm256_storeu_pd(&C[(i*56)+0], c21_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a21_0 = _mm_loadu_pd(&values[97]);
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, b21));
_mm_storeu_pd(&C[(i*56)+0], c21_0);
__m128d c21_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a21_2 = _mm_loadu_pd(&values[99]);
c21_2 = _mm_add_pd(c21_2, _mm_mul_pd(a21_2, b21));
_mm_storeu_pd(&C[(i*56)+2], c21_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a21_4 = _mm256_loadu_pd(&values[101]);
c21_4 = _mm256_add_pd(c21_4, _mm256_mul_pd(a21_4, b21));
_mm256_storeu_pd(&C[(i*56)+4], c21_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a21_4 = _mm_loadu_pd(&values[101]);
c21_4 = _mm_add_pd(c21_4, _mm_mul_pd(a21_4, b21));
_mm_storeu_pd(&C[(i*56)+4], c21_4);
__m128d c21_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a21_6 = _mm_loadu_pd(&values[103]);
c21_6 = _mm_add_pd(c21_6, _mm_mul_pd(a21_6, b21));
_mm_storeu_pd(&C[(i*56)+6], c21_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a21_8 = _mm256_loadu_pd(&values[105]);
c21_8 = _mm256_add_pd(c21_8, _mm256_mul_pd(a21_8, b21));
_mm256_storeu_pd(&C[(i*56)+8], c21_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a21_8 = _mm_loadu_pd(&values[105]);
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, b21));
_mm_storeu_pd(&C[(i*56)+8], c21_8);
__m128d c21_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a21_10 = _mm_loadu_pd(&values[107]);
c21_10 = _mm_add_pd(c21_10, _mm_mul_pd(a21_10, b21));
_mm_storeu_pd(&C[(i*56)+10], c21_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a21_12 = _mm256_loadu_pd(&values[109]);
c21_12 = _mm256_add_pd(c21_12, _mm256_mul_pd(a21_12, b21));
_mm256_storeu_pd(&C[(i*56)+12], c21_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a21_12 = _mm_loadu_pd(&values[109]);
c21_12 = _mm_add_pd(c21_12, _mm_mul_pd(a21_12, b21));
_mm_storeu_pd(&C[(i*56)+12], c21_12);
__m128d c21_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a21_14 = _mm_loadu_pd(&values[111]);
c21_14 = _mm_add_pd(c21_14, _mm_mul_pd(a21_14, b21));
_mm_storeu_pd(&C[(i*56)+14], c21_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a21_16 = _mm256_loadu_pd(&values[113]);
c21_16 = _mm256_add_pd(c21_16, _mm256_mul_pd(a21_16, b21));
_mm256_storeu_pd(&C[(i*56)+16], c21_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a21_16 = _mm_loadu_pd(&values[113]);
c21_16 = _mm_add_pd(c21_16, _mm_mul_pd(a21_16, b21));
_mm_storeu_pd(&C[(i*56)+16], c21_16);
__m128d c21_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a21_18 = _mm_loadu_pd(&values[115]);
c21_18 = _mm_add_pd(c21_18, _mm_mul_pd(a21_18, b21));
_mm_storeu_pd(&C[(i*56)+18], c21_18);
#endif
#else
C[(i*56)+0] += values[97] * B[(i*56)+21];
C[(i*56)+1] += values[98] * B[(i*56)+21];
C[(i*56)+2] += values[99] * B[(i*56)+21];
C[(i*56)+3] += values[100] * B[(i*56)+21];
C[(i*56)+4] += values[101] * B[(i*56)+21];
C[(i*56)+5] += values[102] * B[(i*56)+21];
C[(i*56)+6] += values[103] * B[(i*56)+21];
C[(i*56)+7] += values[104] * B[(i*56)+21];
C[(i*56)+8] += values[105] * B[(i*56)+21];
C[(i*56)+9] += values[106] * B[(i*56)+21];
C[(i*56)+10] += values[107] * B[(i*56)+21];
C[(i*56)+11] += values[108] * B[(i*56)+21];
C[(i*56)+12] += values[109] * B[(i*56)+21];
C[(i*56)+13] += values[110] * B[(i*56)+21];
C[(i*56)+14] += values[111] * B[(i*56)+21];
C[(i*56)+15] += values[112] * B[(i*56)+21];
C[(i*56)+16] += values[113] * B[(i*56)+21];
C[(i*56)+17] += values[114] * B[(i*56)+21];
C[(i*56)+18] += values[115] * B[(i*56)+21];
C[(i*56)+19] += values[116] * B[(i*56)+21];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a22_0 = _mm256_loadu_pd(&values[117]);
c22_0 = _mm256_add_pd(c22_0, _mm256_mul_pd(a22_0, b22));
_mm256_storeu_pd(&C[(i*56)+0], c22_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a22_0 = _mm_loadu_pd(&values[117]);
c22_0 = _mm_add_pd(c22_0, _mm_mul_pd(a22_0, b22));
_mm_storeu_pd(&C[(i*56)+0], c22_0);
__m128d c22_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a22_2 = _mm_loadu_pd(&values[119]);
c22_2 = _mm_add_pd(c22_2, _mm_mul_pd(a22_2, b22));
_mm_storeu_pd(&C[(i*56)+2], c22_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a22_4 = _mm256_loadu_pd(&values[121]);
c22_4 = _mm256_add_pd(c22_4, _mm256_mul_pd(a22_4, b22));
_mm256_storeu_pd(&C[(i*56)+4], c22_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a22_4 = _mm_loadu_pd(&values[121]);
c22_4 = _mm_add_pd(c22_4, _mm_mul_pd(a22_4, b22));
_mm_storeu_pd(&C[(i*56)+4], c22_4);
__m128d c22_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a22_6 = _mm_loadu_pd(&values[123]);
c22_6 = _mm_add_pd(c22_6, _mm_mul_pd(a22_6, b22));
_mm_storeu_pd(&C[(i*56)+6], c22_6);
#endif
__m128d c22_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a22_8 = _mm_loadu_pd(&values[125]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, b22));
#endif
_mm_storeu_pd(&C[(i*56)+8], c22_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_10 = _mm256_loadu_pd(&C[(i*56)+11]);
__m256d a22_10 = _mm256_loadu_pd(&values[127]);
c22_10 = _mm256_add_pd(c22_10, _mm256_mul_pd(a22_10, b22));
_mm256_storeu_pd(&C[(i*56)+11], c22_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_10 = _mm_loadu_pd(&C[(i*56)+11]);
__m128d a22_10 = _mm_loadu_pd(&values[127]);
c22_10 = _mm_add_pd(c22_10, _mm_mul_pd(a22_10, b22));
_mm_storeu_pd(&C[(i*56)+11], c22_10);
__m128d c22_12 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a22_12 = _mm_loadu_pd(&values[129]);
c22_12 = _mm_add_pd(c22_12, _mm_mul_pd(a22_12, b22));
_mm_storeu_pd(&C[(i*56)+13], c22_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_14 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a22_14 = _mm256_loadu_pd(&values[131]);
c22_14 = _mm256_add_pd(c22_14, _mm256_mul_pd(a22_14, b22));
_mm256_storeu_pd(&C[(i*56)+15], c22_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_14 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a22_14 = _mm_loadu_pd(&values[131]);
c22_14 = _mm_add_pd(c22_14, _mm_mul_pd(a22_14, b22));
_mm_storeu_pd(&C[(i*56)+15], c22_14);
__m128d c22_16 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a22_16 = _mm_loadu_pd(&values[133]);
c22_16 = _mm_add_pd(c22_16, _mm_mul_pd(a22_16, b22));
_mm_storeu_pd(&C[(i*56)+17], c22_16);
#endif
__m128d c22_18 = _mm_load_sd(&C[(i*56)+19]);
__m128d a22_18 = _mm_load_sd(&values[135]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, b22));
#endif
_mm_store_sd(&C[(i*56)+19], c22_18);
#else
C[(i*56)+0] += values[117] * B[(i*56)+22];
C[(i*56)+1] += values[118] * B[(i*56)+22];
C[(i*56)+2] += values[119] * B[(i*56)+22];
C[(i*56)+3] += values[120] * B[(i*56)+22];
C[(i*56)+4] += values[121] * B[(i*56)+22];
C[(i*56)+5] += values[122] * B[(i*56)+22];
C[(i*56)+6] += values[123] * B[(i*56)+22];
C[(i*56)+7] += values[124] * B[(i*56)+22];
C[(i*56)+8] += values[125] * B[(i*56)+22];
C[(i*56)+9] += values[126] * B[(i*56)+22];
C[(i*56)+11] += values[127] * B[(i*56)+22];
C[(i*56)+12] += values[128] * B[(i*56)+22];
C[(i*56)+13] += values[129] * B[(i*56)+22];
C[(i*56)+14] += values[130] * B[(i*56)+22];
C[(i*56)+15] += values[131] * B[(i*56)+22];
C[(i*56)+16] += values[132] * B[(i*56)+22];
C[(i*56)+17] += values[133] * B[(i*56)+22];
C[(i*56)+18] += values[134] * B[(i*56)+22];
C[(i*56)+19] += values[135] * B[(i*56)+22];
#endif
#ifndef NDEBUG
num_flops += 38;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a23_0 = _mm256_loadu_pd(&values[136]);
c23_0 = _mm256_add_pd(c23_0, _mm256_mul_pd(a23_0, b23));
_mm256_storeu_pd(&C[(i*56)+0], c23_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a23_0 = _mm_loadu_pd(&values[136]);
c23_0 = _mm_add_pd(c23_0, _mm_mul_pd(a23_0, b23));
_mm_storeu_pd(&C[(i*56)+0], c23_0);
__m128d c23_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a23_2 = _mm_loadu_pd(&values[138]);
c23_2 = _mm_add_pd(c23_2, _mm_mul_pd(a23_2, b23));
_mm_storeu_pd(&C[(i*56)+2], c23_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a23_4 = _mm256_loadu_pd(&values[140]);
c23_4 = _mm256_add_pd(c23_4, _mm256_mul_pd(a23_4, b23));
_mm256_storeu_pd(&C[(i*56)+5], c23_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a23_4 = _mm_loadu_pd(&values[140]);
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, b23));
_mm_storeu_pd(&C[(i*56)+5], c23_4);
__m128d c23_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a23_6 = _mm_loadu_pd(&values[142]);
c23_6 = _mm_add_pd(c23_6, _mm_mul_pd(a23_6, b23));
_mm_storeu_pd(&C[(i*56)+7], c23_6);
#endif
__m128d c23_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a23_8 = _mm_load_sd(&values[144]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, b23));
#endif
_mm_store_sd(&C[(i*56)+9], c23_8);
__m128d c23_9 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a23_9 = _mm_loadu_pd(&values[145]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, b23));
#endif
_mm_storeu_pd(&C[(i*56)+12], c23_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_11 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a23_11 = _mm256_loadu_pd(&values[147]);
c23_11 = _mm256_add_pd(c23_11, _mm256_mul_pd(a23_11, b23));
_mm256_storeu_pd(&C[(i*56)+15], c23_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_11 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a23_11 = _mm_loadu_pd(&values[147]);
c23_11 = _mm_add_pd(c23_11, _mm_mul_pd(a23_11, b23));
_mm_storeu_pd(&C[(i*56)+15], c23_11);
__m128d c23_13 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a23_13 = _mm_loadu_pd(&values[149]);
c23_13 = _mm_add_pd(c23_13, _mm_mul_pd(a23_13, b23));
_mm_storeu_pd(&C[(i*56)+17], c23_13);
#endif
__m128d c23_15 = _mm_load_sd(&C[(i*56)+19]);
__m128d a23_15 = _mm_load_sd(&values[151]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, b23));
#endif
_mm_store_sd(&C[(i*56)+19], c23_15);
#else
C[(i*56)+0] += values[136] * B[(i*56)+23];
C[(i*56)+1] += values[137] * B[(i*56)+23];
C[(i*56)+2] += values[138] * B[(i*56)+23];
C[(i*56)+3] += values[139] * B[(i*56)+23];
C[(i*56)+5] += values[140] * B[(i*56)+23];
C[(i*56)+6] += values[141] * B[(i*56)+23];
C[(i*56)+7] += values[142] * B[(i*56)+23];
C[(i*56)+8] += values[143] * B[(i*56)+23];
C[(i*56)+9] += values[144] * B[(i*56)+23];
C[(i*56)+12] += values[145] * B[(i*56)+23];
C[(i*56)+13] += values[146] * B[(i*56)+23];
C[(i*56)+15] += values[147] * B[(i*56)+23];
C[(i*56)+16] += values[148] * B[(i*56)+23];
C[(i*56)+17] += values[149] * B[(i*56)+23];
C[(i*56)+18] += values[150] * B[(i*56)+23];
C[(i*56)+19] += values[151] * B[(i*56)+23];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b24 = _mm256_broadcast_sd(&B[(i*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b24 = _mm_loaddup_pd(&B[(i*56)+24]);
#endif
__m128d c24_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a24_0 = _mm_load_sd(&values[152]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, b24));
#endif
_mm_store_sd(&C[(i*56)+0], c24_0);
__m128d c24_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a24_1 = _mm_loadu_pd(&values[153]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, b24));
#endif
_mm_storeu_pd(&C[(i*56)+2], c24_1);
__m128d c24_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a24_3 = _mm_load_sd(&values[155]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, b24));
#endif
_mm_store_sd(&C[(i*56)+6], c24_3);
__m128d c24_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a24_4 = _mm_loadu_pd(&values[156]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, b24));
#endif
_mm_storeu_pd(&C[(i*56)+8], c24_4);
__m128d c24_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a24_6 = _mm_load_sd(&values[158]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, b24));
#endif
_mm_store_sd(&C[(i*56)+13], c24_6);
__m128d c24_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a24_7 = _mm_load_sd(&values[159]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, b24));
#endif
_mm_store_sd(&C[(i*56)+16], c24_7);
__m128d c24_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a24_8 = _mm_loadu_pd(&values[160]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, b24));
#endif
_mm_storeu_pd(&C[(i*56)+18], c24_8);
#else
C[(i*56)+0] += values[152] * B[(i*56)+24];
C[(i*56)+2] += values[153] * B[(i*56)+24];
C[(i*56)+3] += values[154] * B[(i*56)+24];
C[(i*56)+6] += values[155] * B[(i*56)+24];
C[(i*56)+8] += values[156] * B[(i*56)+24];
C[(i*56)+9] += values[157] * B[(i*56)+24];
C[(i*56)+13] += values[158] * B[(i*56)+24];
C[(i*56)+16] += values[159] * B[(i*56)+24];
C[(i*56)+18] += values[160] * B[(i*56)+24];
C[(i*56)+19] += values[161] * B[(i*56)+24];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a25_0 = _mm256_loadu_pd(&values[162]);
c25_0 = _mm256_add_pd(c25_0, _mm256_mul_pd(a25_0, b25));
_mm256_storeu_pd(&C[(i*56)+0], c25_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a25_0 = _mm_loadu_pd(&values[162]);
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, b25));
_mm_storeu_pd(&C[(i*56)+0], c25_0);
__m128d c25_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a25_2 = _mm_loadu_pd(&values[164]);
c25_2 = _mm_add_pd(c25_2, _mm_mul_pd(a25_2, b25));
_mm_storeu_pd(&C[(i*56)+2], c25_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a25_4 = _mm256_loadu_pd(&values[166]);
c25_4 = _mm256_add_pd(c25_4, _mm256_mul_pd(a25_4, b25));
_mm256_storeu_pd(&C[(i*56)+4], c25_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a25_4 = _mm_loadu_pd(&values[166]);
c25_4 = _mm_add_pd(c25_4, _mm_mul_pd(a25_4, b25));
_mm_storeu_pd(&C[(i*56)+4], c25_4);
__m128d c25_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a25_6 = _mm_loadu_pd(&values[168]);
c25_6 = _mm_add_pd(c25_6, _mm_mul_pd(a25_6, b25));
_mm_storeu_pd(&C[(i*56)+6], c25_6);
#endif
__m128d c25_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a25_8 = _mm_loadu_pd(&values[170]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, b25));
#endif
_mm_storeu_pd(&C[(i*56)+8], c25_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_10 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a25_10 = _mm256_loadu_pd(&values[172]);
c25_10 = _mm256_add_pd(c25_10, _mm256_mul_pd(a25_10, b25));
_mm256_storeu_pd(&C[(i*56)+14], c25_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_10 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a25_10 = _mm_loadu_pd(&values[172]);
c25_10 = _mm_add_pd(c25_10, _mm_mul_pd(a25_10, b25));
_mm_storeu_pd(&C[(i*56)+14], c25_10);
__m128d c25_12 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a25_12 = _mm_loadu_pd(&values[174]);
c25_12 = _mm_add_pd(c25_12, _mm_mul_pd(a25_12, b25));
_mm_storeu_pd(&C[(i*56)+16], c25_12);
#endif
__m128d c25_14 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a25_14 = _mm_loadu_pd(&values[176]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_14 = _mm_add_pd(c25_14, _mm_mul_pd(a25_14, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_14 = _mm_add_pd(c25_14, _mm_mul_pd(a25_14, b25));
#endif
_mm_storeu_pd(&C[(i*56)+18], c25_14);
#else
C[(i*56)+0] += values[162] * B[(i*56)+25];
C[(i*56)+1] += values[163] * B[(i*56)+25];
C[(i*56)+2] += values[164] * B[(i*56)+25];
C[(i*56)+3] += values[165] * B[(i*56)+25];
C[(i*56)+4] += values[166] * B[(i*56)+25];
C[(i*56)+5] += values[167] * B[(i*56)+25];
C[(i*56)+6] += values[168] * B[(i*56)+25];
C[(i*56)+7] += values[169] * B[(i*56)+25];
C[(i*56)+8] += values[170] * B[(i*56)+25];
C[(i*56)+9] += values[171] * B[(i*56)+25];
C[(i*56)+14] += values[172] * B[(i*56)+25];
C[(i*56)+15] += values[173] * B[(i*56)+25];
C[(i*56)+16] += values[174] * B[(i*56)+25];
C[(i*56)+17] += values[175] * B[(i*56)+25];
C[(i*56)+18] += values[176] * B[(i*56)+25];
C[(i*56)+19] += values[177] * B[(i*56)+25];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a26_0 = _mm256_loadu_pd(&values[178]);
c26_0 = _mm256_add_pd(c26_0, _mm256_mul_pd(a26_0, b26));
_mm256_storeu_pd(&C[(i*56)+0], c26_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a26_0 = _mm_loadu_pd(&values[178]);
c26_0 = _mm_add_pd(c26_0, _mm_mul_pd(a26_0, b26));
_mm_storeu_pd(&C[(i*56)+0], c26_0);
__m128d c26_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a26_2 = _mm_loadu_pd(&values[180]);
c26_2 = _mm_add_pd(c26_2, _mm_mul_pd(a26_2, b26));
_mm_storeu_pd(&C[(i*56)+2], c26_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a26_4 = _mm256_loadu_pd(&values[182]);
c26_4 = _mm256_add_pd(c26_4, _mm256_mul_pd(a26_4, b26));
_mm256_storeu_pd(&C[(i*56)+4], c26_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a26_4 = _mm_loadu_pd(&values[182]);
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, b26));
_mm_storeu_pd(&C[(i*56)+4], c26_4);
__m128d c26_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a26_6 = _mm_loadu_pd(&values[184]);
c26_6 = _mm_add_pd(c26_6, _mm_mul_pd(a26_6, b26));
_mm_storeu_pd(&C[(i*56)+6], c26_6);
#endif
__m128d c26_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a26_8 = _mm_loadu_pd(&values[186]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, b26));
#endif
_mm_storeu_pd(&C[(i*56)+8], c26_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_10 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a26_10 = _mm256_loadu_pd(&values[188]);
c26_10 = _mm256_add_pd(c26_10, _mm256_mul_pd(a26_10, b26));
_mm256_storeu_pd(&C[(i*56)+14], c26_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_10 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a26_10 = _mm_loadu_pd(&values[188]);
c26_10 = _mm_add_pd(c26_10, _mm_mul_pd(a26_10, b26));
_mm_storeu_pd(&C[(i*56)+14], c26_10);
__m128d c26_12 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a26_12 = _mm_loadu_pd(&values[190]);
c26_12 = _mm_add_pd(c26_12, _mm_mul_pd(a26_12, b26));
_mm_storeu_pd(&C[(i*56)+16], c26_12);
#endif
__m128d c26_14 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a26_14 = _mm_loadu_pd(&values[192]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_14 = _mm_add_pd(c26_14, _mm_mul_pd(a26_14, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_14 = _mm_add_pd(c26_14, _mm_mul_pd(a26_14, b26));
#endif
_mm_storeu_pd(&C[(i*56)+18], c26_14);
#else
C[(i*56)+0] += values[178] * B[(i*56)+26];
C[(i*56)+1] += values[179] * B[(i*56)+26];
C[(i*56)+2] += values[180] * B[(i*56)+26];
C[(i*56)+3] += values[181] * B[(i*56)+26];
C[(i*56)+4] += values[182] * B[(i*56)+26];
C[(i*56)+5] += values[183] * B[(i*56)+26];
C[(i*56)+6] += values[184] * B[(i*56)+26];
C[(i*56)+7] += values[185] * B[(i*56)+26];
C[(i*56)+8] += values[186] * B[(i*56)+26];
C[(i*56)+9] += values[187] * B[(i*56)+26];
C[(i*56)+14] += values[188] * B[(i*56)+26];
C[(i*56)+15] += values[189] * B[(i*56)+26];
C[(i*56)+16] += values[190] * B[(i*56)+26];
C[(i*56)+17] += values[191] * B[(i*56)+26];
C[(i*56)+18] += values[192] * B[(i*56)+26];
C[(i*56)+19] += values[193] * B[(i*56)+26];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a27_0 = _mm256_loadu_pd(&values[194]);
c27_0 = _mm256_add_pd(c27_0, _mm256_mul_pd(a27_0, b27));
_mm256_storeu_pd(&C[(i*56)+0], c27_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a27_0 = _mm_loadu_pd(&values[194]);
c27_0 = _mm_add_pd(c27_0, _mm_mul_pd(a27_0, b27));
_mm_storeu_pd(&C[(i*56)+0], c27_0);
__m128d c27_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a27_2 = _mm_loadu_pd(&values[196]);
c27_2 = _mm_add_pd(c27_2, _mm_mul_pd(a27_2, b27));
_mm_storeu_pd(&C[(i*56)+2], c27_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a27_4 = _mm256_loadu_pd(&values[198]);
c27_4 = _mm256_add_pd(c27_4, _mm256_mul_pd(a27_4, b27));
_mm256_storeu_pd(&C[(i*56)+5], c27_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a27_4 = _mm_loadu_pd(&values[198]);
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, b27));
_mm_storeu_pd(&C[(i*56)+5], c27_4);
__m128d c27_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a27_6 = _mm_loadu_pd(&values[200]);
c27_6 = _mm_add_pd(c27_6, _mm_mul_pd(a27_6, b27));
_mm_storeu_pd(&C[(i*56)+7], c27_6);
#endif
__m128d c27_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a27_8 = _mm_load_sd(&values[202]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, b27));
#endif
_mm_store_sd(&C[(i*56)+9], c27_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_9 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a27_9 = _mm256_loadu_pd(&values[203]);
c27_9 = _mm256_add_pd(c27_9, _mm256_mul_pd(a27_9, b27));
_mm256_storeu_pd(&C[(i*56)+15], c27_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_9 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a27_9 = _mm_loadu_pd(&values[203]);
c27_9 = _mm_add_pd(c27_9, _mm_mul_pd(a27_9, b27));
_mm_storeu_pd(&C[(i*56)+15], c27_9);
__m128d c27_11 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a27_11 = _mm_loadu_pd(&values[205]);
c27_11 = _mm_add_pd(c27_11, _mm_mul_pd(a27_11, b27));
_mm_storeu_pd(&C[(i*56)+17], c27_11);
#endif
__m128d c27_13 = _mm_load_sd(&C[(i*56)+19]);
__m128d a27_13 = _mm_load_sd(&values[207]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_13 = _mm_add_sd(c27_13, _mm_mul_sd(a27_13, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_13 = _mm_add_sd(c27_13, _mm_mul_sd(a27_13, b27));
#endif
_mm_store_sd(&C[(i*56)+19], c27_13);
#else
C[(i*56)+0] += values[194] * B[(i*56)+27];
C[(i*56)+1] += values[195] * B[(i*56)+27];
C[(i*56)+2] += values[196] * B[(i*56)+27];
C[(i*56)+3] += values[197] * B[(i*56)+27];
C[(i*56)+5] += values[198] * B[(i*56)+27];
C[(i*56)+6] += values[199] * B[(i*56)+27];
C[(i*56)+7] += values[200] * B[(i*56)+27];
C[(i*56)+8] += values[201] * B[(i*56)+27];
C[(i*56)+9] += values[202] * B[(i*56)+27];
C[(i*56)+15] += values[203] * B[(i*56)+27];
C[(i*56)+16] += values[204] * B[(i*56)+27];
C[(i*56)+17] += values[205] * B[(i*56)+27];
C[(i*56)+18] += values[206] * B[(i*56)+27];
C[(i*56)+19] += values[207] * B[(i*56)+27];
#endif
#ifndef NDEBUG
num_flops += 28;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b28 = _mm256_broadcast_sd(&B[(i*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b28 = _mm_loaddup_pd(&B[(i*56)+28]);
#endif
__m128d c28_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a28_0 = _mm_load_sd(&values[208]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, b28));
#endif
_mm_store_sd(&C[(i*56)+0], c28_0);
__m128d c28_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a28_1 = _mm_loadu_pd(&values[209]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, b28));
#endif
_mm_storeu_pd(&C[(i*56)+2], c28_1);
__m128d c28_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a28_3 = _mm_load_sd(&values[211]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, b28));
#endif
_mm_store_sd(&C[(i*56)+6], c28_3);
__m128d c28_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a28_4 = _mm_loadu_pd(&values[212]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, b28));
#endif
_mm_storeu_pd(&C[(i*56)+8], c28_4);
__m128d c28_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a28_6 = _mm_load_sd(&values[214]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, b28));
#endif
_mm_store_sd(&C[(i*56)+16], c28_6);
__m128d c28_7 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a28_7 = _mm_loadu_pd(&values[215]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_7 = _mm_add_pd(c28_7, _mm_mul_pd(a28_7, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_7 = _mm_add_pd(c28_7, _mm_mul_pd(a28_7, b28));
#endif
_mm_storeu_pd(&C[(i*56)+18], c28_7);
#else
C[(i*56)+0] += values[208] * B[(i*56)+28];
C[(i*56)+2] += values[209] * B[(i*56)+28];
C[(i*56)+3] += values[210] * B[(i*56)+28];
C[(i*56)+6] += values[211] * B[(i*56)+28];
C[(i*56)+8] += values[212] * B[(i*56)+28];
C[(i*56)+9] += values[213] * B[(i*56)+28];
C[(i*56)+16] += values[214] * B[(i*56)+28];
C[(i*56)+18] += values[215] * B[(i*56)+28];
C[(i*56)+19] += values[216] * B[(i*56)+28];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c29_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a29_0 = _mm256_loadu_pd(&values[217]);
c29_0 = _mm256_add_pd(c29_0, _mm256_mul_pd(a29_0, b29));
_mm256_storeu_pd(&C[(i*56)+0], c29_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c29_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a29_0 = _mm_loadu_pd(&values[217]);
c29_0 = _mm_add_pd(c29_0, _mm_mul_pd(a29_0, b29));
_mm_storeu_pd(&C[(i*56)+0], c29_0);
__m128d c29_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a29_2 = _mm_loadu_pd(&values[219]);
c29_2 = _mm_add_pd(c29_2, _mm_mul_pd(a29_2, b29));
_mm_storeu_pd(&C[(i*56)+2], c29_2);
#endif
__m128d c29_4 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a29_4 = _mm_loadu_pd(&values[221]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_4 = _mm_add_pd(c29_4, _mm_mul_pd(a29_4, b29));
#endif
_mm_storeu_pd(&C[(i*56)+7], c29_4);
__m128d c29_6 = _mm_load_sd(&C[(i*56)+9]);
__m128d a29_6 = _mm_load_sd(&values[223]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_6 = _mm_add_sd(c29_6, _mm_mul_sd(a29_6, b29));
#endif
_mm_store_sd(&C[(i*56)+9], c29_6);
__m128d c29_7 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a29_7 = _mm_loadu_pd(&values[224]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_7 = _mm_add_pd(c29_7, _mm_mul_pd(a29_7, b29));
#endif
_mm_storeu_pd(&C[(i*56)+17], c29_7);
__m128d c29_9 = _mm_load_sd(&C[(i*56)+19]);
__m128d a29_9 = _mm_load_sd(&values[226]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_9 = _mm_add_sd(c29_9, _mm_mul_sd(a29_9, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_9 = _mm_add_sd(c29_9, _mm_mul_sd(a29_9, b29));
#endif
_mm_store_sd(&C[(i*56)+19], c29_9);
#else
C[(i*56)+0] += values[217] * B[(i*56)+29];
C[(i*56)+1] += values[218] * B[(i*56)+29];
C[(i*56)+2] += values[219] * B[(i*56)+29];
C[(i*56)+3] += values[220] * B[(i*56)+29];
C[(i*56)+7] += values[221] * B[(i*56)+29];
C[(i*56)+8] += values[222] * B[(i*56)+29];
C[(i*56)+9] += values[223] * B[(i*56)+29];
C[(i*56)+17] += values[224] * B[(i*56)+29];
C[(i*56)+18] += values[225] * B[(i*56)+29];
C[(i*56)+19] += values[226] * B[(i*56)+29];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a30_0 = _mm256_loadu_pd(&values[227]);
c30_0 = _mm256_add_pd(c30_0, _mm256_mul_pd(a30_0, b30));
_mm256_storeu_pd(&C[(i*56)+0], c30_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a30_0 = _mm_loadu_pd(&values[227]);
c30_0 = _mm_add_pd(c30_0, _mm_mul_pd(a30_0, b30));
_mm_storeu_pd(&C[(i*56)+0], c30_0);
__m128d c30_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a30_2 = _mm_loadu_pd(&values[229]);
c30_2 = _mm_add_pd(c30_2, _mm_mul_pd(a30_2, b30));
_mm_storeu_pd(&C[(i*56)+2], c30_2);
#endif
__m128d c30_4 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a30_4 = _mm_loadu_pd(&values[231]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_4 = _mm_add_pd(c30_4, _mm_mul_pd(a30_4, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_4 = _mm_add_pd(c30_4, _mm_mul_pd(a30_4, b30));
#endif
_mm_storeu_pd(&C[(i*56)+7], c30_4);
__m128d c30_6 = _mm_load_sd(&C[(i*56)+9]);
__m128d a30_6 = _mm_load_sd(&values[233]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_6 = _mm_add_sd(c30_6, _mm_mul_sd(a30_6, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_6 = _mm_add_sd(c30_6, _mm_mul_sd(a30_6, b30));
#endif
_mm_store_sd(&C[(i*56)+9], c30_6);
__m128d c30_7 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a30_7 = _mm_loadu_pd(&values[234]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_7 = _mm_add_pd(c30_7, _mm_mul_pd(a30_7, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_7 = _mm_add_pd(c30_7, _mm_mul_pd(a30_7, b30));
#endif
_mm_storeu_pd(&C[(i*56)+17], c30_7);
__m128d c30_9 = _mm_load_sd(&C[(i*56)+19]);
__m128d a30_9 = _mm_load_sd(&values[236]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_9 = _mm_add_sd(c30_9, _mm_mul_sd(a30_9, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_9 = _mm_add_sd(c30_9, _mm_mul_sd(a30_9, b30));
#endif
_mm_store_sd(&C[(i*56)+19], c30_9);
#else
C[(i*56)+0] += values[227] * B[(i*56)+30];
C[(i*56)+1] += values[228] * B[(i*56)+30];
C[(i*56)+2] += values[229] * B[(i*56)+30];
C[(i*56)+3] += values[230] * B[(i*56)+30];
C[(i*56)+7] += values[231] * B[(i*56)+30];
C[(i*56)+8] += values[232] * B[(i*56)+30];
C[(i*56)+9] += values[233] * B[(i*56)+30];
C[(i*56)+17] += values[234] * B[(i*56)+30];
C[(i*56)+18] += values[235] * B[(i*56)+30];
C[(i*56)+19] += values[236] * B[(i*56)+30];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b31 = _mm256_broadcast_sd(&B[(i*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b31 = _mm_loaddup_pd(&B[(i*56)+31]);
#endif
__m128d c31_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a31_0 = _mm_load_sd(&values[237]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, b31));
#endif
_mm_store_sd(&C[(i*56)+0], c31_0);
__m128d c31_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a31_1 = _mm_loadu_pd(&values[238]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, b31));
#endif
_mm_storeu_pd(&C[(i*56)+2], c31_1);
__m128d c31_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a31_3 = _mm_loadu_pd(&values[240]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_3 = _mm_add_pd(c31_3, _mm_mul_pd(a31_3, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_3 = _mm_add_pd(c31_3, _mm_mul_pd(a31_3, b31));
#endif
_mm_storeu_pd(&C[(i*56)+8], c31_3);
__m128d c31_5 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a31_5 = _mm_loadu_pd(&values[242]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_5 = _mm_add_pd(c31_5, _mm_mul_pd(a31_5, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_5 = _mm_add_pd(c31_5, _mm_mul_pd(a31_5, b31));
#endif
_mm_storeu_pd(&C[(i*56)+18], c31_5);
#else
C[(i*56)+0] += values[237] * B[(i*56)+31];
C[(i*56)+2] += values[238] * B[(i*56)+31];
C[(i*56)+3] += values[239] * B[(i*56)+31];
C[(i*56)+8] += values[240] * B[(i*56)+31];
C[(i*56)+9] += values[241] * B[(i*56)+31];
C[(i*56)+18] += values[242] * B[(i*56)+31];
C[(i*56)+19] += values[243] * B[(i*56)+31];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*56)+32]);
#endif
__m128d c32_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a32_0 = _mm_load_sd(&values[244]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_0 = _mm_add_sd(c32_0, _mm_mul_sd(a32_0, b32));
#endif
_mm_store_sd(&C[(i*56)+0], c32_0);
__m128d c32_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a32_1 = _mm_load_sd(&values[245]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_1 = _mm_add_sd(c32_1, _mm_mul_sd(a32_1, b32));
#endif
_mm_store_sd(&C[(i*56)+3], c32_1);
__m128d c32_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a32_2 = _mm_load_sd(&values[246]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, b32));
#endif
_mm_store_sd(&C[(i*56)+9], c32_2);
__m128d c32_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a32_3 = _mm_load_sd(&values[247]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, b32));
#endif
_mm_store_sd(&C[(i*56)+19], c32_3);
#else
C[(i*56)+0] += values[244] * B[(i*56)+32];
C[(i*56)+3] += values[245] * B[(i*56)+32];
C[(i*56)+9] += values[246] * B[(i*56)+32];
C[(i*56)+19] += values[247] * B[(i*56)+32];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b33 = _mm256_broadcast_sd(&B[(i*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b33 = _mm_loaddup_pd(&B[(i*56)+33]);
#endif
__m128d c33_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a33_0 = _mm_load_sd(&values[248]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, b33));
#endif
_mm_store_sd(&C[(i*56)+0], c33_0);
__m128d c33_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a33_1 = _mm_load_sd(&values[249]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_1 = _mm_add_sd(c33_1, _mm_mul_sd(a33_1, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_1 = _mm_add_sd(c33_1, _mm_mul_sd(a33_1, b33));
#endif
_mm_store_sd(&C[(i*56)+3], c33_1);
__m128d c33_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a33_2 = _mm_load_sd(&values[250]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_2 = _mm_add_sd(c33_2, _mm_mul_sd(a33_2, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_2 = _mm_add_sd(c33_2, _mm_mul_sd(a33_2, b33));
#endif
_mm_store_sd(&C[(i*56)+9], c33_2);
__m128d c33_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a33_3 = _mm_load_sd(&values[251]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_3 = _mm_add_sd(c33_3, _mm_mul_sd(a33_3, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_3 = _mm_add_sd(c33_3, _mm_mul_sd(a33_3, b33));
#endif
_mm_store_sd(&C[(i*56)+19], c33_3);
#else
C[(i*56)+0] += values[248] * B[(i*56)+33];
C[(i*56)+3] += values[249] * B[(i*56)+33];
C[(i*56)+9] += values[250] * B[(i*56)+33];
C[(i*56)+19] += values[251] * B[(i*56)+33];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 35, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b35 = _mm256_broadcast_sd(&B[(i*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b35 = _mm_loaddup_pd(&B[(i*56)+35]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a35_0 = _mm256_loadu_pd(&values[252]);
c35_0 = _mm256_add_pd(c35_0, _mm256_mul_pd(a35_0, b35));
_mm256_storeu_pd(&C[(i*56)+0], c35_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a35_0 = _mm_loadu_pd(&values[252]);
c35_0 = _mm_add_pd(c35_0, _mm_mul_pd(a35_0, b35));
_mm_storeu_pd(&C[(i*56)+0], c35_0);
__m128d c35_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a35_2 = _mm_loadu_pd(&values[254]);
c35_2 = _mm_add_pd(c35_2, _mm_mul_pd(a35_2, b35));
_mm_storeu_pd(&C[(i*56)+2], c35_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a35_4 = _mm256_loadu_pd(&values[256]);
c35_4 = _mm256_add_pd(c35_4, _mm256_mul_pd(a35_4, b35));
_mm256_storeu_pd(&C[(i*56)+4], c35_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a35_4 = _mm_loadu_pd(&values[256]);
c35_4 = _mm_add_pd(c35_4, _mm_mul_pd(a35_4, b35));
_mm_storeu_pd(&C[(i*56)+4], c35_4);
__m128d c35_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a35_6 = _mm_loadu_pd(&values[258]);
c35_6 = _mm_add_pd(c35_6, _mm_mul_pd(a35_6, b35));
_mm_storeu_pd(&C[(i*56)+6], c35_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a35_8 = _mm256_loadu_pd(&values[260]);
c35_8 = _mm256_add_pd(c35_8, _mm256_mul_pd(a35_8, b35));
_mm256_storeu_pd(&C[(i*56)+8], c35_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a35_8 = _mm_loadu_pd(&values[260]);
c35_8 = _mm_add_pd(c35_8, _mm_mul_pd(a35_8, b35));
_mm_storeu_pd(&C[(i*56)+8], c35_8);
__m128d c35_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a35_10 = _mm_loadu_pd(&values[262]);
c35_10 = _mm_add_pd(c35_10, _mm_mul_pd(a35_10, b35));
_mm_storeu_pd(&C[(i*56)+10], c35_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a35_12 = _mm256_loadu_pd(&values[264]);
c35_12 = _mm256_add_pd(c35_12, _mm256_mul_pd(a35_12, b35));
_mm256_storeu_pd(&C[(i*56)+12], c35_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a35_12 = _mm_loadu_pd(&values[264]);
c35_12 = _mm_add_pd(c35_12, _mm_mul_pd(a35_12, b35));
_mm_storeu_pd(&C[(i*56)+12], c35_12);
__m128d c35_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a35_14 = _mm_loadu_pd(&values[266]);
c35_14 = _mm_add_pd(c35_14, _mm_mul_pd(a35_14, b35));
_mm_storeu_pd(&C[(i*56)+14], c35_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a35_16 = _mm256_loadu_pd(&values[268]);
c35_16 = _mm256_add_pd(c35_16, _mm256_mul_pd(a35_16, b35));
_mm256_storeu_pd(&C[(i*56)+16], c35_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a35_16 = _mm_loadu_pd(&values[268]);
c35_16 = _mm_add_pd(c35_16, _mm_mul_pd(a35_16, b35));
_mm_storeu_pd(&C[(i*56)+16], c35_16);
__m128d c35_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a35_18 = _mm_loadu_pd(&values[270]);
c35_18 = _mm_add_pd(c35_18, _mm_mul_pd(a35_18, b35));
_mm_storeu_pd(&C[(i*56)+18], c35_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_20 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a35_20 = _mm256_loadu_pd(&values[272]);
c35_20 = _mm256_add_pd(c35_20, _mm256_mul_pd(a35_20, b35));
_mm256_storeu_pd(&C[(i*56)+20], c35_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_20 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a35_20 = _mm_loadu_pd(&values[272]);
c35_20 = _mm_add_pd(c35_20, _mm_mul_pd(a35_20, b35));
_mm_storeu_pd(&C[(i*56)+20], c35_20);
__m128d c35_22 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a35_22 = _mm_loadu_pd(&values[274]);
c35_22 = _mm_add_pd(c35_22, _mm_mul_pd(a35_22, b35));
_mm_storeu_pd(&C[(i*56)+22], c35_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_24 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a35_24 = _mm256_loadu_pd(&values[276]);
c35_24 = _mm256_add_pd(c35_24, _mm256_mul_pd(a35_24, b35));
_mm256_storeu_pd(&C[(i*56)+24], c35_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_24 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a35_24 = _mm_loadu_pd(&values[276]);
c35_24 = _mm_add_pd(c35_24, _mm_mul_pd(a35_24, b35));
_mm_storeu_pd(&C[(i*56)+24], c35_24);
__m128d c35_26 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a35_26 = _mm_loadu_pd(&values[278]);
c35_26 = _mm_add_pd(c35_26, _mm_mul_pd(a35_26, b35));
_mm_storeu_pd(&C[(i*56)+26], c35_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_28 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a35_28 = _mm256_loadu_pd(&values[280]);
c35_28 = _mm256_add_pd(c35_28, _mm256_mul_pd(a35_28, b35));
_mm256_storeu_pd(&C[(i*56)+28], c35_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_28 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a35_28 = _mm_loadu_pd(&values[280]);
c35_28 = _mm_add_pd(c35_28, _mm_mul_pd(a35_28, b35));
_mm_storeu_pd(&C[(i*56)+28], c35_28);
__m128d c35_30 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a35_30 = _mm_loadu_pd(&values[282]);
c35_30 = _mm_add_pd(c35_30, _mm_mul_pd(a35_30, b35));
_mm_storeu_pd(&C[(i*56)+30], c35_30);
#endif
__m128d c35_32 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a35_32 = _mm_loadu_pd(&values[284]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_32 = _mm_add_pd(c35_32, _mm_mul_pd(a35_32, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_32 = _mm_add_pd(c35_32, _mm_mul_pd(a35_32, b35));
#endif
_mm_storeu_pd(&C[(i*56)+32], c35_32);
__m128d c35_34 = _mm_load_sd(&C[(i*56)+34]);
__m128d a35_34 = _mm_load_sd(&values[286]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_34 = _mm_add_sd(c35_34, _mm_mul_sd(a35_34, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_34 = _mm_add_sd(c35_34, _mm_mul_sd(a35_34, b35));
#endif
_mm_store_sd(&C[(i*56)+34], c35_34);
#else
C[(i*56)+0] += values[252] * B[(i*56)+35];
C[(i*56)+1] += values[253] * B[(i*56)+35];
C[(i*56)+2] += values[254] * B[(i*56)+35];
C[(i*56)+3] += values[255] * B[(i*56)+35];
C[(i*56)+4] += values[256] * B[(i*56)+35];
C[(i*56)+5] += values[257] * B[(i*56)+35];
C[(i*56)+6] += values[258] * B[(i*56)+35];
C[(i*56)+7] += values[259] * B[(i*56)+35];
C[(i*56)+8] += values[260] * B[(i*56)+35];
C[(i*56)+9] += values[261] * B[(i*56)+35];
C[(i*56)+10] += values[262] * B[(i*56)+35];
C[(i*56)+11] += values[263] * B[(i*56)+35];
C[(i*56)+12] += values[264] * B[(i*56)+35];
C[(i*56)+13] += values[265] * B[(i*56)+35];
C[(i*56)+14] += values[266] * B[(i*56)+35];
C[(i*56)+15] += values[267] * B[(i*56)+35];
C[(i*56)+16] += values[268] * B[(i*56)+35];
C[(i*56)+17] += values[269] * B[(i*56)+35];
C[(i*56)+18] += values[270] * B[(i*56)+35];
C[(i*56)+19] += values[271] * B[(i*56)+35];
C[(i*56)+20] += values[272] * B[(i*56)+35];
C[(i*56)+21] += values[273] * B[(i*56)+35];
C[(i*56)+22] += values[274] * B[(i*56)+35];
C[(i*56)+23] += values[275] * B[(i*56)+35];
C[(i*56)+24] += values[276] * B[(i*56)+35];
C[(i*56)+25] += values[277] * B[(i*56)+35];
C[(i*56)+26] += values[278] * B[(i*56)+35];
C[(i*56)+27] += values[279] * B[(i*56)+35];
C[(i*56)+28] += values[280] * B[(i*56)+35];
C[(i*56)+29] += values[281] * B[(i*56)+35];
C[(i*56)+30] += values[282] * B[(i*56)+35];
C[(i*56)+31] += values[283] * B[(i*56)+35];
C[(i*56)+32] += values[284] * B[(i*56)+35];
C[(i*56)+33] += values[285] * B[(i*56)+35];
C[(i*56)+34] += values[286] * B[(i*56)+35];
#endif
#ifndef NDEBUG
num_flops += 70;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b36 = _mm256_broadcast_sd(&B[(i*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b36 = _mm_loaddup_pd(&B[(i*56)+36]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a36_0 = _mm256_loadu_pd(&values[287]);
c36_0 = _mm256_add_pd(c36_0, _mm256_mul_pd(a36_0, b36));
_mm256_storeu_pd(&C[(i*56)+0], c36_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a36_0 = _mm_loadu_pd(&values[287]);
c36_0 = _mm_add_pd(c36_0, _mm_mul_pd(a36_0, b36));
_mm_storeu_pd(&C[(i*56)+0], c36_0);
__m128d c36_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a36_2 = _mm_loadu_pd(&values[289]);
c36_2 = _mm_add_pd(c36_2, _mm_mul_pd(a36_2, b36));
_mm_storeu_pd(&C[(i*56)+2], c36_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a36_4 = _mm256_loadu_pd(&values[291]);
c36_4 = _mm256_add_pd(c36_4, _mm256_mul_pd(a36_4, b36));
_mm256_storeu_pd(&C[(i*56)+4], c36_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a36_4 = _mm_loadu_pd(&values[291]);
c36_4 = _mm_add_pd(c36_4, _mm_mul_pd(a36_4, b36));
_mm_storeu_pd(&C[(i*56)+4], c36_4);
__m128d c36_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a36_6 = _mm_loadu_pd(&values[293]);
c36_6 = _mm_add_pd(c36_6, _mm_mul_pd(a36_6, b36));
_mm_storeu_pd(&C[(i*56)+6], c36_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a36_8 = _mm256_loadu_pd(&values[295]);
c36_8 = _mm256_add_pd(c36_8, _mm256_mul_pd(a36_8, b36));
_mm256_storeu_pd(&C[(i*56)+8], c36_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a36_8 = _mm_loadu_pd(&values[295]);
c36_8 = _mm_add_pd(c36_8, _mm_mul_pd(a36_8, b36));
_mm_storeu_pd(&C[(i*56)+8], c36_8);
__m128d c36_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a36_10 = _mm_loadu_pd(&values[297]);
c36_10 = _mm_add_pd(c36_10, _mm_mul_pd(a36_10, b36));
_mm_storeu_pd(&C[(i*56)+10], c36_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a36_12 = _mm256_loadu_pd(&values[299]);
c36_12 = _mm256_add_pd(c36_12, _mm256_mul_pd(a36_12, b36));
_mm256_storeu_pd(&C[(i*56)+12], c36_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a36_12 = _mm_loadu_pd(&values[299]);
c36_12 = _mm_add_pd(c36_12, _mm_mul_pd(a36_12, b36));
_mm_storeu_pd(&C[(i*56)+12], c36_12);
__m128d c36_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a36_14 = _mm_loadu_pd(&values[301]);
c36_14 = _mm_add_pd(c36_14, _mm_mul_pd(a36_14, b36));
_mm_storeu_pd(&C[(i*56)+14], c36_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a36_16 = _mm256_loadu_pd(&values[303]);
c36_16 = _mm256_add_pd(c36_16, _mm256_mul_pd(a36_16, b36));
_mm256_storeu_pd(&C[(i*56)+16], c36_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a36_16 = _mm_loadu_pd(&values[303]);
c36_16 = _mm_add_pd(c36_16, _mm_mul_pd(a36_16, b36));
_mm_storeu_pd(&C[(i*56)+16], c36_16);
__m128d c36_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a36_18 = _mm_loadu_pd(&values[305]);
c36_18 = _mm_add_pd(c36_18, _mm_mul_pd(a36_18, b36));
_mm_storeu_pd(&C[(i*56)+18], c36_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_20 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a36_20 = _mm256_loadu_pd(&values[307]);
c36_20 = _mm256_add_pd(c36_20, _mm256_mul_pd(a36_20, b36));
_mm256_storeu_pd(&C[(i*56)+20], c36_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_20 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a36_20 = _mm_loadu_pd(&values[307]);
c36_20 = _mm_add_pd(c36_20, _mm_mul_pd(a36_20, b36));
_mm_storeu_pd(&C[(i*56)+20], c36_20);
__m128d c36_22 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a36_22 = _mm_loadu_pd(&values[309]);
c36_22 = _mm_add_pd(c36_22, _mm_mul_pd(a36_22, b36));
_mm_storeu_pd(&C[(i*56)+22], c36_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_24 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a36_24 = _mm256_loadu_pd(&values[311]);
c36_24 = _mm256_add_pd(c36_24, _mm256_mul_pd(a36_24, b36));
_mm256_storeu_pd(&C[(i*56)+24], c36_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_24 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a36_24 = _mm_loadu_pd(&values[311]);
c36_24 = _mm_add_pd(c36_24, _mm_mul_pd(a36_24, b36));
_mm_storeu_pd(&C[(i*56)+24], c36_24);
__m128d c36_26 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a36_26 = _mm_loadu_pd(&values[313]);
c36_26 = _mm_add_pd(c36_26, _mm_mul_pd(a36_26, b36));
_mm_storeu_pd(&C[(i*56)+26], c36_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_28 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a36_28 = _mm256_loadu_pd(&values[315]);
c36_28 = _mm256_add_pd(c36_28, _mm256_mul_pd(a36_28, b36));
_mm256_storeu_pd(&C[(i*56)+28], c36_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_28 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a36_28 = _mm_loadu_pd(&values[315]);
c36_28 = _mm_add_pd(c36_28, _mm_mul_pd(a36_28, b36));
_mm_storeu_pd(&C[(i*56)+28], c36_28);
__m128d c36_30 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a36_30 = _mm_loadu_pd(&values[317]);
c36_30 = _mm_add_pd(c36_30, _mm_mul_pd(a36_30, b36));
_mm_storeu_pd(&C[(i*56)+30], c36_30);
#endif
__m128d c36_32 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a36_32 = _mm_loadu_pd(&values[319]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_32 = _mm_add_pd(c36_32, _mm_mul_pd(a36_32, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_32 = _mm_add_pd(c36_32, _mm_mul_pd(a36_32, b36));
#endif
_mm_storeu_pd(&C[(i*56)+32], c36_32);
__m128d c36_34 = _mm_load_sd(&C[(i*56)+34]);
__m128d a36_34 = _mm_load_sd(&values[321]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_34 = _mm_add_sd(c36_34, _mm_mul_sd(a36_34, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_34 = _mm_add_sd(c36_34, _mm_mul_sd(a36_34, b36));
#endif
_mm_store_sd(&C[(i*56)+34], c36_34);
#else
C[(i*56)+0] += values[287] * B[(i*56)+36];
C[(i*56)+1] += values[288] * B[(i*56)+36];
C[(i*56)+2] += values[289] * B[(i*56)+36];
C[(i*56)+3] += values[290] * B[(i*56)+36];
C[(i*56)+4] += values[291] * B[(i*56)+36];
C[(i*56)+5] += values[292] * B[(i*56)+36];
C[(i*56)+6] += values[293] * B[(i*56)+36];
C[(i*56)+7] += values[294] * B[(i*56)+36];
C[(i*56)+8] += values[295] * B[(i*56)+36];
C[(i*56)+9] += values[296] * B[(i*56)+36];
C[(i*56)+10] += values[297] * B[(i*56)+36];
C[(i*56)+11] += values[298] * B[(i*56)+36];
C[(i*56)+12] += values[299] * B[(i*56)+36];
C[(i*56)+13] += values[300] * B[(i*56)+36];
C[(i*56)+14] += values[301] * B[(i*56)+36];
C[(i*56)+15] += values[302] * B[(i*56)+36];
C[(i*56)+16] += values[303] * B[(i*56)+36];
C[(i*56)+17] += values[304] * B[(i*56)+36];
C[(i*56)+18] += values[305] * B[(i*56)+36];
C[(i*56)+19] += values[306] * B[(i*56)+36];
C[(i*56)+20] += values[307] * B[(i*56)+36];
C[(i*56)+21] += values[308] * B[(i*56)+36];
C[(i*56)+22] += values[309] * B[(i*56)+36];
C[(i*56)+23] += values[310] * B[(i*56)+36];
C[(i*56)+24] += values[311] * B[(i*56)+36];
C[(i*56)+25] += values[312] * B[(i*56)+36];
C[(i*56)+26] += values[313] * B[(i*56)+36];
C[(i*56)+27] += values[314] * B[(i*56)+36];
C[(i*56)+28] += values[315] * B[(i*56)+36];
C[(i*56)+29] += values[316] * B[(i*56)+36];
C[(i*56)+30] += values[317] * B[(i*56)+36];
C[(i*56)+31] += values[318] * B[(i*56)+36];
C[(i*56)+32] += values[319] * B[(i*56)+36];
C[(i*56)+33] += values[320] * B[(i*56)+36];
C[(i*56)+34] += values[321] * B[(i*56)+36];
#endif
#ifndef NDEBUG
num_flops += 70;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b37 = _mm256_broadcast_sd(&B[(i*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b37 = _mm_loaddup_pd(&B[(i*56)+37]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a37_0 = _mm256_loadu_pd(&values[322]);
c37_0 = _mm256_add_pd(c37_0, _mm256_mul_pd(a37_0, b37));
_mm256_storeu_pd(&C[(i*56)+0], c37_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a37_0 = _mm_loadu_pd(&values[322]);
c37_0 = _mm_add_pd(c37_0, _mm_mul_pd(a37_0, b37));
_mm_storeu_pd(&C[(i*56)+0], c37_0);
__m128d c37_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a37_2 = _mm_loadu_pd(&values[324]);
c37_2 = _mm_add_pd(c37_2, _mm_mul_pd(a37_2, b37));
_mm_storeu_pd(&C[(i*56)+2], c37_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a37_4 = _mm256_loadu_pd(&values[326]);
c37_4 = _mm256_add_pd(c37_4, _mm256_mul_pd(a37_4, b37));
_mm256_storeu_pd(&C[(i*56)+4], c37_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a37_4 = _mm_loadu_pd(&values[326]);
c37_4 = _mm_add_pd(c37_4, _mm_mul_pd(a37_4, b37));
_mm_storeu_pd(&C[(i*56)+4], c37_4);
__m128d c37_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a37_6 = _mm_loadu_pd(&values[328]);
c37_6 = _mm_add_pd(c37_6, _mm_mul_pd(a37_6, b37));
_mm_storeu_pd(&C[(i*56)+6], c37_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a37_8 = _mm256_loadu_pd(&values[330]);
c37_8 = _mm256_add_pd(c37_8, _mm256_mul_pd(a37_8, b37));
_mm256_storeu_pd(&C[(i*56)+8], c37_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a37_8 = _mm_loadu_pd(&values[330]);
c37_8 = _mm_add_pd(c37_8, _mm_mul_pd(a37_8, b37));
_mm_storeu_pd(&C[(i*56)+8], c37_8);
__m128d c37_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a37_10 = _mm_loadu_pd(&values[332]);
c37_10 = _mm_add_pd(c37_10, _mm_mul_pd(a37_10, b37));
_mm_storeu_pd(&C[(i*56)+10], c37_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a37_12 = _mm256_loadu_pd(&values[334]);
c37_12 = _mm256_add_pd(c37_12, _mm256_mul_pd(a37_12, b37));
_mm256_storeu_pd(&C[(i*56)+12], c37_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a37_12 = _mm_loadu_pd(&values[334]);
c37_12 = _mm_add_pd(c37_12, _mm_mul_pd(a37_12, b37));
_mm_storeu_pd(&C[(i*56)+12], c37_12);
__m128d c37_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a37_14 = _mm_loadu_pd(&values[336]);
c37_14 = _mm_add_pd(c37_14, _mm_mul_pd(a37_14, b37));
_mm_storeu_pd(&C[(i*56)+14], c37_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a37_16 = _mm256_loadu_pd(&values[338]);
c37_16 = _mm256_add_pd(c37_16, _mm256_mul_pd(a37_16, b37));
_mm256_storeu_pd(&C[(i*56)+16], c37_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a37_16 = _mm_loadu_pd(&values[338]);
c37_16 = _mm_add_pd(c37_16, _mm_mul_pd(a37_16, b37));
_mm_storeu_pd(&C[(i*56)+16], c37_16);
__m128d c37_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a37_18 = _mm_loadu_pd(&values[340]);
c37_18 = _mm_add_pd(c37_18, _mm_mul_pd(a37_18, b37));
_mm_storeu_pd(&C[(i*56)+18], c37_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_20 = _mm256_loadu_pd(&C[(i*56)+21]);
__m256d a37_20 = _mm256_loadu_pd(&values[342]);
c37_20 = _mm256_add_pd(c37_20, _mm256_mul_pd(a37_20, b37));
_mm256_storeu_pd(&C[(i*56)+21], c37_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_20 = _mm_loadu_pd(&C[(i*56)+21]);
__m128d a37_20 = _mm_loadu_pd(&values[342]);
c37_20 = _mm_add_pd(c37_20, _mm_mul_pd(a37_20, b37));
_mm_storeu_pd(&C[(i*56)+21], c37_20);
__m128d c37_22 = _mm_loadu_pd(&C[(i*56)+23]);
__m128d a37_22 = _mm_loadu_pd(&values[344]);
c37_22 = _mm_add_pd(c37_22, _mm_mul_pd(a37_22, b37));
_mm_storeu_pd(&C[(i*56)+23], c37_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_24 = _mm256_loadu_pd(&C[(i*56)+25]);
__m256d a37_24 = _mm256_loadu_pd(&values[346]);
c37_24 = _mm256_add_pd(c37_24, _mm256_mul_pd(a37_24, b37));
_mm256_storeu_pd(&C[(i*56)+25], c37_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_24 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a37_24 = _mm_loadu_pd(&values[346]);
c37_24 = _mm_add_pd(c37_24, _mm_mul_pd(a37_24, b37));
_mm_storeu_pd(&C[(i*56)+25], c37_24);
__m128d c37_26 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a37_26 = _mm_loadu_pd(&values[348]);
c37_26 = _mm_add_pd(c37_26, _mm_mul_pd(a37_26, b37));
_mm_storeu_pd(&C[(i*56)+27], c37_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_28 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a37_28 = _mm256_loadu_pd(&values[350]);
c37_28 = _mm256_add_pd(c37_28, _mm256_mul_pd(a37_28, b37));
_mm256_storeu_pd(&C[(i*56)+29], c37_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_28 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a37_28 = _mm_loadu_pd(&values[350]);
c37_28 = _mm_add_pd(c37_28, _mm_mul_pd(a37_28, b37));
_mm_storeu_pd(&C[(i*56)+29], c37_28);
__m128d c37_30 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a37_30 = _mm_loadu_pd(&values[352]);
c37_30 = _mm_add_pd(c37_30, _mm_mul_pd(a37_30, b37));
_mm_storeu_pd(&C[(i*56)+31], c37_30);
#endif
__m128d c37_32 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a37_32 = _mm_loadu_pd(&values[354]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_32 = _mm_add_pd(c37_32, _mm_mul_pd(a37_32, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_32 = _mm_add_pd(c37_32, _mm_mul_pd(a37_32, b37));
#endif
_mm_storeu_pd(&C[(i*56)+33], c37_32);
#else
C[(i*56)+0] += values[322] * B[(i*56)+37];
C[(i*56)+1] += values[323] * B[(i*56)+37];
C[(i*56)+2] += values[324] * B[(i*56)+37];
C[(i*56)+3] += values[325] * B[(i*56)+37];
C[(i*56)+4] += values[326] * B[(i*56)+37];
C[(i*56)+5] += values[327] * B[(i*56)+37];
C[(i*56)+6] += values[328] * B[(i*56)+37];
C[(i*56)+7] += values[329] * B[(i*56)+37];
C[(i*56)+8] += values[330] * B[(i*56)+37];
C[(i*56)+9] += values[331] * B[(i*56)+37];
C[(i*56)+10] += values[332] * B[(i*56)+37];
C[(i*56)+11] += values[333] * B[(i*56)+37];
C[(i*56)+12] += values[334] * B[(i*56)+37];
C[(i*56)+13] += values[335] * B[(i*56)+37];
C[(i*56)+14] += values[336] * B[(i*56)+37];
C[(i*56)+15] += values[337] * B[(i*56)+37];
C[(i*56)+16] += values[338] * B[(i*56)+37];
C[(i*56)+17] += values[339] * B[(i*56)+37];
C[(i*56)+18] += values[340] * B[(i*56)+37];
C[(i*56)+19] += values[341] * B[(i*56)+37];
C[(i*56)+21] += values[342] * B[(i*56)+37];
C[(i*56)+22] += values[343] * B[(i*56)+37];
C[(i*56)+23] += values[344] * B[(i*56)+37];
C[(i*56)+24] += values[345] * B[(i*56)+37];
C[(i*56)+25] += values[346] * B[(i*56)+37];
C[(i*56)+26] += values[347] * B[(i*56)+37];
C[(i*56)+27] += values[348] * B[(i*56)+37];
C[(i*56)+28] += values[349] * B[(i*56)+37];
C[(i*56)+29] += values[350] * B[(i*56)+37];
C[(i*56)+30] += values[351] * B[(i*56)+37];
C[(i*56)+31] += values[352] * B[(i*56)+37];
C[(i*56)+32] += values[353] * B[(i*56)+37];
C[(i*56)+33] += values[354] * B[(i*56)+37];
C[(i*56)+34] += values[355] * B[(i*56)+37];
#endif
#ifndef NDEBUG
num_flops += 68;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b38 = _mm256_broadcast_sd(&B[(i*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b38 = _mm_loaddup_pd(&B[(i*56)+38]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a38_0 = _mm256_loadu_pd(&values[356]);
c38_0 = _mm256_add_pd(c38_0, _mm256_mul_pd(a38_0, b38));
_mm256_storeu_pd(&C[(i*56)+0], c38_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a38_0 = _mm_loadu_pd(&values[356]);
c38_0 = _mm_add_pd(c38_0, _mm_mul_pd(a38_0, b38));
_mm_storeu_pd(&C[(i*56)+0], c38_0);
__m128d c38_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a38_2 = _mm_loadu_pd(&values[358]);
c38_2 = _mm_add_pd(c38_2, _mm_mul_pd(a38_2, b38));
_mm_storeu_pd(&C[(i*56)+2], c38_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a38_4 = _mm256_loadu_pd(&values[360]);
c38_4 = _mm256_add_pd(c38_4, _mm256_mul_pd(a38_4, b38));
_mm256_storeu_pd(&C[(i*56)+4], c38_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a38_4 = _mm_loadu_pd(&values[360]);
c38_4 = _mm_add_pd(c38_4, _mm_mul_pd(a38_4, b38));
_mm_storeu_pd(&C[(i*56)+4], c38_4);
__m128d c38_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a38_6 = _mm_loadu_pd(&values[362]);
c38_6 = _mm_add_pd(c38_6, _mm_mul_pd(a38_6, b38));
_mm_storeu_pd(&C[(i*56)+6], c38_6);
#endif
__m128d c38_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a38_8 = _mm_loadu_pd(&values[364]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_8 = _mm_add_pd(c38_8, _mm_mul_pd(a38_8, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_8 = _mm_add_pd(c38_8, _mm_mul_pd(a38_8, b38));
#endif
_mm_storeu_pd(&C[(i*56)+8], c38_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_10 = _mm256_loadu_pd(&C[(i*56)+11]);
__m256d a38_10 = _mm256_loadu_pd(&values[366]);
c38_10 = _mm256_add_pd(c38_10, _mm256_mul_pd(a38_10, b38));
_mm256_storeu_pd(&C[(i*56)+11], c38_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_10 = _mm_loadu_pd(&C[(i*56)+11]);
__m128d a38_10 = _mm_loadu_pd(&values[366]);
c38_10 = _mm_add_pd(c38_10, _mm_mul_pd(a38_10, b38));
_mm_storeu_pd(&C[(i*56)+11], c38_10);
__m128d c38_12 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a38_12 = _mm_loadu_pd(&values[368]);
c38_12 = _mm_add_pd(c38_12, _mm_mul_pd(a38_12, b38));
_mm_storeu_pd(&C[(i*56)+13], c38_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_14 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a38_14 = _mm256_loadu_pd(&values[370]);
c38_14 = _mm256_add_pd(c38_14, _mm256_mul_pd(a38_14, b38));
_mm256_storeu_pd(&C[(i*56)+15], c38_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_14 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a38_14 = _mm_loadu_pd(&values[370]);
c38_14 = _mm_add_pd(c38_14, _mm_mul_pd(a38_14, b38));
_mm_storeu_pd(&C[(i*56)+15], c38_14);
__m128d c38_16 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a38_16 = _mm_loadu_pd(&values[372]);
c38_16 = _mm_add_pd(c38_16, _mm_mul_pd(a38_16, b38));
_mm_storeu_pd(&C[(i*56)+17], c38_16);
#endif
__m128d c38_18 = _mm_load_sd(&C[(i*56)+19]);
__m128d a38_18 = _mm_load_sd(&values[374]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_18 = _mm_add_sd(c38_18, _mm_mul_sd(a38_18, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_18 = _mm_add_sd(c38_18, _mm_mul_sd(a38_18, b38));
#endif
_mm_store_sd(&C[(i*56)+19], c38_18);
__m128d c38_19 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a38_19 = _mm_loadu_pd(&values[375]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_19 = _mm_add_pd(c38_19, _mm_mul_pd(a38_19, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_19 = _mm_add_pd(c38_19, _mm_mul_pd(a38_19, b38));
#endif
_mm_storeu_pd(&C[(i*56)+22], c38_19);
__m128d c38_21 = _mm_load_sd(&C[(i*56)+24]);
__m128d a38_21 = _mm_load_sd(&values[377]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_21 = _mm_add_sd(c38_21, _mm_mul_sd(a38_21, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_21 = _mm_add_sd(c38_21, _mm_mul_sd(a38_21, b38));
#endif
_mm_store_sd(&C[(i*56)+24], c38_21);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_22 = _mm256_loadu_pd(&C[(i*56)+26]);
__m256d a38_22 = _mm256_loadu_pd(&values[378]);
c38_22 = _mm256_add_pd(c38_22, _mm256_mul_pd(a38_22, b38));
_mm256_storeu_pd(&C[(i*56)+26], c38_22);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_22 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a38_22 = _mm_loadu_pd(&values[378]);
c38_22 = _mm_add_pd(c38_22, _mm_mul_pd(a38_22, b38));
_mm_storeu_pd(&C[(i*56)+26], c38_22);
__m128d c38_24 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a38_24 = _mm_loadu_pd(&values[380]);
c38_24 = _mm_add_pd(c38_24, _mm_mul_pd(a38_24, b38));
_mm_storeu_pd(&C[(i*56)+28], c38_24);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_26 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a38_26 = _mm256_loadu_pd(&values[382]);
c38_26 = _mm256_add_pd(c38_26, _mm256_mul_pd(a38_26, b38));
_mm256_storeu_pd(&C[(i*56)+30], c38_26);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_26 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a38_26 = _mm_loadu_pd(&values[382]);
c38_26 = _mm_add_pd(c38_26, _mm_mul_pd(a38_26, b38));
_mm_storeu_pd(&C[(i*56)+30], c38_26);
__m128d c38_28 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a38_28 = _mm_loadu_pd(&values[384]);
c38_28 = _mm_add_pd(c38_28, _mm_mul_pd(a38_28, b38));
_mm_storeu_pd(&C[(i*56)+32], c38_28);
#endif
__m128d c38_30 = _mm_load_sd(&C[(i*56)+34]);
__m128d a38_30 = _mm_load_sd(&values[386]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_30 = _mm_add_sd(c38_30, _mm_mul_sd(a38_30, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_30 = _mm_add_sd(c38_30, _mm_mul_sd(a38_30, b38));
#endif
_mm_store_sd(&C[(i*56)+34], c38_30);
#else
C[(i*56)+0] += values[356] * B[(i*56)+38];
C[(i*56)+1] += values[357] * B[(i*56)+38];
C[(i*56)+2] += values[358] * B[(i*56)+38];
C[(i*56)+3] += values[359] * B[(i*56)+38];
C[(i*56)+4] += values[360] * B[(i*56)+38];
C[(i*56)+5] += values[361] * B[(i*56)+38];
C[(i*56)+6] += values[362] * B[(i*56)+38];
C[(i*56)+7] += values[363] * B[(i*56)+38];
C[(i*56)+8] += values[364] * B[(i*56)+38];
C[(i*56)+9] += values[365] * B[(i*56)+38];
C[(i*56)+11] += values[366] * B[(i*56)+38];
C[(i*56)+12] += values[367] * B[(i*56)+38];
C[(i*56)+13] += values[368] * B[(i*56)+38];
C[(i*56)+14] += values[369] * B[(i*56)+38];
C[(i*56)+15] += values[370] * B[(i*56)+38];
C[(i*56)+16] += values[371] * B[(i*56)+38];
C[(i*56)+17] += values[372] * B[(i*56)+38];
C[(i*56)+18] += values[373] * B[(i*56)+38];
C[(i*56)+19] += values[374] * B[(i*56)+38];
C[(i*56)+22] += values[375] * B[(i*56)+38];
C[(i*56)+23] += values[376] * B[(i*56)+38];
C[(i*56)+24] += values[377] * B[(i*56)+38];
C[(i*56)+26] += values[378] * B[(i*56)+38];
C[(i*56)+27] += values[379] * B[(i*56)+38];
C[(i*56)+28] += values[380] * B[(i*56)+38];
C[(i*56)+29] += values[381] * B[(i*56)+38];
C[(i*56)+30] += values[382] * B[(i*56)+38];
C[(i*56)+31] += values[383] * B[(i*56)+38];
C[(i*56)+32] += values[384] * B[(i*56)+38];
C[(i*56)+33] += values[385] * B[(i*56)+38];
C[(i*56)+34] += values[386] * B[(i*56)+38];
#endif
#ifndef NDEBUG
num_flops += 62;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b39 = _mm256_broadcast_sd(&B[(i*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b39 = _mm_loaddup_pd(&B[(i*56)+39]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a39_0 = _mm256_loadu_pd(&values[387]);
c39_0 = _mm256_add_pd(c39_0, _mm256_mul_pd(a39_0, b39));
_mm256_storeu_pd(&C[(i*56)+0], c39_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a39_0 = _mm_loadu_pd(&values[387]);
c39_0 = _mm_add_pd(c39_0, _mm_mul_pd(a39_0, b39));
_mm_storeu_pd(&C[(i*56)+0], c39_0);
__m128d c39_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a39_2 = _mm_loadu_pd(&values[389]);
c39_2 = _mm_add_pd(c39_2, _mm_mul_pd(a39_2, b39));
_mm_storeu_pd(&C[(i*56)+2], c39_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a39_4 = _mm256_loadu_pd(&values[391]);
c39_4 = _mm256_add_pd(c39_4, _mm256_mul_pd(a39_4, b39));
_mm256_storeu_pd(&C[(i*56)+5], c39_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a39_4 = _mm_loadu_pd(&values[391]);
c39_4 = _mm_add_pd(c39_4, _mm_mul_pd(a39_4, b39));
_mm_storeu_pd(&C[(i*56)+5], c39_4);
__m128d c39_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a39_6 = _mm_loadu_pd(&values[393]);
c39_6 = _mm_add_pd(c39_6, _mm_mul_pd(a39_6, b39));
_mm_storeu_pd(&C[(i*56)+7], c39_6);
#endif
__m128d c39_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a39_8 = _mm_load_sd(&values[395]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_8 = _mm_add_sd(c39_8, _mm_mul_sd(a39_8, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_8 = _mm_add_sd(c39_8, _mm_mul_sd(a39_8, b39));
#endif
_mm_store_sd(&C[(i*56)+9], c39_8);
__m128d c39_9 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a39_9 = _mm_loadu_pd(&values[396]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_9 = _mm_add_pd(c39_9, _mm_mul_pd(a39_9, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_9 = _mm_add_pd(c39_9, _mm_mul_pd(a39_9, b39));
#endif
_mm_storeu_pd(&C[(i*56)+12], c39_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_11 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a39_11 = _mm256_loadu_pd(&values[398]);
c39_11 = _mm256_add_pd(c39_11, _mm256_mul_pd(a39_11, b39));
_mm256_storeu_pd(&C[(i*56)+15], c39_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_11 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a39_11 = _mm_loadu_pd(&values[398]);
c39_11 = _mm_add_pd(c39_11, _mm_mul_pd(a39_11, b39));
_mm_storeu_pd(&C[(i*56)+15], c39_11);
__m128d c39_13 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a39_13 = _mm_loadu_pd(&values[400]);
c39_13 = _mm_add_pd(c39_13, _mm_mul_pd(a39_13, b39));
_mm_storeu_pd(&C[(i*56)+17], c39_13);
#endif
__m128d c39_15 = _mm_load_sd(&C[(i*56)+19]);
__m128d a39_15 = _mm_load_sd(&values[402]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_15 = _mm_add_sd(c39_15, _mm_mul_sd(a39_15, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_15 = _mm_add_sd(c39_15, _mm_mul_sd(a39_15, b39));
#endif
_mm_store_sd(&C[(i*56)+19], c39_15);
__m128d c39_16 = _mm_loadu_pd(&C[(i*56)+23]);
__m128d a39_16 = _mm_loadu_pd(&values[403]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_16 = _mm_add_pd(c39_16, _mm_mul_pd(a39_16, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_16 = _mm_add_pd(c39_16, _mm_mul_pd(a39_16, b39));
#endif
_mm_storeu_pd(&C[(i*56)+23], c39_16);
__m128d c39_18 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a39_18 = _mm_loadu_pd(&values[405]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_18 = _mm_add_pd(c39_18, _mm_mul_pd(a39_18, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_18 = _mm_add_pd(c39_18, _mm_mul_pd(a39_18, b39));
#endif
_mm_storeu_pd(&C[(i*56)+27], c39_18);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_20 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a39_20 = _mm256_loadu_pd(&values[407]);
c39_20 = _mm256_add_pd(c39_20, _mm256_mul_pd(a39_20, b39));
_mm256_storeu_pd(&C[(i*56)+30], c39_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_20 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a39_20 = _mm_loadu_pd(&values[407]);
c39_20 = _mm_add_pd(c39_20, _mm_mul_pd(a39_20, b39));
_mm_storeu_pd(&C[(i*56)+30], c39_20);
__m128d c39_22 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a39_22 = _mm_loadu_pd(&values[409]);
c39_22 = _mm_add_pd(c39_22, _mm_mul_pd(a39_22, b39));
_mm_storeu_pd(&C[(i*56)+32], c39_22);
#endif
__m128d c39_24 = _mm_load_sd(&C[(i*56)+34]);
__m128d a39_24 = _mm_load_sd(&values[411]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_24 = _mm_add_sd(c39_24, _mm_mul_sd(a39_24, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_24 = _mm_add_sd(c39_24, _mm_mul_sd(a39_24, b39));
#endif
_mm_store_sd(&C[(i*56)+34], c39_24);
#else
C[(i*56)+0] += values[387] * B[(i*56)+39];
C[(i*56)+1] += values[388] * B[(i*56)+39];
C[(i*56)+2] += values[389] * B[(i*56)+39];
C[(i*56)+3] += values[390] * B[(i*56)+39];
C[(i*56)+5] += values[391] * B[(i*56)+39];
C[(i*56)+6] += values[392] * B[(i*56)+39];
C[(i*56)+7] += values[393] * B[(i*56)+39];
C[(i*56)+8] += values[394] * B[(i*56)+39];
C[(i*56)+9] += values[395] * B[(i*56)+39];
C[(i*56)+12] += values[396] * B[(i*56)+39];
C[(i*56)+13] += values[397] * B[(i*56)+39];
C[(i*56)+15] += values[398] * B[(i*56)+39];
C[(i*56)+16] += values[399] * B[(i*56)+39];
C[(i*56)+17] += values[400] * B[(i*56)+39];
C[(i*56)+18] += values[401] * B[(i*56)+39];
C[(i*56)+19] += values[402] * B[(i*56)+39];
C[(i*56)+23] += values[403] * B[(i*56)+39];
C[(i*56)+24] += values[404] * B[(i*56)+39];
C[(i*56)+27] += values[405] * B[(i*56)+39];
C[(i*56)+28] += values[406] * B[(i*56)+39];
C[(i*56)+30] += values[407] * B[(i*56)+39];
C[(i*56)+31] += values[408] * B[(i*56)+39];
C[(i*56)+32] += values[409] * B[(i*56)+39];
C[(i*56)+33] += values[410] * B[(i*56)+39];
C[(i*56)+34] += values[411] * B[(i*56)+39];
#endif
#ifndef NDEBUG
num_flops += 50;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b40 = _mm256_broadcast_sd(&B[(i*56)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b40 = _mm_loaddup_pd(&B[(i*56)+40]);
#endif
__m128d c40_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a40_0 = _mm_load_sd(&values[412]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_0 = _mm_add_sd(c40_0, _mm_mul_sd(a40_0, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_0 = _mm_add_sd(c40_0, _mm_mul_sd(a40_0, b40));
#endif
_mm_store_sd(&C[(i*56)+0], c40_0);
__m128d c40_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a40_1 = _mm_loadu_pd(&values[413]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_1 = _mm_add_pd(c40_1, _mm_mul_pd(a40_1, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_1 = _mm_add_pd(c40_1, _mm_mul_pd(a40_1, b40));
#endif
_mm_storeu_pd(&C[(i*56)+2], c40_1);
__m128d c40_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a40_3 = _mm_load_sd(&values[415]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_3 = _mm_add_sd(c40_3, _mm_mul_sd(a40_3, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_3 = _mm_add_sd(c40_3, _mm_mul_sd(a40_3, b40));
#endif
_mm_store_sd(&C[(i*56)+6], c40_3);
__m128d c40_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a40_4 = _mm_loadu_pd(&values[416]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_4 = _mm_add_pd(c40_4, _mm_mul_pd(a40_4, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_4 = _mm_add_pd(c40_4, _mm_mul_pd(a40_4, b40));
#endif
_mm_storeu_pd(&C[(i*56)+8], c40_4);
__m128d c40_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a40_6 = _mm_load_sd(&values[418]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_6 = _mm_add_sd(c40_6, _mm_mul_sd(a40_6, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_6 = _mm_add_sd(c40_6, _mm_mul_sd(a40_6, b40));
#endif
_mm_store_sd(&C[(i*56)+13], c40_6);
__m128d c40_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a40_7 = _mm_load_sd(&values[419]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_7 = _mm_add_sd(c40_7, _mm_mul_sd(a40_7, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_7 = _mm_add_sd(c40_7, _mm_mul_sd(a40_7, b40));
#endif
_mm_store_sd(&C[(i*56)+16], c40_7);
__m128d c40_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a40_8 = _mm_loadu_pd(&values[420]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_8 = _mm_add_pd(c40_8, _mm_mul_pd(a40_8, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_8 = _mm_add_pd(c40_8, _mm_mul_pd(a40_8, b40));
#endif
_mm_storeu_pd(&C[(i*56)+18], c40_8);
__m128d c40_10 = _mm_load_sd(&C[(i*56)+24]);
__m128d a40_10 = _mm_load_sd(&values[422]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_10 = _mm_add_sd(c40_10, _mm_mul_sd(a40_10, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_10 = _mm_add_sd(c40_10, _mm_mul_sd(a40_10, b40));
#endif
_mm_store_sd(&C[(i*56)+24], c40_10);
__m128d c40_11 = _mm_load_sd(&C[(i*56)+28]);
__m128d a40_11 = _mm_load_sd(&values[423]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_11 = _mm_add_sd(c40_11, _mm_mul_sd(a40_11, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_11 = _mm_add_sd(c40_11, _mm_mul_sd(a40_11, b40));
#endif
_mm_store_sd(&C[(i*56)+28], c40_11);
__m128d c40_12 = _mm_load_sd(&C[(i*56)+31]);
__m128d a40_12 = _mm_load_sd(&values[424]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_12 = _mm_add_sd(c40_12, _mm_mul_sd(a40_12, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_12 = _mm_add_sd(c40_12, _mm_mul_sd(a40_12, b40));
#endif
_mm_store_sd(&C[(i*56)+31], c40_12);
__m128d c40_13 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a40_13 = _mm_loadu_pd(&values[425]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_13 = _mm_add_pd(c40_13, _mm_mul_pd(a40_13, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_13 = _mm_add_pd(c40_13, _mm_mul_pd(a40_13, b40));
#endif
_mm_storeu_pd(&C[(i*56)+33], c40_13);
#else
C[(i*56)+0] += values[412] * B[(i*56)+40];
C[(i*56)+2] += values[413] * B[(i*56)+40];
C[(i*56)+3] += values[414] * B[(i*56)+40];
C[(i*56)+6] += values[415] * B[(i*56)+40];
C[(i*56)+8] += values[416] * B[(i*56)+40];
C[(i*56)+9] += values[417] * B[(i*56)+40];
C[(i*56)+13] += values[418] * B[(i*56)+40];
C[(i*56)+16] += values[419] * B[(i*56)+40];
C[(i*56)+18] += values[420] * B[(i*56)+40];
C[(i*56)+19] += values[421] * B[(i*56)+40];
C[(i*56)+24] += values[422] * B[(i*56)+40];
C[(i*56)+28] += values[423] * B[(i*56)+40];
C[(i*56)+31] += values[424] * B[(i*56)+40];
C[(i*56)+33] += values[425] * B[(i*56)+40];
C[(i*56)+34] += values[426] * B[(i*56)+40];
#endif
#ifndef NDEBUG
num_flops += 30;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b41 = _mm256_broadcast_sd(&B[(i*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b41 = _mm_loaddup_pd(&B[(i*56)+41]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a41_0 = _mm256_loadu_pd(&values[427]);
c41_0 = _mm256_add_pd(c41_0, _mm256_mul_pd(a41_0, b41));
_mm256_storeu_pd(&C[(i*56)+0], c41_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a41_0 = _mm_loadu_pd(&values[427]);
c41_0 = _mm_add_pd(c41_0, _mm_mul_pd(a41_0, b41));
_mm_storeu_pd(&C[(i*56)+0], c41_0);
__m128d c41_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a41_2 = _mm_loadu_pd(&values[429]);
c41_2 = _mm_add_pd(c41_2, _mm_mul_pd(a41_2, b41));
_mm_storeu_pd(&C[(i*56)+2], c41_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a41_4 = _mm256_loadu_pd(&values[431]);
c41_4 = _mm256_add_pd(c41_4, _mm256_mul_pd(a41_4, b41));
_mm256_storeu_pd(&C[(i*56)+4], c41_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a41_4 = _mm_loadu_pd(&values[431]);
c41_4 = _mm_add_pd(c41_4, _mm_mul_pd(a41_4, b41));
_mm_storeu_pd(&C[(i*56)+4], c41_4);
__m128d c41_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a41_6 = _mm_loadu_pd(&values[433]);
c41_6 = _mm_add_pd(c41_6, _mm_mul_pd(a41_6, b41));
_mm_storeu_pd(&C[(i*56)+6], c41_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a41_8 = _mm256_loadu_pd(&values[435]);
c41_8 = _mm256_add_pd(c41_8, _mm256_mul_pd(a41_8, b41));
_mm256_storeu_pd(&C[(i*56)+8], c41_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a41_8 = _mm_loadu_pd(&values[435]);
c41_8 = _mm_add_pd(c41_8, _mm_mul_pd(a41_8, b41));
_mm_storeu_pd(&C[(i*56)+8], c41_8);
__m128d c41_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a41_10 = _mm_loadu_pd(&values[437]);
c41_10 = _mm_add_pd(c41_10, _mm_mul_pd(a41_10, b41));
_mm_storeu_pd(&C[(i*56)+10], c41_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a41_12 = _mm256_loadu_pd(&values[439]);
c41_12 = _mm256_add_pd(c41_12, _mm256_mul_pd(a41_12, b41));
_mm256_storeu_pd(&C[(i*56)+12], c41_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a41_12 = _mm_loadu_pd(&values[439]);
c41_12 = _mm_add_pd(c41_12, _mm_mul_pd(a41_12, b41));
_mm_storeu_pd(&C[(i*56)+12], c41_12);
__m128d c41_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a41_14 = _mm_loadu_pd(&values[441]);
c41_14 = _mm_add_pd(c41_14, _mm_mul_pd(a41_14, b41));
_mm_storeu_pd(&C[(i*56)+14], c41_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a41_16 = _mm256_loadu_pd(&values[443]);
c41_16 = _mm256_add_pd(c41_16, _mm256_mul_pd(a41_16, b41));
_mm256_storeu_pd(&C[(i*56)+16], c41_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a41_16 = _mm_loadu_pd(&values[443]);
c41_16 = _mm_add_pd(c41_16, _mm_mul_pd(a41_16, b41));
_mm_storeu_pd(&C[(i*56)+16], c41_16);
__m128d c41_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a41_18 = _mm_loadu_pd(&values[445]);
c41_18 = _mm_add_pd(c41_18, _mm_mul_pd(a41_18, b41));
_mm_storeu_pd(&C[(i*56)+18], c41_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_20 = _mm256_loadu_pd(&C[(i*56)+25]);
__m256d a41_20 = _mm256_loadu_pd(&values[447]);
c41_20 = _mm256_add_pd(c41_20, _mm256_mul_pd(a41_20, b41));
_mm256_storeu_pd(&C[(i*56)+25], c41_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_20 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a41_20 = _mm_loadu_pd(&values[447]);
c41_20 = _mm_add_pd(c41_20, _mm_mul_pd(a41_20, b41));
_mm_storeu_pd(&C[(i*56)+25], c41_20);
__m128d c41_22 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a41_22 = _mm_loadu_pd(&values[449]);
c41_22 = _mm_add_pd(c41_22, _mm_mul_pd(a41_22, b41));
_mm_storeu_pd(&C[(i*56)+27], c41_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_24 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a41_24 = _mm256_loadu_pd(&values[451]);
c41_24 = _mm256_add_pd(c41_24, _mm256_mul_pd(a41_24, b41));
_mm256_storeu_pd(&C[(i*56)+29], c41_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_24 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a41_24 = _mm_loadu_pd(&values[451]);
c41_24 = _mm_add_pd(c41_24, _mm_mul_pd(a41_24, b41));
_mm_storeu_pd(&C[(i*56)+29], c41_24);
__m128d c41_26 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a41_26 = _mm_loadu_pd(&values[453]);
c41_26 = _mm_add_pd(c41_26, _mm_mul_pd(a41_26, b41));
_mm_storeu_pd(&C[(i*56)+31], c41_26);
#endif
__m128d c41_28 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a41_28 = _mm_loadu_pd(&values[455]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_28 = _mm_add_pd(c41_28, _mm_mul_pd(a41_28, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_28 = _mm_add_pd(c41_28, _mm_mul_pd(a41_28, b41));
#endif
_mm_storeu_pd(&C[(i*56)+33], c41_28);
#else
C[(i*56)+0] += values[427] * B[(i*56)+41];
C[(i*56)+1] += values[428] * B[(i*56)+41];
C[(i*56)+2] += values[429] * B[(i*56)+41];
C[(i*56)+3] += values[430] * B[(i*56)+41];
C[(i*56)+4] += values[431] * B[(i*56)+41];
C[(i*56)+5] += values[432] * B[(i*56)+41];
C[(i*56)+6] += values[433] * B[(i*56)+41];
C[(i*56)+7] += values[434] * B[(i*56)+41];
C[(i*56)+8] += values[435] * B[(i*56)+41];
C[(i*56)+9] += values[436] * B[(i*56)+41];
C[(i*56)+10] += values[437] * B[(i*56)+41];
C[(i*56)+11] += values[438] * B[(i*56)+41];
C[(i*56)+12] += values[439] * B[(i*56)+41];
C[(i*56)+13] += values[440] * B[(i*56)+41];
C[(i*56)+14] += values[441] * B[(i*56)+41];
C[(i*56)+15] += values[442] * B[(i*56)+41];
C[(i*56)+16] += values[443] * B[(i*56)+41];
C[(i*56)+17] += values[444] * B[(i*56)+41];
C[(i*56)+18] += values[445] * B[(i*56)+41];
C[(i*56)+19] += values[446] * B[(i*56)+41];
C[(i*56)+25] += values[447] * B[(i*56)+41];
C[(i*56)+26] += values[448] * B[(i*56)+41];
C[(i*56)+27] += values[449] * B[(i*56)+41];
C[(i*56)+28] += values[450] * B[(i*56)+41];
C[(i*56)+29] += values[451] * B[(i*56)+41];
C[(i*56)+30] += values[452] * B[(i*56)+41];
C[(i*56)+31] += values[453] * B[(i*56)+41];
C[(i*56)+32] += values[454] * B[(i*56)+41];
C[(i*56)+33] += values[455] * B[(i*56)+41];
C[(i*56)+34] += values[456] * B[(i*56)+41];
#endif
#ifndef NDEBUG
num_flops += 60;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b42 = _mm256_broadcast_sd(&B[(i*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b42 = _mm_loaddup_pd(&B[(i*56)+42]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a42_0 = _mm256_loadu_pd(&values[457]);
c42_0 = _mm256_add_pd(c42_0, _mm256_mul_pd(a42_0, b42));
_mm256_storeu_pd(&C[(i*56)+0], c42_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a42_0 = _mm_loadu_pd(&values[457]);
c42_0 = _mm_add_pd(c42_0, _mm_mul_pd(a42_0, b42));
_mm_storeu_pd(&C[(i*56)+0], c42_0);
__m128d c42_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a42_2 = _mm_loadu_pd(&values[459]);
c42_2 = _mm_add_pd(c42_2, _mm_mul_pd(a42_2, b42));
_mm_storeu_pd(&C[(i*56)+2], c42_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a42_4 = _mm256_loadu_pd(&values[461]);
c42_4 = _mm256_add_pd(c42_4, _mm256_mul_pd(a42_4, b42));
_mm256_storeu_pd(&C[(i*56)+4], c42_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a42_4 = _mm_loadu_pd(&values[461]);
c42_4 = _mm_add_pd(c42_4, _mm_mul_pd(a42_4, b42));
_mm_storeu_pd(&C[(i*56)+4], c42_4);
__m128d c42_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a42_6 = _mm_loadu_pd(&values[463]);
c42_6 = _mm_add_pd(c42_6, _mm_mul_pd(a42_6, b42));
_mm_storeu_pd(&C[(i*56)+6], c42_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a42_8 = _mm256_loadu_pd(&values[465]);
c42_8 = _mm256_add_pd(c42_8, _mm256_mul_pd(a42_8, b42));
_mm256_storeu_pd(&C[(i*56)+8], c42_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a42_8 = _mm_loadu_pd(&values[465]);
c42_8 = _mm_add_pd(c42_8, _mm_mul_pd(a42_8, b42));
_mm_storeu_pd(&C[(i*56)+8], c42_8);
__m128d c42_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a42_10 = _mm_loadu_pd(&values[467]);
c42_10 = _mm_add_pd(c42_10, _mm_mul_pd(a42_10, b42));
_mm_storeu_pd(&C[(i*56)+10], c42_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a42_12 = _mm256_loadu_pd(&values[469]);
c42_12 = _mm256_add_pd(c42_12, _mm256_mul_pd(a42_12, b42));
_mm256_storeu_pd(&C[(i*56)+12], c42_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a42_12 = _mm_loadu_pd(&values[469]);
c42_12 = _mm_add_pd(c42_12, _mm_mul_pd(a42_12, b42));
_mm_storeu_pd(&C[(i*56)+12], c42_12);
__m128d c42_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a42_14 = _mm_loadu_pd(&values[471]);
c42_14 = _mm_add_pd(c42_14, _mm_mul_pd(a42_14, b42));
_mm_storeu_pd(&C[(i*56)+14], c42_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a42_16 = _mm256_loadu_pd(&values[473]);
c42_16 = _mm256_add_pd(c42_16, _mm256_mul_pd(a42_16, b42));
_mm256_storeu_pd(&C[(i*56)+16], c42_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a42_16 = _mm_loadu_pd(&values[473]);
c42_16 = _mm_add_pd(c42_16, _mm_mul_pd(a42_16, b42));
_mm_storeu_pd(&C[(i*56)+16], c42_16);
__m128d c42_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a42_18 = _mm_loadu_pd(&values[475]);
c42_18 = _mm_add_pd(c42_18, _mm_mul_pd(a42_18, b42));
_mm_storeu_pd(&C[(i*56)+18], c42_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_20 = _mm256_loadu_pd(&C[(i*56)+25]);
__m256d a42_20 = _mm256_loadu_pd(&values[477]);
c42_20 = _mm256_add_pd(c42_20, _mm256_mul_pd(a42_20, b42));
_mm256_storeu_pd(&C[(i*56)+25], c42_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_20 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a42_20 = _mm_loadu_pd(&values[477]);
c42_20 = _mm_add_pd(c42_20, _mm_mul_pd(a42_20, b42));
_mm_storeu_pd(&C[(i*56)+25], c42_20);
__m128d c42_22 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a42_22 = _mm_loadu_pd(&values[479]);
c42_22 = _mm_add_pd(c42_22, _mm_mul_pd(a42_22, b42));
_mm_storeu_pd(&C[(i*56)+27], c42_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_24 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a42_24 = _mm256_loadu_pd(&values[481]);
c42_24 = _mm256_add_pd(c42_24, _mm256_mul_pd(a42_24, b42));
_mm256_storeu_pd(&C[(i*56)+29], c42_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_24 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a42_24 = _mm_loadu_pd(&values[481]);
c42_24 = _mm_add_pd(c42_24, _mm_mul_pd(a42_24, b42));
_mm_storeu_pd(&C[(i*56)+29], c42_24);
__m128d c42_26 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a42_26 = _mm_loadu_pd(&values[483]);
c42_26 = _mm_add_pd(c42_26, _mm_mul_pd(a42_26, b42));
_mm_storeu_pd(&C[(i*56)+31], c42_26);
#endif
__m128d c42_28 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a42_28 = _mm_loadu_pd(&values[485]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_28 = _mm_add_pd(c42_28, _mm_mul_pd(a42_28, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_28 = _mm_add_pd(c42_28, _mm_mul_pd(a42_28, b42));
#endif
_mm_storeu_pd(&C[(i*56)+33], c42_28);
#else
C[(i*56)+0] += values[457] * B[(i*56)+42];
C[(i*56)+1] += values[458] * B[(i*56)+42];
C[(i*56)+2] += values[459] * B[(i*56)+42];
C[(i*56)+3] += values[460] * B[(i*56)+42];
C[(i*56)+4] += values[461] * B[(i*56)+42];
C[(i*56)+5] += values[462] * B[(i*56)+42];
C[(i*56)+6] += values[463] * B[(i*56)+42];
C[(i*56)+7] += values[464] * B[(i*56)+42];
C[(i*56)+8] += values[465] * B[(i*56)+42];
C[(i*56)+9] += values[466] * B[(i*56)+42];
C[(i*56)+10] += values[467] * B[(i*56)+42];
C[(i*56)+11] += values[468] * B[(i*56)+42];
C[(i*56)+12] += values[469] * B[(i*56)+42];
C[(i*56)+13] += values[470] * B[(i*56)+42];
C[(i*56)+14] += values[471] * B[(i*56)+42];
C[(i*56)+15] += values[472] * B[(i*56)+42];
C[(i*56)+16] += values[473] * B[(i*56)+42];
C[(i*56)+17] += values[474] * B[(i*56)+42];
C[(i*56)+18] += values[475] * B[(i*56)+42];
C[(i*56)+19] += values[476] * B[(i*56)+42];
C[(i*56)+25] += values[477] * B[(i*56)+42];
C[(i*56)+26] += values[478] * B[(i*56)+42];
C[(i*56)+27] += values[479] * B[(i*56)+42];
C[(i*56)+28] += values[480] * B[(i*56)+42];
C[(i*56)+29] += values[481] * B[(i*56)+42];
C[(i*56)+30] += values[482] * B[(i*56)+42];
C[(i*56)+31] += values[483] * B[(i*56)+42];
C[(i*56)+32] += values[484] * B[(i*56)+42];
C[(i*56)+33] += values[485] * B[(i*56)+42];
C[(i*56)+34] += values[486] * B[(i*56)+42];
#endif
#ifndef NDEBUG
num_flops += 60;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b43 = _mm256_broadcast_sd(&B[(i*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b43 = _mm_loaddup_pd(&B[(i*56)+43]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a43_0 = _mm256_loadu_pd(&values[487]);
c43_0 = _mm256_add_pd(c43_0, _mm256_mul_pd(a43_0, b43));
_mm256_storeu_pd(&C[(i*56)+0], c43_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a43_0 = _mm_loadu_pd(&values[487]);
c43_0 = _mm_add_pd(c43_0, _mm_mul_pd(a43_0, b43));
_mm_storeu_pd(&C[(i*56)+0], c43_0);
__m128d c43_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a43_2 = _mm_loadu_pd(&values[489]);
c43_2 = _mm_add_pd(c43_2, _mm_mul_pd(a43_2, b43));
_mm_storeu_pd(&C[(i*56)+2], c43_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a43_4 = _mm256_loadu_pd(&values[491]);
c43_4 = _mm256_add_pd(c43_4, _mm256_mul_pd(a43_4, b43));
_mm256_storeu_pd(&C[(i*56)+4], c43_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a43_4 = _mm_loadu_pd(&values[491]);
c43_4 = _mm_add_pd(c43_4, _mm_mul_pd(a43_4, b43));
_mm_storeu_pd(&C[(i*56)+4], c43_4);
__m128d c43_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a43_6 = _mm_loadu_pd(&values[493]);
c43_6 = _mm_add_pd(c43_6, _mm_mul_pd(a43_6, b43));
_mm_storeu_pd(&C[(i*56)+6], c43_6);
#endif
__m128d c43_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a43_8 = _mm_loadu_pd(&values[495]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_8 = _mm_add_pd(c43_8, _mm_mul_pd(a43_8, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_8 = _mm_add_pd(c43_8, _mm_mul_pd(a43_8, b43));
#endif
_mm_storeu_pd(&C[(i*56)+8], c43_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_10 = _mm256_loadu_pd(&C[(i*56)+11]);
__m256d a43_10 = _mm256_loadu_pd(&values[497]);
c43_10 = _mm256_add_pd(c43_10, _mm256_mul_pd(a43_10, b43));
_mm256_storeu_pd(&C[(i*56)+11], c43_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_10 = _mm_loadu_pd(&C[(i*56)+11]);
__m128d a43_10 = _mm_loadu_pd(&values[497]);
c43_10 = _mm_add_pd(c43_10, _mm_mul_pd(a43_10, b43));
_mm_storeu_pd(&C[(i*56)+11], c43_10);
__m128d c43_12 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a43_12 = _mm_loadu_pd(&values[499]);
c43_12 = _mm_add_pd(c43_12, _mm_mul_pd(a43_12, b43));
_mm_storeu_pd(&C[(i*56)+13], c43_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_14 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a43_14 = _mm256_loadu_pd(&values[501]);
c43_14 = _mm256_add_pd(c43_14, _mm256_mul_pd(a43_14, b43));
_mm256_storeu_pd(&C[(i*56)+15], c43_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_14 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a43_14 = _mm_loadu_pd(&values[501]);
c43_14 = _mm_add_pd(c43_14, _mm_mul_pd(a43_14, b43));
_mm_storeu_pd(&C[(i*56)+15], c43_14);
__m128d c43_16 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a43_16 = _mm_loadu_pd(&values[503]);
c43_16 = _mm_add_pd(c43_16, _mm_mul_pd(a43_16, b43));
_mm_storeu_pd(&C[(i*56)+17], c43_16);
#endif
__m128d c43_18 = _mm_load_sd(&C[(i*56)+19]);
__m128d a43_18 = _mm_load_sd(&values[505]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_18 = _mm_add_sd(c43_18, _mm_mul_sd(a43_18, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_18 = _mm_add_sd(c43_18, _mm_mul_sd(a43_18, b43));
#endif
_mm_store_sd(&C[(i*56)+19], c43_18);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_19 = _mm256_loadu_pd(&C[(i*56)+26]);
__m256d a43_19 = _mm256_loadu_pd(&values[506]);
c43_19 = _mm256_add_pd(c43_19, _mm256_mul_pd(a43_19, b43));
_mm256_storeu_pd(&C[(i*56)+26], c43_19);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_19 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a43_19 = _mm_loadu_pd(&values[506]);
c43_19 = _mm_add_pd(c43_19, _mm_mul_pd(a43_19, b43));
_mm_storeu_pd(&C[(i*56)+26], c43_19);
__m128d c43_21 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a43_21 = _mm_loadu_pd(&values[508]);
c43_21 = _mm_add_pd(c43_21, _mm_mul_pd(a43_21, b43));
_mm_storeu_pd(&C[(i*56)+28], c43_21);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_23 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a43_23 = _mm256_loadu_pd(&values[510]);
c43_23 = _mm256_add_pd(c43_23, _mm256_mul_pd(a43_23, b43));
_mm256_storeu_pd(&C[(i*56)+30], c43_23);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_23 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a43_23 = _mm_loadu_pd(&values[510]);
c43_23 = _mm_add_pd(c43_23, _mm_mul_pd(a43_23, b43));
_mm_storeu_pd(&C[(i*56)+30], c43_23);
__m128d c43_25 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a43_25 = _mm_loadu_pd(&values[512]);
c43_25 = _mm_add_pd(c43_25, _mm_mul_pd(a43_25, b43));
_mm_storeu_pd(&C[(i*56)+32], c43_25);
#endif
__m128d c43_27 = _mm_load_sd(&C[(i*56)+34]);
__m128d a43_27 = _mm_load_sd(&values[514]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_27 = _mm_add_sd(c43_27, _mm_mul_sd(a43_27, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_27 = _mm_add_sd(c43_27, _mm_mul_sd(a43_27, b43));
#endif
_mm_store_sd(&C[(i*56)+34], c43_27);
#else
C[(i*56)+0] += values[487] * B[(i*56)+43];
C[(i*56)+1] += values[488] * B[(i*56)+43];
C[(i*56)+2] += values[489] * B[(i*56)+43];
C[(i*56)+3] += values[490] * B[(i*56)+43];
C[(i*56)+4] += values[491] * B[(i*56)+43];
C[(i*56)+5] += values[492] * B[(i*56)+43];
C[(i*56)+6] += values[493] * B[(i*56)+43];
C[(i*56)+7] += values[494] * B[(i*56)+43];
C[(i*56)+8] += values[495] * B[(i*56)+43];
C[(i*56)+9] += values[496] * B[(i*56)+43];
C[(i*56)+11] += values[497] * B[(i*56)+43];
C[(i*56)+12] += values[498] * B[(i*56)+43];
C[(i*56)+13] += values[499] * B[(i*56)+43];
C[(i*56)+14] += values[500] * B[(i*56)+43];
C[(i*56)+15] += values[501] * B[(i*56)+43];
C[(i*56)+16] += values[502] * B[(i*56)+43];
C[(i*56)+17] += values[503] * B[(i*56)+43];
C[(i*56)+18] += values[504] * B[(i*56)+43];
C[(i*56)+19] += values[505] * B[(i*56)+43];
C[(i*56)+26] += values[506] * B[(i*56)+43];
C[(i*56)+27] += values[507] * B[(i*56)+43];
C[(i*56)+28] += values[508] * B[(i*56)+43];
C[(i*56)+29] += values[509] * B[(i*56)+43];
C[(i*56)+30] += values[510] * B[(i*56)+43];
C[(i*56)+31] += values[511] * B[(i*56)+43];
C[(i*56)+32] += values[512] * B[(i*56)+43];
C[(i*56)+33] += values[513] * B[(i*56)+43];
C[(i*56)+34] += values[514] * B[(i*56)+43];
#endif
#ifndef NDEBUG
num_flops += 56;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b44 = _mm256_broadcast_sd(&B[(i*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b44 = _mm_loaddup_pd(&B[(i*56)+44]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a44_0 = _mm256_loadu_pd(&values[515]);
c44_0 = _mm256_add_pd(c44_0, _mm256_mul_pd(a44_0, b44));
_mm256_storeu_pd(&C[(i*56)+0], c44_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a44_0 = _mm_loadu_pd(&values[515]);
c44_0 = _mm_add_pd(c44_0, _mm_mul_pd(a44_0, b44));
_mm_storeu_pd(&C[(i*56)+0], c44_0);
__m128d c44_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a44_2 = _mm_loadu_pd(&values[517]);
c44_2 = _mm_add_pd(c44_2, _mm_mul_pd(a44_2, b44));
_mm_storeu_pd(&C[(i*56)+2], c44_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a44_4 = _mm256_loadu_pd(&values[519]);
c44_4 = _mm256_add_pd(c44_4, _mm256_mul_pd(a44_4, b44));
_mm256_storeu_pd(&C[(i*56)+5], c44_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a44_4 = _mm_loadu_pd(&values[519]);
c44_4 = _mm_add_pd(c44_4, _mm_mul_pd(a44_4, b44));
_mm_storeu_pd(&C[(i*56)+5], c44_4);
__m128d c44_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a44_6 = _mm_loadu_pd(&values[521]);
c44_6 = _mm_add_pd(c44_6, _mm_mul_pd(a44_6, b44));
_mm_storeu_pd(&C[(i*56)+7], c44_6);
#endif
__m128d c44_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a44_8 = _mm_load_sd(&values[523]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_8 = _mm_add_sd(c44_8, _mm_mul_sd(a44_8, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_8 = _mm_add_sd(c44_8, _mm_mul_sd(a44_8, b44));
#endif
_mm_store_sd(&C[(i*56)+9], c44_8);
__m128d c44_9 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a44_9 = _mm_loadu_pd(&values[524]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_9 = _mm_add_pd(c44_9, _mm_mul_pd(a44_9, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_9 = _mm_add_pd(c44_9, _mm_mul_pd(a44_9, b44));
#endif
_mm_storeu_pd(&C[(i*56)+12], c44_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_11 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a44_11 = _mm256_loadu_pd(&values[526]);
c44_11 = _mm256_add_pd(c44_11, _mm256_mul_pd(a44_11, b44));
_mm256_storeu_pd(&C[(i*56)+15], c44_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_11 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a44_11 = _mm_loadu_pd(&values[526]);
c44_11 = _mm_add_pd(c44_11, _mm_mul_pd(a44_11, b44));
_mm_storeu_pd(&C[(i*56)+15], c44_11);
__m128d c44_13 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a44_13 = _mm_loadu_pd(&values[528]);
c44_13 = _mm_add_pd(c44_13, _mm_mul_pd(a44_13, b44));
_mm_storeu_pd(&C[(i*56)+17], c44_13);
#endif
__m128d c44_15 = _mm_load_sd(&C[(i*56)+19]);
__m128d a44_15 = _mm_load_sd(&values[530]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_15 = _mm_add_sd(c44_15, _mm_mul_sd(a44_15, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_15 = _mm_add_sd(c44_15, _mm_mul_sd(a44_15, b44));
#endif
_mm_store_sd(&C[(i*56)+19], c44_15);
__m128d c44_16 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a44_16 = _mm_loadu_pd(&values[531]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_16 = _mm_add_pd(c44_16, _mm_mul_pd(a44_16, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_16 = _mm_add_pd(c44_16, _mm_mul_pd(a44_16, b44));
#endif
_mm_storeu_pd(&C[(i*56)+27], c44_16);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_18 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a44_18 = _mm256_loadu_pd(&values[533]);
c44_18 = _mm256_add_pd(c44_18, _mm256_mul_pd(a44_18, b44));
_mm256_storeu_pd(&C[(i*56)+30], c44_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_18 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a44_18 = _mm_loadu_pd(&values[533]);
c44_18 = _mm_add_pd(c44_18, _mm_mul_pd(a44_18, b44));
_mm_storeu_pd(&C[(i*56)+30], c44_18);
__m128d c44_20 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a44_20 = _mm_loadu_pd(&values[535]);
c44_20 = _mm_add_pd(c44_20, _mm_mul_pd(a44_20, b44));
_mm_storeu_pd(&C[(i*56)+32], c44_20);
#endif
__m128d c44_22 = _mm_load_sd(&C[(i*56)+34]);
__m128d a44_22 = _mm_load_sd(&values[537]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_22 = _mm_add_sd(c44_22, _mm_mul_sd(a44_22, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_22 = _mm_add_sd(c44_22, _mm_mul_sd(a44_22, b44));
#endif
_mm_store_sd(&C[(i*56)+34], c44_22);
#else
C[(i*56)+0] += values[515] * B[(i*56)+44];
C[(i*56)+1] += values[516] * B[(i*56)+44];
C[(i*56)+2] += values[517] * B[(i*56)+44];
C[(i*56)+3] += values[518] * B[(i*56)+44];
C[(i*56)+5] += values[519] * B[(i*56)+44];
C[(i*56)+6] += values[520] * B[(i*56)+44];
C[(i*56)+7] += values[521] * B[(i*56)+44];
C[(i*56)+8] += values[522] * B[(i*56)+44];
C[(i*56)+9] += values[523] * B[(i*56)+44];
C[(i*56)+12] += values[524] * B[(i*56)+44];
C[(i*56)+13] += values[525] * B[(i*56)+44];
C[(i*56)+15] += values[526] * B[(i*56)+44];
C[(i*56)+16] += values[527] * B[(i*56)+44];
C[(i*56)+17] += values[528] * B[(i*56)+44];
C[(i*56)+18] += values[529] * B[(i*56)+44];
C[(i*56)+19] += values[530] * B[(i*56)+44];
C[(i*56)+27] += values[531] * B[(i*56)+44];
C[(i*56)+28] += values[532] * B[(i*56)+44];
C[(i*56)+30] += values[533] * B[(i*56)+44];
C[(i*56)+31] += values[534] * B[(i*56)+44];
C[(i*56)+32] += values[535] * B[(i*56)+44];
C[(i*56)+33] += values[536] * B[(i*56)+44];
C[(i*56)+34] += values[537] * B[(i*56)+44];
#endif
#ifndef NDEBUG
num_flops += 46;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b45 = _mm256_broadcast_sd(&B[(i*56)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b45 = _mm_loaddup_pd(&B[(i*56)+45]);
#endif
__m128d c45_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a45_0 = _mm_load_sd(&values[538]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_0 = _mm_add_sd(c45_0, _mm_mul_sd(a45_0, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_0 = _mm_add_sd(c45_0, _mm_mul_sd(a45_0, b45));
#endif
_mm_store_sd(&C[(i*56)+0], c45_0);
__m128d c45_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a45_1 = _mm_loadu_pd(&values[539]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_1 = _mm_add_pd(c45_1, _mm_mul_pd(a45_1, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_1 = _mm_add_pd(c45_1, _mm_mul_pd(a45_1, b45));
#endif
_mm_storeu_pd(&C[(i*56)+2], c45_1);
__m128d c45_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a45_3 = _mm_load_sd(&values[541]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_3 = _mm_add_sd(c45_3, _mm_mul_sd(a45_3, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_3 = _mm_add_sd(c45_3, _mm_mul_sd(a45_3, b45));
#endif
_mm_store_sd(&C[(i*56)+6], c45_3);
__m128d c45_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a45_4 = _mm_loadu_pd(&values[542]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_4 = _mm_add_pd(c45_4, _mm_mul_pd(a45_4, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_4 = _mm_add_pd(c45_4, _mm_mul_pd(a45_4, b45));
#endif
_mm_storeu_pd(&C[(i*56)+8], c45_4);
__m128d c45_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a45_6 = _mm_load_sd(&values[544]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_6 = _mm_add_sd(c45_6, _mm_mul_sd(a45_6, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_6 = _mm_add_sd(c45_6, _mm_mul_sd(a45_6, b45));
#endif
_mm_store_sd(&C[(i*56)+13], c45_6);
__m128d c45_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a45_7 = _mm_load_sd(&values[545]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_7 = _mm_add_sd(c45_7, _mm_mul_sd(a45_7, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_7 = _mm_add_sd(c45_7, _mm_mul_sd(a45_7, b45));
#endif
_mm_store_sd(&C[(i*56)+16], c45_7);
__m128d c45_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a45_8 = _mm_loadu_pd(&values[546]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_8 = _mm_add_pd(c45_8, _mm_mul_pd(a45_8, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_8 = _mm_add_pd(c45_8, _mm_mul_pd(a45_8, b45));
#endif
_mm_storeu_pd(&C[(i*56)+18], c45_8);
__m128d c45_10 = _mm_load_sd(&C[(i*56)+28]);
__m128d a45_10 = _mm_load_sd(&values[548]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_10 = _mm_add_sd(c45_10, _mm_mul_sd(a45_10, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_10 = _mm_add_sd(c45_10, _mm_mul_sd(a45_10, b45));
#endif
_mm_store_sd(&C[(i*56)+28], c45_10);
__m128d c45_11 = _mm_load_sd(&C[(i*56)+31]);
__m128d a45_11 = _mm_load_sd(&values[549]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_11 = _mm_add_sd(c45_11, _mm_mul_sd(a45_11, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_11 = _mm_add_sd(c45_11, _mm_mul_sd(a45_11, b45));
#endif
_mm_store_sd(&C[(i*56)+31], c45_11);
__m128d c45_12 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a45_12 = _mm_loadu_pd(&values[550]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_12 = _mm_add_pd(c45_12, _mm_mul_pd(a45_12, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_12 = _mm_add_pd(c45_12, _mm_mul_pd(a45_12, b45));
#endif
_mm_storeu_pd(&C[(i*56)+33], c45_12);
#else
C[(i*56)+0] += values[538] * B[(i*56)+45];
C[(i*56)+2] += values[539] * B[(i*56)+45];
C[(i*56)+3] += values[540] * B[(i*56)+45];
C[(i*56)+6] += values[541] * B[(i*56)+45];
C[(i*56)+8] += values[542] * B[(i*56)+45];
C[(i*56)+9] += values[543] * B[(i*56)+45];
C[(i*56)+13] += values[544] * B[(i*56)+45];
C[(i*56)+16] += values[545] * B[(i*56)+45];
C[(i*56)+18] += values[546] * B[(i*56)+45];
C[(i*56)+19] += values[547] * B[(i*56)+45];
C[(i*56)+28] += values[548] * B[(i*56)+45];
C[(i*56)+31] += values[549] * B[(i*56)+45];
C[(i*56)+33] += values[550] * B[(i*56)+45];
C[(i*56)+34] += values[551] * B[(i*56)+45];
#endif
#ifndef NDEBUG
num_flops += 28;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b46 = _mm256_broadcast_sd(&B[(i*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b46 = _mm_loaddup_pd(&B[(i*56)+46]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a46_0 = _mm256_loadu_pd(&values[552]);
c46_0 = _mm256_add_pd(c46_0, _mm256_mul_pd(a46_0, b46));
_mm256_storeu_pd(&C[(i*56)+0], c46_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a46_0 = _mm_loadu_pd(&values[552]);
c46_0 = _mm_add_pd(c46_0, _mm_mul_pd(a46_0, b46));
_mm_storeu_pd(&C[(i*56)+0], c46_0);
__m128d c46_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a46_2 = _mm_loadu_pd(&values[554]);
c46_2 = _mm_add_pd(c46_2, _mm_mul_pd(a46_2, b46));
_mm_storeu_pd(&C[(i*56)+2], c46_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a46_4 = _mm256_loadu_pd(&values[556]);
c46_4 = _mm256_add_pd(c46_4, _mm256_mul_pd(a46_4, b46));
_mm256_storeu_pd(&C[(i*56)+4], c46_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a46_4 = _mm_loadu_pd(&values[556]);
c46_4 = _mm_add_pd(c46_4, _mm_mul_pd(a46_4, b46));
_mm_storeu_pd(&C[(i*56)+4], c46_4);
__m128d c46_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a46_6 = _mm_loadu_pd(&values[558]);
c46_6 = _mm_add_pd(c46_6, _mm_mul_pd(a46_6, b46));
_mm_storeu_pd(&C[(i*56)+6], c46_6);
#endif
__m128d c46_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a46_8 = _mm_loadu_pd(&values[560]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_8 = _mm_add_pd(c46_8, _mm_mul_pd(a46_8, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_8 = _mm_add_pd(c46_8, _mm_mul_pd(a46_8, b46));
#endif
_mm_storeu_pd(&C[(i*56)+8], c46_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_10 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a46_10 = _mm256_loadu_pd(&values[562]);
c46_10 = _mm256_add_pd(c46_10, _mm256_mul_pd(a46_10, b46));
_mm256_storeu_pd(&C[(i*56)+14], c46_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_10 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a46_10 = _mm_loadu_pd(&values[562]);
c46_10 = _mm_add_pd(c46_10, _mm_mul_pd(a46_10, b46));
_mm_storeu_pd(&C[(i*56)+14], c46_10);
__m128d c46_12 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a46_12 = _mm_loadu_pd(&values[564]);
c46_12 = _mm_add_pd(c46_12, _mm_mul_pd(a46_12, b46));
_mm_storeu_pd(&C[(i*56)+16], c46_12);
#endif
__m128d c46_14 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a46_14 = _mm_loadu_pd(&values[566]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_14 = _mm_add_pd(c46_14, _mm_mul_pd(a46_14, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_14 = _mm_add_pd(c46_14, _mm_mul_pd(a46_14, b46));
#endif
_mm_storeu_pd(&C[(i*56)+18], c46_14);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_16 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a46_16 = _mm256_loadu_pd(&values[568]);
c46_16 = _mm256_add_pd(c46_16, _mm256_mul_pd(a46_16, b46));
_mm256_storeu_pd(&C[(i*56)+29], c46_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_16 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a46_16 = _mm_loadu_pd(&values[568]);
c46_16 = _mm_add_pd(c46_16, _mm_mul_pd(a46_16, b46));
_mm_storeu_pd(&C[(i*56)+29], c46_16);
__m128d c46_18 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a46_18 = _mm_loadu_pd(&values[570]);
c46_18 = _mm_add_pd(c46_18, _mm_mul_pd(a46_18, b46));
_mm_storeu_pd(&C[(i*56)+31], c46_18);
#endif
__m128d c46_20 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a46_20 = _mm_loadu_pd(&values[572]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_20 = _mm_add_pd(c46_20, _mm_mul_pd(a46_20, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_20 = _mm_add_pd(c46_20, _mm_mul_pd(a46_20, b46));
#endif
_mm_storeu_pd(&C[(i*56)+33], c46_20);
#else
C[(i*56)+0] += values[552] * B[(i*56)+46];
C[(i*56)+1] += values[553] * B[(i*56)+46];
C[(i*56)+2] += values[554] * B[(i*56)+46];
C[(i*56)+3] += values[555] * B[(i*56)+46];
C[(i*56)+4] += values[556] * B[(i*56)+46];
C[(i*56)+5] += values[557] * B[(i*56)+46];
C[(i*56)+6] += values[558] * B[(i*56)+46];
C[(i*56)+7] += values[559] * B[(i*56)+46];
C[(i*56)+8] += values[560] * B[(i*56)+46];
C[(i*56)+9] += values[561] * B[(i*56)+46];
C[(i*56)+14] += values[562] * B[(i*56)+46];
C[(i*56)+15] += values[563] * B[(i*56)+46];
C[(i*56)+16] += values[564] * B[(i*56)+46];
C[(i*56)+17] += values[565] * B[(i*56)+46];
C[(i*56)+18] += values[566] * B[(i*56)+46];
C[(i*56)+19] += values[567] * B[(i*56)+46];
C[(i*56)+29] += values[568] * B[(i*56)+46];
C[(i*56)+30] += values[569] * B[(i*56)+46];
C[(i*56)+31] += values[570] * B[(i*56)+46];
C[(i*56)+32] += values[571] * B[(i*56)+46];
C[(i*56)+33] += values[572] * B[(i*56)+46];
C[(i*56)+34] += values[573] * B[(i*56)+46];
#endif
#ifndef NDEBUG
num_flops += 44;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b47 = _mm256_broadcast_sd(&B[(i*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b47 = _mm_loaddup_pd(&B[(i*56)+47]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a47_0 = _mm256_loadu_pd(&values[574]);
c47_0 = _mm256_add_pd(c47_0, _mm256_mul_pd(a47_0, b47));
_mm256_storeu_pd(&C[(i*56)+0], c47_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a47_0 = _mm_loadu_pd(&values[574]);
c47_0 = _mm_add_pd(c47_0, _mm_mul_pd(a47_0, b47));
_mm_storeu_pd(&C[(i*56)+0], c47_0);
__m128d c47_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a47_2 = _mm_loadu_pd(&values[576]);
c47_2 = _mm_add_pd(c47_2, _mm_mul_pd(a47_2, b47));
_mm_storeu_pd(&C[(i*56)+2], c47_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a47_4 = _mm256_loadu_pd(&values[578]);
c47_4 = _mm256_add_pd(c47_4, _mm256_mul_pd(a47_4, b47));
_mm256_storeu_pd(&C[(i*56)+4], c47_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a47_4 = _mm_loadu_pd(&values[578]);
c47_4 = _mm_add_pd(c47_4, _mm_mul_pd(a47_4, b47));
_mm_storeu_pd(&C[(i*56)+4], c47_4);
__m128d c47_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a47_6 = _mm_loadu_pd(&values[580]);
c47_6 = _mm_add_pd(c47_6, _mm_mul_pd(a47_6, b47));
_mm_storeu_pd(&C[(i*56)+6], c47_6);
#endif
__m128d c47_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a47_8 = _mm_loadu_pd(&values[582]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_8 = _mm_add_pd(c47_8, _mm_mul_pd(a47_8, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_8 = _mm_add_pd(c47_8, _mm_mul_pd(a47_8, b47));
#endif
_mm_storeu_pd(&C[(i*56)+8], c47_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_10 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a47_10 = _mm256_loadu_pd(&values[584]);
c47_10 = _mm256_add_pd(c47_10, _mm256_mul_pd(a47_10, b47));
_mm256_storeu_pd(&C[(i*56)+14], c47_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_10 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a47_10 = _mm_loadu_pd(&values[584]);
c47_10 = _mm_add_pd(c47_10, _mm_mul_pd(a47_10, b47));
_mm_storeu_pd(&C[(i*56)+14], c47_10);
__m128d c47_12 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a47_12 = _mm_loadu_pd(&values[586]);
c47_12 = _mm_add_pd(c47_12, _mm_mul_pd(a47_12, b47));
_mm_storeu_pd(&C[(i*56)+16], c47_12);
#endif
__m128d c47_14 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a47_14 = _mm_loadu_pd(&values[588]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_14 = _mm_add_pd(c47_14, _mm_mul_pd(a47_14, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_14 = _mm_add_pd(c47_14, _mm_mul_pd(a47_14, b47));
#endif
_mm_storeu_pd(&C[(i*56)+18], c47_14);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_16 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a47_16 = _mm256_loadu_pd(&values[590]);
c47_16 = _mm256_add_pd(c47_16, _mm256_mul_pd(a47_16, b47));
_mm256_storeu_pd(&C[(i*56)+29], c47_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_16 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a47_16 = _mm_loadu_pd(&values[590]);
c47_16 = _mm_add_pd(c47_16, _mm_mul_pd(a47_16, b47));
_mm_storeu_pd(&C[(i*56)+29], c47_16);
__m128d c47_18 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a47_18 = _mm_loadu_pd(&values[592]);
c47_18 = _mm_add_pd(c47_18, _mm_mul_pd(a47_18, b47));
_mm_storeu_pd(&C[(i*56)+31], c47_18);
#endif
__m128d c47_20 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a47_20 = _mm_loadu_pd(&values[594]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_20 = _mm_add_pd(c47_20, _mm_mul_pd(a47_20, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_20 = _mm_add_pd(c47_20, _mm_mul_pd(a47_20, b47));
#endif
_mm_storeu_pd(&C[(i*56)+33], c47_20);
#else
C[(i*56)+0] += values[574] * B[(i*56)+47];
C[(i*56)+1] += values[575] * B[(i*56)+47];
C[(i*56)+2] += values[576] * B[(i*56)+47];
C[(i*56)+3] += values[577] * B[(i*56)+47];
C[(i*56)+4] += values[578] * B[(i*56)+47];
C[(i*56)+5] += values[579] * B[(i*56)+47];
C[(i*56)+6] += values[580] * B[(i*56)+47];
C[(i*56)+7] += values[581] * B[(i*56)+47];
C[(i*56)+8] += values[582] * B[(i*56)+47];
C[(i*56)+9] += values[583] * B[(i*56)+47];
C[(i*56)+14] += values[584] * B[(i*56)+47];
C[(i*56)+15] += values[585] * B[(i*56)+47];
C[(i*56)+16] += values[586] * B[(i*56)+47];
C[(i*56)+17] += values[587] * B[(i*56)+47];
C[(i*56)+18] += values[588] * B[(i*56)+47];
C[(i*56)+19] += values[589] * B[(i*56)+47];
C[(i*56)+29] += values[590] * B[(i*56)+47];
C[(i*56)+30] += values[591] * B[(i*56)+47];
C[(i*56)+31] += values[592] * B[(i*56)+47];
C[(i*56)+32] += values[593] * B[(i*56)+47];
C[(i*56)+33] += values[594] * B[(i*56)+47];
C[(i*56)+34] += values[595] * B[(i*56)+47];
#endif
#ifndef NDEBUG
num_flops += 44;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b48 = _mm256_broadcast_sd(&B[(i*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b48 = _mm_loaddup_pd(&B[(i*56)+48]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a48_0 = _mm256_loadu_pd(&values[596]);
c48_0 = _mm256_add_pd(c48_0, _mm256_mul_pd(a48_0, b48));
_mm256_storeu_pd(&C[(i*56)+0], c48_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a48_0 = _mm_loadu_pd(&values[596]);
c48_0 = _mm_add_pd(c48_0, _mm_mul_pd(a48_0, b48));
_mm_storeu_pd(&C[(i*56)+0], c48_0);
__m128d c48_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a48_2 = _mm_loadu_pd(&values[598]);
c48_2 = _mm_add_pd(c48_2, _mm_mul_pd(a48_2, b48));
_mm_storeu_pd(&C[(i*56)+2], c48_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a48_4 = _mm256_loadu_pd(&values[600]);
c48_4 = _mm256_add_pd(c48_4, _mm256_mul_pd(a48_4, b48));
_mm256_storeu_pd(&C[(i*56)+5], c48_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a48_4 = _mm_loadu_pd(&values[600]);
c48_4 = _mm_add_pd(c48_4, _mm_mul_pd(a48_4, b48));
_mm_storeu_pd(&C[(i*56)+5], c48_4);
__m128d c48_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a48_6 = _mm_loadu_pd(&values[602]);
c48_6 = _mm_add_pd(c48_6, _mm_mul_pd(a48_6, b48));
_mm_storeu_pd(&C[(i*56)+7], c48_6);
#endif
__m128d c48_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a48_8 = _mm_load_sd(&values[604]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_8 = _mm_add_sd(c48_8, _mm_mul_sd(a48_8, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_8 = _mm_add_sd(c48_8, _mm_mul_sd(a48_8, b48));
#endif
_mm_store_sd(&C[(i*56)+9], c48_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_9 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a48_9 = _mm256_loadu_pd(&values[605]);
c48_9 = _mm256_add_pd(c48_9, _mm256_mul_pd(a48_9, b48));
_mm256_storeu_pd(&C[(i*56)+15], c48_9);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_9 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a48_9 = _mm_loadu_pd(&values[605]);
c48_9 = _mm_add_pd(c48_9, _mm_mul_pd(a48_9, b48));
_mm_storeu_pd(&C[(i*56)+15], c48_9);
__m128d c48_11 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a48_11 = _mm_loadu_pd(&values[607]);
c48_11 = _mm_add_pd(c48_11, _mm_mul_pd(a48_11, b48));
_mm_storeu_pd(&C[(i*56)+17], c48_11);
#endif
__m128d c48_13 = _mm_load_sd(&C[(i*56)+19]);
__m128d a48_13 = _mm_load_sd(&values[609]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_13 = _mm_add_sd(c48_13, _mm_mul_sd(a48_13, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_13 = _mm_add_sd(c48_13, _mm_mul_sd(a48_13, b48));
#endif
_mm_store_sd(&C[(i*56)+19], c48_13);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_14 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a48_14 = _mm256_loadu_pd(&values[610]);
c48_14 = _mm256_add_pd(c48_14, _mm256_mul_pd(a48_14, b48));
_mm256_storeu_pd(&C[(i*56)+30], c48_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_14 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a48_14 = _mm_loadu_pd(&values[610]);
c48_14 = _mm_add_pd(c48_14, _mm_mul_pd(a48_14, b48));
_mm_storeu_pd(&C[(i*56)+30], c48_14);
__m128d c48_16 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a48_16 = _mm_loadu_pd(&values[612]);
c48_16 = _mm_add_pd(c48_16, _mm_mul_pd(a48_16, b48));
_mm_storeu_pd(&C[(i*56)+32], c48_16);
#endif
__m128d c48_18 = _mm_load_sd(&C[(i*56)+34]);
__m128d a48_18 = _mm_load_sd(&values[614]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_18 = _mm_add_sd(c48_18, _mm_mul_sd(a48_18, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_18 = _mm_add_sd(c48_18, _mm_mul_sd(a48_18, b48));
#endif
_mm_store_sd(&C[(i*56)+34], c48_18);
#else
C[(i*56)+0] += values[596] * B[(i*56)+48];
C[(i*56)+1] += values[597] * B[(i*56)+48];
C[(i*56)+2] += values[598] * B[(i*56)+48];
C[(i*56)+3] += values[599] * B[(i*56)+48];
C[(i*56)+5] += values[600] * B[(i*56)+48];
C[(i*56)+6] += values[601] * B[(i*56)+48];
C[(i*56)+7] += values[602] * B[(i*56)+48];
C[(i*56)+8] += values[603] * B[(i*56)+48];
C[(i*56)+9] += values[604] * B[(i*56)+48];
C[(i*56)+15] += values[605] * B[(i*56)+48];
C[(i*56)+16] += values[606] * B[(i*56)+48];
C[(i*56)+17] += values[607] * B[(i*56)+48];
C[(i*56)+18] += values[608] * B[(i*56)+48];
C[(i*56)+19] += values[609] * B[(i*56)+48];
C[(i*56)+30] += values[610] * B[(i*56)+48];
C[(i*56)+31] += values[611] * B[(i*56)+48];
C[(i*56)+32] += values[612] * B[(i*56)+48];
C[(i*56)+33] += values[613] * B[(i*56)+48];
C[(i*56)+34] += values[614] * B[(i*56)+48];
#endif
#ifndef NDEBUG
num_flops += 38;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b49 = _mm256_broadcast_sd(&B[(i*56)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b49 = _mm_loaddup_pd(&B[(i*56)+49]);
#endif
__m128d c49_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a49_0 = _mm_load_sd(&values[615]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_0 = _mm_add_sd(c49_0, _mm_mul_sd(a49_0, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_0 = _mm_add_sd(c49_0, _mm_mul_sd(a49_0, b49));
#endif
_mm_store_sd(&C[(i*56)+0], c49_0);
__m128d c49_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a49_1 = _mm_loadu_pd(&values[616]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_1 = _mm_add_pd(c49_1, _mm_mul_pd(a49_1, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_1 = _mm_add_pd(c49_1, _mm_mul_pd(a49_1, b49));
#endif
_mm_storeu_pd(&C[(i*56)+2], c49_1);
__m128d c49_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a49_3 = _mm_load_sd(&values[618]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_3 = _mm_add_sd(c49_3, _mm_mul_sd(a49_3, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_3 = _mm_add_sd(c49_3, _mm_mul_sd(a49_3, b49));
#endif
_mm_store_sd(&C[(i*56)+6], c49_3);
__m128d c49_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a49_4 = _mm_loadu_pd(&values[619]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_4 = _mm_add_pd(c49_4, _mm_mul_pd(a49_4, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_4 = _mm_add_pd(c49_4, _mm_mul_pd(a49_4, b49));
#endif
_mm_storeu_pd(&C[(i*56)+8], c49_4);
__m128d c49_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a49_6 = _mm_load_sd(&values[621]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_6 = _mm_add_sd(c49_6, _mm_mul_sd(a49_6, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_6 = _mm_add_sd(c49_6, _mm_mul_sd(a49_6, b49));
#endif
_mm_store_sd(&C[(i*56)+16], c49_6);
__m128d c49_7 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a49_7 = _mm_loadu_pd(&values[622]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_7 = _mm_add_pd(c49_7, _mm_mul_pd(a49_7, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_7 = _mm_add_pd(c49_7, _mm_mul_pd(a49_7, b49));
#endif
_mm_storeu_pd(&C[(i*56)+18], c49_7);
__m128d c49_9 = _mm_load_sd(&C[(i*56)+31]);
__m128d a49_9 = _mm_load_sd(&values[624]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_9 = _mm_add_sd(c49_9, _mm_mul_sd(a49_9, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_9 = _mm_add_sd(c49_9, _mm_mul_sd(a49_9, b49));
#endif
_mm_store_sd(&C[(i*56)+31], c49_9);
__m128d c49_10 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a49_10 = _mm_loadu_pd(&values[625]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_10 = _mm_add_pd(c49_10, _mm_mul_pd(a49_10, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_10 = _mm_add_pd(c49_10, _mm_mul_pd(a49_10, b49));
#endif
_mm_storeu_pd(&C[(i*56)+33], c49_10);
#else
C[(i*56)+0] += values[615] * B[(i*56)+49];
C[(i*56)+2] += values[616] * B[(i*56)+49];
C[(i*56)+3] += values[617] * B[(i*56)+49];
C[(i*56)+6] += values[618] * B[(i*56)+49];
C[(i*56)+8] += values[619] * B[(i*56)+49];
C[(i*56)+9] += values[620] * B[(i*56)+49];
C[(i*56)+16] += values[621] * B[(i*56)+49];
C[(i*56)+18] += values[622] * B[(i*56)+49];
C[(i*56)+19] += values[623] * B[(i*56)+49];
C[(i*56)+31] += values[624] * B[(i*56)+49];
C[(i*56)+33] += values[625] * B[(i*56)+49];
C[(i*56)+34] += values[626] * B[(i*56)+49];
#endif
#ifndef NDEBUG
num_flops += 24;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b50 = _mm256_broadcast_sd(&B[(i*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b50 = _mm_loaddup_pd(&B[(i*56)+50]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c50_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a50_0 = _mm256_loadu_pd(&values[627]);
c50_0 = _mm256_add_pd(c50_0, _mm256_mul_pd(a50_0, b50));
_mm256_storeu_pd(&C[(i*56)+0], c50_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c50_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a50_0 = _mm_loadu_pd(&values[627]);
c50_0 = _mm_add_pd(c50_0, _mm_mul_pd(a50_0, b50));
_mm_storeu_pd(&C[(i*56)+0], c50_0);
__m128d c50_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a50_2 = _mm_loadu_pd(&values[629]);
c50_2 = _mm_add_pd(c50_2, _mm_mul_pd(a50_2, b50));
_mm_storeu_pd(&C[(i*56)+2], c50_2);
#endif
__m128d c50_4 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a50_4 = _mm_loadu_pd(&values[631]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_4 = _mm_add_pd(c50_4, _mm_mul_pd(a50_4, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_4 = _mm_add_pd(c50_4, _mm_mul_pd(a50_4, b50));
#endif
_mm_storeu_pd(&C[(i*56)+7], c50_4);
__m128d c50_6 = _mm_load_sd(&C[(i*56)+9]);
__m128d a50_6 = _mm_load_sd(&values[633]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_6 = _mm_add_sd(c50_6, _mm_mul_sd(a50_6, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_6 = _mm_add_sd(c50_6, _mm_mul_sd(a50_6, b50));
#endif
_mm_store_sd(&C[(i*56)+9], c50_6);
__m128d c50_7 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a50_7 = _mm_loadu_pd(&values[634]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_7 = _mm_add_pd(c50_7, _mm_mul_pd(a50_7, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_7 = _mm_add_pd(c50_7, _mm_mul_pd(a50_7, b50));
#endif
_mm_storeu_pd(&C[(i*56)+17], c50_7);
__m128d c50_9 = _mm_load_sd(&C[(i*56)+19]);
__m128d a50_9 = _mm_load_sd(&values[636]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_9 = _mm_add_sd(c50_9, _mm_mul_sd(a50_9, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_9 = _mm_add_sd(c50_9, _mm_mul_sd(a50_9, b50));
#endif
_mm_store_sd(&C[(i*56)+19], c50_9);
__m128d c50_10 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a50_10 = _mm_loadu_pd(&values[637]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_10 = _mm_add_pd(c50_10, _mm_mul_pd(a50_10, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_10 = _mm_add_pd(c50_10, _mm_mul_pd(a50_10, b50));
#endif
_mm_storeu_pd(&C[(i*56)+32], c50_10);
__m128d c50_12 = _mm_load_sd(&C[(i*56)+34]);
__m128d a50_12 = _mm_load_sd(&values[639]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_12 = _mm_add_sd(c50_12, _mm_mul_sd(a50_12, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_12 = _mm_add_sd(c50_12, _mm_mul_sd(a50_12, b50));
#endif
_mm_store_sd(&C[(i*56)+34], c50_12);
#else
C[(i*56)+0] += values[627] * B[(i*56)+50];
C[(i*56)+1] += values[628] * B[(i*56)+50];
C[(i*56)+2] += values[629] * B[(i*56)+50];
C[(i*56)+3] += values[630] * B[(i*56)+50];
C[(i*56)+7] += values[631] * B[(i*56)+50];
C[(i*56)+8] += values[632] * B[(i*56)+50];
C[(i*56)+9] += values[633] * B[(i*56)+50];
C[(i*56)+17] += values[634] * B[(i*56)+50];
C[(i*56)+18] += values[635] * B[(i*56)+50];
C[(i*56)+19] += values[636] * B[(i*56)+50];
C[(i*56)+32] += values[637] * B[(i*56)+50];
C[(i*56)+33] += values[638] * B[(i*56)+50];
C[(i*56)+34] += values[639] * B[(i*56)+50];
#endif
#ifndef NDEBUG
num_flops += 26;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b51 = _mm256_broadcast_sd(&B[(i*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b51 = _mm_loaddup_pd(&B[(i*56)+51]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c51_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a51_0 = _mm256_loadu_pd(&values[640]);
c51_0 = _mm256_add_pd(c51_0, _mm256_mul_pd(a51_0, b51));
_mm256_storeu_pd(&C[(i*56)+0], c51_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c51_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a51_0 = _mm_loadu_pd(&values[640]);
c51_0 = _mm_add_pd(c51_0, _mm_mul_pd(a51_0, b51));
_mm_storeu_pd(&C[(i*56)+0], c51_0);
__m128d c51_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a51_2 = _mm_loadu_pd(&values[642]);
c51_2 = _mm_add_pd(c51_2, _mm_mul_pd(a51_2, b51));
_mm_storeu_pd(&C[(i*56)+2], c51_2);
#endif
__m128d c51_4 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a51_4 = _mm_loadu_pd(&values[644]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_4 = _mm_add_pd(c51_4, _mm_mul_pd(a51_4, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_4 = _mm_add_pd(c51_4, _mm_mul_pd(a51_4, b51));
#endif
_mm_storeu_pd(&C[(i*56)+7], c51_4);
__m128d c51_6 = _mm_load_sd(&C[(i*56)+9]);
__m128d a51_6 = _mm_load_sd(&values[646]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_6 = _mm_add_sd(c51_6, _mm_mul_sd(a51_6, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_6 = _mm_add_sd(c51_6, _mm_mul_sd(a51_6, b51));
#endif
_mm_store_sd(&C[(i*56)+9], c51_6);
__m128d c51_7 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a51_7 = _mm_loadu_pd(&values[647]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_7 = _mm_add_pd(c51_7, _mm_mul_pd(a51_7, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_7 = _mm_add_pd(c51_7, _mm_mul_pd(a51_7, b51));
#endif
_mm_storeu_pd(&C[(i*56)+17], c51_7);
__m128d c51_9 = _mm_load_sd(&C[(i*56)+19]);
__m128d a51_9 = _mm_load_sd(&values[649]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_9 = _mm_add_sd(c51_9, _mm_mul_sd(a51_9, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_9 = _mm_add_sd(c51_9, _mm_mul_sd(a51_9, b51));
#endif
_mm_store_sd(&C[(i*56)+19], c51_9);
__m128d c51_10 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a51_10 = _mm_loadu_pd(&values[650]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_10 = _mm_add_pd(c51_10, _mm_mul_pd(a51_10, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_10 = _mm_add_pd(c51_10, _mm_mul_pd(a51_10, b51));
#endif
_mm_storeu_pd(&C[(i*56)+32], c51_10);
__m128d c51_12 = _mm_load_sd(&C[(i*56)+34]);
__m128d a51_12 = _mm_load_sd(&values[652]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_12 = _mm_add_sd(c51_12, _mm_mul_sd(a51_12, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_12 = _mm_add_sd(c51_12, _mm_mul_sd(a51_12, b51));
#endif
_mm_store_sd(&C[(i*56)+34], c51_12);
#else
C[(i*56)+0] += values[640] * B[(i*56)+51];
C[(i*56)+1] += values[641] * B[(i*56)+51];
C[(i*56)+2] += values[642] * B[(i*56)+51];
C[(i*56)+3] += values[643] * B[(i*56)+51];
C[(i*56)+7] += values[644] * B[(i*56)+51];
C[(i*56)+8] += values[645] * B[(i*56)+51];
C[(i*56)+9] += values[646] * B[(i*56)+51];
C[(i*56)+17] += values[647] * B[(i*56)+51];
C[(i*56)+18] += values[648] * B[(i*56)+51];
C[(i*56)+19] += values[649] * B[(i*56)+51];
C[(i*56)+32] += values[650] * B[(i*56)+51];
C[(i*56)+33] += values[651] * B[(i*56)+51];
C[(i*56)+34] += values[652] * B[(i*56)+51];
#endif
#ifndef NDEBUG
num_flops += 26;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b52 = _mm256_broadcast_sd(&B[(i*56)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b52 = _mm_loaddup_pd(&B[(i*56)+52]);
#endif
__m128d c52_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a52_0 = _mm_load_sd(&values[653]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_0 = _mm_add_sd(c52_0, _mm_mul_sd(a52_0, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_0 = _mm_add_sd(c52_0, _mm_mul_sd(a52_0, b52));
#endif
_mm_store_sd(&C[(i*56)+0], c52_0);
__m128d c52_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a52_1 = _mm_loadu_pd(&values[654]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_1 = _mm_add_pd(c52_1, _mm_mul_pd(a52_1, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_1 = _mm_add_pd(c52_1, _mm_mul_pd(a52_1, b52));
#endif
_mm_storeu_pd(&C[(i*56)+2], c52_1);
__m128d c52_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a52_3 = _mm_loadu_pd(&values[656]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_3 = _mm_add_pd(c52_3, _mm_mul_pd(a52_3, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_3 = _mm_add_pd(c52_3, _mm_mul_pd(a52_3, b52));
#endif
_mm_storeu_pd(&C[(i*56)+8], c52_3);
__m128d c52_5 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a52_5 = _mm_loadu_pd(&values[658]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_5 = _mm_add_pd(c52_5, _mm_mul_pd(a52_5, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_5 = _mm_add_pd(c52_5, _mm_mul_pd(a52_5, b52));
#endif
_mm_storeu_pd(&C[(i*56)+18], c52_5);
__m128d c52_7 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a52_7 = _mm_loadu_pd(&values[660]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_7 = _mm_add_pd(c52_7, _mm_mul_pd(a52_7, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_7 = _mm_add_pd(c52_7, _mm_mul_pd(a52_7, b52));
#endif
_mm_storeu_pd(&C[(i*56)+33], c52_7);
#else
C[(i*56)+0] += values[653] * B[(i*56)+52];
C[(i*56)+2] += values[654] * B[(i*56)+52];
C[(i*56)+3] += values[655] * B[(i*56)+52];
C[(i*56)+8] += values[656] * B[(i*56)+52];
C[(i*56)+9] += values[657] * B[(i*56)+52];
C[(i*56)+18] += values[658] * B[(i*56)+52];
C[(i*56)+19] += values[659] * B[(i*56)+52];
C[(i*56)+33] += values[660] * B[(i*56)+52];
C[(i*56)+34] += values[661] * B[(i*56)+52];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b53 = _mm256_broadcast_sd(&B[(i*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b53 = _mm_loaddup_pd(&B[(i*56)+53]);
#endif
__m128d c53_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a53_0 = _mm_load_sd(&values[662]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_0 = _mm_add_sd(c53_0, _mm_mul_sd(a53_0, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_0 = _mm_add_sd(c53_0, _mm_mul_sd(a53_0, b53));
#endif
_mm_store_sd(&C[(i*56)+0], c53_0);
__m128d c53_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a53_1 = _mm_load_sd(&values[663]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_1 = _mm_add_sd(c53_1, _mm_mul_sd(a53_1, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_1 = _mm_add_sd(c53_1, _mm_mul_sd(a53_1, b53));
#endif
_mm_store_sd(&C[(i*56)+3], c53_1);
__m128d c53_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a53_2 = _mm_load_sd(&values[664]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_2 = _mm_add_sd(c53_2, _mm_mul_sd(a53_2, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_2 = _mm_add_sd(c53_2, _mm_mul_sd(a53_2, b53));
#endif
_mm_store_sd(&C[(i*56)+9], c53_2);
__m128d c53_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a53_3 = _mm_load_sd(&values[665]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_3 = _mm_add_sd(c53_3, _mm_mul_sd(a53_3, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_3 = _mm_add_sd(c53_3, _mm_mul_sd(a53_3, b53));
#endif
_mm_store_sd(&C[(i*56)+19], c53_3);
__m128d c53_4 = _mm_load_sd(&C[(i*56)+34]);
__m128d a53_4 = _mm_load_sd(&values[666]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_4 = _mm_add_sd(c53_4, _mm_mul_sd(a53_4, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_4 = _mm_add_sd(c53_4, _mm_mul_sd(a53_4, b53));
#endif
_mm_store_sd(&C[(i*56)+34], c53_4);
#else
C[(i*56)+0] += values[662] * B[(i*56)+53];
C[(i*56)+3] += values[663] * B[(i*56)+53];
C[(i*56)+9] += values[664] * B[(i*56)+53];
C[(i*56)+19] += values[665] * B[(i*56)+53];
C[(i*56)+34] += values[666] * B[(i*56)+53];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b54 = _mm256_broadcast_sd(&B[(i*56)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b54 = _mm_loaddup_pd(&B[(i*56)+54]);
#endif
__m128d c54_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a54_0 = _mm_load_sd(&values[667]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_0 = _mm_add_sd(c54_0, _mm_mul_sd(a54_0, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_0 = _mm_add_sd(c54_0, _mm_mul_sd(a54_0, b54));
#endif
_mm_store_sd(&C[(i*56)+0], c54_0);
__m128d c54_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a54_1 = _mm_load_sd(&values[668]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_1 = _mm_add_sd(c54_1, _mm_mul_sd(a54_1, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_1 = _mm_add_sd(c54_1, _mm_mul_sd(a54_1, b54));
#endif
_mm_store_sd(&C[(i*56)+3], c54_1);
__m128d c54_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a54_2 = _mm_load_sd(&values[669]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_2 = _mm_add_sd(c54_2, _mm_mul_sd(a54_2, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_2 = _mm_add_sd(c54_2, _mm_mul_sd(a54_2, b54));
#endif
_mm_store_sd(&C[(i*56)+9], c54_2);
__m128d c54_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a54_3 = _mm_load_sd(&values[670]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_3 = _mm_add_sd(c54_3, _mm_mul_sd(a54_3, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_3 = _mm_add_sd(c54_3, _mm_mul_sd(a54_3, b54));
#endif
_mm_store_sd(&C[(i*56)+19], c54_3);
__m128d c54_4 = _mm_load_sd(&C[(i*56)+34]);
__m128d a54_4 = _mm_load_sd(&values[671]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_4 = _mm_add_sd(c54_4, _mm_mul_sd(a54_4, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_4 = _mm_add_sd(c54_4, _mm_mul_sd(a54_4, b54));
#endif
_mm_store_sd(&C[(i*56)+34], c54_4);
#else
C[(i*56)+0] += values[667] * B[(i*56)+54];
C[(i*56)+3] += values[668] * B[(i*56)+54];
C[(i*56)+9] += values[669] * B[(i*56)+54];
C[(i*56)+19] += values[670] * B[(i*56)+54];
C[(i*56)+34] += values[671] * B[(i*56)+54];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}

inline void generatedMatrixMultiplication_kZetaDivMT_9_56(double* values, double* B, double* C, int exit_col, double* A_prefetch = NULL, double* B_prefetch = NULL, double* C_prefetch = NULL)
{
#pragma nounroll
for (int i = 0; i < 9; i++)
{
  #pragma simd
  for (int m = 0; m < exit_col; m++) {
    C[(i*56)+m] = 0.0;
  }
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#else
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b1 = _mm256_broadcast_sd(&B[(i*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b1 = _mm_loaddup_pd(&B[(i*56)+1]);
#endif
__m128d c1_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a1_0 = _mm_load_sd(&values[0]);
#if defined(__SSE3__) && defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, _mm256_castpd256_pd128(b1)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c1_0 = _mm_add_sd(c1_0, _mm_mul_sd(a1_0, b1));
#endif
_mm_store_sd(&C[(i*56)+0], c1_0);
#else
C[(i*56)+0] += values[0] * B[(i*56)+1];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b2 = _mm256_broadcast_sd(&B[(i*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b2 = _mm_loaddup_pd(&B[(i*56)+2]);
#endif
__m128d c2_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a2_0 = _mm_load_sd(&values[1]);
#if defined(__SSE3__) && defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, _mm256_castpd256_pd128(b2)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c2_0 = _mm_add_sd(c2_0, _mm_mul_sd(a2_0, b2));
#endif
_mm_store_sd(&C[(i*56)+0], c2_0);
#else
C[(i*56)+0] += values[1] * B[(i*56)+2];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b3 = _mm256_broadcast_sd(&B[(i*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b3 = _mm_loaddup_pd(&B[(i*56)+3]);
#endif
__m128d c3_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a3_0 = _mm_load_sd(&values[2]);
#if defined(__SSE3__) && defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, _mm256_castpd256_pd128(b3)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c3_0 = _mm_add_sd(c3_0, _mm_mul_sd(a3_0, b3));
#endif
_mm_store_sd(&C[(i*56)+0], c3_0);
#else
C[(i*56)+0] += values[2] * B[(i*56)+3];
#endif
#ifndef NDEBUG
num_flops += 2;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 4, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b4 = _mm256_broadcast_sd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b4 = _mm_loaddup_pd(&B[(i*56)+4]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c4_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a4_0 = _mm256_loadu_pd(&values[3]);
c4_0 = _mm256_add_pd(c4_0, _mm256_mul_pd(a4_0, b4));
_mm256_storeu_pd(&C[(i*56)+0], c4_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c4_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a4_0 = _mm_loadu_pd(&values[3]);
c4_0 = _mm_add_pd(c4_0, _mm_mul_pd(a4_0, b4));
_mm_storeu_pd(&C[(i*56)+0], c4_0);
__m128d c4_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a4_2 = _mm_loadu_pd(&values[5]);
c4_2 = _mm_add_pd(c4_2, _mm_mul_pd(a4_2, b4));
_mm_storeu_pd(&C[(i*56)+2], c4_2);
#endif
#else
C[(i*56)+0] += values[3] * B[(i*56)+4];
C[(i*56)+1] += values[4] * B[(i*56)+4];
C[(i*56)+2] += values[5] * B[(i*56)+4];
C[(i*56)+3] += values[6] * B[(i*56)+4];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b5 = _mm256_broadcast_sd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b5 = _mm_loaddup_pd(&B[(i*56)+5]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c5_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a5_0 = _mm256_loadu_pd(&values[7]);
c5_0 = _mm256_add_pd(c5_0, _mm256_mul_pd(a5_0, b5));
_mm256_storeu_pd(&C[(i*56)+0], c5_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c5_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a5_0 = _mm_loadu_pd(&values[7]);
c5_0 = _mm_add_pd(c5_0, _mm_mul_pd(a5_0, b5));
_mm_storeu_pd(&C[(i*56)+0], c5_0);
__m128d c5_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a5_2 = _mm_loadu_pd(&values[9]);
c5_2 = _mm_add_pd(c5_2, _mm_mul_pd(a5_2, b5));
_mm_storeu_pd(&C[(i*56)+2], c5_2);
#endif
#else
C[(i*56)+0] += values[7] * B[(i*56)+5];
C[(i*56)+1] += values[8] * B[(i*56)+5];
C[(i*56)+2] += values[9] * B[(i*56)+5];
C[(i*56)+3] += values[10] * B[(i*56)+5];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b6 = _mm256_broadcast_sd(&B[(i*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b6 = _mm_loaddup_pd(&B[(i*56)+6]);
#endif
__m128d c6_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a6_0 = _mm_load_sd(&values[11]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_0 = _mm_add_sd(c6_0, _mm_mul_sd(a6_0, b6));
#endif
_mm_store_sd(&C[(i*56)+0], c6_0);
__m128d c6_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a6_1 = _mm_loadu_pd(&values[12]);
#if defined(__SSE3__) && defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, _mm256_castpd256_pd128(b6)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c6_1 = _mm_add_pd(c6_1, _mm_mul_pd(a6_1, b6));
#endif
_mm_storeu_pd(&C[(i*56)+2], c6_1);
#else
C[(i*56)+0] += values[11] * B[(i*56)+6];
C[(i*56)+2] += values[12] * B[(i*56)+6];
C[(i*56)+3] += values[13] * B[(i*56)+6];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b7 = _mm256_broadcast_sd(&B[(i*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b7 = _mm_loaddup_pd(&B[(i*56)+7]);
#endif
__m128d c7_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a7_0 = _mm_loadu_pd(&values[14]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_0 = _mm_add_pd(c7_0, _mm_mul_pd(a7_0, b7));
#endif
_mm_storeu_pd(&C[(i*56)+0], c7_0);
__m128d c7_2 = _mm_load_sd(&C[(i*56)+3]);
__m128d a7_2 = _mm_load_sd(&values[16]);
#if defined(__SSE3__) && defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, _mm256_castpd256_pd128(b7)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c7_2 = _mm_add_sd(c7_2, _mm_mul_sd(a7_2, b7));
#endif
_mm_store_sd(&C[(i*56)+3], c7_2);
#else
C[(i*56)+0] += values[14] * B[(i*56)+7];
C[(i*56)+1] += values[15] * B[(i*56)+7];
C[(i*56)+3] += values[16] * B[(i*56)+7];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b8 = _mm256_broadcast_sd(&B[(i*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b8 = _mm_loaddup_pd(&B[(i*56)+8]);
#endif
__m128d c8_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a8_0 = _mm_load_sd(&values[17]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_0 = _mm_add_sd(c8_0, _mm_mul_sd(a8_0, b8));
#endif
_mm_store_sd(&C[(i*56)+0], c8_0);
__m128d c8_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a8_1 = _mm_loadu_pd(&values[18]);
#if defined(__SSE3__) && defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, _mm256_castpd256_pd128(b8)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c8_1 = _mm_add_pd(c8_1, _mm_mul_pd(a8_1, b8));
#endif
_mm_storeu_pd(&C[(i*56)+2], c8_1);
#else
C[(i*56)+0] += values[17] * B[(i*56)+8];
C[(i*56)+2] += values[18] * B[(i*56)+8];
C[(i*56)+3] += values[19] * B[(i*56)+8];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b9 = _mm256_broadcast_sd(&B[(i*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b9 = _mm_loaddup_pd(&B[(i*56)+9]);
#endif
__m128d c9_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a9_0 = _mm_load_sd(&values[20]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_0 = _mm_add_sd(c9_0, _mm_mul_sd(a9_0, b9));
#endif
_mm_store_sd(&C[(i*56)+0], c9_0);
__m128d c9_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a9_1 = _mm_load_sd(&values[21]);
#if defined(__SSE3__) && defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, _mm256_castpd256_pd128(b9)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c9_1 = _mm_add_sd(c9_1, _mm_mul_sd(a9_1, b9));
#endif
_mm_store_sd(&C[(i*56)+3], c9_1);
#else
C[(i*56)+0] += values[20] * B[(i*56)+9];
C[(i*56)+3] += values[21] * B[(i*56)+9];
#endif
#ifndef NDEBUG
num_flops += 4;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 10, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b10 = _mm256_broadcast_sd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b10 = _mm_loaddup_pd(&B[(i*56)+10]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a10_0 = _mm256_loadu_pd(&values[22]);
c10_0 = _mm256_add_pd(c10_0, _mm256_mul_pd(a10_0, b10));
_mm256_storeu_pd(&C[(i*56)+0], c10_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a10_0 = _mm_loadu_pd(&values[22]);
c10_0 = _mm_add_pd(c10_0, _mm_mul_pd(a10_0, b10));
_mm_storeu_pd(&C[(i*56)+0], c10_0);
__m128d c10_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a10_2 = _mm_loadu_pd(&values[24]);
c10_2 = _mm_add_pd(c10_2, _mm_mul_pd(a10_2, b10));
_mm_storeu_pd(&C[(i*56)+2], c10_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c10_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a10_4 = _mm256_loadu_pd(&values[26]);
c10_4 = _mm256_add_pd(c10_4, _mm256_mul_pd(a10_4, b10));
_mm256_storeu_pd(&C[(i*56)+4], c10_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c10_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a10_4 = _mm_loadu_pd(&values[26]);
c10_4 = _mm_add_pd(c10_4, _mm_mul_pd(a10_4, b10));
_mm_storeu_pd(&C[(i*56)+4], c10_4);
__m128d c10_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a10_6 = _mm_loadu_pd(&values[28]);
c10_6 = _mm_add_pd(c10_6, _mm_mul_pd(a10_6, b10));
_mm_storeu_pd(&C[(i*56)+6], c10_6);
#endif
__m128d c10_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a10_8 = _mm_loadu_pd(&values[30]);
#if defined(__SSE3__) && defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, _mm256_castpd256_pd128(b10)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c10_8 = _mm_add_pd(c10_8, _mm_mul_pd(a10_8, b10));
#endif
_mm_storeu_pd(&C[(i*56)+8], c10_8);
#else
C[(i*56)+0] += values[22] * B[(i*56)+10];
C[(i*56)+1] += values[23] * B[(i*56)+10];
C[(i*56)+2] += values[24] * B[(i*56)+10];
C[(i*56)+3] += values[25] * B[(i*56)+10];
C[(i*56)+4] += values[26] * B[(i*56)+10];
C[(i*56)+5] += values[27] * B[(i*56)+10];
C[(i*56)+6] += values[28] * B[(i*56)+10];
C[(i*56)+7] += values[29] * B[(i*56)+10];
C[(i*56)+8] += values[30] * B[(i*56)+10];
C[(i*56)+9] += values[31] * B[(i*56)+10];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b11 = _mm256_broadcast_sd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b11 = _mm_loaddup_pd(&B[(i*56)+11]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a11_0 = _mm256_loadu_pd(&values[32]);
c11_0 = _mm256_add_pd(c11_0, _mm256_mul_pd(a11_0, b11));
_mm256_storeu_pd(&C[(i*56)+0], c11_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a11_0 = _mm_loadu_pd(&values[32]);
c11_0 = _mm_add_pd(c11_0, _mm_mul_pd(a11_0, b11));
_mm_storeu_pd(&C[(i*56)+0], c11_0);
__m128d c11_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a11_2 = _mm_loadu_pd(&values[34]);
c11_2 = _mm_add_pd(c11_2, _mm_mul_pd(a11_2, b11));
_mm_storeu_pd(&C[(i*56)+2], c11_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c11_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a11_4 = _mm256_loadu_pd(&values[36]);
c11_4 = _mm256_add_pd(c11_4, _mm256_mul_pd(a11_4, b11));
_mm256_storeu_pd(&C[(i*56)+4], c11_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c11_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a11_4 = _mm_loadu_pd(&values[36]);
c11_4 = _mm_add_pd(c11_4, _mm_mul_pd(a11_4, b11));
_mm_storeu_pd(&C[(i*56)+4], c11_4);
__m128d c11_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a11_6 = _mm_loadu_pd(&values[38]);
c11_6 = _mm_add_pd(c11_6, _mm_mul_pd(a11_6, b11));
_mm_storeu_pd(&C[(i*56)+6], c11_6);
#endif
__m128d c11_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a11_8 = _mm_loadu_pd(&values[40]);
#if defined(__SSE3__) && defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, _mm256_castpd256_pd128(b11)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c11_8 = _mm_add_pd(c11_8, _mm_mul_pd(a11_8, b11));
#endif
_mm_storeu_pd(&C[(i*56)+8], c11_8);
#else
C[(i*56)+0] += values[32] * B[(i*56)+11];
C[(i*56)+1] += values[33] * B[(i*56)+11];
C[(i*56)+2] += values[34] * B[(i*56)+11];
C[(i*56)+3] += values[35] * B[(i*56)+11];
C[(i*56)+4] += values[36] * B[(i*56)+11];
C[(i*56)+5] += values[37] * B[(i*56)+11];
C[(i*56)+6] += values[38] * B[(i*56)+11];
C[(i*56)+7] += values[39] * B[(i*56)+11];
C[(i*56)+8] += values[40] * B[(i*56)+11];
C[(i*56)+9] += values[41] * B[(i*56)+11];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b12 = _mm256_broadcast_sd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b12 = _mm_loaddup_pd(&B[(i*56)+12]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a12_0 = _mm256_loadu_pd(&values[42]);
c12_0 = _mm256_add_pd(c12_0, _mm256_mul_pd(a12_0, b12));
_mm256_storeu_pd(&C[(i*56)+0], c12_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a12_0 = _mm_loadu_pd(&values[42]);
c12_0 = _mm_add_pd(c12_0, _mm_mul_pd(a12_0, b12));
_mm_storeu_pd(&C[(i*56)+0], c12_0);
__m128d c12_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a12_2 = _mm_loadu_pd(&values[44]);
c12_2 = _mm_add_pd(c12_2, _mm_mul_pd(a12_2, b12));
_mm_storeu_pd(&C[(i*56)+2], c12_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c12_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a12_4 = _mm256_loadu_pd(&values[46]);
c12_4 = _mm256_add_pd(c12_4, _mm256_mul_pd(a12_4, b12));
_mm256_storeu_pd(&C[(i*56)+5], c12_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c12_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a12_4 = _mm_loadu_pd(&values[46]);
c12_4 = _mm_add_pd(c12_4, _mm_mul_pd(a12_4, b12));
_mm_storeu_pd(&C[(i*56)+5], c12_4);
__m128d c12_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a12_6 = _mm_loadu_pd(&values[48]);
c12_6 = _mm_add_pd(c12_6, _mm_mul_pd(a12_6, b12));
_mm_storeu_pd(&C[(i*56)+7], c12_6);
#endif
__m128d c12_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a12_8 = _mm_load_sd(&values[50]);
#if defined(__SSE3__) && defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, _mm256_castpd256_pd128(b12)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c12_8 = _mm_add_sd(c12_8, _mm_mul_sd(a12_8, b12));
#endif
_mm_store_sd(&C[(i*56)+9], c12_8);
#else
C[(i*56)+0] += values[42] * B[(i*56)+12];
C[(i*56)+1] += values[43] * B[(i*56)+12];
C[(i*56)+2] += values[44] * B[(i*56)+12];
C[(i*56)+3] += values[45] * B[(i*56)+12];
C[(i*56)+5] += values[46] * B[(i*56)+12];
C[(i*56)+6] += values[47] * B[(i*56)+12];
C[(i*56)+7] += values[48] * B[(i*56)+12];
C[(i*56)+8] += values[49] * B[(i*56)+12];
C[(i*56)+9] += values[50] * B[(i*56)+12];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b13 = _mm256_broadcast_sd(&B[(i*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b13 = _mm_loaddup_pd(&B[(i*56)+13]);
#endif
__m128d c13_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a13_0 = _mm_load_sd(&values[51]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_0 = _mm_add_sd(c13_0, _mm_mul_sd(a13_0, b13));
#endif
_mm_store_sd(&C[(i*56)+0], c13_0);
__m128d c13_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a13_1 = _mm_loadu_pd(&values[52]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_1 = _mm_add_pd(c13_1, _mm_mul_pd(a13_1, b13));
#endif
_mm_storeu_pd(&C[(i*56)+2], c13_1);
__m128d c13_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a13_3 = _mm_load_sd(&values[54]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_3 = _mm_add_sd(c13_3, _mm_mul_sd(a13_3, b13));
#endif
_mm_store_sd(&C[(i*56)+6], c13_3);
__m128d c13_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a13_4 = _mm_loadu_pd(&values[55]);
#if defined(__SSE3__) && defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, _mm256_castpd256_pd128(b13)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c13_4 = _mm_add_pd(c13_4, _mm_mul_pd(a13_4, b13));
#endif
_mm_storeu_pd(&C[(i*56)+8], c13_4);
#else
C[(i*56)+0] += values[51] * B[(i*56)+13];
C[(i*56)+2] += values[52] * B[(i*56)+13];
C[(i*56)+3] += values[53] * B[(i*56)+13];
C[(i*56)+6] += values[54] * B[(i*56)+13];
C[(i*56)+8] += values[55] * B[(i*56)+13];
C[(i*56)+9] += values[56] * B[(i*56)+13];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b14 = _mm256_broadcast_sd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b14 = _mm_loaddup_pd(&B[(i*56)+14]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c14_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a14_0 = _mm256_loadu_pd(&values[57]);
c14_0 = _mm256_add_pd(c14_0, _mm256_mul_pd(a14_0, b14));
_mm256_storeu_pd(&C[(i*56)+0], c14_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c14_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a14_0 = _mm_loadu_pd(&values[57]);
c14_0 = _mm_add_pd(c14_0, _mm_mul_pd(a14_0, b14));
_mm_storeu_pd(&C[(i*56)+0], c14_0);
__m128d c14_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a14_2 = _mm_loadu_pd(&values[59]);
c14_2 = _mm_add_pd(c14_2, _mm_mul_pd(a14_2, b14));
_mm_storeu_pd(&C[(i*56)+2], c14_2);
#endif
__m128d c14_4 = _mm_load_sd(&C[(i*56)+4]);
__m128d a14_4 = _mm_load_sd(&values[61]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_4 = _mm_add_sd(c14_4, _mm_mul_sd(a14_4, b14));
#endif
_mm_store_sd(&C[(i*56)+4], c14_4);
__m128d c14_5 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a14_5 = _mm_loadu_pd(&values[62]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_5 = _mm_add_pd(c14_5, _mm_mul_pd(a14_5, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_5 = _mm_add_pd(c14_5, _mm_mul_pd(a14_5, b14));
#endif
_mm_storeu_pd(&C[(i*56)+7], c14_5);
__m128d c14_7 = _mm_load_sd(&C[(i*56)+9]);
__m128d a14_7 = _mm_load_sd(&values[64]);
#if defined(__SSE3__) && defined(__AVX256__)
c14_7 = _mm_add_sd(c14_7, _mm_mul_sd(a14_7, _mm256_castpd256_pd128(b14)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c14_7 = _mm_add_sd(c14_7, _mm_mul_sd(a14_7, b14));
#endif
_mm_store_sd(&C[(i*56)+9], c14_7);
#else
C[(i*56)+0] += values[57] * B[(i*56)+14];
C[(i*56)+1] += values[58] * B[(i*56)+14];
C[(i*56)+2] += values[59] * B[(i*56)+14];
C[(i*56)+3] += values[60] * B[(i*56)+14];
C[(i*56)+4] += values[61] * B[(i*56)+14];
C[(i*56)+7] += values[62] * B[(i*56)+14];
C[(i*56)+8] += values[63] * B[(i*56)+14];
C[(i*56)+9] += values[64] * B[(i*56)+14];
#endif
#ifndef NDEBUG
num_flops += 16;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b15 = _mm256_broadcast_sd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b15 = _mm_loaddup_pd(&B[(i*56)+15]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c15_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a15_0 = _mm256_loadu_pd(&values[65]);
c15_0 = _mm256_add_pd(c15_0, _mm256_mul_pd(a15_0, b15));
_mm256_storeu_pd(&C[(i*56)+0], c15_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c15_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a15_0 = _mm_loadu_pd(&values[65]);
c15_0 = _mm_add_pd(c15_0, _mm_mul_pd(a15_0, b15));
_mm_storeu_pd(&C[(i*56)+0], c15_0);
__m128d c15_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a15_2 = _mm_loadu_pd(&values[67]);
c15_2 = _mm_add_pd(c15_2, _mm_mul_pd(a15_2, b15));
_mm_storeu_pd(&C[(i*56)+2], c15_2);
#endif
__m128d c15_4 = _mm_load_sd(&C[(i*56)+5]);
__m128d a15_4 = _mm_load_sd(&values[69]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_4 = _mm_add_sd(c15_4, _mm_mul_sd(a15_4, b15));
#endif
_mm_store_sd(&C[(i*56)+5], c15_4);
__m128d c15_5 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a15_5 = _mm_loadu_pd(&values[70]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_5 = _mm_add_pd(c15_5, _mm_mul_pd(a15_5, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_5 = _mm_add_pd(c15_5, _mm_mul_pd(a15_5, b15));
#endif
_mm_storeu_pd(&C[(i*56)+7], c15_5);
__m128d c15_7 = _mm_load_sd(&C[(i*56)+9]);
__m128d a15_7 = _mm_load_sd(&values[72]);
#if defined(__SSE3__) && defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, _mm256_castpd256_pd128(b15)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c15_7 = _mm_add_sd(c15_7, _mm_mul_sd(a15_7, b15));
#endif
_mm_store_sd(&C[(i*56)+9], c15_7);
#else
C[(i*56)+0] += values[65] * B[(i*56)+15];
C[(i*56)+1] += values[66] * B[(i*56)+15];
C[(i*56)+2] += values[67] * B[(i*56)+15];
C[(i*56)+3] += values[68] * B[(i*56)+15];
C[(i*56)+5] += values[69] * B[(i*56)+15];
C[(i*56)+7] += values[70] * B[(i*56)+15];
C[(i*56)+8] += values[71] * B[(i*56)+15];
C[(i*56)+9] += values[72] * B[(i*56)+15];
#endif
#ifndef NDEBUG
num_flops += 16;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b16 = _mm256_broadcast_sd(&B[(i*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b16 = _mm_loaddup_pd(&B[(i*56)+16]);
#endif
__m128d c16_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a16_0 = _mm_load_sd(&values[73]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_0 = _mm_add_sd(c16_0, _mm_mul_sd(a16_0, b16));
#endif
_mm_store_sd(&C[(i*56)+0], c16_0);
__m128d c16_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a16_1 = _mm_loadu_pd(&values[74]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_1 = _mm_add_pd(c16_1, _mm_mul_pd(a16_1, b16));
#endif
_mm_storeu_pd(&C[(i*56)+2], c16_1);
__m128d c16_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a16_3 = _mm_load_sd(&values[76]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_3 = _mm_add_sd(c16_3, _mm_mul_sd(a16_3, b16));
#endif
_mm_store_sd(&C[(i*56)+6], c16_3);
__m128d c16_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a16_4 = _mm_loadu_pd(&values[77]);
#if defined(__SSE3__) && defined(__AVX256__)
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, _mm256_castpd256_pd128(b16)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c16_4 = _mm_add_pd(c16_4, _mm_mul_pd(a16_4, b16));
#endif
_mm_storeu_pd(&C[(i*56)+8], c16_4);
#else
C[(i*56)+0] += values[73] * B[(i*56)+16];
C[(i*56)+2] += values[74] * B[(i*56)+16];
C[(i*56)+3] += values[75] * B[(i*56)+16];
C[(i*56)+6] += values[76] * B[(i*56)+16];
C[(i*56)+8] += values[77] * B[(i*56)+16];
C[(i*56)+9] += values[78] * B[(i*56)+16];
#endif
#ifndef NDEBUG
num_flops += 12;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b17 = _mm256_broadcast_sd(&B[(i*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b17 = _mm_loaddup_pd(&B[(i*56)+17]);
#endif
__m128d c17_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a17_0 = _mm_loadu_pd(&values[79]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_0 = _mm_add_pd(c17_0, _mm_mul_pd(a17_0, b17));
#endif
_mm_storeu_pd(&C[(i*56)+0], c17_0);
__m128d c17_2 = _mm_load_sd(&C[(i*56)+3]);
__m128d a17_2 = _mm_load_sd(&values[81]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_2 = _mm_add_sd(c17_2, _mm_mul_sd(a17_2, b17));
#endif
_mm_store_sd(&C[(i*56)+3], c17_2);
__m128d c17_3 = _mm_load_sd(&C[(i*56)+7]);
__m128d a17_3 = _mm_load_sd(&values[82]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_3 = _mm_add_sd(c17_3, _mm_mul_sd(a17_3, b17));
#endif
_mm_store_sd(&C[(i*56)+7], c17_3);
__m128d c17_4 = _mm_load_sd(&C[(i*56)+9]);
__m128d a17_4 = _mm_load_sd(&values[83]);
#if defined(__SSE3__) && defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, _mm256_castpd256_pd128(b17)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c17_4 = _mm_add_sd(c17_4, _mm_mul_sd(a17_4, b17));
#endif
_mm_store_sd(&C[(i*56)+9], c17_4);
#else
C[(i*56)+0] += values[79] * B[(i*56)+17];
C[(i*56)+1] += values[80] * B[(i*56)+17];
C[(i*56)+3] += values[81] * B[(i*56)+17];
C[(i*56)+7] += values[82] * B[(i*56)+17];
C[(i*56)+9] += values[83] * B[(i*56)+17];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b18 = _mm256_broadcast_sd(&B[(i*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b18 = _mm_loaddup_pd(&B[(i*56)+18]);
#endif
__m128d c18_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a18_0 = _mm_load_sd(&values[84]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_0 = _mm_add_sd(c18_0, _mm_mul_sd(a18_0, b18));
#endif
_mm_store_sd(&C[(i*56)+0], c18_0);
__m128d c18_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a18_1 = _mm_loadu_pd(&values[85]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_1 = _mm_add_pd(c18_1, _mm_mul_pd(a18_1, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_1 = _mm_add_pd(c18_1, _mm_mul_pd(a18_1, b18));
#endif
_mm_storeu_pd(&C[(i*56)+2], c18_1);
__m128d c18_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a18_3 = _mm_loadu_pd(&values[87]);
#if defined(__SSE3__) && defined(__AVX256__)
c18_3 = _mm_add_pd(c18_3, _mm_mul_pd(a18_3, _mm256_castpd256_pd128(b18)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c18_3 = _mm_add_pd(c18_3, _mm_mul_pd(a18_3, b18));
#endif
_mm_storeu_pd(&C[(i*56)+8], c18_3);
#else
C[(i*56)+0] += values[84] * B[(i*56)+18];
C[(i*56)+2] += values[85] * B[(i*56)+18];
C[(i*56)+3] += values[86] * B[(i*56)+18];
C[(i*56)+8] += values[87] * B[(i*56)+18];
C[(i*56)+9] += values[88] * B[(i*56)+18];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b19 = _mm256_broadcast_sd(&B[(i*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b19 = _mm_loaddup_pd(&B[(i*56)+19]);
#endif
__m128d c19_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a19_0 = _mm_load_sd(&values[89]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_0 = _mm_add_sd(c19_0, _mm_mul_sd(a19_0, b19));
#endif
_mm_store_sd(&C[(i*56)+0], c19_0);
__m128d c19_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a19_1 = _mm_load_sd(&values[90]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_1 = _mm_add_sd(c19_1, _mm_mul_sd(a19_1, b19));
#endif
_mm_store_sd(&C[(i*56)+3], c19_1);
__m128d c19_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a19_2 = _mm_load_sd(&values[91]);
#if defined(__SSE3__) && defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, _mm256_castpd256_pd128(b19)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c19_2 = _mm_add_sd(c19_2, _mm_mul_sd(a19_2, b19));
#endif
_mm_store_sd(&C[(i*56)+9], c19_2);
#else
C[(i*56)+0] += values[89] * B[(i*56)+19];
C[(i*56)+3] += values[90] * B[(i*56)+19];
C[(i*56)+9] += values[91] * B[(i*56)+19];
#endif
#ifndef NDEBUG
num_flops += 6;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 20, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b20 = _mm256_broadcast_sd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b20 = _mm_loaddup_pd(&B[(i*56)+20]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a20_0 = _mm256_loadu_pd(&values[92]);
c20_0 = _mm256_add_pd(c20_0, _mm256_mul_pd(a20_0, b20));
_mm256_storeu_pd(&C[(i*56)+0], c20_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a20_0 = _mm_loadu_pd(&values[92]);
c20_0 = _mm_add_pd(c20_0, _mm_mul_pd(a20_0, b20));
_mm_storeu_pd(&C[(i*56)+0], c20_0);
__m128d c20_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a20_2 = _mm_loadu_pd(&values[94]);
c20_2 = _mm_add_pd(c20_2, _mm_mul_pd(a20_2, b20));
_mm_storeu_pd(&C[(i*56)+2], c20_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a20_4 = _mm256_loadu_pd(&values[96]);
c20_4 = _mm256_add_pd(c20_4, _mm256_mul_pd(a20_4, b20));
_mm256_storeu_pd(&C[(i*56)+4], c20_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a20_4 = _mm_loadu_pd(&values[96]);
c20_4 = _mm_add_pd(c20_4, _mm_mul_pd(a20_4, b20));
_mm_storeu_pd(&C[(i*56)+4], c20_4);
__m128d c20_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a20_6 = _mm_loadu_pd(&values[98]);
c20_6 = _mm_add_pd(c20_6, _mm_mul_pd(a20_6, b20));
_mm_storeu_pd(&C[(i*56)+6], c20_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a20_8 = _mm256_loadu_pd(&values[100]);
c20_8 = _mm256_add_pd(c20_8, _mm256_mul_pd(a20_8, b20));
_mm256_storeu_pd(&C[(i*56)+8], c20_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a20_8 = _mm_loadu_pd(&values[100]);
c20_8 = _mm_add_pd(c20_8, _mm_mul_pd(a20_8, b20));
_mm_storeu_pd(&C[(i*56)+8], c20_8);
__m128d c20_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a20_10 = _mm_loadu_pd(&values[102]);
c20_10 = _mm_add_pd(c20_10, _mm_mul_pd(a20_10, b20));
_mm_storeu_pd(&C[(i*56)+10], c20_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a20_12 = _mm256_loadu_pd(&values[104]);
c20_12 = _mm256_add_pd(c20_12, _mm256_mul_pd(a20_12, b20));
_mm256_storeu_pd(&C[(i*56)+12], c20_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a20_12 = _mm_loadu_pd(&values[104]);
c20_12 = _mm_add_pd(c20_12, _mm_mul_pd(a20_12, b20));
_mm_storeu_pd(&C[(i*56)+12], c20_12);
__m128d c20_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a20_14 = _mm_loadu_pd(&values[106]);
c20_14 = _mm_add_pd(c20_14, _mm_mul_pd(a20_14, b20));
_mm_storeu_pd(&C[(i*56)+14], c20_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c20_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a20_16 = _mm256_loadu_pd(&values[108]);
c20_16 = _mm256_add_pd(c20_16, _mm256_mul_pd(a20_16, b20));
_mm256_storeu_pd(&C[(i*56)+16], c20_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c20_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a20_16 = _mm_loadu_pd(&values[108]);
c20_16 = _mm_add_pd(c20_16, _mm_mul_pd(a20_16, b20));
_mm_storeu_pd(&C[(i*56)+16], c20_16);
__m128d c20_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a20_18 = _mm_loadu_pd(&values[110]);
c20_18 = _mm_add_pd(c20_18, _mm_mul_pd(a20_18, b20));
_mm_storeu_pd(&C[(i*56)+18], c20_18);
#endif
#else
C[(i*56)+0] += values[92] * B[(i*56)+20];
C[(i*56)+1] += values[93] * B[(i*56)+20];
C[(i*56)+2] += values[94] * B[(i*56)+20];
C[(i*56)+3] += values[95] * B[(i*56)+20];
C[(i*56)+4] += values[96] * B[(i*56)+20];
C[(i*56)+5] += values[97] * B[(i*56)+20];
C[(i*56)+6] += values[98] * B[(i*56)+20];
C[(i*56)+7] += values[99] * B[(i*56)+20];
C[(i*56)+8] += values[100] * B[(i*56)+20];
C[(i*56)+9] += values[101] * B[(i*56)+20];
C[(i*56)+10] += values[102] * B[(i*56)+20];
C[(i*56)+11] += values[103] * B[(i*56)+20];
C[(i*56)+12] += values[104] * B[(i*56)+20];
C[(i*56)+13] += values[105] * B[(i*56)+20];
C[(i*56)+14] += values[106] * B[(i*56)+20];
C[(i*56)+15] += values[107] * B[(i*56)+20];
C[(i*56)+16] += values[108] * B[(i*56)+20];
C[(i*56)+17] += values[109] * B[(i*56)+20];
C[(i*56)+18] += values[110] * B[(i*56)+20];
C[(i*56)+19] += values[111] * B[(i*56)+20];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b21 = _mm256_broadcast_sd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b21 = _mm_loaddup_pd(&B[(i*56)+21]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a21_0 = _mm256_loadu_pd(&values[112]);
c21_0 = _mm256_add_pd(c21_0, _mm256_mul_pd(a21_0, b21));
_mm256_storeu_pd(&C[(i*56)+0], c21_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a21_0 = _mm_loadu_pd(&values[112]);
c21_0 = _mm_add_pd(c21_0, _mm_mul_pd(a21_0, b21));
_mm_storeu_pd(&C[(i*56)+0], c21_0);
__m128d c21_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a21_2 = _mm_loadu_pd(&values[114]);
c21_2 = _mm_add_pd(c21_2, _mm_mul_pd(a21_2, b21));
_mm_storeu_pd(&C[(i*56)+2], c21_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a21_4 = _mm256_loadu_pd(&values[116]);
c21_4 = _mm256_add_pd(c21_4, _mm256_mul_pd(a21_4, b21));
_mm256_storeu_pd(&C[(i*56)+4], c21_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a21_4 = _mm_loadu_pd(&values[116]);
c21_4 = _mm_add_pd(c21_4, _mm_mul_pd(a21_4, b21));
_mm_storeu_pd(&C[(i*56)+4], c21_4);
__m128d c21_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a21_6 = _mm_loadu_pd(&values[118]);
c21_6 = _mm_add_pd(c21_6, _mm_mul_pd(a21_6, b21));
_mm_storeu_pd(&C[(i*56)+6], c21_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a21_8 = _mm256_loadu_pd(&values[120]);
c21_8 = _mm256_add_pd(c21_8, _mm256_mul_pd(a21_8, b21));
_mm256_storeu_pd(&C[(i*56)+8], c21_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a21_8 = _mm_loadu_pd(&values[120]);
c21_8 = _mm_add_pd(c21_8, _mm_mul_pd(a21_8, b21));
_mm_storeu_pd(&C[(i*56)+8], c21_8);
__m128d c21_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a21_10 = _mm_loadu_pd(&values[122]);
c21_10 = _mm_add_pd(c21_10, _mm_mul_pd(a21_10, b21));
_mm_storeu_pd(&C[(i*56)+10], c21_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a21_12 = _mm256_loadu_pd(&values[124]);
c21_12 = _mm256_add_pd(c21_12, _mm256_mul_pd(a21_12, b21));
_mm256_storeu_pd(&C[(i*56)+12], c21_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a21_12 = _mm_loadu_pd(&values[124]);
c21_12 = _mm_add_pd(c21_12, _mm_mul_pd(a21_12, b21));
_mm_storeu_pd(&C[(i*56)+12], c21_12);
__m128d c21_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a21_14 = _mm_loadu_pd(&values[126]);
c21_14 = _mm_add_pd(c21_14, _mm_mul_pd(a21_14, b21));
_mm_storeu_pd(&C[(i*56)+14], c21_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c21_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a21_16 = _mm256_loadu_pd(&values[128]);
c21_16 = _mm256_add_pd(c21_16, _mm256_mul_pd(a21_16, b21));
_mm256_storeu_pd(&C[(i*56)+16], c21_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c21_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a21_16 = _mm_loadu_pd(&values[128]);
c21_16 = _mm_add_pd(c21_16, _mm_mul_pd(a21_16, b21));
_mm_storeu_pd(&C[(i*56)+16], c21_16);
__m128d c21_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a21_18 = _mm_loadu_pd(&values[130]);
c21_18 = _mm_add_pd(c21_18, _mm_mul_pd(a21_18, b21));
_mm_storeu_pd(&C[(i*56)+18], c21_18);
#endif
#else
C[(i*56)+0] += values[112] * B[(i*56)+21];
C[(i*56)+1] += values[113] * B[(i*56)+21];
C[(i*56)+2] += values[114] * B[(i*56)+21];
C[(i*56)+3] += values[115] * B[(i*56)+21];
C[(i*56)+4] += values[116] * B[(i*56)+21];
C[(i*56)+5] += values[117] * B[(i*56)+21];
C[(i*56)+6] += values[118] * B[(i*56)+21];
C[(i*56)+7] += values[119] * B[(i*56)+21];
C[(i*56)+8] += values[120] * B[(i*56)+21];
C[(i*56)+9] += values[121] * B[(i*56)+21];
C[(i*56)+10] += values[122] * B[(i*56)+21];
C[(i*56)+11] += values[123] * B[(i*56)+21];
C[(i*56)+12] += values[124] * B[(i*56)+21];
C[(i*56)+13] += values[125] * B[(i*56)+21];
C[(i*56)+14] += values[126] * B[(i*56)+21];
C[(i*56)+15] += values[127] * B[(i*56)+21];
C[(i*56)+16] += values[128] * B[(i*56)+21];
C[(i*56)+17] += values[129] * B[(i*56)+21];
C[(i*56)+18] += values[130] * B[(i*56)+21];
C[(i*56)+19] += values[131] * B[(i*56)+21];
#endif
#ifndef NDEBUG
num_flops += 40;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b22 = _mm256_broadcast_sd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b22 = _mm_loaddup_pd(&B[(i*56)+22]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a22_0 = _mm256_loadu_pd(&values[132]);
c22_0 = _mm256_add_pd(c22_0, _mm256_mul_pd(a22_0, b22));
_mm256_storeu_pd(&C[(i*56)+0], c22_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a22_0 = _mm_loadu_pd(&values[132]);
c22_0 = _mm_add_pd(c22_0, _mm_mul_pd(a22_0, b22));
_mm_storeu_pd(&C[(i*56)+0], c22_0);
__m128d c22_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a22_2 = _mm_loadu_pd(&values[134]);
c22_2 = _mm_add_pd(c22_2, _mm_mul_pd(a22_2, b22));
_mm_storeu_pd(&C[(i*56)+2], c22_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a22_4 = _mm256_loadu_pd(&values[136]);
c22_4 = _mm256_add_pd(c22_4, _mm256_mul_pd(a22_4, b22));
_mm256_storeu_pd(&C[(i*56)+4], c22_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a22_4 = _mm_loadu_pd(&values[136]);
c22_4 = _mm_add_pd(c22_4, _mm_mul_pd(a22_4, b22));
_mm_storeu_pd(&C[(i*56)+4], c22_4);
__m128d c22_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a22_6 = _mm_loadu_pd(&values[138]);
c22_6 = _mm_add_pd(c22_6, _mm_mul_pd(a22_6, b22));
_mm_storeu_pd(&C[(i*56)+6], c22_6);
#endif
__m128d c22_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a22_8 = _mm_loadu_pd(&values[140]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_8 = _mm_add_pd(c22_8, _mm_mul_pd(a22_8, b22));
#endif
_mm_storeu_pd(&C[(i*56)+8], c22_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_10 = _mm256_loadu_pd(&C[(i*56)+11]);
__m256d a22_10 = _mm256_loadu_pd(&values[142]);
c22_10 = _mm256_add_pd(c22_10, _mm256_mul_pd(a22_10, b22));
_mm256_storeu_pd(&C[(i*56)+11], c22_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_10 = _mm_loadu_pd(&C[(i*56)+11]);
__m128d a22_10 = _mm_loadu_pd(&values[142]);
c22_10 = _mm_add_pd(c22_10, _mm_mul_pd(a22_10, b22));
_mm_storeu_pd(&C[(i*56)+11], c22_10);
__m128d c22_12 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a22_12 = _mm_loadu_pd(&values[144]);
c22_12 = _mm_add_pd(c22_12, _mm_mul_pd(a22_12, b22));
_mm_storeu_pd(&C[(i*56)+13], c22_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c22_14 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a22_14 = _mm256_loadu_pd(&values[146]);
c22_14 = _mm256_add_pd(c22_14, _mm256_mul_pd(a22_14, b22));
_mm256_storeu_pd(&C[(i*56)+15], c22_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c22_14 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a22_14 = _mm_loadu_pd(&values[146]);
c22_14 = _mm_add_pd(c22_14, _mm_mul_pd(a22_14, b22));
_mm_storeu_pd(&C[(i*56)+15], c22_14);
__m128d c22_16 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a22_16 = _mm_loadu_pd(&values[148]);
c22_16 = _mm_add_pd(c22_16, _mm_mul_pd(a22_16, b22));
_mm_storeu_pd(&C[(i*56)+17], c22_16);
#endif
__m128d c22_18 = _mm_load_sd(&C[(i*56)+19]);
__m128d a22_18 = _mm_load_sd(&values[150]);
#if defined(__SSE3__) && defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, _mm256_castpd256_pd128(b22)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c22_18 = _mm_add_sd(c22_18, _mm_mul_sd(a22_18, b22));
#endif
_mm_store_sd(&C[(i*56)+19], c22_18);
#else
C[(i*56)+0] += values[132] * B[(i*56)+22];
C[(i*56)+1] += values[133] * B[(i*56)+22];
C[(i*56)+2] += values[134] * B[(i*56)+22];
C[(i*56)+3] += values[135] * B[(i*56)+22];
C[(i*56)+4] += values[136] * B[(i*56)+22];
C[(i*56)+5] += values[137] * B[(i*56)+22];
C[(i*56)+6] += values[138] * B[(i*56)+22];
C[(i*56)+7] += values[139] * B[(i*56)+22];
C[(i*56)+8] += values[140] * B[(i*56)+22];
C[(i*56)+9] += values[141] * B[(i*56)+22];
C[(i*56)+11] += values[142] * B[(i*56)+22];
C[(i*56)+12] += values[143] * B[(i*56)+22];
C[(i*56)+13] += values[144] * B[(i*56)+22];
C[(i*56)+14] += values[145] * B[(i*56)+22];
C[(i*56)+15] += values[146] * B[(i*56)+22];
C[(i*56)+16] += values[147] * B[(i*56)+22];
C[(i*56)+17] += values[148] * B[(i*56)+22];
C[(i*56)+18] += values[149] * B[(i*56)+22];
C[(i*56)+19] += values[150] * B[(i*56)+22];
#endif
#ifndef NDEBUG
num_flops += 38;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b23 = _mm256_broadcast_sd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b23 = _mm_loaddup_pd(&B[(i*56)+23]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a23_0 = _mm256_loadu_pd(&values[151]);
c23_0 = _mm256_add_pd(c23_0, _mm256_mul_pd(a23_0, b23));
_mm256_storeu_pd(&C[(i*56)+0], c23_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a23_0 = _mm_loadu_pd(&values[151]);
c23_0 = _mm_add_pd(c23_0, _mm_mul_pd(a23_0, b23));
_mm_storeu_pd(&C[(i*56)+0], c23_0);
__m128d c23_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a23_2 = _mm_loadu_pd(&values[153]);
c23_2 = _mm_add_pd(c23_2, _mm_mul_pd(a23_2, b23));
_mm_storeu_pd(&C[(i*56)+2], c23_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a23_4 = _mm256_loadu_pd(&values[155]);
c23_4 = _mm256_add_pd(c23_4, _mm256_mul_pd(a23_4, b23));
_mm256_storeu_pd(&C[(i*56)+5], c23_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a23_4 = _mm_loadu_pd(&values[155]);
c23_4 = _mm_add_pd(c23_4, _mm_mul_pd(a23_4, b23));
_mm_storeu_pd(&C[(i*56)+5], c23_4);
__m128d c23_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a23_6 = _mm_loadu_pd(&values[157]);
c23_6 = _mm_add_pd(c23_6, _mm_mul_pd(a23_6, b23));
_mm_storeu_pd(&C[(i*56)+7], c23_6);
#endif
__m128d c23_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a23_8 = _mm_load_sd(&values[159]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_8 = _mm_add_sd(c23_8, _mm_mul_sd(a23_8, b23));
#endif
_mm_store_sd(&C[(i*56)+9], c23_8);
__m128d c23_9 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a23_9 = _mm_loadu_pd(&values[160]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_9 = _mm_add_pd(c23_9, _mm_mul_pd(a23_9, b23));
#endif
_mm_storeu_pd(&C[(i*56)+12], c23_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c23_11 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a23_11 = _mm256_loadu_pd(&values[162]);
c23_11 = _mm256_add_pd(c23_11, _mm256_mul_pd(a23_11, b23));
_mm256_storeu_pd(&C[(i*56)+15], c23_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c23_11 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a23_11 = _mm_loadu_pd(&values[162]);
c23_11 = _mm_add_pd(c23_11, _mm_mul_pd(a23_11, b23));
_mm_storeu_pd(&C[(i*56)+15], c23_11);
__m128d c23_13 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a23_13 = _mm_loadu_pd(&values[164]);
c23_13 = _mm_add_pd(c23_13, _mm_mul_pd(a23_13, b23));
_mm_storeu_pd(&C[(i*56)+17], c23_13);
#endif
__m128d c23_15 = _mm_load_sd(&C[(i*56)+19]);
__m128d a23_15 = _mm_load_sd(&values[166]);
#if defined(__SSE3__) && defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, _mm256_castpd256_pd128(b23)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c23_15 = _mm_add_sd(c23_15, _mm_mul_sd(a23_15, b23));
#endif
_mm_store_sd(&C[(i*56)+19], c23_15);
#else
C[(i*56)+0] += values[151] * B[(i*56)+23];
C[(i*56)+1] += values[152] * B[(i*56)+23];
C[(i*56)+2] += values[153] * B[(i*56)+23];
C[(i*56)+3] += values[154] * B[(i*56)+23];
C[(i*56)+5] += values[155] * B[(i*56)+23];
C[(i*56)+6] += values[156] * B[(i*56)+23];
C[(i*56)+7] += values[157] * B[(i*56)+23];
C[(i*56)+8] += values[158] * B[(i*56)+23];
C[(i*56)+9] += values[159] * B[(i*56)+23];
C[(i*56)+12] += values[160] * B[(i*56)+23];
C[(i*56)+13] += values[161] * B[(i*56)+23];
C[(i*56)+15] += values[162] * B[(i*56)+23];
C[(i*56)+16] += values[163] * B[(i*56)+23];
C[(i*56)+17] += values[164] * B[(i*56)+23];
C[(i*56)+18] += values[165] * B[(i*56)+23];
C[(i*56)+19] += values[166] * B[(i*56)+23];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b24 = _mm256_broadcast_sd(&B[(i*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b24 = _mm_loaddup_pd(&B[(i*56)+24]);
#endif
__m128d c24_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a24_0 = _mm_load_sd(&values[167]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_0 = _mm_add_sd(c24_0, _mm_mul_sd(a24_0, b24));
#endif
_mm_store_sd(&C[(i*56)+0], c24_0);
__m128d c24_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a24_1 = _mm_loadu_pd(&values[168]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_1 = _mm_add_pd(c24_1, _mm_mul_pd(a24_1, b24));
#endif
_mm_storeu_pd(&C[(i*56)+2], c24_1);
__m128d c24_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a24_3 = _mm_load_sd(&values[170]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_3 = _mm_add_sd(c24_3, _mm_mul_sd(a24_3, b24));
#endif
_mm_store_sd(&C[(i*56)+6], c24_3);
__m128d c24_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a24_4 = _mm_loadu_pd(&values[171]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_4 = _mm_add_pd(c24_4, _mm_mul_pd(a24_4, b24));
#endif
_mm_storeu_pd(&C[(i*56)+8], c24_4);
__m128d c24_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a24_6 = _mm_load_sd(&values[173]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_6 = _mm_add_sd(c24_6, _mm_mul_sd(a24_6, b24));
#endif
_mm_store_sd(&C[(i*56)+13], c24_6);
__m128d c24_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a24_7 = _mm_load_sd(&values[174]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_7 = _mm_add_sd(c24_7, _mm_mul_sd(a24_7, b24));
#endif
_mm_store_sd(&C[(i*56)+16], c24_7);
__m128d c24_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a24_8 = _mm_loadu_pd(&values[175]);
#if defined(__SSE3__) && defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, _mm256_castpd256_pd128(b24)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c24_8 = _mm_add_pd(c24_8, _mm_mul_pd(a24_8, b24));
#endif
_mm_storeu_pd(&C[(i*56)+18], c24_8);
#else
C[(i*56)+0] += values[167] * B[(i*56)+24];
C[(i*56)+2] += values[168] * B[(i*56)+24];
C[(i*56)+3] += values[169] * B[(i*56)+24];
C[(i*56)+6] += values[170] * B[(i*56)+24];
C[(i*56)+8] += values[171] * B[(i*56)+24];
C[(i*56)+9] += values[172] * B[(i*56)+24];
C[(i*56)+13] += values[173] * B[(i*56)+24];
C[(i*56)+16] += values[174] * B[(i*56)+24];
C[(i*56)+18] += values[175] * B[(i*56)+24];
C[(i*56)+19] += values[176] * B[(i*56)+24];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b25 = _mm256_broadcast_sd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b25 = _mm_loaddup_pd(&B[(i*56)+25]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a25_0 = _mm256_loadu_pd(&values[177]);
c25_0 = _mm256_add_pd(c25_0, _mm256_mul_pd(a25_0, b25));
_mm256_storeu_pd(&C[(i*56)+0], c25_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a25_0 = _mm_loadu_pd(&values[177]);
c25_0 = _mm_add_pd(c25_0, _mm_mul_pd(a25_0, b25));
_mm_storeu_pd(&C[(i*56)+0], c25_0);
__m128d c25_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a25_2 = _mm_loadu_pd(&values[179]);
c25_2 = _mm_add_pd(c25_2, _mm_mul_pd(a25_2, b25));
_mm_storeu_pd(&C[(i*56)+2], c25_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a25_4 = _mm256_loadu_pd(&values[181]);
c25_4 = _mm256_add_pd(c25_4, _mm256_mul_pd(a25_4, b25));
_mm256_storeu_pd(&C[(i*56)+4], c25_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a25_4 = _mm_loadu_pd(&values[181]);
c25_4 = _mm_add_pd(c25_4, _mm_mul_pd(a25_4, b25));
_mm_storeu_pd(&C[(i*56)+4], c25_4);
__m128d c25_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a25_6 = _mm_loadu_pd(&values[183]);
c25_6 = _mm_add_pd(c25_6, _mm_mul_pd(a25_6, b25));
_mm_storeu_pd(&C[(i*56)+6], c25_6);
#endif
__m128d c25_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a25_8 = _mm_loadu_pd(&values[185]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_8 = _mm_add_pd(c25_8, _mm_mul_pd(a25_8, b25));
#endif
_mm_storeu_pd(&C[(i*56)+8], c25_8);
__m128d c25_10 = _mm_load_sd(&C[(i*56)+10]);
__m128d a25_10 = _mm_load_sd(&values[187]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_10 = _mm_add_sd(c25_10, _mm_mul_sd(a25_10, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_10 = _mm_add_sd(c25_10, _mm_mul_sd(a25_10, b25));
#endif
_mm_store_sd(&C[(i*56)+10], c25_10);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c25_11 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a25_11 = _mm256_loadu_pd(&values[188]);
c25_11 = _mm256_add_pd(c25_11, _mm256_mul_pd(a25_11, b25));
_mm256_storeu_pd(&C[(i*56)+14], c25_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c25_11 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a25_11 = _mm_loadu_pd(&values[188]);
c25_11 = _mm_add_pd(c25_11, _mm_mul_pd(a25_11, b25));
_mm_storeu_pd(&C[(i*56)+14], c25_11);
__m128d c25_13 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a25_13 = _mm_loadu_pd(&values[190]);
c25_13 = _mm_add_pd(c25_13, _mm_mul_pd(a25_13, b25));
_mm_storeu_pd(&C[(i*56)+16], c25_13);
#endif
__m128d c25_15 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a25_15 = _mm_loadu_pd(&values[192]);
#if defined(__SSE3__) && defined(__AVX256__)
c25_15 = _mm_add_pd(c25_15, _mm_mul_pd(a25_15, _mm256_castpd256_pd128(b25)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c25_15 = _mm_add_pd(c25_15, _mm_mul_pd(a25_15, b25));
#endif
_mm_storeu_pd(&C[(i*56)+18], c25_15);
#else
C[(i*56)+0] += values[177] * B[(i*56)+25];
C[(i*56)+1] += values[178] * B[(i*56)+25];
C[(i*56)+2] += values[179] * B[(i*56)+25];
C[(i*56)+3] += values[180] * B[(i*56)+25];
C[(i*56)+4] += values[181] * B[(i*56)+25];
C[(i*56)+5] += values[182] * B[(i*56)+25];
C[(i*56)+6] += values[183] * B[(i*56)+25];
C[(i*56)+7] += values[184] * B[(i*56)+25];
C[(i*56)+8] += values[185] * B[(i*56)+25];
C[(i*56)+9] += values[186] * B[(i*56)+25];
C[(i*56)+10] += values[187] * B[(i*56)+25];
C[(i*56)+14] += values[188] * B[(i*56)+25];
C[(i*56)+15] += values[189] * B[(i*56)+25];
C[(i*56)+16] += values[190] * B[(i*56)+25];
C[(i*56)+17] += values[191] * B[(i*56)+25];
C[(i*56)+18] += values[192] * B[(i*56)+25];
C[(i*56)+19] += values[193] * B[(i*56)+25];
#endif
#ifndef NDEBUG
num_flops += 34;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b26 = _mm256_broadcast_sd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b26 = _mm_loaddup_pd(&B[(i*56)+26]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a26_0 = _mm256_loadu_pd(&values[194]);
c26_0 = _mm256_add_pd(c26_0, _mm256_mul_pd(a26_0, b26));
_mm256_storeu_pd(&C[(i*56)+0], c26_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a26_0 = _mm_loadu_pd(&values[194]);
c26_0 = _mm_add_pd(c26_0, _mm_mul_pd(a26_0, b26));
_mm_storeu_pd(&C[(i*56)+0], c26_0);
__m128d c26_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a26_2 = _mm_loadu_pd(&values[196]);
c26_2 = _mm_add_pd(c26_2, _mm_mul_pd(a26_2, b26));
_mm_storeu_pd(&C[(i*56)+2], c26_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a26_4 = _mm256_loadu_pd(&values[198]);
c26_4 = _mm256_add_pd(c26_4, _mm256_mul_pd(a26_4, b26));
_mm256_storeu_pd(&C[(i*56)+4], c26_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a26_4 = _mm_loadu_pd(&values[198]);
c26_4 = _mm_add_pd(c26_4, _mm_mul_pd(a26_4, b26));
_mm_storeu_pd(&C[(i*56)+4], c26_4);
__m128d c26_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a26_6 = _mm_loadu_pd(&values[200]);
c26_6 = _mm_add_pd(c26_6, _mm_mul_pd(a26_6, b26));
_mm_storeu_pd(&C[(i*56)+6], c26_6);
#endif
__m128d c26_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a26_8 = _mm_loadu_pd(&values[202]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_8 = _mm_add_pd(c26_8, _mm_mul_pd(a26_8, b26));
#endif
_mm_storeu_pd(&C[(i*56)+8], c26_8);
__m128d c26_10 = _mm_load_sd(&C[(i*56)+11]);
__m128d a26_10 = _mm_load_sd(&values[204]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_10 = _mm_add_sd(c26_10, _mm_mul_sd(a26_10, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_10 = _mm_add_sd(c26_10, _mm_mul_sd(a26_10, b26));
#endif
_mm_store_sd(&C[(i*56)+11], c26_10);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c26_11 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a26_11 = _mm256_loadu_pd(&values[205]);
c26_11 = _mm256_add_pd(c26_11, _mm256_mul_pd(a26_11, b26));
_mm256_storeu_pd(&C[(i*56)+14], c26_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c26_11 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a26_11 = _mm_loadu_pd(&values[205]);
c26_11 = _mm_add_pd(c26_11, _mm_mul_pd(a26_11, b26));
_mm_storeu_pd(&C[(i*56)+14], c26_11);
__m128d c26_13 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a26_13 = _mm_loadu_pd(&values[207]);
c26_13 = _mm_add_pd(c26_13, _mm_mul_pd(a26_13, b26));
_mm_storeu_pd(&C[(i*56)+16], c26_13);
#endif
__m128d c26_15 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a26_15 = _mm_loadu_pd(&values[209]);
#if defined(__SSE3__) && defined(__AVX256__)
c26_15 = _mm_add_pd(c26_15, _mm_mul_pd(a26_15, _mm256_castpd256_pd128(b26)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c26_15 = _mm_add_pd(c26_15, _mm_mul_pd(a26_15, b26));
#endif
_mm_storeu_pd(&C[(i*56)+18], c26_15);
#else
C[(i*56)+0] += values[194] * B[(i*56)+26];
C[(i*56)+1] += values[195] * B[(i*56)+26];
C[(i*56)+2] += values[196] * B[(i*56)+26];
C[(i*56)+3] += values[197] * B[(i*56)+26];
C[(i*56)+4] += values[198] * B[(i*56)+26];
C[(i*56)+5] += values[199] * B[(i*56)+26];
C[(i*56)+6] += values[200] * B[(i*56)+26];
C[(i*56)+7] += values[201] * B[(i*56)+26];
C[(i*56)+8] += values[202] * B[(i*56)+26];
C[(i*56)+9] += values[203] * B[(i*56)+26];
C[(i*56)+11] += values[204] * B[(i*56)+26];
C[(i*56)+14] += values[205] * B[(i*56)+26];
C[(i*56)+15] += values[206] * B[(i*56)+26];
C[(i*56)+16] += values[207] * B[(i*56)+26];
C[(i*56)+17] += values[208] * B[(i*56)+26];
C[(i*56)+18] += values[209] * B[(i*56)+26];
C[(i*56)+19] += values[210] * B[(i*56)+26];
#endif
#ifndef NDEBUG
num_flops += 34;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b27 = _mm256_broadcast_sd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b27 = _mm_loaddup_pd(&B[(i*56)+27]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a27_0 = _mm256_loadu_pd(&values[211]);
c27_0 = _mm256_add_pd(c27_0, _mm256_mul_pd(a27_0, b27));
_mm256_storeu_pd(&C[(i*56)+0], c27_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a27_0 = _mm_loadu_pd(&values[211]);
c27_0 = _mm_add_pd(c27_0, _mm_mul_pd(a27_0, b27));
_mm_storeu_pd(&C[(i*56)+0], c27_0);
__m128d c27_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a27_2 = _mm_loadu_pd(&values[213]);
c27_2 = _mm_add_pd(c27_2, _mm_mul_pd(a27_2, b27));
_mm_storeu_pd(&C[(i*56)+2], c27_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a27_4 = _mm256_loadu_pd(&values[215]);
c27_4 = _mm256_add_pd(c27_4, _mm256_mul_pd(a27_4, b27));
_mm256_storeu_pd(&C[(i*56)+5], c27_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a27_4 = _mm_loadu_pd(&values[215]);
c27_4 = _mm_add_pd(c27_4, _mm_mul_pd(a27_4, b27));
_mm_storeu_pd(&C[(i*56)+5], c27_4);
__m128d c27_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a27_6 = _mm_loadu_pd(&values[217]);
c27_6 = _mm_add_pd(c27_6, _mm_mul_pd(a27_6, b27));
_mm_storeu_pd(&C[(i*56)+7], c27_6);
#endif
__m128d c27_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a27_8 = _mm_load_sd(&values[219]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_8 = _mm_add_sd(c27_8, _mm_mul_sd(a27_8, b27));
#endif
_mm_store_sd(&C[(i*56)+9], c27_8);
__m128d c27_9 = _mm_load_sd(&C[(i*56)+12]);
__m128d a27_9 = _mm_load_sd(&values[220]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_9 = _mm_add_sd(c27_9, _mm_mul_sd(a27_9, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_9 = _mm_add_sd(c27_9, _mm_mul_sd(a27_9, b27));
#endif
_mm_store_sd(&C[(i*56)+12], c27_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c27_10 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a27_10 = _mm256_loadu_pd(&values[221]);
c27_10 = _mm256_add_pd(c27_10, _mm256_mul_pd(a27_10, b27));
_mm256_storeu_pd(&C[(i*56)+15], c27_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c27_10 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a27_10 = _mm_loadu_pd(&values[221]);
c27_10 = _mm_add_pd(c27_10, _mm_mul_pd(a27_10, b27));
_mm_storeu_pd(&C[(i*56)+15], c27_10);
__m128d c27_12 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a27_12 = _mm_loadu_pd(&values[223]);
c27_12 = _mm_add_pd(c27_12, _mm_mul_pd(a27_12, b27));
_mm_storeu_pd(&C[(i*56)+17], c27_12);
#endif
__m128d c27_14 = _mm_load_sd(&C[(i*56)+19]);
__m128d a27_14 = _mm_load_sd(&values[225]);
#if defined(__SSE3__) && defined(__AVX256__)
c27_14 = _mm_add_sd(c27_14, _mm_mul_sd(a27_14, _mm256_castpd256_pd128(b27)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c27_14 = _mm_add_sd(c27_14, _mm_mul_sd(a27_14, b27));
#endif
_mm_store_sd(&C[(i*56)+19], c27_14);
#else
C[(i*56)+0] += values[211] * B[(i*56)+27];
C[(i*56)+1] += values[212] * B[(i*56)+27];
C[(i*56)+2] += values[213] * B[(i*56)+27];
C[(i*56)+3] += values[214] * B[(i*56)+27];
C[(i*56)+5] += values[215] * B[(i*56)+27];
C[(i*56)+6] += values[216] * B[(i*56)+27];
C[(i*56)+7] += values[217] * B[(i*56)+27];
C[(i*56)+8] += values[218] * B[(i*56)+27];
C[(i*56)+9] += values[219] * B[(i*56)+27];
C[(i*56)+12] += values[220] * B[(i*56)+27];
C[(i*56)+15] += values[221] * B[(i*56)+27];
C[(i*56)+16] += values[222] * B[(i*56)+27];
C[(i*56)+17] += values[223] * B[(i*56)+27];
C[(i*56)+18] += values[224] * B[(i*56)+27];
C[(i*56)+19] += values[225] * B[(i*56)+27];
#endif
#ifndef NDEBUG
num_flops += 30;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b28 = _mm256_broadcast_sd(&B[(i*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b28 = _mm_loaddup_pd(&B[(i*56)+28]);
#endif
__m128d c28_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a28_0 = _mm_load_sd(&values[226]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_0 = _mm_add_sd(c28_0, _mm_mul_sd(a28_0, b28));
#endif
_mm_store_sd(&C[(i*56)+0], c28_0);
__m128d c28_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a28_1 = _mm_loadu_pd(&values[227]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_1 = _mm_add_pd(c28_1, _mm_mul_pd(a28_1, b28));
#endif
_mm_storeu_pd(&C[(i*56)+2], c28_1);
__m128d c28_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a28_3 = _mm_load_sd(&values[229]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_3 = _mm_add_sd(c28_3, _mm_mul_sd(a28_3, b28));
#endif
_mm_store_sd(&C[(i*56)+6], c28_3);
__m128d c28_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a28_4 = _mm_loadu_pd(&values[230]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_4 = _mm_add_pd(c28_4, _mm_mul_pd(a28_4, b28));
#endif
_mm_storeu_pd(&C[(i*56)+8], c28_4);
__m128d c28_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a28_6 = _mm_load_sd(&values[232]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_6 = _mm_add_sd(c28_6, _mm_mul_sd(a28_6, b28));
#endif
_mm_store_sd(&C[(i*56)+13], c28_6);
__m128d c28_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a28_7 = _mm_load_sd(&values[233]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_7 = _mm_add_sd(c28_7, _mm_mul_sd(a28_7, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_7 = _mm_add_sd(c28_7, _mm_mul_sd(a28_7, b28));
#endif
_mm_store_sd(&C[(i*56)+16], c28_7);
__m128d c28_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a28_8 = _mm_loadu_pd(&values[234]);
#if defined(__SSE3__) && defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, _mm256_castpd256_pd128(b28)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c28_8 = _mm_add_pd(c28_8, _mm_mul_pd(a28_8, b28));
#endif
_mm_storeu_pd(&C[(i*56)+18], c28_8);
#else
C[(i*56)+0] += values[226] * B[(i*56)+28];
C[(i*56)+2] += values[227] * B[(i*56)+28];
C[(i*56)+3] += values[228] * B[(i*56)+28];
C[(i*56)+6] += values[229] * B[(i*56)+28];
C[(i*56)+8] += values[230] * B[(i*56)+28];
C[(i*56)+9] += values[231] * B[(i*56)+28];
C[(i*56)+13] += values[232] * B[(i*56)+28];
C[(i*56)+16] += values[233] * B[(i*56)+28];
C[(i*56)+18] += values[234] * B[(i*56)+28];
C[(i*56)+19] += values[235] * B[(i*56)+28];
#endif
#ifndef NDEBUG
num_flops += 20;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b29 = _mm256_broadcast_sd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b29 = _mm_loaddup_pd(&B[(i*56)+29]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c29_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a29_0 = _mm256_loadu_pd(&values[236]);
c29_0 = _mm256_add_pd(c29_0, _mm256_mul_pd(a29_0, b29));
_mm256_storeu_pd(&C[(i*56)+0], c29_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c29_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a29_0 = _mm_loadu_pd(&values[236]);
c29_0 = _mm_add_pd(c29_0, _mm_mul_pd(a29_0, b29));
_mm_storeu_pd(&C[(i*56)+0], c29_0);
__m128d c29_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a29_2 = _mm_loadu_pd(&values[238]);
c29_2 = _mm_add_pd(c29_2, _mm_mul_pd(a29_2, b29));
_mm_storeu_pd(&C[(i*56)+2], c29_2);
#endif
__m128d c29_4 = _mm_load_sd(&C[(i*56)+4]);
__m128d a29_4 = _mm_load_sd(&values[240]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_4 = _mm_add_sd(c29_4, _mm_mul_sd(a29_4, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_4 = _mm_add_sd(c29_4, _mm_mul_sd(a29_4, b29));
#endif
_mm_store_sd(&C[(i*56)+4], c29_4);
__m128d c29_5 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a29_5 = _mm_loadu_pd(&values[241]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_5 = _mm_add_pd(c29_5, _mm_mul_pd(a29_5, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_5 = _mm_add_pd(c29_5, _mm_mul_pd(a29_5, b29));
#endif
_mm_storeu_pd(&C[(i*56)+7], c29_5);
__m128d c29_7 = _mm_load_sd(&C[(i*56)+9]);
__m128d a29_7 = _mm_load_sd(&values[243]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_7 = _mm_add_sd(c29_7, _mm_mul_sd(a29_7, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_7 = _mm_add_sd(c29_7, _mm_mul_sd(a29_7, b29));
#endif
_mm_store_sd(&C[(i*56)+9], c29_7);
__m128d c29_8 = _mm_load_sd(&C[(i*56)+14]);
__m128d a29_8 = _mm_load_sd(&values[244]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_8 = _mm_add_sd(c29_8, _mm_mul_sd(a29_8, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_8 = _mm_add_sd(c29_8, _mm_mul_sd(a29_8, b29));
#endif
_mm_store_sd(&C[(i*56)+14], c29_8);
__m128d c29_9 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a29_9 = _mm_loadu_pd(&values[245]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_9 = _mm_add_pd(c29_9, _mm_mul_pd(a29_9, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_9 = _mm_add_pd(c29_9, _mm_mul_pd(a29_9, b29));
#endif
_mm_storeu_pd(&C[(i*56)+17], c29_9);
__m128d c29_11 = _mm_load_sd(&C[(i*56)+19]);
__m128d a29_11 = _mm_load_sd(&values[247]);
#if defined(__SSE3__) && defined(__AVX256__)
c29_11 = _mm_add_sd(c29_11, _mm_mul_sd(a29_11, _mm256_castpd256_pd128(b29)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c29_11 = _mm_add_sd(c29_11, _mm_mul_sd(a29_11, b29));
#endif
_mm_store_sd(&C[(i*56)+19], c29_11);
#else
C[(i*56)+0] += values[236] * B[(i*56)+29];
C[(i*56)+1] += values[237] * B[(i*56)+29];
C[(i*56)+2] += values[238] * B[(i*56)+29];
C[(i*56)+3] += values[239] * B[(i*56)+29];
C[(i*56)+4] += values[240] * B[(i*56)+29];
C[(i*56)+7] += values[241] * B[(i*56)+29];
C[(i*56)+8] += values[242] * B[(i*56)+29];
C[(i*56)+9] += values[243] * B[(i*56)+29];
C[(i*56)+14] += values[244] * B[(i*56)+29];
C[(i*56)+17] += values[245] * B[(i*56)+29];
C[(i*56)+18] += values[246] * B[(i*56)+29];
C[(i*56)+19] += values[247] * B[(i*56)+29];
#endif
#ifndef NDEBUG
num_flops += 24;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b30 = _mm256_broadcast_sd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b30 = _mm_loaddup_pd(&B[(i*56)+30]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c30_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a30_0 = _mm256_loadu_pd(&values[248]);
c30_0 = _mm256_add_pd(c30_0, _mm256_mul_pd(a30_0, b30));
_mm256_storeu_pd(&C[(i*56)+0], c30_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c30_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a30_0 = _mm_loadu_pd(&values[248]);
c30_0 = _mm_add_pd(c30_0, _mm_mul_pd(a30_0, b30));
_mm_storeu_pd(&C[(i*56)+0], c30_0);
__m128d c30_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a30_2 = _mm_loadu_pd(&values[250]);
c30_2 = _mm_add_pd(c30_2, _mm_mul_pd(a30_2, b30));
_mm_storeu_pd(&C[(i*56)+2], c30_2);
#endif
__m128d c30_4 = _mm_load_sd(&C[(i*56)+5]);
__m128d a30_4 = _mm_load_sd(&values[252]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_4 = _mm_add_sd(c30_4, _mm_mul_sd(a30_4, b30));
#endif
_mm_store_sd(&C[(i*56)+5], c30_4);
__m128d c30_5 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a30_5 = _mm_loadu_pd(&values[253]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_5 = _mm_add_pd(c30_5, _mm_mul_pd(a30_5, b30));
#endif
_mm_storeu_pd(&C[(i*56)+7], c30_5);
__m128d c30_7 = _mm_load_sd(&C[(i*56)+9]);
__m128d a30_7 = _mm_load_sd(&values[255]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_7 = _mm_add_sd(c30_7, _mm_mul_sd(a30_7, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_7 = _mm_add_sd(c30_7, _mm_mul_sd(a30_7, b30));
#endif
_mm_store_sd(&C[(i*56)+9], c30_7);
__m128d c30_8 = _mm_load_sd(&C[(i*56)+15]);
__m128d a30_8 = _mm_load_sd(&values[256]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_8 = _mm_add_sd(c30_8, _mm_mul_sd(a30_8, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_8 = _mm_add_sd(c30_8, _mm_mul_sd(a30_8, b30));
#endif
_mm_store_sd(&C[(i*56)+15], c30_8);
__m128d c30_9 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a30_9 = _mm_loadu_pd(&values[257]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_9 = _mm_add_pd(c30_9, _mm_mul_pd(a30_9, b30));
#endif
_mm_storeu_pd(&C[(i*56)+17], c30_9);
__m128d c30_11 = _mm_load_sd(&C[(i*56)+19]);
__m128d a30_11 = _mm_load_sd(&values[259]);
#if defined(__SSE3__) && defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, _mm256_castpd256_pd128(b30)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c30_11 = _mm_add_sd(c30_11, _mm_mul_sd(a30_11, b30));
#endif
_mm_store_sd(&C[(i*56)+19], c30_11);
#else
C[(i*56)+0] += values[248] * B[(i*56)+30];
C[(i*56)+1] += values[249] * B[(i*56)+30];
C[(i*56)+2] += values[250] * B[(i*56)+30];
C[(i*56)+3] += values[251] * B[(i*56)+30];
C[(i*56)+5] += values[252] * B[(i*56)+30];
C[(i*56)+7] += values[253] * B[(i*56)+30];
C[(i*56)+8] += values[254] * B[(i*56)+30];
C[(i*56)+9] += values[255] * B[(i*56)+30];
C[(i*56)+15] += values[256] * B[(i*56)+30];
C[(i*56)+17] += values[257] * B[(i*56)+30];
C[(i*56)+18] += values[258] * B[(i*56)+30];
C[(i*56)+19] += values[259] * B[(i*56)+30];
#endif
#ifndef NDEBUG
num_flops += 24;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b31 = _mm256_broadcast_sd(&B[(i*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b31 = _mm_loaddup_pd(&B[(i*56)+31]);
#endif
__m128d c31_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a31_0 = _mm_load_sd(&values[260]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_0 = _mm_add_sd(c31_0, _mm_mul_sd(a31_0, b31));
#endif
_mm_store_sd(&C[(i*56)+0], c31_0);
__m128d c31_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a31_1 = _mm_loadu_pd(&values[261]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_1 = _mm_add_pd(c31_1, _mm_mul_pd(a31_1, b31));
#endif
_mm_storeu_pd(&C[(i*56)+2], c31_1);
__m128d c31_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a31_3 = _mm_load_sd(&values[263]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_3 = _mm_add_sd(c31_3, _mm_mul_sd(a31_3, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_3 = _mm_add_sd(c31_3, _mm_mul_sd(a31_3, b31));
#endif
_mm_store_sd(&C[(i*56)+6], c31_3);
__m128d c31_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a31_4 = _mm_loadu_pd(&values[264]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_4 = _mm_add_pd(c31_4, _mm_mul_pd(a31_4, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_4 = _mm_add_pd(c31_4, _mm_mul_pd(a31_4, b31));
#endif
_mm_storeu_pd(&C[(i*56)+8], c31_4);
__m128d c31_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a31_6 = _mm_load_sd(&values[266]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_6 = _mm_add_sd(c31_6, _mm_mul_sd(a31_6, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_6 = _mm_add_sd(c31_6, _mm_mul_sd(a31_6, b31));
#endif
_mm_store_sd(&C[(i*56)+16], c31_6);
__m128d c31_7 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a31_7 = _mm_loadu_pd(&values[267]);
#if defined(__SSE3__) && defined(__AVX256__)
c31_7 = _mm_add_pd(c31_7, _mm_mul_pd(a31_7, _mm256_castpd256_pd128(b31)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c31_7 = _mm_add_pd(c31_7, _mm_mul_pd(a31_7, b31));
#endif
_mm_storeu_pd(&C[(i*56)+18], c31_7);
#else
C[(i*56)+0] += values[260] * B[(i*56)+31];
C[(i*56)+2] += values[261] * B[(i*56)+31];
C[(i*56)+3] += values[262] * B[(i*56)+31];
C[(i*56)+6] += values[263] * B[(i*56)+31];
C[(i*56)+8] += values[264] * B[(i*56)+31];
C[(i*56)+9] += values[265] * B[(i*56)+31];
C[(i*56)+16] += values[266] * B[(i*56)+31];
C[(i*56)+18] += values[267] * B[(i*56)+31];
C[(i*56)+19] += values[268] * B[(i*56)+31];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b32 = _mm256_broadcast_sd(&B[(i*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b32 = _mm_loaddup_pd(&B[(i*56)+32]);
#endif
__m128d c32_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a32_0 = _mm_loadu_pd(&values[269]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_0 = _mm_add_pd(c32_0, _mm_mul_pd(a32_0, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_0 = _mm_add_pd(c32_0, _mm_mul_pd(a32_0, b32));
#endif
_mm_storeu_pd(&C[(i*56)+0], c32_0);
__m128d c32_2 = _mm_load_sd(&C[(i*56)+3]);
__m128d a32_2 = _mm_load_sd(&values[271]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_2 = _mm_add_sd(c32_2, _mm_mul_sd(a32_2, b32));
#endif
_mm_store_sd(&C[(i*56)+3], c32_2);
__m128d c32_3 = _mm_load_sd(&C[(i*56)+7]);
__m128d a32_3 = _mm_load_sd(&values[272]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_3 = _mm_add_sd(c32_3, _mm_mul_sd(a32_3, b32));
#endif
_mm_store_sd(&C[(i*56)+7], c32_3);
__m128d c32_4 = _mm_load_sd(&C[(i*56)+9]);
__m128d a32_4 = _mm_load_sd(&values[273]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_4 = _mm_add_sd(c32_4, _mm_mul_sd(a32_4, b32));
#endif
_mm_store_sd(&C[(i*56)+9], c32_4);
__m128d c32_5 = _mm_load_sd(&C[(i*56)+17]);
__m128d a32_5 = _mm_load_sd(&values[274]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_5 = _mm_add_sd(c32_5, _mm_mul_sd(a32_5, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_5 = _mm_add_sd(c32_5, _mm_mul_sd(a32_5, b32));
#endif
_mm_store_sd(&C[(i*56)+17], c32_5);
__m128d c32_6 = _mm_load_sd(&C[(i*56)+19]);
__m128d a32_6 = _mm_load_sd(&values[275]);
#if defined(__SSE3__) && defined(__AVX256__)
c32_6 = _mm_add_sd(c32_6, _mm_mul_sd(a32_6, _mm256_castpd256_pd128(b32)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c32_6 = _mm_add_sd(c32_6, _mm_mul_sd(a32_6, b32));
#endif
_mm_store_sd(&C[(i*56)+19], c32_6);
#else
C[(i*56)+0] += values[269] * B[(i*56)+32];
C[(i*56)+1] += values[270] * B[(i*56)+32];
C[(i*56)+3] += values[271] * B[(i*56)+32];
C[(i*56)+7] += values[272] * B[(i*56)+32];
C[(i*56)+9] += values[273] * B[(i*56)+32];
C[(i*56)+17] += values[274] * B[(i*56)+32];
C[(i*56)+19] += values[275] * B[(i*56)+32];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b33 = _mm256_broadcast_sd(&B[(i*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b33 = _mm_loaddup_pd(&B[(i*56)+33]);
#endif
__m128d c33_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a33_0 = _mm_load_sd(&values[276]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_0 = _mm_add_sd(c33_0, _mm_mul_sd(a33_0, b33));
#endif
_mm_store_sd(&C[(i*56)+0], c33_0);
__m128d c33_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a33_1 = _mm_loadu_pd(&values[277]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_1 = _mm_add_pd(c33_1, _mm_mul_pd(a33_1, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_1 = _mm_add_pd(c33_1, _mm_mul_pd(a33_1, b33));
#endif
_mm_storeu_pd(&C[(i*56)+2], c33_1);
__m128d c33_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a33_3 = _mm_loadu_pd(&values[279]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_3 = _mm_add_pd(c33_3, _mm_mul_pd(a33_3, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_3 = _mm_add_pd(c33_3, _mm_mul_pd(a33_3, b33));
#endif
_mm_storeu_pd(&C[(i*56)+8], c33_3);
__m128d c33_5 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a33_5 = _mm_loadu_pd(&values[281]);
#if defined(__SSE3__) && defined(__AVX256__)
c33_5 = _mm_add_pd(c33_5, _mm_mul_pd(a33_5, _mm256_castpd256_pd128(b33)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c33_5 = _mm_add_pd(c33_5, _mm_mul_pd(a33_5, b33));
#endif
_mm_storeu_pd(&C[(i*56)+18], c33_5);
#else
C[(i*56)+0] += values[276] * B[(i*56)+33];
C[(i*56)+2] += values[277] * B[(i*56)+33];
C[(i*56)+3] += values[278] * B[(i*56)+33];
C[(i*56)+8] += values[279] * B[(i*56)+33];
C[(i*56)+9] += values[280] * B[(i*56)+33];
C[(i*56)+18] += values[281] * B[(i*56)+33];
C[(i*56)+19] += values[282] * B[(i*56)+33];
#endif
#ifndef NDEBUG
num_flops += 14;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b34 = _mm256_broadcast_sd(&B[(i*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b34 = _mm_loaddup_pd(&B[(i*56)+34]);
#endif
__m128d c34_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a34_0 = _mm_load_sd(&values[283]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_0 = _mm_add_sd(c34_0, _mm_mul_sd(a34_0, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_0 = _mm_add_sd(c34_0, _mm_mul_sd(a34_0, b34));
#endif
_mm_store_sd(&C[(i*56)+0], c34_0);
__m128d c34_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a34_1 = _mm_load_sd(&values[284]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_1 = _mm_add_sd(c34_1, _mm_mul_sd(a34_1, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_1 = _mm_add_sd(c34_1, _mm_mul_sd(a34_1, b34));
#endif
_mm_store_sd(&C[(i*56)+3], c34_1);
__m128d c34_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a34_2 = _mm_load_sd(&values[285]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_2 = _mm_add_sd(c34_2, _mm_mul_sd(a34_2, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_2 = _mm_add_sd(c34_2, _mm_mul_sd(a34_2, b34));
#endif
_mm_store_sd(&C[(i*56)+9], c34_2);
__m128d c34_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a34_3 = _mm_load_sd(&values[286]);
#if defined(__SSE3__) && defined(__AVX256__)
c34_3 = _mm_add_sd(c34_3, _mm_mul_sd(a34_3, _mm256_castpd256_pd128(b34)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c34_3 = _mm_add_sd(c34_3, _mm_mul_sd(a34_3, b34));
#endif
_mm_store_sd(&C[(i*56)+19], c34_3);
#else
C[(i*56)+0] += values[283] * B[(i*56)+34];
C[(i*56)+3] += values[284] * B[(i*56)+34];
C[(i*56)+9] += values[285] * B[(i*56)+34];
C[(i*56)+19] += values[286] * B[(i*56)+34];
#endif
#ifndef NDEBUG
num_flops += 8;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
if ( __builtin_expect(exit_col == 35, false) ) { continue; }
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b35 = _mm256_broadcast_sd(&B[(i*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b35 = _mm_loaddup_pd(&B[(i*56)+35]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a35_0 = _mm256_loadu_pd(&values[287]);
c35_0 = _mm256_add_pd(c35_0, _mm256_mul_pd(a35_0, b35));
_mm256_storeu_pd(&C[(i*56)+0], c35_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a35_0 = _mm_loadu_pd(&values[287]);
c35_0 = _mm_add_pd(c35_0, _mm_mul_pd(a35_0, b35));
_mm_storeu_pd(&C[(i*56)+0], c35_0);
__m128d c35_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a35_2 = _mm_loadu_pd(&values[289]);
c35_2 = _mm_add_pd(c35_2, _mm_mul_pd(a35_2, b35));
_mm_storeu_pd(&C[(i*56)+2], c35_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a35_4 = _mm256_loadu_pd(&values[291]);
c35_4 = _mm256_add_pd(c35_4, _mm256_mul_pd(a35_4, b35));
_mm256_storeu_pd(&C[(i*56)+4], c35_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a35_4 = _mm_loadu_pd(&values[291]);
c35_4 = _mm_add_pd(c35_4, _mm_mul_pd(a35_4, b35));
_mm_storeu_pd(&C[(i*56)+4], c35_4);
__m128d c35_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a35_6 = _mm_loadu_pd(&values[293]);
c35_6 = _mm_add_pd(c35_6, _mm_mul_pd(a35_6, b35));
_mm_storeu_pd(&C[(i*56)+6], c35_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a35_8 = _mm256_loadu_pd(&values[295]);
c35_8 = _mm256_add_pd(c35_8, _mm256_mul_pd(a35_8, b35));
_mm256_storeu_pd(&C[(i*56)+8], c35_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a35_8 = _mm_loadu_pd(&values[295]);
c35_8 = _mm_add_pd(c35_8, _mm_mul_pd(a35_8, b35));
_mm_storeu_pd(&C[(i*56)+8], c35_8);
__m128d c35_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a35_10 = _mm_loadu_pd(&values[297]);
c35_10 = _mm_add_pd(c35_10, _mm_mul_pd(a35_10, b35));
_mm_storeu_pd(&C[(i*56)+10], c35_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a35_12 = _mm256_loadu_pd(&values[299]);
c35_12 = _mm256_add_pd(c35_12, _mm256_mul_pd(a35_12, b35));
_mm256_storeu_pd(&C[(i*56)+12], c35_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a35_12 = _mm_loadu_pd(&values[299]);
c35_12 = _mm_add_pd(c35_12, _mm_mul_pd(a35_12, b35));
_mm_storeu_pd(&C[(i*56)+12], c35_12);
__m128d c35_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a35_14 = _mm_loadu_pd(&values[301]);
c35_14 = _mm_add_pd(c35_14, _mm_mul_pd(a35_14, b35));
_mm_storeu_pd(&C[(i*56)+14], c35_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a35_16 = _mm256_loadu_pd(&values[303]);
c35_16 = _mm256_add_pd(c35_16, _mm256_mul_pd(a35_16, b35));
_mm256_storeu_pd(&C[(i*56)+16], c35_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a35_16 = _mm_loadu_pd(&values[303]);
c35_16 = _mm_add_pd(c35_16, _mm_mul_pd(a35_16, b35));
_mm_storeu_pd(&C[(i*56)+16], c35_16);
__m128d c35_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a35_18 = _mm_loadu_pd(&values[305]);
c35_18 = _mm_add_pd(c35_18, _mm_mul_pd(a35_18, b35));
_mm_storeu_pd(&C[(i*56)+18], c35_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_20 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a35_20 = _mm256_loadu_pd(&values[307]);
c35_20 = _mm256_add_pd(c35_20, _mm256_mul_pd(a35_20, b35));
_mm256_storeu_pd(&C[(i*56)+20], c35_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_20 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a35_20 = _mm_loadu_pd(&values[307]);
c35_20 = _mm_add_pd(c35_20, _mm_mul_pd(a35_20, b35));
_mm_storeu_pd(&C[(i*56)+20], c35_20);
__m128d c35_22 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a35_22 = _mm_loadu_pd(&values[309]);
c35_22 = _mm_add_pd(c35_22, _mm_mul_pd(a35_22, b35));
_mm_storeu_pd(&C[(i*56)+22], c35_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_24 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a35_24 = _mm256_loadu_pd(&values[311]);
c35_24 = _mm256_add_pd(c35_24, _mm256_mul_pd(a35_24, b35));
_mm256_storeu_pd(&C[(i*56)+24], c35_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_24 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a35_24 = _mm_loadu_pd(&values[311]);
c35_24 = _mm_add_pd(c35_24, _mm_mul_pd(a35_24, b35));
_mm_storeu_pd(&C[(i*56)+24], c35_24);
__m128d c35_26 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a35_26 = _mm_loadu_pd(&values[313]);
c35_26 = _mm_add_pd(c35_26, _mm_mul_pd(a35_26, b35));
_mm_storeu_pd(&C[(i*56)+26], c35_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c35_28 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a35_28 = _mm256_loadu_pd(&values[315]);
c35_28 = _mm256_add_pd(c35_28, _mm256_mul_pd(a35_28, b35));
_mm256_storeu_pd(&C[(i*56)+28], c35_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c35_28 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a35_28 = _mm_loadu_pd(&values[315]);
c35_28 = _mm_add_pd(c35_28, _mm_mul_pd(a35_28, b35));
_mm_storeu_pd(&C[(i*56)+28], c35_28);
__m128d c35_30 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a35_30 = _mm_loadu_pd(&values[317]);
c35_30 = _mm_add_pd(c35_30, _mm_mul_pd(a35_30, b35));
_mm_storeu_pd(&C[(i*56)+30], c35_30);
#endif
__m128d c35_32 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a35_32 = _mm_loadu_pd(&values[319]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_32 = _mm_add_pd(c35_32, _mm_mul_pd(a35_32, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_32 = _mm_add_pd(c35_32, _mm_mul_pd(a35_32, b35));
#endif
_mm_storeu_pd(&C[(i*56)+32], c35_32);
__m128d c35_34 = _mm_load_sd(&C[(i*56)+34]);
__m128d a35_34 = _mm_load_sd(&values[321]);
#if defined(__SSE3__) && defined(__AVX256__)
c35_34 = _mm_add_sd(c35_34, _mm_mul_sd(a35_34, _mm256_castpd256_pd128(b35)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c35_34 = _mm_add_sd(c35_34, _mm_mul_sd(a35_34, b35));
#endif
_mm_store_sd(&C[(i*56)+34], c35_34);
#else
C[(i*56)+0] += values[287] * B[(i*56)+35];
C[(i*56)+1] += values[288] * B[(i*56)+35];
C[(i*56)+2] += values[289] * B[(i*56)+35];
C[(i*56)+3] += values[290] * B[(i*56)+35];
C[(i*56)+4] += values[291] * B[(i*56)+35];
C[(i*56)+5] += values[292] * B[(i*56)+35];
C[(i*56)+6] += values[293] * B[(i*56)+35];
C[(i*56)+7] += values[294] * B[(i*56)+35];
C[(i*56)+8] += values[295] * B[(i*56)+35];
C[(i*56)+9] += values[296] * B[(i*56)+35];
C[(i*56)+10] += values[297] * B[(i*56)+35];
C[(i*56)+11] += values[298] * B[(i*56)+35];
C[(i*56)+12] += values[299] * B[(i*56)+35];
C[(i*56)+13] += values[300] * B[(i*56)+35];
C[(i*56)+14] += values[301] * B[(i*56)+35];
C[(i*56)+15] += values[302] * B[(i*56)+35];
C[(i*56)+16] += values[303] * B[(i*56)+35];
C[(i*56)+17] += values[304] * B[(i*56)+35];
C[(i*56)+18] += values[305] * B[(i*56)+35];
C[(i*56)+19] += values[306] * B[(i*56)+35];
C[(i*56)+20] += values[307] * B[(i*56)+35];
C[(i*56)+21] += values[308] * B[(i*56)+35];
C[(i*56)+22] += values[309] * B[(i*56)+35];
C[(i*56)+23] += values[310] * B[(i*56)+35];
C[(i*56)+24] += values[311] * B[(i*56)+35];
C[(i*56)+25] += values[312] * B[(i*56)+35];
C[(i*56)+26] += values[313] * B[(i*56)+35];
C[(i*56)+27] += values[314] * B[(i*56)+35];
C[(i*56)+28] += values[315] * B[(i*56)+35];
C[(i*56)+29] += values[316] * B[(i*56)+35];
C[(i*56)+30] += values[317] * B[(i*56)+35];
C[(i*56)+31] += values[318] * B[(i*56)+35];
C[(i*56)+32] += values[319] * B[(i*56)+35];
C[(i*56)+33] += values[320] * B[(i*56)+35];
C[(i*56)+34] += values[321] * B[(i*56)+35];
#endif
#ifndef NDEBUG
num_flops += 70;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b36 = _mm256_broadcast_sd(&B[(i*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b36 = _mm_loaddup_pd(&B[(i*56)+36]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a36_0 = _mm256_loadu_pd(&values[322]);
c36_0 = _mm256_add_pd(c36_0, _mm256_mul_pd(a36_0, b36));
_mm256_storeu_pd(&C[(i*56)+0], c36_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a36_0 = _mm_loadu_pd(&values[322]);
c36_0 = _mm_add_pd(c36_0, _mm_mul_pd(a36_0, b36));
_mm_storeu_pd(&C[(i*56)+0], c36_0);
__m128d c36_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a36_2 = _mm_loadu_pd(&values[324]);
c36_2 = _mm_add_pd(c36_2, _mm_mul_pd(a36_2, b36));
_mm_storeu_pd(&C[(i*56)+2], c36_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a36_4 = _mm256_loadu_pd(&values[326]);
c36_4 = _mm256_add_pd(c36_4, _mm256_mul_pd(a36_4, b36));
_mm256_storeu_pd(&C[(i*56)+4], c36_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a36_4 = _mm_loadu_pd(&values[326]);
c36_4 = _mm_add_pd(c36_4, _mm_mul_pd(a36_4, b36));
_mm_storeu_pd(&C[(i*56)+4], c36_4);
__m128d c36_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a36_6 = _mm_loadu_pd(&values[328]);
c36_6 = _mm_add_pd(c36_6, _mm_mul_pd(a36_6, b36));
_mm_storeu_pd(&C[(i*56)+6], c36_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a36_8 = _mm256_loadu_pd(&values[330]);
c36_8 = _mm256_add_pd(c36_8, _mm256_mul_pd(a36_8, b36));
_mm256_storeu_pd(&C[(i*56)+8], c36_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a36_8 = _mm_loadu_pd(&values[330]);
c36_8 = _mm_add_pd(c36_8, _mm_mul_pd(a36_8, b36));
_mm_storeu_pd(&C[(i*56)+8], c36_8);
__m128d c36_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a36_10 = _mm_loadu_pd(&values[332]);
c36_10 = _mm_add_pd(c36_10, _mm_mul_pd(a36_10, b36));
_mm_storeu_pd(&C[(i*56)+10], c36_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a36_12 = _mm256_loadu_pd(&values[334]);
c36_12 = _mm256_add_pd(c36_12, _mm256_mul_pd(a36_12, b36));
_mm256_storeu_pd(&C[(i*56)+12], c36_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a36_12 = _mm_loadu_pd(&values[334]);
c36_12 = _mm_add_pd(c36_12, _mm_mul_pd(a36_12, b36));
_mm_storeu_pd(&C[(i*56)+12], c36_12);
__m128d c36_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a36_14 = _mm_loadu_pd(&values[336]);
c36_14 = _mm_add_pd(c36_14, _mm_mul_pd(a36_14, b36));
_mm_storeu_pd(&C[(i*56)+14], c36_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a36_16 = _mm256_loadu_pd(&values[338]);
c36_16 = _mm256_add_pd(c36_16, _mm256_mul_pd(a36_16, b36));
_mm256_storeu_pd(&C[(i*56)+16], c36_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a36_16 = _mm_loadu_pd(&values[338]);
c36_16 = _mm_add_pd(c36_16, _mm_mul_pd(a36_16, b36));
_mm_storeu_pd(&C[(i*56)+16], c36_16);
__m128d c36_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a36_18 = _mm_loadu_pd(&values[340]);
c36_18 = _mm_add_pd(c36_18, _mm_mul_pd(a36_18, b36));
_mm_storeu_pd(&C[(i*56)+18], c36_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_20 = _mm256_loadu_pd(&C[(i*56)+20]);
__m256d a36_20 = _mm256_loadu_pd(&values[342]);
c36_20 = _mm256_add_pd(c36_20, _mm256_mul_pd(a36_20, b36));
_mm256_storeu_pd(&C[(i*56)+20], c36_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_20 = _mm_loadu_pd(&C[(i*56)+20]);
__m128d a36_20 = _mm_loadu_pd(&values[342]);
c36_20 = _mm_add_pd(c36_20, _mm_mul_pd(a36_20, b36));
_mm_storeu_pd(&C[(i*56)+20], c36_20);
__m128d c36_22 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a36_22 = _mm_loadu_pd(&values[344]);
c36_22 = _mm_add_pd(c36_22, _mm_mul_pd(a36_22, b36));
_mm_storeu_pd(&C[(i*56)+22], c36_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_24 = _mm256_loadu_pd(&C[(i*56)+24]);
__m256d a36_24 = _mm256_loadu_pd(&values[346]);
c36_24 = _mm256_add_pd(c36_24, _mm256_mul_pd(a36_24, b36));
_mm256_storeu_pd(&C[(i*56)+24], c36_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_24 = _mm_loadu_pd(&C[(i*56)+24]);
__m128d a36_24 = _mm_loadu_pd(&values[346]);
c36_24 = _mm_add_pd(c36_24, _mm_mul_pd(a36_24, b36));
_mm_storeu_pd(&C[(i*56)+24], c36_24);
__m128d c36_26 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a36_26 = _mm_loadu_pd(&values[348]);
c36_26 = _mm_add_pd(c36_26, _mm_mul_pd(a36_26, b36));
_mm_storeu_pd(&C[(i*56)+26], c36_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c36_28 = _mm256_loadu_pd(&C[(i*56)+28]);
__m256d a36_28 = _mm256_loadu_pd(&values[350]);
c36_28 = _mm256_add_pd(c36_28, _mm256_mul_pd(a36_28, b36));
_mm256_storeu_pd(&C[(i*56)+28], c36_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c36_28 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a36_28 = _mm_loadu_pd(&values[350]);
c36_28 = _mm_add_pd(c36_28, _mm_mul_pd(a36_28, b36));
_mm_storeu_pd(&C[(i*56)+28], c36_28);
__m128d c36_30 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a36_30 = _mm_loadu_pd(&values[352]);
c36_30 = _mm_add_pd(c36_30, _mm_mul_pd(a36_30, b36));
_mm_storeu_pd(&C[(i*56)+30], c36_30);
#endif
__m128d c36_32 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a36_32 = _mm_loadu_pd(&values[354]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_32 = _mm_add_pd(c36_32, _mm_mul_pd(a36_32, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_32 = _mm_add_pd(c36_32, _mm_mul_pd(a36_32, b36));
#endif
_mm_storeu_pd(&C[(i*56)+32], c36_32);
__m128d c36_34 = _mm_load_sd(&C[(i*56)+34]);
__m128d a36_34 = _mm_load_sd(&values[356]);
#if defined(__SSE3__) && defined(__AVX256__)
c36_34 = _mm_add_sd(c36_34, _mm_mul_sd(a36_34, _mm256_castpd256_pd128(b36)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c36_34 = _mm_add_sd(c36_34, _mm_mul_sd(a36_34, b36));
#endif
_mm_store_sd(&C[(i*56)+34], c36_34);
#else
C[(i*56)+0] += values[322] * B[(i*56)+36];
C[(i*56)+1] += values[323] * B[(i*56)+36];
C[(i*56)+2] += values[324] * B[(i*56)+36];
C[(i*56)+3] += values[325] * B[(i*56)+36];
C[(i*56)+4] += values[326] * B[(i*56)+36];
C[(i*56)+5] += values[327] * B[(i*56)+36];
C[(i*56)+6] += values[328] * B[(i*56)+36];
C[(i*56)+7] += values[329] * B[(i*56)+36];
C[(i*56)+8] += values[330] * B[(i*56)+36];
C[(i*56)+9] += values[331] * B[(i*56)+36];
C[(i*56)+10] += values[332] * B[(i*56)+36];
C[(i*56)+11] += values[333] * B[(i*56)+36];
C[(i*56)+12] += values[334] * B[(i*56)+36];
C[(i*56)+13] += values[335] * B[(i*56)+36];
C[(i*56)+14] += values[336] * B[(i*56)+36];
C[(i*56)+15] += values[337] * B[(i*56)+36];
C[(i*56)+16] += values[338] * B[(i*56)+36];
C[(i*56)+17] += values[339] * B[(i*56)+36];
C[(i*56)+18] += values[340] * B[(i*56)+36];
C[(i*56)+19] += values[341] * B[(i*56)+36];
C[(i*56)+20] += values[342] * B[(i*56)+36];
C[(i*56)+21] += values[343] * B[(i*56)+36];
C[(i*56)+22] += values[344] * B[(i*56)+36];
C[(i*56)+23] += values[345] * B[(i*56)+36];
C[(i*56)+24] += values[346] * B[(i*56)+36];
C[(i*56)+25] += values[347] * B[(i*56)+36];
C[(i*56)+26] += values[348] * B[(i*56)+36];
C[(i*56)+27] += values[349] * B[(i*56)+36];
C[(i*56)+28] += values[350] * B[(i*56)+36];
C[(i*56)+29] += values[351] * B[(i*56)+36];
C[(i*56)+30] += values[352] * B[(i*56)+36];
C[(i*56)+31] += values[353] * B[(i*56)+36];
C[(i*56)+32] += values[354] * B[(i*56)+36];
C[(i*56)+33] += values[355] * B[(i*56)+36];
C[(i*56)+34] += values[356] * B[(i*56)+36];
#endif
#ifndef NDEBUG
num_flops += 70;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b37 = _mm256_broadcast_sd(&B[(i*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b37 = _mm_loaddup_pd(&B[(i*56)+37]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a37_0 = _mm256_loadu_pd(&values[357]);
c37_0 = _mm256_add_pd(c37_0, _mm256_mul_pd(a37_0, b37));
_mm256_storeu_pd(&C[(i*56)+0], c37_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a37_0 = _mm_loadu_pd(&values[357]);
c37_0 = _mm_add_pd(c37_0, _mm_mul_pd(a37_0, b37));
_mm_storeu_pd(&C[(i*56)+0], c37_0);
__m128d c37_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a37_2 = _mm_loadu_pd(&values[359]);
c37_2 = _mm_add_pd(c37_2, _mm_mul_pd(a37_2, b37));
_mm_storeu_pd(&C[(i*56)+2], c37_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a37_4 = _mm256_loadu_pd(&values[361]);
c37_4 = _mm256_add_pd(c37_4, _mm256_mul_pd(a37_4, b37));
_mm256_storeu_pd(&C[(i*56)+4], c37_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a37_4 = _mm_loadu_pd(&values[361]);
c37_4 = _mm_add_pd(c37_4, _mm_mul_pd(a37_4, b37));
_mm_storeu_pd(&C[(i*56)+4], c37_4);
__m128d c37_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a37_6 = _mm_loadu_pd(&values[363]);
c37_6 = _mm_add_pd(c37_6, _mm_mul_pd(a37_6, b37));
_mm_storeu_pd(&C[(i*56)+6], c37_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a37_8 = _mm256_loadu_pd(&values[365]);
c37_8 = _mm256_add_pd(c37_8, _mm256_mul_pd(a37_8, b37));
_mm256_storeu_pd(&C[(i*56)+8], c37_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a37_8 = _mm_loadu_pd(&values[365]);
c37_8 = _mm_add_pd(c37_8, _mm_mul_pd(a37_8, b37));
_mm_storeu_pd(&C[(i*56)+8], c37_8);
__m128d c37_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a37_10 = _mm_loadu_pd(&values[367]);
c37_10 = _mm_add_pd(c37_10, _mm_mul_pd(a37_10, b37));
_mm_storeu_pd(&C[(i*56)+10], c37_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a37_12 = _mm256_loadu_pd(&values[369]);
c37_12 = _mm256_add_pd(c37_12, _mm256_mul_pd(a37_12, b37));
_mm256_storeu_pd(&C[(i*56)+12], c37_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a37_12 = _mm_loadu_pd(&values[369]);
c37_12 = _mm_add_pd(c37_12, _mm_mul_pd(a37_12, b37));
_mm_storeu_pd(&C[(i*56)+12], c37_12);
__m128d c37_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a37_14 = _mm_loadu_pd(&values[371]);
c37_14 = _mm_add_pd(c37_14, _mm_mul_pd(a37_14, b37));
_mm_storeu_pd(&C[(i*56)+14], c37_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a37_16 = _mm256_loadu_pd(&values[373]);
c37_16 = _mm256_add_pd(c37_16, _mm256_mul_pd(a37_16, b37));
_mm256_storeu_pd(&C[(i*56)+16], c37_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a37_16 = _mm_loadu_pd(&values[373]);
c37_16 = _mm_add_pd(c37_16, _mm_mul_pd(a37_16, b37));
_mm_storeu_pd(&C[(i*56)+16], c37_16);
__m128d c37_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a37_18 = _mm_loadu_pd(&values[375]);
c37_18 = _mm_add_pd(c37_18, _mm_mul_pd(a37_18, b37));
_mm_storeu_pd(&C[(i*56)+18], c37_18);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_20 = _mm256_loadu_pd(&C[(i*56)+21]);
__m256d a37_20 = _mm256_loadu_pd(&values[377]);
c37_20 = _mm256_add_pd(c37_20, _mm256_mul_pd(a37_20, b37));
_mm256_storeu_pd(&C[(i*56)+21], c37_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_20 = _mm_loadu_pd(&C[(i*56)+21]);
__m128d a37_20 = _mm_loadu_pd(&values[377]);
c37_20 = _mm_add_pd(c37_20, _mm_mul_pd(a37_20, b37));
_mm_storeu_pd(&C[(i*56)+21], c37_20);
__m128d c37_22 = _mm_loadu_pd(&C[(i*56)+23]);
__m128d a37_22 = _mm_loadu_pd(&values[379]);
c37_22 = _mm_add_pd(c37_22, _mm_mul_pd(a37_22, b37));
_mm_storeu_pd(&C[(i*56)+23], c37_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_24 = _mm256_loadu_pd(&C[(i*56)+25]);
__m256d a37_24 = _mm256_loadu_pd(&values[381]);
c37_24 = _mm256_add_pd(c37_24, _mm256_mul_pd(a37_24, b37));
_mm256_storeu_pd(&C[(i*56)+25], c37_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_24 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a37_24 = _mm_loadu_pd(&values[381]);
c37_24 = _mm_add_pd(c37_24, _mm_mul_pd(a37_24, b37));
_mm_storeu_pd(&C[(i*56)+25], c37_24);
__m128d c37_26 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a37_26 = _mm_loadu_pd(&values[383]);
c37_26 = _mm_add_pd(c37_26, _mm_mul_pd(a37_26, b37));
_mm_storeu_pd(&C[(i*56)+27], c37_26);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c37_28 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a37_28 = _mm256_loadu_pd(&values[385]);
c37_28 = _mm256_add_pd(c37_28, _mm256_mul_pd(a37_28, b37));
_mm256_storeu_pd(&C[(i*56)+29], c37_28);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c37_28 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a37_28 = _mm_loadu_pd(&values[385]);
c37_28 = _mm_add_pd(c37_28, _mm_mul_pd(a37_28, b37));
_mm_storeu_pd(&C[(i*56)+29], c37_28);
__m128d c37_30 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a37_30 = _mm_loadu_pd(&values[387]);
c37_30 = _mm_add_pd(c37_30, _mm_mul_pd(a37_30, b37));
_mm_storeu_pd(&C[(i*56)+31], c37_30);
#endif
__m128d c37_32 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a37_32 = _mm_loadu_pd(&values[389]);
#if defined(__SSE3__) && defined(__AVX256__)
c37_32 = _mm_add_pd(c37_32, _mm_mul_pd(a37_32, _mm256_castpd256_pd128(b37)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c37_32 = _mm_add_pd(c37_32, _mm_mul_pd(a37_32, b37));
#endif
_mm_storeu_pd(&C[(i*56)+33], c37_32);
#else
C[(i*56)+0] += values[357] * B[(i*56)+37];
C[(i*56)+1] += values[358] * B[(i*56)+37];
C[(i*56)+2] += values[359] * B[(i*56)+37];
C[(i*56)+3] += values[360] * B[(i*56)+37];
C[(i*56)+4] += values[361] * B[(i*56)+37];
C[(i*56)+5] += values[362] * B[(i*56)+37];
C[(i*56)+6] += values[363] * B[(i*56)+37];
C[(i*56)+7] += values[364] * B[(i*56)+37];
C[(i*56)+8] += values[365] * B[(i*56)+37];
C[(i*56)+9] += values[366] * B[(i*56)+37];
C[(i*56)+10] += values[367] * B[(i*56)+37];
C[(i*56)+11] += values[368] * B[(i*56)+37];
C[(i*56)+12] += values[369] * B[(i*56)+37];
C[(i*56)+13] += values[370] * B[(i*56)+37];
C[(i*56)+14] += values[371] * B[(i*56)+37];
C[(i*56)+15] += values[372] * B[(i*56)+37];
C[(i*56)+16] += values[373] * B[(i*56)+37];
C[(i*56)+17] += values[374] * B[(i*56)+37];
C[(i*56)+18] += values[375] * B[(i*56)+37];
C[(i*56)+19] += values[376] * B[(i*56)+37];
C[(i*56)+21] += values[377] * B[(i*56)+37];
C[(i*56)+22] += values[378] * B[(i*56)+37];
C[(i*56)+23] += values[379] * B[(i*56)+37];
C[(i*56)+24] += values[380] * B[(i*56)+37];
C[(i*56)+25] += values[381] * B[(i*56)+37];
C[(i*56)+26] += values[382] * B[(i*56)+37];
C[(i*56)+27] += values[383] * B[(i*56)+37];
C[(i*56)+28] += values[384] * B[(i*56)+37];
C[(i*56)+29] += values[385] * B[(i*56)+37];
C[(i*56)+30] += values[386] * B[(i*56)+37];
C[(i*56)+31] += values[387] * B[(i*56)+37];
C[(i*56)+32] += values[388] * B[(i*56)+37];
C[(i*56)+33] += values[389] * B[(i*56)+37];
C[(i*56)+34] += values[390] * B[(i*56)+37];
#endif
#ifndef NDEBUG
num_flops += 68;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b38 = _mm256_broadcast_sd(&B[(i*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b38 = _mm_loaddup_pd(&B[(i*56)+38]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a38_0 = _mm256_loadu_pd(&values[391]);
c38_0 = _mm256_add_pd(c38_0, _mm256_mul_pd(a38_0, b38));
_mm256_storeu_pd(&C[(i*56)+0], c38_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a38_0 = _mm_loadu_pd(&values[391]);
c38_0 = _mm_add_pd(c38_0, _mm_mul_pd(a38_0, b38));
_mm_storeu_pd(&C[(i*56)+0], c38_0);
__m128d c38_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a38_2 = _mm_loadu_pd(&values[393]);
c38_2 = _mm_add_pd(c38_2, _mm_mul_pd(a38_2, b38));
_mm_storeu_pd(&C[(i*56)+2], c38_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a38_4 = _mm256_loadu_pd(&values[395]);
c38_4 = _mm256_add_pd(c38_4, _mm256_mul_pd(a38_4, b38));
_mm256_storeu_pd(&C[(i*56)+4], c38_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a38_4 = _mm_loadu_pd(&values[395]);
c38_4 = _mm_add_pd(c38_4, _mm_mul_pd(a38_4, b38));
_mm_storeu_pd(&C[(i*56)+4], c38_4);
__m128d c38_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a38_6 = _mm_loadu_pd(&values[397]);
c38_6 = _mm_add_pd(c38_6, _mm_mul_pd(a38_6, b38));
_mm_storeu_pd(&C[(i*56)+6], c38_6);
#endif
__m128d c38_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a38_8 = _mm_loadu_pd(&values[399]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_8 = _mm_add_pd(c38_8, _mm_mul_pd(a38_8, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_8 = _mm_add_pd(c38_8, _mm_mul_pd(a38_8, b38));
#endif
_mm_storeu_pd(&C[(i*56)+8], c38_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_10 = _mm256_loadu_pd(&C[(i*56)+11]);
__m256d a38_10 = _mm256_loadu_pd(&values[401]);
c38_10 = _mm256_add_pd(c38_10, _mm256_mul_pd(a38_10, b38));
_mm256_storeu_pd(&C[(i*56)+11], c38_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_10 = _mm_loadu_pd(&C[(i*56)+11]);
__m128d a38_10 = _mm_loadu_pd(&values[401]);
c38_10 = _mm_add_pd(c38_10, _mm_mul_pd(a38_10, b38));
_mm_storeu_pd(&C[(i*56)+11], c38_10);
__m128d c38_12 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a38_12 = _mm_loadu_pd(&values[403]);
c38_12 = _mm_add_pd(c38_12, _mm_mul_pd(a38_12, b38));
_mm_storeu_pd(&C[(i*56)+13], c38_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_14 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a38_14 = _mm256_loadu_pd(&values[405]);
c38_14 = _mm256_add_pd(c38_14, _mm256_mul_pd(a38_14, b38));
_mm256_storeu_pd(&C[(i*56)+15], c38_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_14 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a38_14 = _mm_loadu_pd(&values[405]);
c38_14 = _mm_add_pd(c38_14, _mm_mul_pd(a38_14, b38));
_mm_storeu_pd(&C[(i*56)+15], c38_14);
__m128d c38_16 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a38_16 = _mm_loadu_pd(&values[407]);
c38_16 = _mm_add_pd(c38_16, _mm_mul_pd(a38_16, b38));
_mm_storeu_pd(&C[(i*56)+17], c38_16);
#endif
__m128d c38_18 = _mm_load_sd(&C[(i*56)+19]);
__m128d a38_18 = _mm_load_sd(&values[409]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_18 = _mm_add_sd(c38_18, _mm_mul_sd(a38_18, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_18 = _mm_add_sd(c38_18, _mm_mul_sd(a38_18, b38));
#endif
_mm_store_sd(&C[(i*56)+19], c38_18);
__m128d c38_19 = _mm_loadu_pd(&C[(i*56)+22]);
__m128d a38_19 = _mm_loadu_pd(&values[410]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_19 = _mm_add_pd(c38_19, _mm_mul_pd(a38_19, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_19 = _mm_add_pd(c38_19, _mm_mul_pd(a38_19, b38));
#endif
_mm_storeu_pd(&C[(i*56)+22], c38_19);
__m128d c38_21 = _mm_load_sd(&C[(i*56)+24]);
__m128d a38_21 = _mm_load_sd(&values[412]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_21 = _mm_add_sd(c38_21, _mm_mul_sd(a38_21, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_21 = _mm_add_sd(c38_21, _mm_mul_sd(a38_21, b38));
#endif
_mm_store_sd(&C[(i*56)+24], c38_21);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_22 = _mm256_loadu_pd(&C[(i*56)+26]);
__m256d a38_22 = _mm256_loadu_pd(&values[413]);
c38_22 = _mm256_add_pd(c38_22, _mm256_mul_pd(a38_22, b38));
_mm256_storeu_pd(&C[(i*56)+26], c38_22);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_22 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a38_22 = _mm_loadu_pd(&values[413]);
c38_22 = _mm_add_pd(c38_22, _mm_mul_pd(a38_22, b38));
_mm_storeu_pd(&C[(i*56)+26], c38_22);
__m128d c38_24 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a38_24 = _mm_loadu_pd(&values[415]);
c38_24 = _mm_add_pd(c38_24, _mm_mul_pd(a38_24, b38));
_mm_storeu_pd(&C[(i*56)+28], c38_24);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c38_26 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a38_26 = _mm256_loadu_pd(&values[417]);
c38_26 = _mm256_add_pd(c38_26, _mm256_mul_pd(a38_26, b38));
_mm256_storeu_pd(&C[(i*56)+30], c38_26);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c38_26 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a38_26 = _mm_loadu_pd(&values[417]);
c38_26 = _mm_add_pd(c38_26, _mm_mul_pd(a38_26, b38));
_mm_storeu_pd(&C[(i*56)+30], c38_26);
__m128d c38_28 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a38_28 = _mm_loadu_pd(&values[419]);
c38_28 = _mm_add_pd(c38_28, _mm_mul_pd(a38_28, b38));
_mm_storeu_pd(&C[(i*56)+32], c38_28);
#endif
__m128d c38_30 = _mm_load_sd(&C[(i*56)+34]);
__m128d a38_30 = _mm_load_sd(&values[421]);
#if defined(__SSE3__) && defined(__AVX256__)
c38_30 = _mm_add_sd(c38_30, _mm_mul_sd(a38_30, _mm256_castpd256_pd128(b38)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c38_30 = _mm_add_sd(c38_30, _mm_mul_sd(a38_30, b38));
#endif
_mm_store_sd(&C[(i*56)+34], c38_30);
#else
C[(i*56)+0] += values[391] * B[(i*56)+38];
C[(i*56)+1] += values[392] * B[(i*56)+38];
C[(i*56)+2] += values[393] * B[(i*56)+38];
C[(i*56)+3] += values[394] * B[(i*56)+38];
C[(i*56)+4] += values[395] * B[(i*56)+38];
C[(i*56)+5] += values[396] * B[(i*56)+38];
C[(i*56)+6] += values[397] * B[(i*56)+38];
C[(i*56)+7] += values[398] * B[(i*56)+38];
C[(i*56)+8] += values[399] * B[(i*56)+38];
C[(i*56)+9] += values[400] * B[(i*56)+38];
C[(i*56)+11] += values[401] * B[(i*56)+38];
C[(i*56)+12] += values[402] * B[(i*56)+38];
C[(i*56)+13] += values[403] * B[(i*56)+38];
C[(i*56)+14] += values[404] * B[(i*56)+38];
C[(i*56)+15] += values[405] * B[(i*56)+38];
C[(i*56)+16] += values[406] * B[(i*56)+38];
C[(i*56)+17] += values[407] * B[(i*56)+38];
C[(i*56)+18] += values[408] * B[(i*56)+38];
C[(i*56)+19] += values[409] * B[(i*56)+38];
C[(i*56)+22] += values[410] * B[(i*56)+38];
C[(i*56)+23] += values[411] * B[(i*56)+38];
C[(i*56)+24] += values[412] * B[(i*56)+38];
C[(i*56)+26] += values[413] * B[(i*56)+38];
C[(i*56)+27] += values[414] * B[(i*56)+38];
C[(i*56)+28] += values[415] * B[(i*56)+38];
C[(i*56)+29] += values[416] * B[(i*56)+38];
C[(i*56)+30] += values[417] * B[(i*56)+38];
C[(i*56)+31] += values[418] * B[(i*56)+38];
C[(i*56)+32] += values[419] * B[(i*56)+38];
C[(i*56)+33] += values[420] * B[(i*56)+38];
C[(i*56)+34] += values[421] * B[(i*56)+38];
#endif
#ifndef NDEBUG
num_flops += 62;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b39 = _mm256_broadcast_sd(&B[(i*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b39 = _mm_loaddup_pd(&B[(i*56)+39]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a39_0 = _mm256_loadu_pd(&values[422]);
c39_0 = _mm256_add_pd(c39_0, _mm256_mul_pd(a39_0, b39));
_mm256_storeu_pd(&C[(i*56)+0], c39_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a39_0 = _mm_loadu_pd(&values[422]);
c39_0 = _mm_add_pd(c39_0, _mm_mul_pd(a39_0, b39));
_mm_storeu_pd(&C[(i*56)+0], c39_0);
__m128d c39_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a39_2 = _mm_loadu_pd(&values[424]);
c39_2 = _mm_add_pd(c39_2, _mm_mul_pd(a39_2, b39));
_mm_storeu_pd(&C[(i*56)+2], c39_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a39_4 = _mm256_loadu_pd(&values[426]);
c39_4 = _mm256_add_pd(c39_4, _mm256_mul_pd(a39_4, b39));
_mm256_storeu_pd(&C[(i*56)+5], c39_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a39_4 = _mm_loadu_pd(&values[426]);
c39_4 = _mm_add_pd(c39_4, _mm_mul_pd(a39_4, b39));
_mm_storeu_pd(&C[(i*56)+5], c39_4);
__m128d c39_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a39_6 = _mm_loadu_pd(&values[428]);
c39_6 = _mm_add_pd(c39_6, _mm_mul_pd(a39_6, b39));
_mm_storeu_pd(&C[(i*56)+7], c39_6);
#endif
__m128d c39_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a39_8 = _mm_load_sd(&values[430]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_8 = _mm_add_sd(c39_8, _mm_mul_sd(a39_8, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_8 = _mm_add_sd(c39_8, _mm_mul_sd(a39_8, b39));
#endif
_mm_store_sd(&C[(i*56)+9], c39_8);
__m128d c39_9 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a39_9 = _mm_loadu_pd(&values[431]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_9 = _mm_add_pd(c39_9, _mm_mul_pd(a39_9, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_9 = _mm_add_pd(c39_9, _mm_mul_pd(a39_9, b39));
#endif
_mm_storeu_pd(&C[(i*56)+12], c39_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_11 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a39_11 = _mm256_loadu_pd(&values[433]);
c39_11 = _mm256_add_pd(c39_11, _mm256_mul_pd(a39_11, b39));
_mm256_storeu_pd(&C[(i*56)+15], c39_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_11 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a39_11 = _mm_loadu_pd(&values[433]);
c39_11 = _mm_add_pd(c39_11, _mm_mul_pd(a39_11, b39));
_mm_storeu_pd(&C[(i*56)+15], c39_11);
__m128d c39_13 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a39_13 = _mm_loadu_pd(&values[435]);
c39_13 = _mm_add_pd(c39_13, _mm_mul_pd(a39_13, b39));
_mm_storeu_pd(&C[(i*56)+17], c39_13);
#endif
__m128d c39_15 = _mm_load_sd(&C[(i*56)+19]);
__m128d a39_15 = _mm_load_sd(&values[437]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_15 = _mm_add_sd(c39_15, _mm_mul_sd(a39_15, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_15 = _mm_add_sd(c39_15, _mm_mul_sd(a39_15, b39));
#endif
_mm_store_sd(&C[(i*56)+19], c39_15);
__m128d c39_16 = _mm_loadu_pd(&C[(i*56)+23]);
__m128d a39_16 = _mm_loadu_pd(&values[438]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_16 = _mm_add_pd(c39_16, _mm_mul_pd(a39_16, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_16 = _mm_add_pd(c39_16, _mm_mul_pd(a39_16, b39));
#endif
_mm_storeu_pd(&C[(i*56)+23], c39_16);
__m128d c39_18 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a39_18 = _mm_loadu_pd(&values[440]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_18 = _mm_add_pd(c39_18, _mm_mul_pd(a39_18, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_18 = _mm_add_pd(c39_18, _mm_mul_pd(a39_18, b39));
#endif
_mm_storeu_pd(&C[(i*56)+27], c39_18);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c39_20 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a39_20 = _mm256_loadu_pd(&values[442]);
c39_20 = _mm256_add_pd(c39_20, _mm256_mul_pd(a39_20, b39));
_mm256_storeu_pd(&C[(i*56)+30], c39_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c39_20 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a39_20 = _mm_loadu_pd(&values[442]);
c39_20 = _mm_add_pd(c39_20, _mm_mul_pd(a39_20, b39));
_mm_storeu_pd(&C[(i*56)+30], c39_20);
__m128d c39_22 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a39_22 = _mm_loadu_pd(&values[444]);
c39_22 = _mm_add_pd(c39_22, _mm_mul_pd(a39_22, b39));
_mm_storeu_pd(&C[(i*56)+32], c39_22);
#endif
__m128d c39_24 = _mm_load_sd(&C[(i*56)+34]);
__m128d a39_24 = _mm_load_sd(&values[446]);
#if defined(__SSE3__) && defined(__AVX256__)
c39_24 = _mm_add_sd(c39_24, _mm_mul_sd(a39_24, _mm256_castpd256_pd128(b39)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c39_24 = _mm_add_sd(c39_24, _mm_mul_sd(a39_24, b39));
#endif
_mm_store_sd(&C[(i*56)+34], c39_24);
#else
C[(i*56)+0] += values[422] * B[(i*56)+39];
C[(i*56)+1] += values[423] * B[(i*56)+39];
C[(i*56)+2] += values[424] * B[(i*56)+39];
C[(i*56)+3] += values[425] * B[(i*56)+39];
C[(i*56)+5] += values[426] * B[(i*56)+39];
C[(i*56)+6] += values[427] * B[(i*56)+39];
C[(i*56)+7] += values[428] * B[(i*56)+39];
C[(i*56)+8] += values[429] * B[(i*56)+39];
C[(i*56)+9] += values[430] * B[(i*56)+39];
C[(i*56)+12] += values[431] * B[(i*56)+39];
C[(i*56)+13] += values[432] * B[(i*56)+39];
C[(i*56)+15] += values[433] * B[(i*56)+39];
C[(i*56)+16] += values[434] * B[(i*56)+39];
C[(i*56)+17] += values[435] * B[(i*56)+39];
C[(i*56)+18] += values[436] * B[(i*56)+39];
C[(i*56)+19] += values[437] * B[(i*56)+39];
C[(i*56)+23] += values[438] * B[(i*56)+39];
C[(i*56)+24] += values[439] * B[(i*56)+39];
C[(i*56)+27] += values[440] * B[(i*56)+39];
C[(i*56)+28] += values[441] * B[(i*56)+39];
C[(i*56)+30] += values[442] * B[(i*56)+39];
C[(i*56)+31] += values[443] * B[(i*56)+39];
C[(i*56)+32] += values[444] * B[(i*56)+39];
C[(i*56)+33] += values[445] * B[(i*56)+39];
C[(i*56)+34] += values[446] * B[(i*56)+39];
#endif
#ifndef NDEBUG
num_flops += 50;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b40 = _mm256_broadcast_sd(&B[(i*56)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b40 = _mm_loaddup_pd(&B[(i*56)+40]);
#endif
__m128d c40_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a40_0 = _mm_load_sd(&values[447]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_0 = _mm_add_sd(c40_0, _mm_mul_sd(a40_0, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_0 = _mm_add_sd(c40_0, _mm_mul_sd(a40_0, b40));
#endif
_mm_store_sd(&C[(i*56)+0], c40_0);
__m128d c40_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a40_1 = _mm_loadu_pd(&values[448]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_1 = _mm_add_pd(c40_1, _mm_mul_pd(a40_1, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_1 = _mm_add_pd(c40_1, _mm_mul_pd(a40_1, b40));
#endif
_mm_storeu_pd(&C[(i*56)+2], c40_1);
__m128d c40_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a40_3 = _mm_load_sd(&values[450]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_3 = _mm_add_sd(c40_3, _mm_mul_sd(a40_3, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_3 = _mm_add_sd(c40_3, _mm_mul_sd(a40_3, b40));
#endif
_mm_store_sd(&C[(i*56)+6], c40_3);
__m128d c40_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a40_4 = _mm_loadu_pd(&values[451]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_4 = _mm_add_pd(c40_4, _mm_mul_pd(a40_4, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_4 = _mm_add_pd(c40_4, _mm_mul_pd(a40_4, b40));
#endif
_mm_storeu_pd(&C[(i*56)+8], c40_4);
__m128d c40_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a40_6 = _mm_load_sd(&values[453]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_6 = _mm_add_sd(c40_6, _mm_mul_sd(a40_6, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_6 = _mm_add_sd(c40_6, _mm_mul_sd(a40_6, b40));
#endif
_mm_store_sd(&C[(i*56)+13], c40_6);
__m128d c40_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a40_7 = _mm_load_sd(&values[454]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_7 = _mm_add_sd(c40_7, _mm_mul_sd(a40_7, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_7 = _mm_add_sd(c40_7, _mm_mul_sd(a40_7, b40));
#endif
_mm_store_sd(&C[(i*56)+16], c40_7);
__m128d c40_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a40_8 = _mm_loadu_pd(&values[455]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_8 = _mm_add_pd(c40_8, _mm_mul_pd(a40_8, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_8 = _mm_add_pd(c40_8, _mm_mul_pd(a40_8, b40));
#endif
_mm_storeu_pd(&C[(i*56)+18], c40_8);
__m128d c40_10 = _mm_load_sd(&C[(i*56)+24]);
__m128d a40_10 = _mm_load_sd(&values[457]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_10 = _mm_add_sd(c40_10, _mm_mul_sd(a40_10, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_10 = _mm_add_sd(c40_10, _mm_mul_sd(a40_10, b40));
#endif
_mm_store_sd(&C[(i*56)+24], c40_10);
__m128d c40_11 = _mm_load_sd(&C[(i*56)+28]);
__m128d a40_11 = _mm_load_sd(&values[458]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_11 = _mm_add_sd(c40_11, _mm_mul_sd(a40_11, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_11 = _mm_add_sd(c40_11, _mm_mul_sd(a40_11, b40));
#endif
_mm_store_sd(&C[(i*56)+28], c40_11);
__m128d c40_12 = _mm_load_sd(&C[(i*56)+31]);
__m128d a40_12 = _mm_load_sd(&values[459]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_12 = _mm_add_sd(c40_12, _mm_mul_sd(a40_12, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_12 = _mm_add_sd(c40_12, _mm_mul_sd(a40_12, b40));
#endif
_mm_store_sd(&C[(i*56)+31], c40_12);
__m128d c40_13 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a40_13 = _mm_loadu_pd(&values[460]);
#if defined(__SSE3__) && defined(__AVX256__)
c40_13 = _mm_add_pd(c40_13, _mm_mul_pd(a40_13, _mm256_castpd256_pd128(b40)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c40_13 = _mm_add_pd(c40_13, _mm_mul_pd(a40_13, b40));
#endif
_mm_storeu_pd(&C[(i*56)+33], c40_13);
#else
C[(i*56)+0] += values[447] * B[(i*56)+40];
C[(i*56)+2] += values[448] * B[(i*56)+40];
C[(i*56)+3] += values[449] * B[(i*56)+40];
C[(i*56)+6] += values[450] * B[(i*56)+40];
C[(i*56)+8] += values[451] * B[(i*56)+40];
C[(i*56)+9] += values[452] * B[(i*56)+40];
C[(i*56)+13] += values[453] * B[(i*56)+40];
C[(i*56)+16] += values[454] * B[(i*56)+40];
C[(i*56)+18] += values[455] * B[(i*56)+40];
C[(i*56)+19] += values[456] * B[(i*56)+40];
C[(i*56)+24] += values[457] * B[(i*56)+40];
C[(i*56)+28] += values[458] * B[(i*56)+40];
C[(i*56)+31] += values[459] * B[(i*56)+40];
C[(i*56)+33] += values[460] * B[(i*56)+40];
C[(i*56)+34] += values[461] * B[(i*56)+40];
#endif
#ifndef NDEBUG
num_flops += 30;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b41 = _mm256_broadcast_sd(&B[(i*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b41 = _mm_loaddup_pd(&B[(i*56)+41]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a41_0 = _mm256_loadu_pd(&values[462]);
c41_0 = _mm256_add_pd(c41_0, _mm256_mul_pd(a41_0, b41));
_mm256_storeu_pd(&C[(i*56)+0], c41_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a41_0 = _mm_loadu_pd(&values[462]);
c41_0 = _mm_add_pd(c41_0, _mm_mul_pd(a41_0, b41));
_mm_storeu_pd(&C[(i*56)+0], c41_0);
__m128d c41_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a41_2 = _mm_loadu_pd(&values[464]);
c41_2 = _mm_add_pd(c41_2, _mm_mul_pd(a41_2, b41));
_mm_storeu_pd(&C[(i*56)+2], c41_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a41_4 = _mm256_loadu_pd(&values[466]);
c41_4 = _mm256_add_pd(c41_4, _mm256_mul_pd(a41_4, b41));
_mm256_storeu_pd(&C[(i*56)+4], c41_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a41_4 = _mm_loadu_pd(&values[466]);
c41_4 = _mm_add_pd(c41_4, _mm_mul_pd(a41_4, b41));
_mm_storeu_pd(&C[(i*56)+4], c41_4);
__m128d c41_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a41_6 = _mm_loadu_pd(&values[468]);
c41_6 = _mm_add_pd(c41_6, _mm_mul_pd(a41_6, b41));
_mm_storeu_pd(&C[(i*56)+6], c41_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a41_8 = _mm256_loadu_pd(&values[470]);
c41_8 = _mm256_add_pd(c41_8, _mm256_mul_pd(a41_8, b41));
_mm256_storeu_pd(&C[(i*56)+8], c41_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a41_8 = _mm_loadu_pd(&values[470]);
c41_8 = _mm_add_pd(c41_8, _mm_mul_pd(a41_8, b41));
_mm_storeu_pd(&C[(i*56)+8], c41_8);
__m128d c41_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a41_10 = _mm_loadu_pd(&values[472]);
c41_10 = _mm_add_pd(c41_10, _mm_mul_pd(a41_10, b41));
_mm_storeu_pd(&C[(i*56)+10], c41_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a41_12 = _mm256_loadu_pd(&values[474]);
c41_12 = _mm256_add_pd(c41_12, _mm256_mul_pd(a41_12, b41));
_mm256_storeu_pd(&C[(i*56)+12], c41_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a41_12 = _mm_loadu_pd(&values[474]);
c41_12 = _mm_add_pd(c41_12, _mm_mul_pd(a41_12, b41));
_mm_storeu_pd(&C[(i*56)+12], c41_12);
__m128d c41_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a41_14 = _mm_loadu_pd(&values[476]);
c41_14 = _mm_add_pd(c41_14, _mm_mul_pd(a41_14, b41));
_mm_storeu_pd(&C[(i*56)+14], c41_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a41_16 = _mm256_loadu_pd(&values[478]);
c41_16 = _mm256_add_pd(c41_16, _mm256_mul_pd(a41_16, b41));
_mm256_storeu_pd(&C[(i*56)+16], c41_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a41_16 = _mm_loadu_pd(&values[478]);
c41_16 = _mm_add_pd(c41_16, _mm_mul_pd(a41_16, b41));
_mm_storeu_pd(&C[(i*56)+16], c41_16);
__m128d c41_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a41_18 = _mm_loadu_pd(&values[480]);
c41_18 = _mm_add_pd(c41_18, _mm_mul_pd(a41_18, b41));
_mm_storeu_pd(&C[(i*56)+18], c41_18);
#endif
__m128d c41_20 = _mm_load_sd(&C[(i*56)+20]);
__m128d a41_20 = _mm_load_sd(&values[482]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_20 = _mm_add_sd(c41_20, _mm_mul_sd(a41_20, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_20 = _mm_add_sd(c41_20, _mm_mul_sd(a41_20, b41));
#endif
_mm_store_sd(&C[(i*56)+20], c41_20);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_21 = _mm256_loadu_pd(&C[(i*56)+25]);
__m256d a41_21 = _mm256_loadu_pd(&values[483]);
c41_21 = _mm256_add_pd(c41_21, _mm256_mul_pd(a41_21, b41));
_mm256_storeu_pd(&C[(i*56)+25], c41_21);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_21 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a41_21 = _mm_loadu_pd(&values[483]);
c41_21 = _mm_add_pd(c41_21, _mm_mul_pd(a41_21, b41));
_mm_storeu_pd(&C[(i*56)+25], c41_21);
__m128d c41_23 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a41_23 = _mm_loadu_pd(&values[485]);
c41_23 = _mm_add_pd(c41_23, _mm_mul_pd(a41_23, b41));
_mm_storeu_pd(&C[(i*56)+27], c41_23);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c41_25 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a41_25 = _mm256_loadu_pd(&values[487]);
c41_25 = _mm256_add_pd(c41_25, _mm256_mul_pd(a41_25, b41));
_mm256_storeu_pd(&C[(i*56)+29], c41_25);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c41_25 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a41_25 = _mm_loadu_pd(&values[487]);
c41_25 = _mm_add_pd(c41_25, _mm_mul_pd(a41_25, b41));
_mm_storeu_pd(&C[(i*56)+29], c41_25);
__m128d c41_27 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a41_27 = _mm_loadu_pd(&values[489]);
c41_27 = _mm_add_pd(c41_27, _mm_mul_pd(a41_27, b41));
_mm_storeu_pd(&C[(i*56)+31], c41_27);
#endif
__m128d c41_29 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a41_29 = _mm_loadu_pd(&values[491]);
#if defined(__SSE3__) && defined(__AVX256__)
c41_29 = _mm_add_pd(c41_29, _mm_mul_pd(a41_29, _mm256_castpd256_pd128(b41)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c41_29 = _mm_add_pd(c41_29, _mm_mul_pd(a41_29, b41));
#endif
_mm_storeu_pd(&C[(i*56)+33], c41_29);
#else
C[(i*56)+0] += values[462] * B[(i*56)+41];
C[(i*56)+1] += values[463] * B[(i*56)+41];
C[(i*56)+2] += values[464] * B[(i*56)+41];
C[(i*56)+3] += values[465] * B[(i*56)+41];
C[(i*56)+4] += values[466] * B[(i*56)+41];
C[(i*56)+5] += values[467] * B[(i*56)+41];
C[(i*56)+6] += values[468] * B[(i*56)+41];
C[(i*56)+7] += values[469] * B[(i*56)+41];
C[(i*56)+8] += values[470] * B[(i*56)+41];
C[(i*56)+9] += values[471] * B[(i*56)+41];
C[(i*56)+10] += values[472] * B[(i*56)+41];
C[(i*56)+11] += values[473] * B[(i*56)+41];
C[(i*56)+12] += values[474] * B[(i*56)+41];
C[(i*56)+13] += values[475] * B[(i*56)+41];
C[(i*56)+14] += values[476] * B[(i*56)+41];
C[(i*56)+15] += values[477] * B[(i*56)+41];
C[(i*56)+16] += values[478] * B[(i*56)+41];
C[(i*56)+17] += values[479] * B[(i*56)+41];
C[(i*56)+18] += values[480] * B[(i*56)+41];
C[(i*56)+19] += values[481] * B[(i*56)+41];
C[(i*56)+20] += values[482] * B[(i*56)+41];
C[(i*56)+25] += values[483] * B[(i*56)+41];
C[(i*56)+26] += values[484] * B[(i*56)+41];
C[(i*56)+27] += values[485] * B[(i*56)+41];
C[(i*56)+28] += values[486] * B[(i*56)+41];
C[(i*56)+29] += values[487] * B[(i*56)+41];
C[(i*56)+30] += values[488] * B[(i*56)+41];
C[(i*56)+31] += values[489] * B[(i*56)+41];
C[(i*56)+32] += values[490] * B[(i*56)+41];
C[(i*56)+33] += values[491] * B[(i*56)+41];
C[(i*56)+34] += values[492] * B[(i*56)+41];
#endif
#ifndef NDEBUG
num_flops += 62;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b42 = _mm256_broadcast_sd(&B[(i*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b42 = _mm_loaddup_pd(&B[(i*56)+42]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a42_0 = _mm256_loadu_pd(&values[493]);
c42_0 = _mm256_add_pd(c42_0, _mm256_mul_pd(a42_0, b42));
_mm256_storeu_pd(&C[(i*56)+0], c42_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a42_0 = _mm_loadu_pd(&values[493]);
c42_0 = _mm_add_pd(c42_0, _mm_mul_pd(a42_0, b42));
_mm_storeu_pd(&C[(i*56)+0], c42_0);
__m128d c42_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a42_2 = _mm_loadu_pd(&values[495]);
c42_2 = _mm_add_pd(c42_2, _mm_mul_pd(a42_2, b42));
_mm_storeu_pd(&C[(i*56)+2], c42_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a42_4 = _mm256_loadu_pd(&values[497]);
c42_4 = _mm256_add_pd(c42_4, _mm256_mul_pd(a42_4, b42));
_mm256_storeu_pd(&C[(i*56)+4], c42_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a42_4 = _mm_loadu_pd(&values[497]);
c42_4 = _mm_add_pd(c42_4, _mm_mul_pd(a42_4, b42));
_mm_storeu_pd(&C[(i*56)+4], c42_4);
__m128d c42_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a42_6 = _mm_loadu_pd(&values[499]);
c42_6 = _mm_add_pd(c42_6, _mm_mul_pd(a42_6, b42));
_mm_storeu_pd(&C[(i*56)+6], c42_6);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_8 = _mm256_loadu_pd(&C[(i*56)+8]);
__m256d a42_8 = _mm256_loadu_pd(&values[501]);
c42_8 = _mm256_add_pd(c42_8, _mm256_mul_pd(a42_8, b42));
_mm256_storeu_pd(&C[(i*56)+8], c42_8);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a42_8 = _mm_loadu_pd(&values[501]);
c42_8 = _mm_add_pd(c42_8, _mm_mul_pd(a42_8, b42));
_mm_storeu_pd(&C[(i*56)+8], c42_8);
__m128d c42_10 = _mm_loadu_pd(&C[(i*56)+10]);
__m128d a42_10 = _mm_loadu_pd(&values[503]);
c42_10 = _mm_add_pd(c42_10, _mm_mul_pd(a42_10, b42));
_mm_storeu_pd(&C[(i*56)+10], c42_10);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_12 = _mm256_loadu_pd(&C[(i*56)+12]);
__m256d a42_12 = _mm256_loadu_pd(&values[505]);
c42_12 = _mm256_add_pd(c42_12, _mm256_mul_pd(a42_12, b42));
_mm256_storeu_pd(&C[(i*56)+12], c42_12);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_12 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a42_12 = _mm_loadu_pd(&values[505]);
c42_12 = _mm_add_pd(c42_12, _mm_mul_pd(a42_12, b42));
_mm_storeu_pd(&C[(i*56)+12], c42_12);
__m128d c42_14 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a42_14 = _mm_loadu_pd(&values[507]);
c42_14 = _mm_add_pd(c42_14, _mm_mul_pd(a42_14, b42));
_mm_storeu_pd(&C[(i*56)+14], c42_14);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_16 = _mm256_loadu_pd(&C[(i*56)+16]);
__m256d a42_16 = _mm256_loadu_pd(&values[509]);
c42_16 = _mm256_add_pd(c42_16, _mm256_mul_pd(a42_16, b42));
_mm256_storeu_pd(&C[(i*56)+16], c42_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_16 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a42_16 = _mm_loadu_pd(&values[509]);
c42_16 = _mm_add_pd(c42_16, _mm_mul_pd(a42_16, b42));
_mm_storeu_pd(&C[(i*56)+16], c42_16);
__m128d c42_18 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a42_18 = _mm_loadu_pd(&values[511]);
c42_18 = _mm_add_pd(c42_18, _mm_mul_pd(a42_18, b42));
_mm_storeu_pd(&C[(i*56)+18], c42_18);
#endif
__m128d c42_20 = _mm_load_sd(&C[(i*56)+21]);
__m128d a42_20 = _mm_load_sd(&values[513]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_20 = _mm_add_sd(c42_20, _mm_mul_sd(a42_20, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_20 = _mm_add_sd(c42_20, _mm_mul_sd(a42_20, b42));
#endif
_mm_store_sd(&C[(i*56)+21], c42_20);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_21 = _mm256_loadu_pd(&C[(i*56)+25]);
__m256d a42_21 = _mm256_loadu_pd(&values[514]);
c42_21 = _mm256_add_pd(c42_21, _mm256_mul_pd(a42_21, b42));
_mm256_storeu_pd(&C[(i*56)+25], c42_21);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_21 = _mm_loadu_pd(&C[(i*56)+25]);
__m128d a42_21 = _mm_loadu_pd(&values[514]);
c42_21 = _mm_add_pd(c42_21, _mm_mul_pd(a42_21, b42));
_mm_storeu_pd(&C[(i*56)+25], c42_21);
__m128d c42_23 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a42_23 = _mm_loadu_pd(&values[516]);
c42_23 = _mm_add_pd(c42_23, _mm_mul_pd(a42_23, b42));
_mm_storeu_pd(&C[(i*56)+27], c42_23);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c42_25 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a42_25 = _mm256_loadu_pd(&values[518]);
c42_25 = _mm256_add_pd(c42_25, _mm256_mul_pd(a42_25, b42));
_mm256_storeu_pd(&C[(i*56)+29], c42_25);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c42_25 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a42_25 = _mm_loadu_pd(&values[518]);
c42_25 = _mm_add_pd(c42_25, _mm_mul_pd(a42_25, b42));
_mm_storeu_pd(&C[(i*56)+29], c42_25);
__m128d c42_27 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a42_27 = _mm_loadu_pd(&values[520]);
c42_27 = _mm_add_pd(c42_27, _mm_mul_pd(a42_27, b42));
_mm_storeu_pd(&C[(i*56)+31], c42_27);
#endif
__m128d c42_29 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a42_29 = _mm_loadu_pd(&values[522]);
#if defined(__SSE3__) && defined(__AVX256__)
c42_29 = _mm_add_pd(c42_29, _mm_mul_pd(a42_29, _mm256_castpd256_pd128(b42)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c42_29 = _mm_add_pd(c42_29, _mm_mul_pd(a42_29, b42));
#endif
_mm_storeu_pd(&C[(i*56)+33], c42_29);
#else
C[(i*56)+0] += values[493] * B[(i*56)+42];
C[(i*56)+1] += values[494] * B[(i*56)+42];
C[(i*56)+2] += values[495] * B[(i*56)+42];
C[(i*56)+3] += values[496] * B[(i*56)+42];
C[(i*56)+4] += values[497] * B[(i*56)+42];
C[(i*56)+5] += values[498] * B[(i*56)+42];
C[(i*56)+6] += values[499] * B[(i*56)+42];
C[(i*56)+7] += values[500] * B[(i*56)+42];
C[(i*56)+8] += values[501] * B[(i*56)+42];
C[(i*56)+9] += values[502] * B[(i*56)+42];
C[(i*56)+10] += values[503] * B[(i*56)+42];
C[(i*56)+11] += values[504] * B[(i*56)+42];
C[(i*56)+12] += values[505] * B[(i*56)+42];
C[(i*56)+13] += values[506] * B[(i*56)+42];
C[(i*56)+14] += values[507] * B[(i*56)+42];
C[(i*56)+15] += values[508] * B[(i*56)+42];
C[(i*56)+16] += values[509] * B[(i*56)+42];
C[(i*56)+17] += values[510] * B[(i*56)+42];
C[(i*56)+18] += values[511] * B[(i*56)+42];
C[(i*56)+19] += values[512] * B[(i*56)+42];
C[(i*56)+21] += values[513] * B[(i*56)+42];
C[(i*56)+25] += values[514] * B[(i*56)+42];
C[(i*56)+26] += values[515] * B[(i*56)+42];
C[(i*56)+27] += values[516] * B[(i*56)+42];
C[(i*56)+28] += values[517] * B[(i*56)+42];
C[(i*56)+29] += values[518] * B[(i*56)+42];
C[(i*56)+30] += values[519] * B[(i*56)+42];
C[(i*56)+31] += values[520] * B[(i*56)+42];
C[(i*56)+32] += values[521] * B[(i*56)+42];
C[(i*56)+33] += values[522] * B[(i*56)+42];
C[(i*56)+34] += values[523] * B[(i*56)+42];
#endif
#ifndef NDEBUG
num_flops += 62;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b43 = _mm256_broadcast_sd(&B[(i*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b43 = _mm_loaddup_pd(&B[(i*56)+43]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a43_0 = _mm256_loadu_pd(&values[524]);
c43_0 = _mm256_add_pd(c43_0, _mm256_mul_pd(a43_0, b43));
_mm256_storeu_pd(&C[(i*56)+0], c43_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a43_0 = _mm_loadu_pd(&values[524]);
c43_0 = _mm_add_pd(c43_0, _mm_mul_pd(a43_0, b43));
_mm_storeu_pd(&C[(i*56)+0], c43_0);
__m128d c43_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a43_2 = _mm_loadu_pd(&values[526]);
c43_2 = _mm_add_pd(c43_2, _mm_mul_pd(a43_2, b43));
_mm_storeu_pd(&C[(i*56)+2], c43_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a43_4 = _mm256_loadu_pd(&values[528]);
c43_4 = _mm256_add_pd(c43_4, _mm256_mul_pd(a43_4, b43));
_mm256_storeu_pd(&C[(i*56)+4], c43_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a43_4 = _mm_loadu_pd(&values[528]);
c43_4 = _mm_add_pd(c43_4, _mm_mul_pd(a43_4, b43));
_mm_storeu_pd(&C[(i*56)+4], c43_4);
__m128d c43_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a43_6 = _mm_loadu_pd(&values[530]);
c43_6 = _mm_add_pd(c43_6, _mm_mul_pd(a43_6, b43));
_mm_storeu_pd(&C[(i*56)+6], c43_6);
#endif
__m128d c43_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a43_8 = _mm_loadu_pd(&values[532]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_8 = _mm_add_pd(c43_8, _mm_mul_pd(a43_8, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_8 = _mm_add_pd(c43_8, _mm_mul_pd(a43_8, b43));
#endif
_mm_storeu_pd(&C[(i*56)+8], c43_8);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_10 = _mm256_loadu_pd(&C[(i*56)+11]);
__m256d a43_10 = _mm256_loadu_pd(&values[534]);
c43_10 = _mm256_add_pd(c43_10, _mm256_mul_pd(a43_10, b43));
_mm256_storeu_pd(&C[(i*56)+11], c43_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_10 = _mm_loadu_pd(&C[(i*56)+11]);
__m128d a43_10 = _mm_loadu_pd(&values[534]);
c43_10 = _mm_add_pd(c43_10, _mm_mul_pd(a43_10, b43));
_mm_storeu_pd(&C[(i*56)+11], c43_10);
__m128d c43_12 = _mm_loadu_pd(&C[(i*56)+13]);
__m128d a43_12 = _mm_loadu_pd(&values[536]);
c43_12 = _mm_add_pd(c43_12, _mm_mul_pd(a43_12, b43));
_mm_storeu_pd(&C[(i*56)+13], c43_12);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_14 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a43_14 = _mm256_loadu_pd(&values[538]);
c43_14 = _mm256_add_pd(c43_14, _mm256_mul_pd(a43_14, b43));
_mm256_storeu_pd(&C[(i*56)+15], c43_14);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_14 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a43_14 = _mm_loadu_pd(&values[538]);
c43_14 = _mm_add_pd(c43_14, _mm_mul_pd(a43_14, b43));
_mm_storeu_pd(&C[(i*56)+15], c43_14);
__m128d c43_16 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a43_16 = _mm_loadu_pd(&values[540]);
c43_16 = _mm_add_pd(c43_16, _mm_mul_pd(a43_16, b43));
_mm_storeu_pd(&C[(i*56)+17], c43_16);
#endif
__m128d c43_18 = _mm_load_sd(&C[(i*56)+19]);
__m128d a43_18 = _mm_load_sd(&values[542]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_18 = _mm_add_sd(c43_18, _mm_mul_sd(a43_18, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_18 = _mm_add_sd(c43_18, _mm_mul_sd(a43_18, b43));
#endif
_mm_store_sd(&C[(i*56)+19], c43_18);
__m128d c43_19 = _mm_load_sd(&C[(i*56)+22]);
__m128d a43_19 = _mm_load_sd(&values[543]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_19 = _mm_add_sd(c43_19, _mm_mul_sd(a43_19, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_19 = _mm_add_sd(c43_19, _mm_mul_sd(a43_19, b43));
#endif
_mm_store_sd(&C[(i*56)+22], c43_19);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_20 = _mm256_loadu_pd(&C[(i*56)+26]);
__m256d a43_20 = _mm256_loadu_pd(&values[544]);
c43_20 = _mm256_add_pd(c43_20, _mm256_mul_pd(a43_20, b43));
_mm256_storeu_pd(&C[(i*56)+26], c43_20);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_20 = _mm_loadu_pd(&C[(i*56)+26]);
__m128d a43_20 = _mm_loadu_pd(&values[544]);
c43_20 = _mm_add_pd(c43_20, _mm_mul_pd(a43_20, b43));
_mm_storeu_pd(&C[(i*56)+26], c43_20);
__m128d c43_22 = _mm_loadu_pd(&C[(i*56)+28]);
__m128d a43_22 = _mm_loadu_pd(&values[546]);
c43_22 = _mm_add_pd(c43_22, _mm_mul_pd(a43_22, b43));
_mm_storeu_pd(&C[(i*56)+28], c43_22);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c43_24 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a43_24 = _mm256_loadu_pd(&values[548]);
c43_24 = _mm256_add_pd(c43_24, _mm256_mul_pd(a43_24, b43));
_mm256_storeu_pd(&C[(i*56)+30], c43_24);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c43_24 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a43_24 = _mm_loadu_pd(&values[548]);
c43_24 = _mm_add_pd(c43_24, _mm_mul_pd(a43_24, b43));
_mm_storeu_pd(&C[(i*56)+30], c43_24);
__m128d c43_26 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a43_26 = _mm_loadu_pd(&values[550]);
c43_26 = _mm_add_pd(c43_26, _mm_mul_pd(a43_26, b43));
_mm_storeu_pd(&C[(i*56)+32], c43_26);
#endif
__m128d c43_28 = _mm_load_sd(&C[(i*56)+34]);
__m128d a43_28 = _mm_load_sd(&values[552]);
#if defined(__SSE3__) && defined(__AVX256__)
c43_28 = _mm_add_sd(c43_28, _mm_mul_sd(a43_28, _mm256_castpd256_pd128(b43)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c43_28 = _mm_add_sd(c43_28, _mm_mul_sd(a43_28, b43));
#endif
_mm_store_sd(&C[(i*56)+34], c43_28);
#else
C[(i*56)+0] += values[524] * B[(i*56)+43];
C[(i*56)+1] += values[525] * B[(i*56)+43];
C[(i*56)+2] += values[526] * B[(i*56)+43];
C[(i*56)+3] += values[527] * B[(i*56)+43];
C[(i*56)+4] += values[528] * B[(i*56)+43];
C[(i*56)+5] += values[529] * B[(i*56)+43];
C[(i*56)+6] += values[530] * B[(i*56)+43];
C[(i*56)+7] += values[531] * B[(i*56)+43];
C[(i*56)+8] += values[532] * B[(i*56)+43];
C[(i*56)+9] += values[533] * B[(i*56)+43];
C[(i*56)+11] += values[534] * B[(i*56)+43];
C[(i*56)+12] += values[535] * B[(i*56)+43];
C[(i*56)+13] += values[536] * B[(i*56)+43];
C[(i*56)+14] += values[537] * B[(i*56)+43];
C[(i*56)+15] += values[538] * B[(i*56)+43];
C[(i*56)+16] += values[539] * B[(i*56)+43];
C[(i*56)+17] += values[540] * B[(i*56)+43];
C[(i*56)+18] += values[541] * B[(i*56)+43];
C[(i*56)+19] += values[542] * B[(i*56)+43];
C[(i*56)+22] += values[543] * B[(i*56)+43];
C[(i*56)+26] += values[544] * B[(i*56)+43];
C[(i*56)+27] += values[545] * B[(i*56)+43];
C[(i*56)+28] += values[546] * B[(i*56)+43];
C[(i*56)+29] += values[547] * B[(i*56)+43];
C[(i*56)+30] += values[548] * B[(i*56)+43];
C[(i*56)+31] += values[549] * B[(i*56)+43];
C[(i*56)+32] += values[550] * B[(i*56)+43];
C[(i*56)+33] += values[551] * B[(i*56)+43];
C[(i*56)+34] += values[552] * B[(i*56)+43];
#endif
#ifndef NDEBUG
num_flops += 58;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b44 = _mm256_broadcast_sd(&B[(i*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b44 = _mm_loaddup_pd(&B[(i*56)+44]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a44_0 = _mm256_loadu_pd(&values[553]);
c44_0 = _mm256_add_pd(c44_0, _mm256_mul_pd(a44_0, b44));
_mm256_storeu_pd(&C[(i*56)+0], c44_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a44_0 = _mm_loadu_pd(&values[553]);
c44_0 = _mm_add_pd(c44_0, _mm_mul_pd(a44_0, b44));
_mm_storeu_pd(&C[(i*56)+0], c44_0);
__m128d c44_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a44_2 = _mm_loadu_pd(&values[555]);
c44_2 = _mm_add_pd(c44_2, _mm_mul_pd(a44_2, b44));
_mm_storeu_pd(&C[(i*56)+2], c44_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a44_4 = _mm256_loadu_pd(&values[557]);
c44_4 = _mm256_add_pd(c44_4, _mm256_mul_pd(a44_4, b44));
_mm256_storeu_pd(&C[(i*56)+5], c44_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a44_4 = _mm_loadu_pd(&values[557]);
c44_4 = _mm_add_pd(c44_4, _mm_mul_pd(a44_4, b44));
_mm_storeu_pd(&C[(i*56)+5], c44_4);
__m128d c44_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a44_6 = _mm_loadu_pd(&values[559]);
c44_6 = _mm_add_pd(c44_6, _mm_mul_pd(a44_6, b44));
_mm_storeu_pd(&C[(i*56)+7], c44_6);
#endif
__m128d c44_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a44_8 = _mm_load_sd(&values[561]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_8 = _mm_add_sd(c44_8, _mm_mul_sd(a44_8, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_8 = _mm_add_sd(c44_8, _mm_mul_sd(a44_8, b44));
#endif
_mm_store_sd(&C[(i*56)+9], c44_8);
__m128d c44_9 = _mm_loadu_pd(&C[(i*56)+12]);
__m128d a44_9 = _mm_loadu_pd(&values[562]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_9 = _mm_add_pd(c44_9, _mm_mul_pd(a44_9, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_9 = _mm_add_pd(c44_9, _mm_mul_pd(a44_9, b44));
#endif
_mm_storeu_pd(&C[(i*56)+12], c44_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_11 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a44_11 = _mm256_loadu_pd(&values[564]);
c44_11 = _mm256_add_pd(c44_11, _mm256_mul_pd(a44_11, b44));
_mm256_storeu_pd(&C[(i*56)+15], c44_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_11 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a44_11 = _mm_loadu_pd(&values[564]);
c44_11 = _mm_add_pd(c44_11, _mm_mul_pd(a44_11, b44));
_mm_storeu_pd(&C[(i*56)+15], c44_11);
__m128d c44_13 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a44_13 = _mm_loadu_pd(&values[566]);
c44_13 = _mm_add_pd(c44_13, _mm_mul_pd(a44_13, b44));
_mm_storeu_pd(&C[(i*56)+17], c44_13);
#endif
__m128d c44_15 = _mm_load_sd(&C[(i*56)+19]);
__m128d a44_15 = _mm_load_sd(&values[568]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_15 = _mm_add_sd(c44_15, _mm_mul_sd(a44_15, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_15 = _mm_add_sd(c44_15, _mm_mul_sd(a44_15, b44));
#endif
_mm_store_sd(&C[(i*56)+19], c44_15);
__m128d c44_16 = _mm_load_sd(&C[(i*56)+23]);
__m128d a44_16 = _mm_load_sd(&values[569]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_16 = _mm_add_sd(c44_16, _mm_mul_sd(a44_16, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_16 = _mm_add_sd(c44_16, _mm_mul_sd(a44_16, b44));
#endif
_mm_store_sd(&C[(i*56)+23], c44_16);
__m128d c44_17 = _mm_loadu_pd(&C[(i*56)+27]);
__m128d a44_17 = _mm_loadu_pd(&values[570]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_17 = _mm_add_pd(c44_17, _mm_mul_pd(a44_17, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_17 = _mm_add_pd(c44_17, _mm_mul_pd(a44_17, b44));
#endif
_mm_storeu_pd(&C[(i*56)+27], c44_17);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c44_19 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a44_19 = _mm256_loadu_pd(&values[572]);
c44_19 = _mm256_add_pd(c44_19, _mm256_mul_pd(a44_19, b44));
_mm256_storeu_pd(&C[(i*56)+30], c44_19);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c44_19 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a44_19 = _mm_loadu_pd(&values[572]);
c44_19 = _mm_add_pd(c44_19, _mm_mul_pd(a44_19, b44));
_mm_storeu_pd(&C[(i*56)+30], c44_19);
__m128d c44_21 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a44_21 = _mm_loadu_pd(&values[574]);
c44_21 = _mm_add_pd(c44_21, _mm_mul_pd(a44_21, b44));
_mm_storeu_pd(&C[(i*56)+32], c44_21);
#endif
__m128d c44_23 = _mm_load_sd(&C[(i*56)+34]);
__m128d a44_23 = _mm_load_sd(&values[576]);
#if defined(__SSE3__) && defined(__AVX256__)
c44_23 = _mm_add_sd(c44_23, _mm_mul_sd(a44_23, _mm256_castpd256_pd128(b44)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c44_23 = _mm_add_sd(c44_23, _mm_mul_sd(a44_23, b44));
#endif
_mm_store_sd(&C[(i*56)+34], c44_23);
#else
C[(i*56)+0] += values[553] * B[(i*56)+44];
C[(i*56)+1] += values[554] * B[(i*56)+44];
C[(i*56)+2] += values[555] * B[(i*56)+44];
C[(i*56)+3] += values[556] * B[(i*56)+44];
C[(i*56)+5] += values[557] * B[(i*56)+44];
C[(i*56)+6] += values[558] * B[(i*56)+44];
C[(i*56)+7] += values[559] * B[(i*56)+44];
C[(i*56)+8] += values[560] * B[(i*56)+44];
C[(i*56)+9] += values[561] * B[(i*56)+44];
C[(i*56)+12] += values[562] * B[(i*56)+44];
C[(i*56)+13] += values[563] * B[(i*56)+44];
C[(i*56)+15] += values[564] * B[(i*56)+44];
C[(i*56)+16] += values[565] * B[(i*56)+44];
C[(i*56)+17] += values[566] * B[(i*56)+44];
C[(i*56)+18] += values[567] * B[(i*56)+44];
C[(i*56)+19] += values[568] * B[(i*56)+44];
C[(i*56)+23] += values[569] * B[(i*56)+44];
C[(i*56)+27] += values[570] * B[(i*56)+44];
C[(i*56)+28] += values[571] * B[(i*56)+44];
C[(i*56)+30] += values[572] * B[(i*56)+44];
C[(i*56)+31] += values[573] * B[(i*56)+44];
C[(i*56)+32] += values[574] * B[(i*56)+44];
C[(i*56)+33] += values[575] * B[(i*56)+44];
C[(i*56)+34] += values[576] * B[(i*56)+44];
#endif
#ifndef NDEBUG
num_flops += 48;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b45 = _mm256_broadcast_sd(&B[(i*56)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b45 = _mm_loaddup_pd(&B[(i*56)+45]);
#endif
__m128d c45_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a45_0 = _mm_load_sd(&values[577]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_0 = _mm_add_sd(c45_0, _mm_mul_sd(a45_0, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_0 = _mm_add_sd(c45_0, _mm_mul_sd(a45_0, b45));
#endif
_mm_store_sd(&C[(i*56)+0], c45_0);
__m128d c45_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a45_1 = _mm_loadu_pd(&values[578]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_1 = _mm_add_pd(c45_1, _mm_mul_pd(a45_1, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_1 = _mm_add_pd(c45_1, _mm_mul_pd(a45_1, b45));
#endif
_mm_storeu_pd(&C[(i*56)+2], c45_1);
__m128d c45_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a45_3 = _mm_load_sd(&values[580]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_3 = _mm_add_sd(c45_3, _mm_mul_sd(a45_3, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_3 = _mm_add_sd(c45_3, _mm_mul_sd(a45_3, b45));
#endif
_mm_store_sd(&C[(i*56)+6], c45_3);
__m128d c45_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a45_4 = _mm_loadu_pd(&values[581]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_4 = _mm_add_pd(c45_4, _mm_mul_pd(a45_4, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_4 = _mm_add_pd(c45_4, _mm_mul_pd(a45_4, b45));
#endif
_mm_storeu_pd(&C[(i*56)+8], c45_4);
__m128d c45_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a45_6 = _mm_load_sd(&values[583]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_6 = _mm_add_sd(c45_6, _mm_mul_sd(a45_6, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_6 = _mm_add_sd(c45_6, _mm_mul_sd(a45_6, b45));
#endif
_mm_store_sd(&C[(i*56)+13], c45_6);
__m128d c45_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a45_7 = _mm_load_sd(&values[584]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_7 = _mm_add_sd(c45_7, _mm_mul_sd(a45_7, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_7 = _mm_add_sd(c45_7, _mm_mul_sd(a45_7, b45));
#endif
_mm_store_sd(&C[(i*56)+16], c45_7);
__m128d c45_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a45_8 = _mm_loadu_pd(&values[585]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_8 = _mm_add_pd(c45_8, _mm_mul_pd(a45_8, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_8 = _mm_add_pd(c45_8, _mm_mul_pd(a45_8, b45));
#endif
_mm_storeu_pd(&C[(i*56)+18], c45_8);
__m128d c45_10 = _mm_load_sd(&C[(i*56)+24]);
__m128d a45_10 = _mm_load_sd(&values[587]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_10 = _mm_add_sd(c45_10, _mm_mul_sd(a45_10, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_10 = _mm_add_sd(c45_10, _mm_mul_sd(a45_10, b45));
#endif
_mm_store_sd(&C[(i*56)+24], c45_10);
__m128d c45_11 = _mm_load_sd(&C[(i*56)+28]);
__m128d a45_11 = _mm_load_sd(&values[588]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_11 = _mm_add_sd(c45_11, _mm_mul_sd(a45_11, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_11 = _mm_add_sd(c45_11, _mm_mul_sd(a45_11, b45));
#endif
_mm_store_sd(&C[(i*56)+28], c45_11);
__m128d c45_12 = _mm_load_sd(&C[(i*56)+31]);
__m128d a45_12 = _mm_load_sd(&values[589]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_12 = _mm_add_sd(c45_12, _mm_mul_sd(a45_12, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_12 = _mm_add_sd(c45_12, _mm_mul_sd(a45_12, b45));
#endif
_mm_store_sd(&C[(i*56)+31], c45_12);
__m128d c45_13 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a45_13 = _mm_loadu_pd(&values[590]);
#if defined(__SSE3__) && defined(__AVX256__)
c45_13 = _mm_add_pd(c45_13, _mm_mul_pd(a45_13, _mm256_castpd256_pd128(b45)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c45_13 = _mm_add_pd(c45_13, _mm_mul_pd(a45_13, b45));
#endif
_mm_storeu_pd(&C[(i*56)+33], c45_13);
#else
C[(i*56)+0] += values[577] * B[(i*56)+45];
C[(i*56)+2] += values[578] * B[(i*56)+45];
C[(i*56)+3] += values[579] * B[(i*56)+45];
C[(i*56)+6] += values[580] * B[(i*56)+45];
C[(i*56)+8] += values[581] * B[(i*56)+45];
C[(i*56)+9] += values[582] * B[(i*56)+45];
C[(i*56)+13] += values[583] * B[(i*56)+45];
C[(i*56)+16] += values[584] * B[(i*56)+45];
C[(i*56)+18] += values[585] * B[(i*56)+45];
C[(i*56)+19] += values[586] * B[(i*56)+45];
C[(i*56)+24] += values[587] * B[(i*56)+45];
C[(i*56)+28] += values[588] * B[(i*56)+45];
C[(i*56)+31] += values[589] * B[(i*56)+45];
C[(i*56)+33] += values[590] * B[(i*56)+45];
C[(i*56)+34] += values[591] * B[(i*56)+45];
#endif
#ifndef NDEBUG
num_flops += 30;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b46 = _mm256_broadcast_sd(&B[(i*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b46 = _mm_loaddup_pd(&B[(i*56)+46]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a46_0 = _mm256_loadu_pd(&values[592]);
c46_0 = _mm256_add_pd(c46_0, _mm256_mul_pd(a46_0, b46));
_mm256_storeu_pd(&C[(i*56)+0], c46_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a46_0 = _mm_loadu_pd(&values[592]);
c46_0 = _mm_add_pd(c46_0, _mm_mul_pd(a46_0, b46));
_mm_storeu_pd(&C[(i*56)+0], c46_0);
__m128d c46_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a46_2 = _mm_loadu_pd(&values[594]);
c46_2 = _mm_add_pd(c46_2, _mm_mul_pd(a46_2, b46));
_mm_storeu_pd(&C[(i*56)+2], c46_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a46_4 = _mm256_loadu_pd(&values[596]);
c46_4 = _mm256_add_pd(c46_4, _mm256_mul_pd(a46_4, b46));
_mm256_storeu_pd(&C[(i*56)+4], c46_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a46_4 = _mm_loadu_pd(&values[596]);
c46_4 = _mm_add_pd(c46_4, _mm_mul_pd(a46_4, b46));
_mm_storeu_pd(&C[(i*56)+4], c46_4);
__m128d c46_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a46_6 = _mm_loadu_pd(&values[598]);
c46_6 = _mm_add_pd(c46_6, _mm_mul_pd(a46_6, b46));
_mm_storeu_pd(&C[(i*56)+6], c46_6);
#endif
__m128d c46_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a46_8 = _mm_loadu_pd(&values[600]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_8 = _mm_add_pd(c46_8, _mm_mul_pd(a46_8, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_8 = _mm_add_pd(c46_8, _mm_mul_pd(a46_8, b46));
#endif
_mm_storeu_pd(&C[(i*56)+8], c46_8);
__m128d c46_10 = _mm_load_sd(&C[(i*56)+10]);
__m128d a46_10 = _mm_load_sd(&values[602]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_10 = _mm_add_sd(c46_10, _mm_mul_sd(a46_10, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_10 = _mm_add_sd(c46_10, _mm_mul_sd(a46_10, b46));
#endif
_mm_store_sd(&C[(i*56)+10], c46_10);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_11 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a46_11 = _mm256_loadu_pd(&values[603]);
c46_11 = _mm256_add_pd(c46_11, _mm256_mul_pd(a46_11, b46));
_mm256_storeu_pd(&C[(i*56)+14], c46_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_11 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a46_11 = _mm_loadu_pd(&values[603]);
c46_11 = _mm_add_pd(c46_11, _mm_mul_pd(a46_11, b46));
_mm_storeu_pd(&C[(i*56)+14], c46_11);
__m128d c46_13 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a46_13 = _mm_loadu_pd(&values[605]);
c46_13 = _mm_add_pd(c46_13, _mm_mul_pd(a46_13, b46));
_mm_storeu_pd(&C[(i*56)+16], c46_13);
#endif
__m128d c46_15 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a46_15 = _mm_loadu_pd(&values[607]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_15 = _mm_add_pd(c46_15, _mm_mul_pd(a46_15, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_15 = _mm_add_pd(c46_15, _mm_mul_pd(a46_15, b46));
#endif
_mm_storeu_pd(&C[(i*56)+18], c46_15);
__m128d c46_17 = _mm_load_sd(&C[(i*56)+25]);
__m128d a46_17 = _mm_load_sd(&values[609]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_17 = _mm_add_sd(c46_17, _mm_mul_sd(a46_17, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_17 = _mm_add_sd(c46_17, _mm_mul_sd(a46_17, b46));
#endif
_mm_store_sd(&C[(i*56)+25], c46_17);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c46_18 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a46_18 = _mm256_loadu_pd(&values[610]);
c46_18 = _mm256_add_pd(c46_18, _mm256_mul_pd(a46_18, b46));
_mm256_storeu_pd(&C[(i*56)+29], c46_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c46_18 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a46_18 = _mm_loadu_pd(&values[610]);
c46_18 = _mm_add_pd(c46_18, _mm_mul_pd(a46_18, b46));
_mm_storeu_pd(&C[(i*56)+29], c46_18);
__m128d c46_20 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a46_20 = _mm_loadu_pd(&values[612]);
c46_20 = _mm_add_pd(c46_20, _mm_mul_pd(a46_20, b46));
_mm_storeu_pd(&C[(i*56)+31], c46_20);
#endif
__m128d c46_22 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a46_22 = _mm_loadu_pd(&values[614]);
#if defined(__SSE3__) && defined(__AVX256__)
c46_22 = _mm_add_pd(c46_22, _mm_mul_pd(a46_22, _mm256_castpd256_pd128(b46)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c46_22 = _mm_add_pd(c46_22, _mm_mul_pd(a46_22, b46));
#endif
_mm_storeu_pd(&C[(i*56)+33], c46_22);
#else
C[(i*56)+0] += values[592] * B[(i*56)+46];
C[(i*56)+1] += values[593] * B[(i*56)+46];
C[(i*56)+2] += values[594] * B[(i*56)+46];
C[(i*56)+3] += values[595] * B[(i*56)+46];
C[(i*56)+4] += values[596] * B[(i*56)+46];
C[(i*56)+5] += values[597] * B[(i*56)+46];
C[(i*56)+6] += values[598] * B[(i*56)+46];
C[(i*56)+7] += values[599] * B[(i*56)+46];
C[(i*56)+8] += values[600] * B[(i*56)+46];
C[(i*56)+9] += values[601] * B[(i*56)+46];
C[(i*56)+10] += values[602] * B[(i*56)+46];
C[(i*56)+14] += values[603] * B[(i*56)+46];
C[(i*56)+15] += values[604] * B[(i*56)+46];
C[(i*56)+16] += values[605] * B[(i*56)+46];
C[(i*56)+17] += values[606] * B[(i*56)+46];
C[(i*56)+18] += values[607] * B[(i*56)+46];
C[(i*56)+19] += values[608] * B[(i*56)+46];
C[(i*56)+25] += values[609] * B[(i*56)+46];
C[(i*56)+29] += values[610] * B[(i*56)+46];
C[(i*56)+30] += values[611] * B[(i*56)+46];
C[(i*56)+31] += values[612] * B[(i*56)+46];
C[(i*56)+32] += values[613] * B[(i*56)+46];
C[(i*56)+33] += values[614] * B[(i*56)+46];
C[(i*56)+34] += values[615] * B[(i*56)+46];
#endif
#ifndef NDEBUG
num_flops += 48;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b47 = _mm256_broadcast_sd(&B[(i*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b47 = _mm_loaddup_pd(&B[(i*56)+47]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a47_0 = _mm256_loadu_pd(&values[616]);
c47_0 = _mm256_add_pd(c47_0, _mm256_mul_pd(a47_0, b47));
_mm256_storeu_pd(&C[(i*56)+0], c47_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a47_0 = _mm_loadu_pd(&values[616]);
c47_0 = _mm_add_pd(c47_0, _mm_mul_pd(a47_0, b47));
_mm_storeu_pd(&C[(i*56)+0], c47_0);
__m128d c47_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a47_2 = _mm_loadu_pd(&values[618]);
c47_2 = _mm_add_pd(c47_2, _mm_mul_pd(a47_2, b47));
_mm_storeu_pd(&C[(i*56)+2], c47_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_4 = _mm256_loadu_pd(&C[(i*56)+4]);
__m256d a47_4 = _mm256_loadu_pd(&values[620]);
c47_4 = _mm256_add_pd(c47_4, _mm256_mul_pd(a47_4, b47));
_mm256_storeu_pd(&C[(i*56)+4], c47_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_4 = _mm_loadu_pd(&C[(i*56)+4]);
__m128d a47_4 = _mm_loadu_pd(&values[620]);
c47_4 = _mm_add_pd(c47_4, _mm_mul_pd(a47_4, b47));
_mm_storeu_pd(&C[(i*56)+4], c47_4);
__m128d c47_6 = _mm_loadu_pd(&C[(i*56)+6]);
__m128d a47_6 = _mm_loadu_pd(&values[622]);
c47_6 = _mm_add_pd(c47_6, _mm_mul_pd(a47_6, b47));
_mm_storeu_pd(&C[(i*56)+6], c47_6);
#endif
__m128d c47_8 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a47_8 = _mm_loadu_pd(&values[624]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_8 = _mm_add_pd(c47_8, _mm_mul_pd(a47_8, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_8 = _mm_add_pd(c47_8, _mm_mul_pd(a47_8, b47));
#endif
_mm_storeu_pd(&C[(i*56)+8], c47_8);
__m128d c47_10 = _mm_load_sd(&C[(i*56)+11]);
__m128d a47_10 = _mm_load_sd(&values[626]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_10 = _mm_add_sd(c47_10, _mm_mul_sd(a47_10, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_10 = _mm_add_sd(c47_10, _mm_mul_sd(a47_10, b47));
#endif
_mm_store_sd(&C[(i*56)+11], c47_10);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_11 = _mm256_loadu_pd(&C[(i*56)+14]);
__m256d a47_11 = _mm256_loadu_pd(&values[627]);
c47_11 = _mm256_add_pd(c47_11, _mm256_mul_pd(a47_11, b47));
_mm256_storeu_pd(&C[(i*56)+14], c47_11);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_11 = _mm_loadu_pd(&C[(i*56)+14]);
__m128d a47_11 = _mm_loadu_pd(&values[627]);
c47_11 = _mm_add_pd(c47_11, _mm_mul_pd(a47_11, b47));
_mm_storeu_pd(&C[(i*56)+14], c47_11);
__m128d c47_13 = _mm_loadu_pd(&C[(i*56)+16]);
__m128d a47_13 = _mm_loadu_pd(&values[629]);
c47_13 = _mm_add_pd(c47_13, _mm_mul_pd(a47_13, b47));
_mm_storeu_pd(&C[(i*56)+16], c47_13);
#endif
__m128d c47_15 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a47_15 = _mm_loadu_pd(&values[631]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_15 = _mm_add_pd(c47_15, _mm_mul_pd(a47_15, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_15 = _mm_add_pd(c47_15, _mm_mul_pd(a47_15, b47));
#endif
_mm_storeu_pd(&C[(i*56)+18], c47_15);
__m128d c47_17 = _mm_load_sd(&C[(i*56)+26]);
__m128d a47_17 = _mm_load_sd(&values[633]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_17 = _mm_add_sd(c47_17, _mm_mul_sd(a47_17, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_17 = _mm_add_sd(c47_17, _mm_mul_sd(a47_17, b47));
#endif
_mm_store_sd(&C[(i*56)+26], c47_17);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c47_18 = _mm256_loadu_pd(&C[(i*56)+29]);
__m256d a47_18 = _mm256_loadu_pd(&values[634]);
c47_18 = _mm256_add_pd(c47_18, _mm256_mul_pd(a47_18, b47));
_mm256_storeu_pd(&C[(i*56)+29], c47_18);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c47_18 = _mm_loadu_pd(&C[(i*56)+29]);
__m128d a47_18 = _mm_loadu_pd(&values[634]);
c47_18 = _mm_add_pd(c47_18, _mm_mul_pd(a47_18, b47));
_mm_storeu_pd(&C[(i*56)+29], c47_18);
__m128d c47_20 = _mm_loadu_pd(&C[(i*56)+31]);
__m128d a47_20 = _mm_loadu_pd(&values[636]);
c47_20 = _mm_add_pd(c47_20, _mm_mul_pd(a47_20, b47));
_mm_storeu_pd(&C[(i*56)+31], c47_20);
#endif
__m128d c47_22 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a47_22 = _mm_loadu_pd(&values[638]);
#if defined(__SSE3__) && defined(__AVX256__)
c47_22 = _mm_add_pd(c47_22, _mm_mul_pd(a47_22, _mm256_castpd256_pd128(b47)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c47_22 = _mm_add_pd(c47_22, _mm_mul_pd(a47_22, b47));
#endif
_mm_storeu_pd(&C[(i*56)+33], c47_22);
#else
C[(i*56)+0] += values[616] * B[(i*56)+47];
C[(i*56)+1] += values[617] * B[(i*56)+47];
C[(i*56)+2] += values[618] * B[(i*56)+47];
C[(i*56)+3] += values[619] * B[(i*56)+47];
C[(i*56)+4] += values[620] * B[(i*56)+47];
C[(i*56)+5] += values[621] * B[(i*56)+47];
C[(i*56)+6] += values[622] * B[(i*56)+47];
C[(i*56)+7] += values[623] * B[(i*56)+47];
C[(i*56)+8] += values[624] * B[(i*56)+47];
C[(i*56)+9] += values[625] * B[(i*56)+47];
C[(i*56)+11] += values[626] * B[(i*56)+47];
C[(i*56)+14] += values[627] * B[(i*56)+47];
C[(i*56)+15] += values[628] * B[(i*56)+47];
C[(i*56)+16] += values[629] * B[(i*56)+47];
C[(i*56)+17] += values[630] * B[(i*56)+47];
C[(i*56)+18] += values[631] * B[(i*56)+47];
C[(i*56)+19] += values[632] * B[(i*56)+47];
C[(i*56)+26] += values[633] * B[(i*56)+47];
C[(i*56)+29] += values[634] * B[(i*56)+47];
C[(i*56)+30] += values[635] * B[(i*56)+47];
C[(i*56)+31] += values[636] * B[(i*56)+47];
C[(i*56)+32] += values[637] * B[(i*56)+47];
C[(i*56)+33] += values[638] * B[(i*56)+47];
C[(i*56)+34] += values[639] * B[(i*56)+47];
#endif
#ifndef NDEBUG
num_flops += 48;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b48 = _mm256_broadcast_sd(&B[(i*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b48 = _mm_loaddup_pd(&B[(i*56)+48]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a48_0 = _mm256_loadu_pd(&values[640]);
c48_0 = _mm256_add_pd(c48_0, _mm256_mul_pd(a48_0, b48));
_mm256_storeu_pd(&C[(i*56)+0], c48_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a48_0 = _mm_loadu_pd(&values[640]);
c48_0 = _mm_add_pd(c48_0, _mm_mul_pd(a48_0, b48));
_mm_storeu_pd(&C[(i*56)+0], c48_0);
__m128d c48_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a48_2 = _mm_loadu_pd(&values[642]);
c48_2 = _mm_add_pd(c48_2, _mm_mul_pd(a48_2, b48));
_mm_storeu_pd(&C[(i*56)+2], c48_2);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_4 = _mm256_loadu_pd(&C[(i*56)+5]);
__m256d a48_4 = _mm256_loadu_pd(&values[644]);
c48_4 = _mm256_add_pd(c48_4, _mm256_mul_pd(a48_4, b48));
_mm256_storeu_pd(&C[(i*56)+5], c48_4);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_4 = _mm_loadu_pd(&C[(i*56)+5]);
__m128d a48_4 = _mm_loadu_pd(&values[644]);
c48_4 = _mm_add_pd(c48_4, _mm_mul_pd(a48_4, b48));
_mm_storeu_pd(&C[(i*56)+5], c48_4);
__m128d c48_6 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a48_6 = _mm_loadu_pd(&values[646]);
c48_6 = _mm_add_pd(c48_6, _mm_mul_pd(a48_6, b48));
_mm_storeu_pd(&C[(i*56)+7], c48_6);
#endif
__m128d c48_8 = _mm_load_sd(&C[(i*56)+9]);
__m128d a48_8 = _mm_load_sd(&values[648]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_8 = _mm_add_sd(c48_8, _mm_mul_sd(a48_8, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_8 = _mm_add_sd(c48_8, _mm_mul_sd(a48_8, b48));
#endif
_mm_store_sd(&C[(i*56)+9], c48_8);
__m128d c48_9 = _mm_load_sd(&C[(i*56)+12]);
__m128d a48_9 = _mm_load_sd(&values[649]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_9 = _mm_add_sd(c48_9, _mm_mul_sd(a48_9, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_9 = _mm_add_sd(c48_9, _mm_mul_sd(a48_9, b48));
#endif
_mm_store_sd(&C[(i*56)+12], c48_9);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_10 = _mm256_loadu_pd(&C[(i*56)+15]);
__m256d a48_10 = _mm256_loadu_pd(&values[650]);
c48_10 = _mm256_add_pd(c48_10, _mm256_mul_pd(a48_10, b48));
_mm256_storeu_pd(&C[(i*56)+15], c48_10);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_10 = _mm_loadu_pd(&C[(i*56)+15]);
__m128d a48_10 = _mm_loadu_pd(&values[650]);
c48_10 = _mm_add_pd(c48_10, _mm_mul_pd(a48_10, b48));
_mm_storeu_pd(&C[(i*56)+15], c48_10);
__m128d c48_12 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a48_12 = _mm_loadu_pd(&values[652]);
c48_12 = _mm_add_pd(c48_12, _mm_mul_pd(a48_12, b48));
_mm_storeu_pd(&C[(i*56)+17], c48_12);
#endif
__m128d c48_14 = _mm_load_sd(&C[(i*56)+19]);
__m128d a48_14 = _mm_load_sd(&values[654]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_14 = _mm_add_sd(c48_14, _mm_mul_sd(a48_14, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_14 = _mm_add_sd(c48_14, _mm_mul_sd(a48_14, b48));
#endif
_mm_store_sd(&C[(i*56)+19], c48_14);
__m128d c48_15 = _mm_load_sd(&C[(i*56)+27]);
__m128d a48_15 = _mm_load_sd(&values[655]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_15 = _mm_add_sd(c48_15, _mm_mul_sd(a48_15, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_15 = _mm_add_sd(c48_15, _mm_mul_sd(a48_15, b48));
#endif
_mm_store_sd(&C[(i*56)+27], c48_15);
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c48_16 = _mm256_loadu_pd(&C[(i*56)+30]);
__m256d a48_16 = _mm256_loadu_pd(&values[656]);
c48_16 = _mm256_add_pd(c48_16, _mm256_mul_pd(a48_16, b48));
_mm256_storeu_pd(&C[(i*56)+30], c48_16);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c48_16 = _mm_loadu_pd(&C[(i*56)+30]);
__m128d a48_16 = _mm_loadu_pd(&values[656]);
c48_16 = _mm_add_pd(c48_16, _mm_mul_pd(a48_16, b48));
_mm_storeu_pd(&C[(i*56)+30], c48_16);
__m128d c48_18 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a48_18 = _mm_loadu_pd(&values[658]);
c48_18 = _mm_add_pd(c48_18, _mm_mul_pd(a48_18, b48));
_mm_storeu_pd(&C[(i*56)+32], c48_18);
#endif
__m128d c48_20 = _mm_load_sd(&C[(i*56)+34]);
__m128d a48_20 = _mm_load_sd(&values[660]);
#if defined(__SSE3__) && defined(__AVX256__)
c48_20 = _mm_add_sd(c48_20, _mm_mul_sd(a48_20, _mm256_castpd256_pd128(b48)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c48_20 = _mm_add_sd(c48_20, _mm_mul_sd(a48_20, b48));
#endif
_mm_store_sd(&C[(i*56)+34], c48_20);
#else
C[(i*56)+0] += values[640] * B[(i*56)+48];
C[(i*56)+1] += values[641] * B[(i*56)+48];
C[(i*56)+2] += values[642] * B[(i*56)+48];
C[(i*56)+3] += values[643] * B[(i*56)+48];
C[(i*56)+5] += values[644] * B[(i*56)+48];
C[(i*56)+6] += values[645] * B[(i*56)+48];
C[(i*56)+7] += values[646] * B[(i*56)+48];
C[(i*56)+8] += values[647] * B[(i*56)+48];
C[(i*56)+9] += values[648] * B[(i*56)+48];
C[(i*56)+12] += values[649] * B[(i*56)+48];
C[(i*56)+15] += values[650] * B[(i*56)+48];
C[(i*56)+16] += values[651] * B[(i*56)+48];
C[(i*56)+17] += values[652] * B[(i*56)+48];
C[(i*56)+18] += values[653] * B[(i*56)+48];
C[(i*56)+19] += values[654] * B[(i*56)+48];
C[(i*56)+27] += values[655] * B[(i*56)+48];
C[(i*56)+30] += values[656] * B[(i*56)+48];
C[(i*56)+31] += values[657] * B[(i*56)+48];
C[(i*56)+32] += values[658] * B[(i*56)+48];
C[(i*56)+33] += values[659] * B[(i*56)+48];
C[(i*56)+34] += values[660] * B[(i*56)+48];
#endif
#ifndef NDEBUG
num_flops += 42;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b49 = _mm256_broadcast_sd(&B[(i*56)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b49 = _mm_loaddup_pd(&B[(i*56)+49]);
#endif
__m128d c49_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a49_0 = _mm_load_sd(&values[661]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_0 = _mm_add_sd(c49_0, _mm_mul_sd(a49_0, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_0 = _mm_add_sd(c49_0, _mm_mul_sd(a49_0, b49));
#endif
_mm_store_sd(&C[(i*56)+0], c49_0);
__m128d c49_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a49_1 = _mm_loadu_pd(&values[662]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_1 = _mm_add_pd(c49_1, _mm_mul_pd(a49_1, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_1 = _mm_add_pd(c49_1, _mm_mul_pd(a49_1, b49));
#endif
_mm_storeu_pd(&C[(i*56)+2], c49_1);
__m128d c49_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a49_3 = _mm_load_sd(&values[664]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_3 = _mm_add_sd(c49_3, _mm_mul_sd(a49_3, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_3 = _mm_add_sd(c49_3, _mm_mul_sd(a49_3, b49));
#endif
_mm_store_sd(&C[(i*56)+6], c49_3);
__m128d c49_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a49_4 = _mm_loadu_pd(&values[665]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_4 = _mm_add_pd(c49_4, _mm_mul_pd(a49_4, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_4 = _mm_add_pd(c49_4, _mm_mul_pd(a49_4, b49));
#endif
_mm_storeu_pd(&C[(i*56)+8], c49_4);
__m128d c49_6 = _mm_load_sd(&C[(i*56)+13]);
__m128d a49_6 = _mm_load_sd(&values[667]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_6 = _mm_add_sd(c49_6, _mm_mul_sd(a49_6, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_6 = _mm_add_sd(c49_6, _mm_mul_sd(a49_6, b49));
#endif
_mm_store_sd(&C[(i*56)+13], c49_6);
__m128d c49_7 = _mm_load_sd(&C[(i*56)+16]);
__m128d a49_7 = _mm_load_sd(&values[668]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_7 = _mm_add_sd(c49_7, _mm_mul_sd(a49_7, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_7 = _mm_add_sd(c49_7, _mm_mul_sd(a49_7, b49));
#endif
_mm_store_sd(&C[(i*56)+16], c49_7);
__m128d c49_8 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a49_8 = _mm_loadu_pd(&values[669]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_8 = _mm_add_pd(c49_8, _mm_mul_pd(a49_8, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_8 = _mm_add_pd(c49_8, _mm_mul_pd(a49_8, b49));
#endif
_mm_storeu_pd(&C[(i*56)+18], c49_8);
__m128d c49_10 = _mm_load_sd(&C[(i*56)+28]);
__m128d a49_10 = _mm_load_sd(&values[671]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_10 = _mm_add_sd(c49_10, _mm_mul_sd(a49_10, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_10 = _mm_add_sd(c49_10, _mm_mul_sd(a49_10, b49));
#endif
_mm_store_sd(&C[(i*56)+28], c49_10);
__m128d c49_11 = _mm_load_sd(&C[(i*56)+31]);
__m128d a49_11 = _mm_load_sd(&values[672]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_11 = _mm_add_sd(c49_11, _mm_mul_sd(a49_11, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_11 = _mm_add_sd(c49_11, _mm_mul_sd(a49_11, b49));
#endif
_mm_store_sd(&C[(i*56)+31], c49_11);
__m128d c49_12 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a49_12 = _mm_loadu_pd(&values[673]);
#if defined(__SSE3__) && defined(__AVX256__)
c49_12 = _mm_add_pd(c49_12, _mm_mul_pd(a49_12, _mm256_castpd256_pd128(b49)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c49_12 = _mm_add_pd(c49_12, _mm_mul_pd(a49_12, b49));
#endif
_mm_storeu_pd(&C[(i*56)+33], c49_12);
#else
C[(i*56)+0] += values[661] * B[(i*56)+49];
C[(i*56)+2] += values[662] * B[(i*56)+49];
C[(i*56)+3] += values[663] * B[(i*56)+49];
C[(i*56)+6] += values[664] * B[(i*56)+49];
C[(i*56)+8] += values[665] * B[(i*56)+49];
C[(i*56)+9] += values[666] * B[(i*56)+49];
C[(i*56)+13] += values[667] * B[(i*56)+49];
C[(i*56)+16] += values[668] * B[(i*56)+49];
C[(i*56)+18] += values[669] * B[(i*56)+49];
C[(i*56)+19] += values[670] * B[(i*56)+49];
C[(i*56)+28] += values[671] * B[(i*56)+49];
C[(i*56)+31] += values[672] * B[(i*56)+49];
C[(i*56)+33] += values[673] * B[(i*56)+49];
C[(i*56)+34] += values[674] * B[(i*56)+49];
#endif
#ifndef NDEBUG
num_flops += 28;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b50 = _mm256_broadcast_sd(&B[(i*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b50 = _mm_loaddup_pd(&B[(i*56)+50]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c50_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a50_0 = _mm256_loadu_pd(&values[675]);
c50_0 = _mm256_add_pd(c50_0, _mm256_mul_pd(a50_0, b50));
_mm256_storeu_pd(&C[(i*56)+0], c50_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c50_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a50_0 = _mm_loadu_pd(&values[675]);
c50_0 = _mm_add_pd(c50_0, _mm_mul_pd(a50_0, b50));
_mm_storeu_pd(&C[(i*56)+0], c50_0);
__m128d c50_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a50_2 = _mm_loadu_pd(&values[677]);
c50_2 = _mm_add_pd(c50_2, _mm_mul_pd(a50_2, b50));
_mm_storeu_pd(&C[(i*56)+2], c50_2);
#endif
__m128d c50_4 = _mm_load_sd(&C[(i*56)+4]);
__m128d a50_4 = _mm_load_sd(&values[679]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_4 = _mm_add_sd(c50_4, _mm_mul_sd(a50_4, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_4 = _mm_add_sd(c50_4, _mm_mul_sd(a50_4, b50));
#endif
_mm_store_sd(&C[(i*56)+4], c50_4);
__m128d c50_5 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a50_5 = _mm_loadu_pd(&values[680]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_5 = _mm_add_pd(c50_5, _mm_mul_pd(a50_5, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_5 = _mm_add_pd(c50_5, _mm_mul_pd(a50_5, b50));
#endif
_mm_storeu_pd(&C[(i*56)+7], c50_5);
__m128d c50_7 = _mm_load_sd(&C[(i*56)+9]);
__m128d a50_7 = _mm_load_sd(&values[682]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_7 = _mm_add_sd(c50_7, _mm_mul_sd(a50_7, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_7 = _mm_add_sd(c50_7, _mm_mul_sd(a50_7, b50));
#endif
_mm_store_sd(&C[(i*56)+9], c50_7);
__m128d c50_8 = _mm_load_sd(&C[(i*56)+14]);
__m128d a50_8 = _mm_load_sd(&values[683]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_8 = _mm_add_sd(c50_8, _mm_mul_sd(a50_8, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_8 = _mm_add_sd(c50_8, _mm_mul_sd(a50_8, b50));
#endif
_mm_store_sd(&C[(i*56)+14], c50_8);
__m128d c50_9 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a50_9 = _mm_loadu_pd(&values[684]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_9 = _mm_add_pd(c50_9, _mm_mul_pd(a50_9, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_9 = _mm_add_pd(c50_9, _mm_mul_pd(a50_9, b50));
#endif
_mm_storeu_pd(&C[(i*56)+17], c50_9);
__m128d c50_11 = _mm_load_sd(&C[(i*56)+19]);
__m128d a50_11 = _mm_load_sd(&values[686]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_11 = _mm_add_sd(c50_11, _mm_mul_sd(a50_11, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_11 = _mm_add_sd(c50_11, _mm_mul_sd(a50_11, b50));
#endif
_mm_store_sd(&C[(i*56)+19], c50_11);
__m128d c50_12 = _mm_load_sd(&C[(i*56)+29]);
__m128d a50_12 = _mm_load_sd(&values[687]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_12 = _mm_add_sd(c50_12, _mm_mul_sd(a50_12, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_12 = _mm_add_sd(c50_12, _mm_mul_sd(a50_12, b50));
#endif
_mm_store_sd(&C[(i*56)+29], c50_12);
__m128d c50_13 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a50_13 = _mm_loadu_pd(&values[688]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_13 = _mm_add_pd(c50_13, _mm_mul_pd(a50_13, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_13 = _mm_add_pd(c50_13, _mm_mul_pd(a50_13, b50));
#endif
_mm_storeu_pd(&C[(i*56)+32], c50_13);
__m128d c50_15 = _mm_load_sd(&C[(i*56)+34]);
__m128d a50_15 = _mm_load_sd(&values[690]);
#if defined(__SSE3__) && defined(__AVX256__)
c50_15 = _mm_add_sd(c50_15, _mm_mul_sd(a50_15, _mm256_castpd256_pd128(b50)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c50_15 = _mm_add_sd(c50_15, _mm_mul_sd(a50_15, b50));
#endif
_mm_store_sd(&C[(i*56)+34], c50_15);
#else
C[(i*56)+0] += values[675] * B[(i*56)+50];
C[(i*56)+1] += values[676] * B[(i*56)+50];
C[(i*56)+2] += values[677] * B[(i*56)+50];
C[(i*56)+3] += values[678] * B[(i*56)+50];
C[(i*56)+4] += values[679] * B[(i*56)+50];
C[(i*56)+7] += values[680] * B[(i*56)+50];
C[(i*56)+8] += values[681] * B[(i*56)+50];
C[(i*56)+9] += values[682] * B[(i*56)+50];
C[(i*56)+14] += values[683] * B[(i*56)+50];
C[(i*56)+17] += values[684] * B[(i*56)+50];
C[(i*56)+18] += values[685] * B[(i*56)+50];
C[(i*56)+19] += values[686] * B[(i*56)+50];
C[(i*56)+29] += values[687] * B[(i*56)+50];
C[(i*56)+32] += values[688] * B[(i*56)+50];
C[(i*56)+33] += values[689] * B[(i*56)+50];
C[(i*56)+34] += values[690] * B[(i*56)+50];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b51 = _mm256_broadcast_sd(&B[(i*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b51 = _mm_loaddup_pd(&B[(i*56)+51]);
#endif
#if defined(__SSE3__) && defined(__AVX256__)
__m256d c51_0 = _mm256_loadu_pd(&C[(i*56)+0]);
__m256d a51_0 = _mm256_loadu_pd(&values[691]);
c51_0 = _mm256_add_pd(c51_0, _mm256_mul_pd(a51_0, b51));
_mm256_storeu_pd(&C[(i*56)+0], c51_0);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d c51_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a51_0 = _mm_loadu_pd(&values[691]);
c51_0 = _mm_add_pd(c51_0, _mm_mul_pd(a51_0, b51));
_mm_storeu_pd(&C[(i*56)+0], c51_0);
__m128d c51_2 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a51_2 = _mm_loadu_pd(&values[693]);
c51_2 = _mm_add_pd(c51_2, _mm_mul_pd(a51_2, b51));
_mm_storeu_pd(&C[(i*56)+2], c51_2);
#endif
__m128d c51_4 = _mm_load_sd(&C[(i*56)+5]);
__m128d a51_4 = _mm_load_sd(&values[695]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_4 = _mm_add_sd(c51_4, _mm_mul_sd(a51_4, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_4 = _mm_add_sd(c51_4, _mm_mul_sd(a51_4, b51));
#endif
_mm_store_sd(&C[(i*56)+5], c51_4);
__m128d c51_5 = _mm_loadu_pd(&C[(i*56)+7]);
__m128d a51_5 = _mm_loadu_pd(&values[696]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_5 = _mm_add_pd(c51_5, _mm_mul_pd(a51_5, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_5 = _mm_add_pd(c51_5, _mm_mul_pd(a51_5, b51));
#endif
_mm_storeu_pd(&C[(i*56)+7], c51_5);
__m128d c51_7 = _mm_load_sd(&C[(i*56)+9]);
__m128d a51_7 = _mm_load_sd(&values[698]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_7 = _mm_add_sd(c51_7, _mm_mul_sd(a51_7, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_7 = _mm_add_sd(c51_7, _mm_mul_sd(a51_7, b51));
#endif
_mm_store_sd(&C[(i*56)+9], c51_7);
__m128d c51_8 = _mm_load_sd(&C[(i*56)+15]);
__m128d a51_8 = _mm_load_sd(&values[699]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_8 = _mm_add_sd(c51_8, _mm_mul_sd(a51_8, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_8 = _mm_add_sd(c51_8, _mm_mul_sd(a51_8, b51));
#endif
_mm_store_sd(&C[(i*56)+15], c51_8);
__m128d c51_9 = _mm_loadu_pd(&C[(i*56)+17]);
__m128d a51_9 = _mm_loadu_pd(&values[700]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_9 = _mm_add_pd(c51_9, _mm_mul_pd(a51_9, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_9 = _mm_add_pd(c51_9, _mm_mul_pd(a51_9, b51));
#endif
_mm_storeu_pd(&C[(i*56)+17], c51_9);
__m128d c51_11 = _mm_load_sd(&C[(i*56)+19]);
__m128d a51_11 = _mm_load_sd(&values[702]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_11 = _mm_add_sd(c51_11, _mm_mul_sd(a51_11, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_11 = _mm_add_sd(c51_11, _mm_mul_sd(a51_11, b51));
#endif
_mm_store_sd(&C[(i*56)+19], c51_11);
__m128d c51_12 = _mm_load_sd(&C[(i*56)+30]);
__m128d a51_12 = _mm_load_sd(&values[703]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_12 = _mm_add_sd(c51_12, _mm_mul_sd(a51_12, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_12 = _mm_add_sd(c51_12, _mm_mul_sd(a51_12, b51));
#endif
_mm_store_sd(&C[(i*56)+30], c51_12);
__m128d c51_13 = _mm_loadu_pd(&C[(i*56)+32]);
__m128d a51_13 = _mm_loadu_pd(&values[704]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_13 = _mm_add_pd(c51_13, _mm_mul_pd(a51_13, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_13 = _mm_add_pd(c51_13, _mm_mul_pd(a51_13, b51));
#endif
_mm_storeu_pd(&C[(i*56)+32], c51_13);
__m128d c51_15 = _mm_load_sd(&C[(i*56)+34]);
__m128d a51_15 = _mm_load_sd(&values[706]);
#if defined(__SSE3__) && defined(__AVX256__)
c51_15 = _mm_add_sd(c51_15, _mm_mul_sd(a51_15, _mm256_castpd256_pd128(b51)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c51_15 = _mm_add_sd(c51_15, _mm_mul_sd(a51_15, b51));
#endif
_mm_store_sd(&C[(i*56)+34], c51_15);
#else
C[(i*56)+0] += values[691] * B[(i*56)+51];
C[(i*56)+1] += values[692] * B[(i*56)+51];
C[(i*56)+2] += values[693] * B[(i*56)+51];
C[(i*56)+3] += values[694] * B[(i*56)+51];
C[(i*56)+5] += values[695] * B[(i*56)+51];
C[(i*56)+7] += values[696] * B[(i*56)+51];
C[(i*56)+8] += values[697] * B[(i*56)+51];
C[(i*56)+9] += values[698] * B[(i*56)+51];
C[(i*56)+15] += values[699] * B[(i*56)+51];
C[(i*56)+17] += values[700] * B[(i*56)+51];
C[(i*56)+18] += values[701] * B[(i*56)+51];
C[(i*56)+19] += values[702] * B[(i*56)+51];
C[(i*56)+30] += values[703] * B[(i*56)+51];
C[(i*56)+32] += values[704] * B[(i*56)+51];
C[(i*56)+33] += values[705] * B[(i*56)+51];
C[(i*56)+34] += values[706] * B[(i*56)+51];
#endif
#ifndef NDEBUG
num_flops += 32;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b52 = _mm256_broadcast_sd(&B[(i*56)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b52 = _mm_loaddup_pd(&B[(i*56)+52]);
#endif
__m128d c52_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a52_0 = _mm_load_sd(&values[707]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_0 = _mm_add_sd(c52_0, _mm_mul_sd(a52_0, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_0 = _mm_add_sd(c52_0, _mm_mul_sd(a52_0, b52));
#endif
_mm_store_sd(&C[(i*56)+0], c52_0);
__m128d c52_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a52_1 = _mm_loadu_pd(&values[708]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_1 = _mm_add_pd(c52_1, _mm_mul_pd(a52_1, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_1 = _mm_add_pd(c52_1, _mm_mul_pd(a52_1, b52));
#endif
_mm_storeu_pd(&C[(i*56)+2], c52_1);
__m128d c52_3 = _mm_load_sd(&C[(i*56)+6]);
__m128d a52_3 = _mm_load_sd(&values[710]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_3 = _mm_add_sd(c52_3, _mm_mul_sd(a52_3, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_3 = _mm_add_sd(c52_3, _mm_mul_sd(a52_3, b52));
#endif
_mm_store_sd(&C[(i*56)+6], c52_3);
__m128d c52_4 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a52_4 = _mm_loadu_pd(&values[711]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_4 = _mm_add_pd(c52_4, _mm_mul_pd(a52_4, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_4 = _mm_add_pd(c52_4, _mm_mul_pd(a52_4, b52));
#endif
_mm_storeu_pd(&C[(i*56)+8], c52_4);
__m128d c52_6 = _mm_load_sd(&C[(i*56)+16]);
__m128d a52_6 = _mm_load_sd(&values[713]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_6 = _mm_add_sd(c52_6, _mm_mul_sd(a52_6, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_6 = _mm_add_sd(c52_6, _mm_mul_sd(a52_6, b52));
#endif
_mm_store_sd(&C[(i*56)+16], c52_6);
__m128d c52_7 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a52_7 = _mm_loadu_pd(&values[714]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_7 = _mm_add_pd(c52_7, _mm_mul_pd(a52_7, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_7 = _mm_add_pd(c52_7, _mm_mul_pd(a52_7, b52));
#endif
_mm_storeu_pd(&C[(i*56)+18], c52_7);
__m128d c52_9 = _mm_load_sd(&C[(i*56)+31]);
__m128d a52_9 = _mm_load_sd(&values[716]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_9 = _mm_add_sd(c52_9, _mm_mul_sd(a52_9, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_9 = _mm_add_sd(c52_9, _mm_mul_sd(a52_9, b52));
#endif
_mm_store_sd(&C[(i*56)+31], c52_9);
__m128d c52_10 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a52_10 = _mm_loadu_pd(&values[717]);
#if defined(__SSE3__) && defined(__AVX256__)
c52_10 = _mm_add_pd(c52_10, _mm_mul_pd(a52_10, _mm256_castpd256_pd128(b52)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c52_10 = _mm_add_pd(c52_10, _mm_mul_pd(a52_10, b52));
#endif
_mm_storeu_pd(&C[(i*56)+33], c52_10);
#else
C[(i*56)+0] += values[707] * B[(i*56)+52];
C[(i*56)+2] += values[708] * B[(i*56)+52];
C[(i*56)+3] += values[709] * B[(i*56)+52];
C[(i*56)+6] += values[710] * B[(i*56)+52];
C[(i*56)+8] += values[711] * B[(i*56)+52];
C[(i*56)+9] += values[712] * B[(i*56)+52];
C[(i*56)+16] += values[713] * B[(i*56)+52];
C[(i*56)+18] += values[714] * B[(i*56)+52];
C[(i*56)+19] += values[715] * B[(i*56)+52];
C[(i*56)+31] += values[716] * B[(i*56)+52];
C[(i*56)+33] += values[717] * B[(i*56)+52];
C[(i*56)+34] += values[718] * B[(i*56)+52];
#endif
#ifndef NDEBUG
num_flops += 24;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b53 = _mm256_broadcast_sd(&B[(i*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b53 = _mm_loaddup_pd(&B[(i*56)+53]);
#endif
__m128d c53_0 = _mm_loadu_pd(&C[(i*56)+0]);
__m128d a53_0 = _mm_loadu_pd(&values[719]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_0 = _mm_add_pd(c53_0, _mm_mul_pd(a53_0, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_0 = _mm_add_pd(c53_0, _mm_mul_pd(a53_0, b53));
#endif
_mm_storeu_pd(&C[(i*56)+0], c53_0);
__m128d c53_2 = _mm_load_sd(&C[(i*56)+3]);
__m128d a53_2 = _mm_load_sd(&values[721]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_2 = _mm_add_sd(c53_2, _mm_mul_sd(a53_2, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_2 = _mm_add_sd(c53_2, _mm_mul_sd(a53_2, b53));
#endif
_mm_store_sd(&C[(i*56)+3], c53_2);
__m128d c53_3 = _mm_load_sd(&C[(i*56)+7]);
__m128d a53_3 = _mm_load_sd(&values[722]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_3 = _mm_add_sd(c53_3, _mm_mul_sd(a53_3, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_3 = _mm_add_sd(c53_3, _mm_mul_sd(a53_3, b53));
#endif
_mm_store_sd(&C[(i*56)+7], c53_3);
__m128d c53_4 = _mm_load_sd(&C[(i*56)+9]);
__m128d a53_4 = _mm_load_sd(&values[723]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_4 = _mm_add_sd(c53_4, _mm_mul_sd(a53_4, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_4 = _mm_add_sd(c53_4, _mm_mul_sd(a53_4, b53));
#endif
_mm_store_sd(&C[(i*56)+9], c53_4);
__m128d c53_5 = _mm_load_sd(&C[(i*56)+17]);
__m128d a53_5 = _mm_load_sd(&values[724]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_5 = _mm_add_sd(c53_5, _mm_mul_sd(a53_5, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_5 = _mm_add_sd(c53_5, _mm_mul_sd(a53_5, b53));
#endif
_mm_store_sd(&C[(i*56)+17], c53_5);
__m128d c53_6 = _mm_load_sd(&C[(i*56)+19]);
__m128d a53_6 = _mm_load_sd(&values[725]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_6 = _mm_add_sd(c53_6, _mm_mul_sd(a53_6, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_6 = _mm_add_sd(c53_6, _mm_mul_sd(a53_6, b53));
#endif
_mm_store_sd(&C[(i*56)+19], c53_6);
__m128d c53_7 = _mm_load_sd(&C[(i*56)+32]);
__m128d a53_7 = _mm_load_sd(&values[726]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_7 = _mm_add_sd(c53_7, _mm_mul_sd(a53_7, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_7 = _mm_add_sd(c53_7, _mm_mul_sd(a53_7, b53));
#endif
_mm_store_sd(&C[(i*56)+32], c53_7);
__m128d c53_8 = _mm_load_sd(&C[(i*56)+34]);
__m128d a53_8 = _mm_load_sd(&values[727]);
#if defined(__SSE3__) && defined(__AVX256__)
c53_8 = _mm_add_sd(c53_8, _mm_mul_sd(a53_8, _mm256_castpd256_pd128(b53)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c53_8 = _mm_add_sd(c53_8, _mm_mul_sd(a53_8, b53));
#endif
_mm_store_sd(&C[(i*56)+34], c53_8);
#else
C[(i*56)+0] += values[719] * B[(i*56)+53];
C[(i*56)+1] += values[720] * B[(i*56)+53];
C[(i*56)+3] += values[721] * B[(i*56)+53];
C[(i*56)+7] += values[722] * B[(i*56)+53];
C[(i*56)+9] += values[723] * B[(i*56)+53];
C[(i*56)+17] += values[724] * B[(i*56)+53];
C[(i*56)+19] += values[725] * B[(i*56)+53];
C[(i*56)+32] += values[726] * B[(i*56)+53];
C[(i*56)+34] += values[727] * B[(i*56)+53];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b54 = _mm256_broadcast_sd(&B[(i*56)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b54 = _mm_loaddup_pd(&B[(i*56)+54]);
#endif
__m128d c54_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a54_0 = _mm_load_sd(&values[728]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_0 = _mm_add_sd(c54_0, _mm_mul_sd(a54_0, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_0 = _mm_add_sd(c54_0, _mm_mul_sd(a54_0, b54));
#endif
_mm_store_sd(&C[(i*56)+0], c54_0);
__m128d c54_1 = _mm_loadu_pd(&C[(i*56)+2]);
__m128d a54_1 = _mm_loadu_pd(&values[729]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_1 = _mm_add_pd(c54_1, _mm_mul_pd(a54_1, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_1 = _mm_add_pd(c54_1, _mm_mul_pd(a54_1, b54));
#endif
_mm_storeu_pd(&C[(i*56)+2], c54_1);
__m128d c54_3 = _mm_loadu_pd(&C[(i*56)+8]);
__m128d a54_3 = _mm_loadu_pd(&values[731]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_3 = _mm_add_pd(c54_3, _mm_mul_pd(a54_3, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_3 = _mm_add_pd(c54_3, _mm_mul_pd(a54_3, b54));
#endif
_mm_storeu_pd(&C[(i*56)+8], c54_3);
__m128d c54_5 = _mm_loadu_pd(&C[(i*56)+18]);
__m128d a54_5 = _mm_loadu_pd(&values[733]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_5 = _mm_add_pd(c54_5, _mm_mul_pd(a54_5, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_5 = _mm_add_pd(c54_5, _mm_mul_pd(a54_5, b54));
#endif
_mm_storeu_pd(&C[(i*56)+18], c54_5);
__m128d c54_7 = _mm_loadu_pd(&C[(i*56)+33]);
__m128d a54_7 = _mm_loadu_pd(&values[735]);
#if defined(__SSE3__) && defined(__AVX256__)
c54_7 = _mm_add_pd(c54_7, _mm_mul_pd(a54_7, _mm256_castpd256_pd128(b54)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c54_7 = _mm_add_pd(c54_7, _mm_mul_pd(a54_7, b54));
#endif
_mm_storeu_pd(&C[(i*56)+33], c54_7);
#else
C[(i*56)+0] += values[728] * B[(i*56)+54];
C[(i*56)+2] += values[729] * B[(i*56)+54];
C[(i*56)+3] += values[730] * B[(i*56)+54];
C[(i*56)+8] += values[731] * B[(i*56)+54];
C[(i*56)+9] += values[732] * B[(i*56)+54];
C[(i*56)+18] += values[733] * B[(i*56)+54];
C[(i*56)+19] += values[734] * B[(i*56)+54];
C[(i*56)+33] += values[735] * B[(i*56)+54];
C[(i*56)+34] += values[736] * B[(i*56)+54];
#endif
#ifndef NDEBUG
num_flops += 18;
#endif
#ifndef NDEBUG
num_flops += 0;
#endif
#if defined(__SSE3__) || defined(__AVX256__)
#if defined(__SSE3__) && defined(__AVX256__)
__m256d b55 = _mm256_broadcast_sd(&B[(i*56)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
__m128d b55 = _mm_loaddup_pd(&B[(i*56)+55]);
#endif
__m128d c55_0 = _mm_load_sd(&C[(i*56)+0]);
__m128d a55_0 = _mm_load_sd(&values[737]);
#if defined(__SSE3__) && defined(__AVX256__)
c55_0 = _mm_add_sd(c55_0, _mm_mul_sd(a55_0, _mm256_castpd256_pd128(b55)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c55_0 = _mm_add_sd(c55_0, _mm_mul_sd(a55_0, b55));
#endif
_mm_store_sd(&C[(i*56)+0], c55_0);
__m128d c55_1 = _mm_load_sd(&C[(i*56)+3]);
__m128d a55_1 = _mm_load_sd(&values[738]);
#if defined(__SSE3__) && defined(__AVX256__)
c55_1 = _mm_add_sd(c55_1, _mm_mul_sd(a55_1, _mm256_castpd256_pd128(b55)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c55_1 = _mm_add_sd(c55_1, _mm_mul_sd(a55_1, b55));
#endif
_mm_store_sd(&C[(i*56)+3], c55_1);
__m128d c55_2 = _mm_load_sd(&C[(i*56)+9]);
__m128d a55_2 = _mm_load_sd(&values[739]);
#if defined(__SSE3__) && defined(__AVX256__)
c55_2 = _mm_add_sd(c55_2, _mm_mul_sd(a55_2, _mm256_castpd256_pd128(b55)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c55_2 = _mm_add_sd(c55_2, _mm_mul_sd(a55_2, b55));
#endif
_mm_store_sd(&C[(i*56)+9], c55_2);
__m128d c55_3 = _mm_load_sd(&C[(i*56)+19]);
__m128d a55_3 = _mm_load_sd(&values[740]);
#if defined(__SSE3__) && defined(__AVX256__)
c55_3 = _mm_add_sd(c55_3, _mm_mul_sd(a55_3, _mm256_castpd256_pd128(b55)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c55_3 = _mm_add_sd(c55_3, _mm_mul_sd(a55_3, b55));
#endif
_mm_store_sd(&C[(i*56)+19], c55_3);
__m128d c55_4 = _mm_load_sd(&C[(i*56)+34]);
__m128d a55_4 = _mm_load_sd(&values[741]);
#if defined(__SSE3__) && defined(__AVX256__)
c55_4 = _mm_add_sd(c55_4, _mm_mul_sd(a55_4, _mm256_castpd256_pd128(b55)));
#endif
#if defined(__SSE3__) && !defined(__AVX256__)
c55_4 = _mm_add_sd(c55_4, _mm_mul_sd(a55_4, b55));
#endif
_mm_store_sd(&C[(i*56)+34], c55_4);
#else
C[(i*56)+0] += values[737] * B[(i*56)+55];
C[(i*56)+3] += values[738] * B[(i*56)+55];
C[(i*56)+9] += values[739] * B[(i*56)+55];
C[(i*56)+19] += values[740] * B[(i*56)+55];
C[(i*56)+34] += values[741] * B[(i*56)+55];
#endif
#ifndef NDEBUG
num_flops += 10;
#endif

}

#ifndef NDEBUG
num_flops += 0;
#endif

}


#endif